{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d476befa-20d3-4104-b70a-2048da8db4c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.302327301857172\n",
      "=== epoch:1, train acc:0.109, test acc:0.088 ===\n",
      "train loss:2.3024350168863017\n",
      "train loss:2.302999776383075\n",
      "train loss:2.303685682128154\n",
      "train loss:2.3020597794836926\n",
      "train loss:2.3025446987890272\n",
      "train loss:2.3026421219653184\n",
      "train loss:2.3013832706687603\n",
      "train loss:2.302978278464221\n",
      "train loss:2.302997774624338\n",
      "train loss:2.301174223504152\n",
      "train loss:2.302991421689626\n",
      "train loss:2.3023600709352987\n",
      "train loss:2.3025371596204165\n",
      "train loss:2.3020926553534284\n",
      "train loss:2.302811409616474\n",
      "train loss:2.3015391551655044\n",
      "train loss:2.3022224323800464\n",
      "train loss:2.3019468607646196\n",
      "train loss:2.3034405485838665\n",
      "train loss:2.2995766390949433\n",
      "train loss:2.302882485714448\n",
      "train loss:2.30324387650021\n",
      "train loss:2.303214063990147\n",
      "train loss:2.301711316726939\n",
      "train loss:2.3020787640244054\n",
      "train loss:2.3022636534730414\n",
      "train loss:2.304731370142651\n",
      "train loss:2.300921146024305\n",
      "train loss:2.301859448637565\n",
      "train loss:2.3016803518681384\n",
      "train loss:2.3007414558983057\n",
      "train loss:2.301450772862166\n",
      "train loss:2.302568135792512\n",
      "train loss:2.303011355871179\n",
      "train loss:2.3018308201603754\n",
      "train loss:2.303002300762557\n",
      "train loss:2.3021827669727557\n",
      "train loss:2.3019030334641695\n",
      "train loss:2.2991266572157274\n",
      "train loss:2.298714262859594\n",
      "train loss:2.2996965659765696\n",
      "train loss:2.298316838374196\n",
      "train loss:2.3004163286139003\n",
      "train loss:2.3024512243068553\n",
      "train loss:2.299792487049118\n",
      "train loss:2.298115659677342\n",
      "train loss:2.30057597100343\n",
      "train loss:2.2996637125798145\n",
      "train loss:2.2987264257153743\n",
      "train loss:2.297967738462778\n",
      "train loss:2.297896585522851\n",
      "train loss:2.295502689976636\n",
      "train loss:2.2994925593784403\n",
      "train loss:2.297636550498463\n",
      "train loss:2.296241770474934\n",
      "train loss:2.287815737436499\n",
      "train loss:2.296333915678386\n",
      "train loss:2.296253838526907\n",
      "train loss:2.2912069358817906\n",
      "train loss:2.2904869180176863\n",
      "train loss:2.2859780669577354\n",
      "train loss:2.283731842947997\n",
      "train loss:2.286509731479491\n",
      "train loss:2.2843230237902943\n",
      "train loss:2.281590010061771\n",
      "train loss:2.2771330947372412\n",
      "train loss:2.2719881529230346\n",
      "train loss:2.2719137451598117\n",
      "train loss:2.273166420713092\n",
      "train loss:2.2753443242570075\n",
      "train loss:2.2681523781006034\n",
      "train loss:2.262111794568187\n",
      "train loss:2.238684249912894\n",
      "train loss:2.265366301521124\n",
      "train loss:2.2164919071485194\n",
      "train loss:2.2520165300355086\n",
      "train loss:2.234137212723695\n",
      "train loss:2.2152403936583944\n",
      "train loss:2.2415528048160267\n",
      "train loss:2.230049484497827\n",
      "train loss:2.214138173351136\n",
      "train loss:2.1922695918547763\n",
      "train loss:2.179215585431418\n",
      "train loss:2.157986300067487\n",
      "train loss:2.1759021314520854\n",
      "train loss:2.1542814800416696\n",
      "train loss:2.143957648926783\n",
      "train loss:2.148950287220499\n",
      "train loss:2.1854398128898227\n",
      "train loss:2.135269784343642\n",
      "train loss:2.132293164931027\n",
      "train loss:2.1180738671885964\n",
      "train loss:2.1334824940890096\n",
      "train loss:2.0651141629015397\n",
      "train loss:2.0900291054776265\n",
      "train loss:2.078926714958739\n",
      "train loss:2.1695273844985863\n",
      "train loss:2.175276757632173\n",
      "train loss:2.126792180272326\n",
      "train loss:2.1469003199239025\n",
      "train loss:2.122746205070913\n",
      "train loss:2.103407097146024\n",
      "train loss:2.1227404478797274\n",
      "train loss:2.0805952923665827\n",
      "train loss:1.9818640692866407\n",
      "train loss:2.080514281754576\n",
      "train loss:2.083371219539622\n",
      "train loss:2.0812502430574766\n",
      "train loss:2.1039068595969126\n",
      "train loss:2.051418557274935\n",
      "train loss:2.028944892811935\n",
      "train loss:2.0117506243215377\n",
      "train loss:2.049272003336898\n",
      "train loss:2.0563123497199034\n",
      "train loss:2.0684305995263657\n",
      "train loss:2.0543968053814403\n",
      "train loss:2.0541202765922595\n",
      "train loss:2.080756192071971\n",
      "train loss:2.080938068069179\n",
      "train loss:2.016822100338005\n",
      "train loss:2.065270417802282\n",
      "train loss:2.052707226492107\n",
      "train loss:2.0420548367554674\n",
      "train loss:2.090584321316314\n",
      "train loss:2.0567464987997828\n",
      "train loss:2.085311920525038\n",
      "train loss:2.0775526536830835\n",
      "train loss:2.080380220504308\n",
      "train loss:2.0654414613089673\n",
      "train loss:2.045556376174074\n",
      "train loss:2.0267350927205747\n",
      "train loss:2.0502741688729653\n",
      "train loss:2.023232067612069\n",
      "train loss:2.0086385871982735\n",
      "train loss:2.0637378198978786\n",
      "train loss:2.012502047614257\n",
      "train loss:2.0840114986058835\n",
      "train loss:2.0425498044586115\n",
      "train loss:2.0633178932778837\n",
      "train loss:2.009488344084128\n",
      "train loss:2.0010071686241533\n",
      "train loss:1.9569107654599731\n",
      "train loss:1.989229636554913\n",
      "train loss:2.026385588600444\n",
      "train loss:1.9887503237989541\n",
      "train loss:1.992621762301255\n",
      "train loss:2.049685820038139\n",
      "train loss:2.0870200888139405\n",
      "train loss:1.997987890456952\n",
      "train loss:1.9243487686535559\n",
      "train loss:2.0946798007916745\n",
      "train loss:2.014609154324644\n",
      "train loss:2.068796404622576\n",
      "train loss:2.0535678916068285\n",
      "train loss:2.0597983840233085\n",
      "train loss:2.035398601526932\n",
      "train loss:2.0219304727154723\n",
      "train loss:2.0257080831581344\n",
      "train loss:2.0077704961739626\n",
      "train loss:2.0340586863939816\n",
      "train loss:2.0068190162534365\n",
      "train loss:1.9172239724298663\n",
      "train loss:2.0260108403024883\n",
      "train loss:1.9855429075796973\n",
      "train loss:2.03305050934389\n",
      "train loss:2.0161769490060615\n",
      "train loss:1.8482021644160576\n",
      "train loss:1.974170267511061\n",
      "train loss:1.966799779173337\n",
      "train loss:2.0209570453780916\n",
      "train loss:1.8956432509281296\n",
      "train loss:1.9913510097871978\n",
      "train loss:2.02748020111005\n",
      "train loss:1.9509117677911354\n",
      "train loss:1.8840222146674188\n",
      "train loss:1.9890598465972524\n",
      "train loss:1.993847871432211\n",
      "train loss:2.0181182744824984\n",
      "train loss:1.931537308534591\n",
      "train loss:1.9947526970263474\n",
      "train loss:2.0317698260795214\n",
      "train loss:1.966863575561408\n",
      "train loss:1.9796101623539528\n",
      "train loss:1.9843736339419875\n",
      "train loss:2.0263651319173537\n",
      "train loss:2.033534191070077\n",
      "train loss:2.0281032561565113\n",
      "train loss:2.072422050935744\n",
      "train loss:1.9197677618445972\n",
      "train loss:1.9892494810655095\n",
      "train loss:2.030378872643789\n",
      "train loss:2.0012565230614543\n",
      "train loss:1.9281732608086615\n",
      "train loss:1.977226703422094\n",
      "train loss:2.0717475491798165\n",
      "train loss:1.9563597372205905\n",
      "train loss:1.8755839533939107\n",
      "train loss:1.9341831601844723\n",
      "train loss:2.0259540142890855\n",
      "train loss:1.9112734076230538\n",
      "train loss:1.8816054936893465\n",
      "train loss:1.915138942575525\n",
      "train loss:2.019141628835714\n",
      "train loss:1.9470229004865607\n",
      "train loss:1.8940231652789634\n",
      "train loss:1.9973962536488545\n",
      "train loss:2.022929287939622\n",
      "train loss:1.9758077826523277\n",
      "train loss:1.9401682610443165\n",
      "train loss:2.0447631506661104\n",
      "train loss:2.017302074387139\n",
      "train loss:1.9250534526881058\n",
      "train loss:1.9422996790822444\n",
      "train loss:1.9135760545648484\n",
      "train loss:1.9336977994196696\n",
      "train loss:1.9828434262937895\n",
      "train loss:1.9375160955986095\n",
      "train loss:1.83741934063932\n",
      "train loss:1.9755344815995384\n",
      "train loss:1.983918551701931\n",
      "train loss:1.956400179941638\n",
      "train loss:1.9275127121140372\n",
      "train loss:1.9235441265300022\n",
      "train loss:1.8267467154935013\n",
      "train loss:1.9709375022316706\n",
      "train loss:1.9220978747476687\n",
      "train loss:1.9683569769033193\n",
      "train loss:1.865499466282617\n",
      "train loss:1.9626744401726528\n",
      "train loss:1.962590634694265\n",
      "train loss:1.9183490653828543\n",
      "train loss:1.8979733075033132\n",
      "train loss:1.8347588384652591\n",
      "train loss:1.8891721173319853\n",
      "train loss:1.8564555972600225\n",
      "train loss:1.879703991406847\n",
      "train loss:1.8227137142828593\n",
      "train loss:1.881647541273548\n",
      "train loss:1.915634556510434\n",
      "train loss:1.914513332877242\n",
      "train loss:1.8902376732792092\n",
      "train loss:1.885860423939056\n",
      "train loss:1.8263068947347958\n",
      "train loss:1.8955431278263821\n",
      "train loss:1.8976140573223514\n",
      "train loss:1.8419999598473935\n",
      "train loss:1.8863608533499698\n",
      "train loss:1.9790724022804975\n",
      "train loss:1.9177403478160295\n",
      "train loss:1.8892922173090023\n",
      "train loss:1.8077221826182868\n",
      "train loss:1.8691706843510678\n",
      "train loss:1.8888138685829632\n",
      "train loss:1.9714285526573716\n",
      "train loss:1.9104827121104904\n",
      "train loss:1.9030512154126538\n",
      "train loss:1.8554538160439042\n",
      "train loss:1.8955409674431203\n",
      "train loss:1.9004288049002351\n",
      "train loss:1.9277918012790751\n",
      "train loss:1.9066299565697526\n",
      "train loss:1.786058627530881\n",
      "train loss:1.8522353638562175\n",
      "train loss:1.8686591709812281\n",
      "train loss:1.9205188842522165\n",
      "train loss:1.8139232017732327\n",
      "train loss:1.8094199290688302\n",
      "train loss:1.8423725287364883\n",
      "train loss:1.8674017162812888\n",
      "train loss:1.8821488858073712\n",
      "train loss:2.0043045171202927\n",
      "train loss:1.8155089778978952\n",
      "train loss:1.9137898077611661\n",
      "train loss:1.8970050324375776\n",
      "train loss:1.864553500914775\n",
      "train loss:1.891799899452864\n",
      "train loss:1.8108759925482372\n",
      "train loss:1.7986895017898394\n",
      "train loss:1.8504091662666395\n",
      "train loss:1.8530347186393485\n",
      "train loss:1.8010092679185237\n",
      "train loss:1.8621000005597237\n",
      "train loss:1.8304952282676885\n",
      "train loss:1.8328641276651831\n",
      "train loss:1.824316925378117\n",
      "train loss:1.8352174312585214\n",
      "train loss:1.8732842367446063\n",
      "train loss:1.8072906506020112\n",
      "train loss:1.8337460394454737\n",
      "train loss:1.7928574521795346\n",
      "train loss:1.7770987837428418\n",
      "train loss:1.8061574302807804\n",
      "train loss:1.7770211393745605\n",
      "train loss:1.7848526146179995\n",
      "train loss:1.7864319439493672\n",
      "train loss:1.7239532512505193\n",
      "train loss:1.8323313131748618\n",
      "train loss:1.8014041249299055\n",
      "train loss:1.7468764788860056\n",
      "train loss:1.7574098876176052\n",
      "train loss:1.8932878095422225\n",
      "train loss:1.7349741980138063\n",
      "train loss:1.8593955403551194\n",
      "train loss:1.8554258596600863\n",
      "train loss:1.775755140292472\n",
      "train loss:1.8844548078100112\n",
      "train loss:1.8611310845655717\n",
      "train loss:1.9110512786603286\n",
      "train loss:1.8602522840169444\n",
      "train loss:1.8602714947096515\n",
      "train loss:1.9336811053380274\n",
      "train loss:1.8733731671606566\n",
      "train loss:1.8623050437382906\n",
      "train loss:1.8382967125741627\n",
      "train loss:1.7729437123230911\n",
      "train loss:1.7861232125238822\n",
      "train loss:1.780595861420879\n",
      "train loss:1.7768181879094962\n",
      "train loss:1.77978969723264\n",
      "train loss:1.7632092328929272\n",
      "train loss:1.6939501637924588\n",
      "train loss:1.8945229158126824\n",
      "train loss:1.8118607319857774\n",
      "train loss:1.8344470891312634\n",
      "train loss:1.7914434456332504\n",
      "train loss:1.8958506048889692\n",
      "train loss:1.8118284995161562\n",
      "train loss:1.8936382350480716\n",
      "train loss:1.7272139238860533\n",
      "train loss:1.714459554177358\n",
      "train loss:1.811571428146258\n",
      "train loss:1.754482991985397\n",
      "train loss:1.7356443651281146\n",
      "train loss:1.810584392779678\n",
      "train loss:1.79169595799252\n",
      "train loss:1.7649153442815377\n",
      "train loss:1.8486950058876166\n",
      "train loss:1.747976891963465\n",
      "train loss:1.8579138304274148\n",
      "train loss:1.7098024563462693\n",
      "train loss:1.7441173579309188\n",
      "train loss:1.7712695094052444\n",
      "train loss:1.859341870027958\n",
      "train loss:1.7790521983295013\n",
      "train loss:1.821221507590203\n",
      "train loss:1.6416526831846177\n",
      "train loss:1.7036669251866365\n",
      "train loss:1.7674349914925833\n",
      "train loss:1.6662971833820794\n",
      "train loss:1.7072079828780398\n",
      "train loss:1.6761515448452389\n",
      "train loss:1.7132691910984659\n",
      "train loss:1.7011314880093484\n",
      "train loss:1.6575865868780466\n",
      "train loss:1.798082644997171\n",
      "train loss:1.7661667564284773\n",
      "train loss:1.7675666652959856\n",
      "train loss:1.66900731171275\n",
      "train loss:1.7760483540176717\n",
      "train loss:1.6914994612807923\n",
      "train loss:1.7232022345819331\n",
      "train loss:1.7172280993657616\n",
      "train loss:1.7443190291449986\n",
      "train loss:1.73577303109909\n",
      "train loss:1.7420740595302269\n",
      "train loss:1.633348700266464\n",
      "train loss:1.6673690897985562\n",
      "train loss:1.7436062725952148\n",
      "train loss:1.7505015125902559\n",
      "train loss:1.7589929946727865\n",
      "train loss:1.709667948032511\n",
      "train loss:1.6304616531099003\n",
      "train loss:1.579511000119327\n",
      "train loss:1.6136162807544137\n",
      "train loss:1.695956029893798\n",
      "train loss:1.5679112720591208\n",
      "train loss:1.7249362989768784\n",
      "train loss:1.6300231730147277\n",
      "train loss:1.7213022390688493\n",
      "train loss:1.6599070342559086\n",
      "train loss:1.6596563452179693\n",
      "train loss:1.6281417614819198\n",
      "train loss:1.6026888334510756\n",
      "train loss:1.5846845898495976\n",
      "train loss:1.5454060822826658\n",
      "train loss:1.6593323939501243\n",
      "train loss:1.6145523960570785\n",
      "train loss:1.6354142871377886\n",
      "train loss:1.6312051523007611\n",
      "train loss:1.6693760502378256\n",
      "train loss:1.7181448972015694\n",
      "train loss:1.5882225233544498\n",
      "train loss:1.6707265469110792\n",
      "train loss:1.7404616618733382\n",
      "train loss:1.6989984494578008\n",
      "train loss:1.6745658066117324\n",
      "train loss:1.607516946929551\n",
      "train loss:1.5751287112281647\n",
      "train loss:1.6505367773755888\n",
      "train loss:1.5968326130677348\n",
      "train loss:1.507121314829953\n",
      "train loss:1.6566568004644453\n",
      "train loss:1.5987986907276275\n",
      "train loss:1.6712908649949383\n",
      "train loss:1.5682963502165967\n",
      "train loss:1.6600901723217805\n",
      "train loss:1.5880839133942926\n",
      "train loss:1.6347027073397888\n",
      "train loss:1.5838964072695743\n",
      "train loss:1.5267973308948544\n",
      "train loss:1.5366777158786025\n",
      "train loss:1.5804013218288406\n",
      "train loss:1.5908127917530777\n",
      "train loss:1.6074644247856127\n",
      "train loss:1.597870211807808\n",
      "train loss:1.5280471897339052\n",
      "train loss:1.4239987680473083\n",
      "train loss:1.5321454067835796\n",
      "train loss:1.4906889640933927\n",
      "train loss:1.552183779659419\n",
      "train loss:1.5179735451593879\n",
      "train loss:1.4849708733965272\n",
      "train loss:1.503390423649314\n",
      "train loss:1.5017052637833166\n",
      "train loss:1.6024122472904794\n",
      "train loss:1.5542976305582477\n",
      "train loss:1.4143848336767193\n",
      "train loss:1.3556428461671879\n",
      "train loss:1.4857671271030088\n",
      "train loss:1.576039876118243\n",
      "train loss:1.5302584338083023\n",
      "train loss:1.4795715986895168\n",
      "train loss:1.4007004163179468\n",
      "train loss:1.4910074166692064\n",
      "train loss:1.6273269457352348\n",
      "train loss:1.4703044288002747\n",
      "train loss:1.402093180356314\n",
      "train loss:1.4177422389743273\n",
      "train loss:1.413430432768216\n",
      "train loss:1.4368386984422719\n",
      "train loss:1.3900293176082723\n",
      "train loss:1.4246115113843205\n",
      "train loss:1.368699136330236\n",
      "train loss:1.2957244425276764\n",
      "train loss:1.2936512528243913\n",
      "train loss:1.4866275524103318\n",
      "train loss:1.5154041858223808\n",
      "train loss:1.506434676570584\n",
      "train loss:1.5310099673642514\n",
      "train loss:1.3633739975539803\n",
      "train loss:1.4120210445163572\n",
      "train loss:1.356550060044479\n",
      "train loss:1.3432632082495046\n",
      "train loss:1.4571801465126586\n",
      "train loss:1.4304055097068897\n",
      "train loss:1.396231953895392\n",
      "train loss:1.4978594893171802\n",
      "train loss:1.4581908096621021\n",
      "train loss:1.2727997036868282\n",
      "train loss:1.256088573411207\n",
      "train loss:1.4220597406843336\n",
      "train loss:1.3590568587991556\n",
      "train loss:1.4124897171327269\n",
      "train loss:1.3498734244272712\n",
      "train loss:1.3324232650529921\n",
      "train loss:1.4495048694465043\n",
      "train loss:1.2611879499956018\n",
      "train loss:1.3693704157639273\n",
      "train loss:1.2553484967577229\n",
      "train loss:1.4488641286031738\n",
      "train loss:1.3497540815045201\n",
      "train loss:1.2113138887411787\n",
      "train loss:1.2327953405880907\n",
      "train loss:1.2593183645471542\n",
      "train loss:1.1748701975735303\n",
      "train loss:1.3323676891960199\n",
      "train loss:1.3111193280012259\n",
      "train loss:1.3079337649650387\n",
      "train loss:1.4436514729056376\n",
      "train loss:1.170867120073651\n",
      "train loss:1.149446395848689\n",
      "train loss:1.5155159508533318\n",
      "train loss:1.3450540149017933\n",
      "train loss:1.4695693135564452\n",
      "train loss:1.3796785419450046\n",
      "train loss:1.4750848178612583\n",
      "train loss:1.3762228632816147\n",
      "train loss:1.3692115919365329\n",
      "train loss:1.3300785733637026\n",
      "train loss:1.4226257702778526\n",
      "train loss:1.3832180955183246\n",
      "train loss:1.2456867646103853\n",
      "train loss:1.3054432343980666\n",
      "train loss:1.2100055581665932\n",
      "train loss:1.1922812460947285\n",
      "train loss:1.2671520134555336\n",
      "train loss:1.306660331816495\n",
      "train loss:1.2931671888397105\n",
      "train loss:1.2028234047744126\n",
      "train loss:1.2785772223770244\n",
      "train loss:1.3155186502365925\n",
      "train loss:1.3581991928471575\n",
      "train loss:1.278826203173499\n",
      "train loss:1.1870198089978827\n",
      "train loss:1.1751942284455539\n",
      "train loss:1.2767922199935844\n",
      "train loss:1.2027003411215522\n",
      "train loss:1.2455844536341507\n",
      "train loss:1.290006564723299\n",
      "train loss:1.3816033619451786\n",
      "train loss:1.3615791460411006\n",
      "train loss:1.4561335984493036\n",
      "train loss:1.33831636531781\n",
      "train loss:1.2723518023229554\n",
      "train loss:1.2407639463844597\n",
      "train loss:1.3227819881269056\n",
      "train loss:1.2276942844984517\n",
      "train loss:1.2346384208120513\n",
      "train loss:1.2577854516555311\n",
      "train loss:1.2094239972205214\n",
      "train loss:1.1719391213386465\n",
      "train loss:1.2437881967230797\n",
      "train loss:1.1586431624087032\n",
      "train loss:1.3458658404641064\n",
      "train loss:1.2136217065081416\n",
      "train loss:1.3162037592553737\n",
      "train loss:1.1915913233001776\n",
      "train loss:1.1537020574874493\n",
      "train loss:1.2657317810228859\n",
      "train loss:1.2733204275648522\n",
      "train loss:1.180360906297791\n",
      "train loss:1.2890104168575995\n",
      "train loss:1.2501548319130722\n",
      "train loss:1.2188293294596673\n",
      "train loss:1.253345808554826\n",
      "train loss:1.239789868887315\n",
      "train loss:1.2095186342723856\n",
      "train loss:1.179177084519376\n",
      "train loss:1.3906590982713056\n",
      "train loss:1.2790712122719194\n",
      "train loss:1.453106142065976\n",
      "train loss:1.2433920448711921\n",
      "train loss:1.4034453147835202\n",
      "train loss:1.4141179157849293\n",
      "train loss:1.3415869097795712\n",
      "train loss:1.1316089133283902\n",
      "train loss:1.2725597363086079\n",
      "train loss:1.34480924922321\n",
      "train loss:1.2503403990027815\n",
      "train loss:1.3374707637524548\n",
      "train loss:1.2846968601229156\n",
      "train loss:1.2628870921568391\n",
      "train loss:1.2024033418930662\n",
      "train loss:1.2279144229593344\n",
      "train loss:1.3391660160413272\n",
      "train loss:1.1572571795162263\n",
      "train loss:1.3019965335717187\n",
      "train loss:1.1691931248566365\n",
      "train loss:1.238055918240659\n",
      "train loss:1.2693501215683003\n",
      "train loss:1.1944118592604407\n",
      "train loss:1.2405212299554431\n",
      "train loss:1.184386177353008\n",
      "train loss:1.2169652235126163\n",
      "train loss:1.1921416736865382\n",
      "train loss:1.2680291358706246\n",
      "train loss:1.1991468583336722\n",
      "train loss:1.177055265985016\n",
      "train loss:1.184691431609175\n",
      "train loss:1.3044336925579603\n",
      "train loss:1.1626641090835041\n",
      "train loss:1.2561949846537448\n",
      "train loss:1.243431412815655\n",
      "train loss:1.3316633029077514\n",
      "train loss:1.2839046141414534\n",
      "train loss:1.2886876129046334\n",
      "train loss:1.2192349213447258\n",
      "train loss:1.173706913151038\n",
      "train loss:1.2524885438560682\n",
      "train loss:1.1859332073375124\n",
      "train loss:1.1115779007513984\n",
      "train loss:1.1012060083131583\n",
      "train loss:1.316562388830596\n",
      "train loss:1.1551086274564437\n",
      "train loss:1.3056927021341542\n",
      "train loss:1.2108338339044078\n",
      "train loss:1.1873099373283953\n",
      "train loss:1.1769417150561043\n",
      "train loss:1.2658893460380216\n",
      "train loss:1.2262636723732765\n",
      "train loss:1.1555808487776833\n",
      "train loss:1.4469363454055568\n",
      "train loss:1.2995060761940598\n",
      "train loss:1.2766183143925227\n",
      "train loss:1.1422293145302178\n",
      "train loss:1.2874911567441993\n",
      "train loss:1.243273195327187\n",
      "train loss:1.1511495385308361\n",
      "train loss:1.198177219714919\n",
      "train loss:1.1447712054601824\n",
      "=== epoch:2, train acc:0.446, test acc:0.457 ===\n",
      "train loss:1.2751550905322129\n",
      "train loss:1.2230653587259674\n",
      "train loss:1.2429789975035364\n",
      "train loss:1.2578659377315775\n",
      "train loss:1.3236487134608372\n",
      "train loss:1.166517118067006\n",
      "train loss:1.1927071566378624\n",
      "train loss:1.1669578330276276\n",
      "train loss:1.177842971124799\n",
      "train loss:1.2215167345658382\n",
      "train loss:1.0231560529447687\n",
      "train loss:1.128063267212277\n",
      "train loss:1.127067892859699\n",
      "train loss:1.1589841732152537\n",
      "train loss:1.2555411877537934\n",
      "train loss:1.2894990553365515\n",
      "train loss:1.2728682304261603\n",
      "train loss:1.2423975112541263\n",
      "train loss:1.1302396824681806\n",
      "train loss:1.2845447813943205\n",
      "train loss:1.3220278116949922\n",
      "train loss:1.4014502279616443\n",
      "train loss:1.2360201133222501\n",
      "train loss:1.098773303019741\n",
      "train loss:1.295037744672743\n",
      "train loss:1.3276243196579478\n",
      "train loss:1.2026451254735304\n",
      "train loss:1.3362704459508463\n",
      "train loss:1.2038645278989109\n",
      "train loss:1.1558823720660631\n",
      "train loss:1.1332984420670358\n",
      "train loss:1.261549642574227\n",
      "train loss:1.1533796975201163\n",
      "train loss:1.081199597542667\n",
      "train loss:1.2147470640726683\n",
      "train loss:1.1488780677948995\n",
      "train loss:1.1603764615445085\n",
      "train loss:1.1671489697079875\n",
      "train loss:1.077754879256768\n",
      "train loss:1.138087695749561\n",
      "train loss:1.209071300613668\n",
      "train loss:1.1020851309851816\n",
      "train loss:1.2980451042551306\n",
      "train loss:1.2034507313258793\n",
      "train loss:1.0915747357066734\n",
      "train loss:1.154075671004018\n",
      "train loss:1.1457034325867463\n",
      "train loss:1.179402900265575\n",
      "train loss:1.1428165203181657\n",
      "train loss:1.3679517426536585\n",
      "train loss:1.2362373857258204\n",
      "train loss:1.183045389166927\n",
      "train loss:1.1985420108550608\n",
      "train loss:1.2271506863376322\n",
      "train loss:1.3251761469275387\n",
      "train loss:1.1969005505328507\n",
      "train loss:1.179088349812823\n",
      "train loss:1.2607184687039241\n",
      "train loss:1.3610460760421352\n",
      "train loss:1.237050963377229\n",
      "train loss:1.1521760032754056\n",
      "train loss:1.1571407766219854\n",
      "train loss:1.271224060306582\n",
      "train loss:1.3081442611301255\n",
      "train loss:1.2076919146314524\n",
      "train loss:1.1831224728501508\n",
      "train loss:1.137397730602964\n",
      "train loss:1.2040088390894883\n",
      "train loss:1.1268218689145464\n",
      "train loss:1.227772983864971\n",
      "train loss:1.1574702734600406\n",
      "train loss:1.198156669220042\n",
      "train loss:1.3169161264401887\n",
      "train loss:1.054750453424658\n",
      "train loss:1.323301345247594\n",
      "train loss:1.1446013490592202\n",
      "train loss:1.1224138296261637\n",
      "train loss:1.1113514372883102\n",
      "train loss:1.1514672156182848\n",
      "train loss:1.198815657475247\n",
      "train loss:1.152367887577266\n",
      "train loss:1.2430626711377548\n",
      "train loss:1.1688244613969423\n",
      "train loss:1.21546209196286\n",
      "train loss:1.1775680034307456\n",
      "train loss:1.1771542148304943\n",
      "train loss:1.2375301885488676\n",
      "train loss:1.2143191779206364\n",
      "train loss:1.1105661238653002\n",
      "train loss:1.0863348278917717\n",
      "train loss:1.1979299310734013\n",
      "train loss:1.2875943225544282\n",
      "train loss:1.1660635037709028\n",
      "train loss:1.1461156925825495\n",
      "train loss:1.1167792510942534\n",
      "train loss:1.2296284466331\n",
      "train loss:1.211792064724593\n",
      "train loss:1.1567306310289938\n",
      "train loss:1.2510760579493008\n",
      "train loss:1.1961253461319785\n",
      "train loss:1.1656163461935984\n",
      "train loss:1.2102120226094366\n",
      "train loss:1.3052554948223005\n",
      "train loss:1.2173274161478067\n",
      "train loss:1.1209303528074712\n",
      "train loss:1.2345028514935674\n",
      "train loss:1.239783685011248\n",
      "train loss:1.2379321122872762\n",
      "train loss:1.1624582965037753\n",
      "train loss:1.180676667330959\n",
      "train loss:1.316764520704942\n",
      "train loss:1.152446741951319\n",
      "train loss:1.1366406198224426\n",
      "train loss:1.103592456712414\n",
      "train loss:1.1119633577624999\n",
      "train loss:1.1071358371391633\n",
      "train loss:1.1890230145921525\n",
      "train loss:1.143305150467966\n",
      "train loss:1.2470271611535586\n",
      "train loss:1.2302784927291182\n",
      "train loss:1.217285708279554\n",
      "train loss:1.071358951421749\n",
      "train loss:1.1102325854168664\n",
      "train loss:1.1431005737680338\n",
      "train loss:1.160657265171428\n",
      "train loss:1.1655664307880398\n",
      "train loss:1.1470836269462117\n",
      "train loss:1.3107069305309753\n",
      "train loss:1.1816530146484272\n",
      "train loss:1.0731396032984828\n",
      "train loss:1.1961337388848978\n",
      "train loss:1.1421772478205752\n",
      "train loss:1.1728290090901337\n",
      "train loss:1.1424276009648395\n",
      "train loss:1.2600850128337109\n",
      "train loss:1.2026807214623352\n",
      "train loss:1.2272501383723107\n",
      "train loss:1.3845364749250917\n",
      "train loss:1.1372551926513785\n",
      "train loss:1.2274123823413245\n",
      "train loss:1.294361437270716\n",
      "train loss:1.0804671951784475\n",
      "train loss:1.2526348117925559\n",
      "train loss:1.21039736269262\n",
      "train loss:1.2897739884794985\n",
      "train loss:1.218921784787609\n",
      "train loss:1.2786575646696339\n",
      "train loss:1.2663386869811895\n",
      "train loss:1.28081618667406\n",
      "train loss:1.1322931307056543\n",
      "train loss:1.1711519396590264\n",
      "train loss:1.2080512063148736\n",
      "train loss:1.1505398387184433\n",
      "train loss:1.139478759258767\n",
      "train loss:1.0759268767031427\n",
      "train loss:1.0828330488401106\n",
      "train loss:1.1286920805940392\n",
      "train loss:1.2503874699867863\n",
      "train loss:1.068911819433853\n",
      "train loss:1.198171373763541\n",
      "train loss:1.1442698721053417\n",
      "train loss:1.1274986256296358\n",
      "train loss:1.1367715075504354\n",
      "train loss:1.2033365345814058\n",
      "train loss:1.1274416495070916\n",
      "train loss:1.4553718289732809\n",
      "train loss:1.2510739744300963\n",
      "train loss:1.2539846360763691\n",
      "train loss:1.1444567178837313\n",
      "train loss:1.3128438329766197\n",
      "train loss:1.2030136616583151\n",
      "train loss:1.1246356159778506\n",
      "train loss:1.1242040556892794\n",
      "train loss:1.087825990003696\n",
      "train loss:1.1843863358444215\n",
      "train loss:1.173143023269014\n",
      "train loss:1.1131333988970429\n",
      "train loss:1.126998670078202\n",
      "train loss:1.1745058419269079\n",
      "train loss:1.1775448148696854\n",
      "train loss:1.0663631101955076\n",
      "train loss:1.0763414394909845\n",
      "train loss:1.1726213442950288\n",
      "train loss:1.193745333093664\n",
      "train loss:1.149650420482223\n",
      "train loss:1.1938219262208378\n",
      "train loss:1.0756507488510862\n",
      "train loss:1.1436128652231314\n",
      "train loss:1.1509151697057072\n",
      "train loss:1.367088802640388\n",
      "train loss:1.1704249412963994\n",
      "train loss:1.2718436511375517\n",
      "train loss:1.2320120581210223\n",
      "train loss:1.2141270782165106\n",
      "train loss:1.1696789608594442\n",
      "train loss:1.0195734714127629\n",
      "train loss:1.1107512411940517\n",
      "train loss:1.013982010621693\n",
      "train loss:1.1408374016070502\n",
      "train loss:1.1156564906151347\n",
      "train loss:1.038930636925051\n",
      "train loss:1.166796187720097\n",
      "train loss:1.079647840680719\n",
      "train loss:1.133315949554257\n",
      "train loss:1.1493632342204843\n",
      "train loss:1.1735949251240176\n",
      "train loss:1.1230636728736327\n",
      "train loss:1.1334709761584316\n",
      "train loss:1.1709577492806043\n",
      "train loss:0.9784697837042452\n",
      "train loss:1.2266083431926147\n",
      "train loss:1.0958764719558989\n",
      "train loss:1.2410550836462133\n",
      "train loss:1.1206752026375038\n",
      "train loss:1.2147802536785466\n",
      "train loss:1.1253899527146236\n",
      "train loss:1.2002485925460116\n",
      "train loss:1.1259305238026136\n",
      "train loss:1.1001996771913978\n",
      "train loss:1.1676723865275918\n",
      "train loss:1.0508012562252569\n",
      "train loss:1.1080743639427642\n",
      "train loss:1.1246762144962255\n",
      "train loss:1.044756554412401\n",
      "train loss:1.1583258572195683\n",
      "train loss:1.0447825611717767\n",
      "train loss:1.0943073148467617\n",
      "train loss:1.3244871569949412\n",
      "train loss:1.0937162032932508\n",
      "train loss:1.1428679229108036\n",
      "train loss:0.9908948086172381\n",
      "train loss:1.208142445948344\n",
      "train loss:1.1449384229099702\n",
      "train loss:1.1668994486119537\n",
      "train loss:1.087609454051877\n",
      "train loss:1.3338155283665682\n",
      "train loss:1.1168149056575452\n",
      "train loss:1.1322208132403748\n",
      "train loss:1.1936145977002013\n",
      "train loss:1.160822604703959\n",
      "train loss:1.1925231010901902\n",
      "train loss:1.022119479084721\n",
      "train loss:1.0530685190001972\n",
      "train loss:1.1727810795180735\n",
      "train loss:1.0681111508683907\n",
      "train loss:1.1862541727679683\n",
      "train loss:1.1643116491811467\n",
      "train loss:1.1763538753848612\n",
      "train loss:1.1521866705848902\n",
      "train loss:1.0671256241836127\n",
      "train loss:1.0402016954510005\n",
      "train loss:1.3762054850776477\n",
      "train loss:1.1080128462358028\n",
      "train loss:1.2406668103786485\n",
      "train loss:1.1674888422604015\n",
      "train loss:1.0457135772329733\n",
      "train loss:1.1408263824528182\n",
      "train loss:1.0697861794750818\n",
      "train loss:1.0723011034093093\n",
      "train loss:1.1545725845749162\n",
      "train loss:1.133565078258849\n",
      "train loss:0.9767828380717887\n",
      "train loss:1.0706789179295675\n",
      "train loss:1.0105056808314483\n",
      "train loss:1.0809347631641952\n",
      "train loss:1.0553522102080375\n",
      "train loss:1.0995209065461977\n",
      "train loss:1.1457470360259396\n",
      "train loss:1.112254422207336\n",
      "train loss:1.1582596529425577\n",
      "train loss:1.2045884590107634\n",
      "train loss:1.1340521857854258\n",
      "train loss:1.1094649653276354\n",
      "train loss:1.3455552796475632\n",
      "train loss:1.2016057535959461\n",
      "train loss:1.2396915080478004\n",
      "train loss:1.1495681318458986\n",
      "train loss:1.2485332089300767\n",
      "train loss:1.2052327685765234\n",
      "train loss:1.2983735210494844\n",
      "train loss:1.336255140586145\n",
      "train loss:1.0287989451061612\n",
      "train loss:1.235858828465865\n",
      "train loss:1.1159381238077584\n",
      "train loss:1.2323850037754054\n",
      "train loss:1.207232830644216\n",
      "train loss:1.1468853858450256\n",
      "train loss:1.1328710141278686\n",
      "train loss:1.183474051849866\n",
      "train loss:1.070405386749803\n",
      "train loss:1.0690039667263884\n",
      "train loss:1.0668358210542142\n",
      "train loss:1.0914619572500386\n",
      "train loss:1.3187717560118486\n",
      "train loss:1.1820931104935426\n",
      "train loss:1.051469220064079\n",
      "train loss:0.9762961196681976\n",
      "train loss:1.0903755872466898\n",
      "train loss:0.9976891023308062\n",
      "train loss:1.1547869020556698\n",
      "train loss:1.1442819650351397\n",
      "train loss:1.4154757643538982\n",
      "train loss:1.156166195827747\n",
      "train loss:1.1755530244969556\n",
      "train loss:1.122140591574376\n",
      "train loss:1.2187235981239288\n",
      "train loss:1.1728925783210264\n",
      "train loss:1.0699710593174556\n",
      "train loss:1.185257726345894\n",
      "train loss:1.0959828834541836\n",
      "train loss:1.2620621548280984\n",
      "train loss:1.1267129541163732\n",
      "train loss:1.0503577601624834\n",
      "train loss:1.0591997483058122\n",
      "train loss:1.2344122960455441\n",
      "train loss:1.034486677831663\n",
      "train loss:1.2324665666304453\n",
      "train loss:1.0943306228797633\n",
      "train loss:1.2139016862104806\n",
      "train loss:1.1309105143937175\n",
      "train loss:1.2047612785921509\n",
      "train loss:1.1868624646893928\n",
      "train loss:1.1011023748371975\n",
      "train loss:1.0691884401608773\n",
      "train loss:1.278651297371825\n",
      "train loss:1.1393892416525138\n",
      "train loss:1.110267494875561\n",
      "train loss:1.1077664778397611\n",
      "train loss:1.1079458587284285\n",
      "train loss:1.1006919371700212\n",
      "train loss:1.1356302016898911\n",
      "train loss:1.26714082794652\n",
      "train loss:1.1521092805914068\n",
      "train loss:1.146268470376994\n",
      "train loss:1.1266180523217262\n",
      "train loss:1.0819755835796792\n",
      "train loss:1.174079803654658\n",
      "train loss:1.0477562304517642\n",
      "train loss:0.9890039636880675\n",
      "train loss:1.1639241627299954\n",
      "train loss:1.175118289075275\n",
      "train loss:1.2175617232602038\n",
      "train loss:1.1140423635847376\n",
      "train loss:1.284757992904517\n",
      "train loss:1.247956007718331\n",
      "train loss:1.0015498629446165\n",
      "train loss:1.1407001491372786\n",
      "train loss:1.134871658885725\n",
      "train loss:1.1212757163024052\n",
      "train loss:1.0861905583445826\n",
      "train loss:1.110815636108375\n",
      "train loss:1.0402454501195044\n",
      "train loss:1.190480045916327\n",
      "train loss:1.1249967433594146\n",
      "train loss:1.0858040628926329\n",
      "train loss:1.198814666155261\n",
      "train loss:1.1130808866531348\n",
      "train loss:0.9482159366521791\n",
      "train loss:1.0816124987641005\n",
      "train loss:1.2737572419247767\n",
      "train loss:1.2238984478213781\n",
      "train loss:1.2070719152265896\n",
      "train loss:1.2079568901652282\n",
      "train loss:1.1030376009380392\n",
      "train loss:1.1434408682291035\n",
      "train loss:1.0476197243866232\n",
      "train loss:1.127758297617726\n",
      "train loss:1.0499143270268625\n",
      "train loss:1.178955703522815\n",
      "train loss:1.3114294758284442\n",
      "train loss:1.3633481952539623\n",
      "train loss:1.0867698984249288\n",
      "train loss:1.1417562406817994\n",
      "train loss:1.0308235518476947\n",
      "train loss:1.1618013752040084\n",
      "train loss:1.0375028091571166\n",
      "train loss:0.959697348994728\n",
      "train loss:1.1416086206218319\n",
      "train loss:1.2451771424504006\n",
      "train loss:1.0133436823398614\n",
      "train loss:1.0488572325627368\n",
      "train loss:1.0638460637169505\n",
      "train loss:1.0388912894484041\n",
      "train loss:1.084866742062643\n",
      "train loss:1.1477716939286982\n",
      "train loss:1.1293184580919435\n",
      "train loss:1.2523305755117888\n",
      "train loss:1.075753203040602\n",
      "train loss:1.1814235137947473\n",
      "train loss:1.292827122265441\n",
      "train loss:1.1714515837440997\n",
      "train loss:1.1243677068364963\n",
      "train loss:1.1002897246799963\n",
      "train loss:0.9969709346660296\n",
      "train loss:1.103830289606805\n",
      "train loss:1.2107887088512934\n",
      "train loss:1.0675173953505739\n",
      "train loss:1.0551455144769306\n",
      "train loss:1.2166892051846203\n",
      "train loss:1.1711888230061323\n",
      "train loss:1.031342784970906\n",
      "train loss:1.0979254984513604\n",
      "train loss:1.0965554185713247\n",
      "train loss:1.0987033163594078\n",
      "train loss:0.9703468404249931\n",
      "train loss:1.1093221076518223\n",
      "train loss:1.0987797294653443\n",
      "train loss:1.06141134214496\n",
      "train loss:1.0493290186244228\n",
      "train loss:1.1418813244864565\n",
      "train loss:0.9850804556462035\n",
      "train loss:1.163252273652396\n",
      "train loss:0.9849724490371448\n",
      "train loss:1.2930200036887312\n",
      "train loss:1.1680080289305226\n",
      "train loss:0.8919638274081941\n",
      "train loss:1.0646873536936439\n",
      "train loss:0.9955588084216523\n",
      "train loss:1.065725490045899\n",
      "train loss:0.9929135450670931\n",
      "train loss:0.9436521131179174\n",
      "train loss:1.1139258092722133\n",
      "train loss:1.0209341494025326\n",
      "train loss:1.2456557577682679\n",
      "train loss:1.0923256757562207\n",
      "train loss:1.2755143563376028\n",
      "train loss:1.0836269097569131\n",
      "train loss:1.1947684075650893\n",
      "train loss:1.1959009259268383\n",
      "train loss:0.9451363284162333\n",
      "train loss:1.0196194354567687\n",
      "train loss:1.2197384691963828\n",
      "train loss:1.089593450948944\n",
      "train loss:1.2781871736425097\n",
      "train loss:1.1042604756640235\n",
      "train loss:1.0630719209075097\n",
      "train loss:1.1435050445970274\n",
      "train loss:1.1776101853774688\n",
      "train loss:1.0675753522349058\n",
      "train loss:0.9572119091824092\n",
      "train loss:1.2678338288911088\n",
      "train loss:1.1103126366489988\n",
      "train loss:1.134574668836163\n",
      "train loss:1.34703922328132\n",
      "train loss:1.075900643960721\n",
      "train loss:1.0366687074752536\n",
      "train loss:1.197962218190992\n",
      "train loss:1.2887990741463262\n",
      "train loss:1.0965670644328656\n",
      "train loss:0.9431450452336989\n",
      "train loss:1.0702032198869356\n",
      "train loss:1.1891753721005753\n",
      "train loss:0.9952058561805122\n",
      "train loss:1.0715802650400954\n",
      "train loss:1.0693899933727098\n",
      "train loss:1.300725215990992\n",
      "train loss:1.1340387078727128\n",
      "train loss:0.9618196177626887\n",
      "train loss:1.0311458033922616\n",
      "train loss:0.95392621956358\n",
      "train loss:1.040241961651665\n",
      "train loss:1.0683710465449783\n",
      "train loss:1.0690688689498127\n",
      "train loss:0.9546663545833307\n",
      "train loss:0.8903852469605034\n",
      "train loss:1.0449727070758725\n",
      "train loss:1.2177795641144828\n",
      "train loss:1.1250002039469846\n",
      "train loss:1.026247260439346\n",
      "train loss:1.0041880098630824\n",
      "train loss:1.2197289867904404\n",
      "train loss:1.265121207693026\n",
      "train loss:1.1330136383948328\n",
      "train loss:1.0372453692029497\n",
      "train loss:1.070351413481748\n",
      "train loss:0.9945414144339928\n",
      "train loss:1.1354224254490672\n",
      "train loss:1.2481019310758348\n",
      "train loss:1.0397853477829806\n",
      "train loss:1.0206869485886496\n",
      "train loss:1.1838419529758644\n",
      "train loss:1.1190805464822746\n",
      "train loss:1.1891199206162448\n",
      "train loss:0.9074674153116767\n",
      "train loss:1.0451580732240877\n",
      "train loss:0.9854889557939447\n",
      "train loss:0.931478558952188\n",
      "train loss:0.9022032067229059\n",
      "train loss:1.010599468454374\n",
      "train loss:0.9432906789400672\n",
      "train loss:0.9498686337449124\n",
      "train loss:1.267188193784556\n",
      "train loss:1.0284182010520828\n",
      "train loss:1.0469899893480175\n",
      "train loss:1.1795121673642022\n",
      "train loss:1.1064412467266507\n",
      "train loss:1.2918053109770862\n",
      "train loss:1.0125242206593414\n",
      "train loss:1.1941262283170753\n",
      "train loss:1.4585699675616357\n",
      "train loss:1.0860555909699903\n",
      "train loss:1.128979340527274\n",
      "train loss:1.082679830879513\n",
      "train loss:1.0198219330727356\n",
      "train loss:0.9620463697254419\n",
      "train loss:1.0166205958084582\n",
      "train loss:1.1147040814567049\n",
      "train loss:1.0164732346638685\n",
      "train loss:1.1048854968223045\n",
      "train loss:1.1043957801289015\n",
      "train loss:1.0956135498853632\n",
      "train loss:1.1855899036585849\n",
      "train loss:1.0710428698487728\n",
      "train loss:0.9731767885969282\n",
      "train loss:1.095472939820023\n",
      "train loss:1.1028319440301415\n",
      "train loss:0.8495317252281186\n",
      "train loss:1.0595329524269037\n",
      "train loss:0.975876325086729\n",
      "train loss:1.1807038643528194\n",
      "train loss:0.9880866277504259\n",
      "train loss:1.0437797593912153\n",
      "train loss:1.0418204635157997\n",
      "train loss:0.9958956033867448\n",
      "train loss:1.1695576280308688\n",
      "train loss:1.0799412865066917\n",
      "train loss:1.1322675123271981\n",
      "train loss:1.0951565289877383\n",
      "train loss:1.123777412989652\n",
      "train loss:1.14952183211149\n",
      "train loss:0.9445961217816433\n",
      "train loss:1.079070440152394\n",
      "train loss:1.1778461997992589\n",
      "train loss:1.0202148918436242\n",
      "train loss:1.0055318979899432\n",
      "train loss:1.0089720433324934\n",
      "train loss:1.0130459472716544\n",
      "train loss:1.0790269505028298\n",
      "train loss:1.0153609469687053\n",
      "train loss:0.9501847058634734\n",
      "train loss:1.0522582502212565\n",
      "train loss:0.9554872563636811\n",
      "train loss:0.9574971558355287\n",
      "train loss:1.0269743759748733\n",
      "train loss:1.1501354624508207\n",
      "train loss:0.9793070032956159\n",
      "train loss:1.152498552339474\n",
      "train loss:1.0272221489997582\n",
      "train loss:1.0276061429350722\n",
      "train loss:1.0892399484448039\n",
      "train loss:1.1129154060425792\n",
      "train loss:1.009944040272396\n",
      "train loss:0.8950209728938952\n",
      "train loss:1.0961699114801005\n",
      "train loss:1.076903382837743\n",
      "train loss:1.0459340778380835\n",
      "train loss:0.9165132097852016\n",
      "train loss:0.9889958972509367\n",
      "train loss:0.9760235753628191\n",
      "train loss:0.945517716271836\n",
      "train loss:1.0402893998929807\n",
      "train loss:1.0453190498535803\n",
      "train loss:0.9711977498024308\n",
      "train loss:0.9771996749745724\n",
      "train loss:1.0321759285143888\n",
      "train loss:1.0283771307645047\n",
      "train loss:1.0952581180596233\n",
      "train loss:1.024467656229539\n",
      "train loss:1.0507311555084902\n",
      "train loss:0.9039396548897932\n",
      "train loss:1.0801759496023202\n",
      "train loss:1.1185087425928524\n",
      "train loss:0.9811727659386861\n",
      "train loss:0.9838231623674496\n",
      "train loss:0.9119186669313816\n",
      "train loss:0.9283059185275156\n",
      "train loss:0.9507623354024689\n",
      "train loss:0.8510900479652416\n",
      "train loss:0.8488160270762619\n",
      "train loss:1.1136673786423024\n",
      "train loss:1.0127443732451546\n",
      "train loss:0.9172074035122187\n",
      "train loss:1.0404716810304502\n",
      "train loss:1.0850511260684277\n",
      "train loss:1.157203895768675\n",
      "train loss:1.0238363616114527\n",
      "train loss:1.1510925634427192\n",
      "train loss:1.0326305854510878\n",
      "train loss:1.089666730577648\n",
      "train loss:0.9289063014214493\n",
      "train loss:1.0433118319865395\n",
      "train loss:1.0698685528535672\n",
      "train loss:1.114475122339821\n",
      "train loss:0.9454160769626277\n",
      "train loss:0.8153747016857331\n",
      "train loss:0.8770523773449844\n",
      "train loss:1.1914715879780464\n",
      "train loss:1.0106230448861373\n",
      "train loss:1.1586602331213778\n",
      "train loss:0.9577236777421768\n",
      "=== epoch:3, train acc:0.561, test acc:0.555 ===\n",
      "train loss:1.121756336907923\n",
      "train loss:0.9758399492539859\n",
      "train loss:1.0316883628749585\n",
      "train loss:1.0282320848783797\n",
      "train loss:1.088160257919962\n",
      "train loss:1.0308148457852142\n",
      "train loss:0.9813036136146716\n",
      "train loss:0.9966857340650104\n",
      "train loss:1.068361841415207\n",
      "train loss:0.9002664963720934\n",
      "train loss:1.0552033898247493\n",
      "train loss:0.9042322081887303\n",
      "train loss:0.983503355121693\n",
      "train loss:0.9709226841181712\n",
      "train loss:0.937454273552718\n",
      "train loss:0.950460564328586\n",
      "train loss:0.9968575109637619\n",
      "train loss:1.0591036535591223\n",
      "train loss:0.9031023293058776\n",
      "train loss:0.9351253303000668\n",
      "train loss:0.9643339771635041\n",
      "train loss:0.9710931255501376\n",
      "train loss:1.1402225812898248\n",
      "train loss:0.9994146758093337\n",
      "train loss:1.0614304594091615\n",
      "train loss:0.9640018913135806\n",
      "train loss:1.053430793622308\n",
      "train loss:1.0713925070157166\n",
      "train loss:1.0429540186059543\n",
      "train loss:1.25877332335644\n",
      "train loss:1.0372598925220287\n",
      "train loss:1.1908587114588929\n",
      "train loss:1.0446861132616705\n",
      "train loss:1.0066111959996775\n",
      "train loss:0.9931342070559294\n",
      "train loss:0.9449401456693947\n",
      "train loss:1.163077166108518\n",
      "train loss:1.0086505266044334\n",
      "train loss:0.9891287334836031\n",
      "train loss:1.124182461222833\n",
      "train loss:1.0175674000791246\n",
      "train loss:1.0528170285383016\n",
      "train loss:0.9807873505140114\n",
      "train loss:0.9553786450958244\n",
      "train loss:0.9302811795875934\n",
      "train loss:0.9377253480373637\n",
      "train loss:0.9965500796865371\n",
      "train loss:0.9998571908109831\n",
      "train loss:0.9384929278869812\n",
      "train loss:0.9544738522855917\n",
      "train loss:0.9642434086243066\n",
      "train loss:0.9914701749229229\n",
      "train loss:0.8898269014945638\n",
      "train loss:0.8807686596586572\n",
      "train loss:1.0520949562776785\n",
      "train loss:0.8418529665942907\n",
      "train loss:0.8985067729195881\n",
      "train loss:0.907367402738258\n",
      "train loss:1.0272711865018287\n",
      "train loss:0.8418738761060273\n",
      "train loss:1.1893211807055308\n",
      "train loss:1.1312596912640331\n",
      "train loss:1.0925219532967436\n",
      "train loss:1.0083906640999631\n",
      "train loss:0.9254205644016998\n",
      "train loss:1.0392490982001663\n",
      "train loss:1.0193412281969167\n",
      "train loss:0.8802333210104948\n",
      "train loss:1.0183591071752187\n",
      "train loss:1.1027632986073046\n",
      "train loss:0.8695356925863159\n",
      "train loss:0.9901081865263734\n",
      "train loss:1.0103381768431658\n",
      "train loss:1.1354941622473391\n",
      "train loss:1.2472557419672978\n",
      "train loss:1.1446249750218858\n",
      "train loss:1.1119184857696278\n",
      "train loss:1.215206166851395\n",
      "train loss:0.99788092007072\n",
      "train loss:1.0012506355807798\n",
      "train loss:1.0087115764523293\n",
      "train loss:1.1285732032477929\n",
      "train loss:0.8984247617627555\n",
      "train loss:1.0097379172236018\n",
      "train loss:0.9479379762475617\n",
      "train loss:0.9432681489399503\n",
      "train loss:1.0327223145559983\n",
      "train loss:1.1318640475552948\n",
      "train loss:0.9617118909210726\n",
      "train loss:1.027896659546707\n",
      "train loss:0.929899175836507\n",
      "train loss:1.0911568771969369\n",
      "train loss:0.9610601572487671\n",
      "train loss:0.9017136661039676\n",
      "train loss:1.1552038892330434\n",
      "train loss:1.0930081246059669\n",
      "train loss:1.0033162101237145\n",
      "train loss:0.8436387464224804\n",
      "train loss:0.8159543225829835\n",
      "train loss:1.1030814865389176\n",
      "train loss:0.8780149242750926\n",
      "train loss:1.1047434766617863\n",
      "train loss:0.9345416778237152\n",
      "train loss:0.9522789088178394\n",
      "train loss:0.8443103425408391\n",
      "train loss:0.9729435186753642\n",
      "train loss:1.084397343402643\n",
      "train loss:0.9297274603234814\n",
      "train loss:0.8409105927826943\n",
      "train loss:0.9252123049002448\n",
      "train loss:0.8557613159764013\n",
      "train loss:1.130633258785347\n",
      "train loss:1.0045744044318359\n",
      "train loss:0.9375584854920591\n",
      "train loss:0.7685392947974664\n",
      "train loss:0.9034216696793926\n",
      "train loss:0.9770001702534011\n",
      "train loss:1.0152499872480818\n",
      "train loss:0.9038817566120546\n",
      "train loss:0.9478617629496994\n",
      "train loss:0.9277640615669326\n",
      "train loss:0.9551527392269693\n",
      "train loss:0.9503886452604253\n",
      "train loss:1.207153911235955\n",
      "train loss:1.0240026601903074\n",
      "train loss:0.9845564174110046\n",
      "train loss:0.8851112178752694\n",
      "train loss:1.2156119171047421\n",
      "train loss:0.8162982680101571\n",
      "train loss:0.918236818326751\n",
      "train loss:0.9010545506842947\n",
      "train loss:0.9809539977769428\n",
      "train loss:1.0128782355368384\n",
      "train loss:1.0777886443726943\n",
      "train loss:0.9006587325839501\n",
      "train loss:0.8676055394719251\n",
      "train loss:0.8912690121004446\n",
      "train loss:0.881230302164492\n",
      "train loss:0.9998157134725548\n",
      "train loss:0.8853574217602913\n",
      "train loss:0.8828669107914614\n",
      "train loss:0.9161136992133118\n",
      "train loss:0.9201352098196112\n",
      "train loss:0.8641117422614522\n",
      "train loss:0.8353719033572777\n",
      "train loss:0.7664278578778208\n",
      "train loss:1.0195881351686829\n",
      "train loss:1.0489984702133366\n",
      "train loss:1.0103962829330198\n",
      "train loss:1.0850163210775288\n",
      "train loss:1.0851470697971313\n",
      "train loss:0.9165663627745598\n",
      "train loss:0.9340082141893764\n",
      "train loss:1.050099402165936\n",
      "train loss:1.0176163505342557\n",
      "train loss:0.8826484876511461\n",
      "train loss:0.9610136527023141\n",
      "train loss:0.9822477166626716\n",
      "train loss:0.8485205448883228\n",
      "train loss:1.0004743946200993\n",
      "train loss:0.9717936344989184\n",
      "train loss:0.9279398421794112\n",
      "train loss:1.0201877116891949\n",
      "train loss:0.9294948072094412\n",
      "train loss:0.9116531753045538\n",
      "train loss:0.9416730668737077\n",
      "train loss:0.8826468581101352\n",
      "train loss:0.8809314210238081\n",
      "train loss:0.8618643061839579\n",
      "train loss:0.9953064345147362\n",
      "train loss:0.9990165590949905\n",
      "train loss:0.9873541151046539\n",
      "train loss:0.8875402212858625\n",
      "train loss:0.9979075910076932\n",
      "train loss:1.0480975294837664\n",
      "train loss:0.9285380422131594\n",
      "train loss:1.096237410945378\n",
      "train loss:0.8703857952939432\n",
      "train loss:1.0755892071608222\n",
      "train loss:0.9100144891202868\n",
      "train loss:1.0161585579314407\n",
      "train loss:1.23993460276685\n",
      "train loss:1.0755997180489067\n",
      "train loss:0.9434575372489165\n",
      "train loss:1.0333881681354269\n",
      "train loss:0.9260160537667139\n",
      "train loss:1.0016110232795805\n",
      "train loss:0.972918550679833\n",
      "train loss:0.8870346765560748\n",
      "train loss:0.8523498371720402\n",
      "train loss:0.7733681403155618\n",
      "train loss:0.9509884980558735\n",
      "train loss:0.9694232504573396\n",
      "train loss:0.987623285488213\n",
      "train loss:0.9078797427842514\n",
      "train loss:0.9413723711688309\n",
      "train loss:0.885862599716197\n",
      "train loss:0.9891831159386274\n",
      "train loss:0.9037881310127625\n",
      "train loss:0.8519615640981395\n",
      "train loss:0.9298369728402582\n",
      "train loss:0.9504616570478059\n",
      "train loss:0.9770761431992713\n",
      "train loss:0.8577635369701077\n",
      "train loss:0.8607440767345358\n",
      "train loss:0.9233937461863252\n",
      "train loss:1.0465265907520394\n",
      "train loss:0.9542096031911906\n",
      "train loss:0.9062750499967623\n",
      "train loss:0.8584263437426426\n",
      "train loss:0.8590553424514011\n",
      "train loss:0.8815498326589679\n",
      "train loss:0.9628276612485207\n",
      "train loss:0.8553455833441838\n",
      "train loss:0.865628525797122\n",
      "train loss:0.8318162639326095\n",
      "train loss:0.915964011372377\n",
      "train loss:0.7912326630486818\n",
      "train loss:0.9960359144032651\n",
      "train loss:0.9922768377161226\n",
      "train loss:0.9644149596326715\n",
      "train loss:0.8525386139656878\n",
      "train loss:0.9872428046333837\n",
      "train loss:1.046972773559156\n",
      "train loss:0.9556611578455282\n",
      "train loss:1.0899556642006099\n",
      "train loss:0.9095971638472962\n",
      "train loss:0.9332579038171215\n",
      "train loss:0.9721602178260227\n",
      "train loss:0.8769296494265281\n",
      "train loss:0.9613665333190604\n",
      "train loss:0.799347349634187\n",
      "train loss:0.9053299046991128\n",
      "train loss:1.001614264416866\n",
      "train loss:0.9400459060104189\n",
      "train loss:0.7974366300695032\n",
      "train loss:0.9912473665249142\n",
      "train loss:0.9226111404347751\n",
      "train loss:1.029224753049059\n",
      "train loss:0.917556016152857\n",
      "train loss:1.045103610401496\n",
      "train loss:1.1019310300659304\n",
      "train loss:0.8243466150323858\n",
      "train loss:0.9155861641104427\n",
      "train loss:0.8608679243643373\n",
      "train loss:0.8422632294184526\n",
      "train loss:0.8836198275580004\n",
      "train loss:0.9352855383505614\n",
      "train loss:0.9239397591917939\n",
      "train loss:1.1506491854420993\n",
      "train loss:0.9755804106935392\n",
      "train loss:0.9087194333931743\n",
      "train loss:0.8775300990520906\n",
      "train loss:0.9974194426245984\n",
      "train loss:0.8235200802704964\n",
      "train loss:0.9636371867790193\n",
      "train loss:0.9223541851822887\n",
      "train loss:0.8739808661982623\n",
      "train loss:0.8423942885531462\n",
      "train loss:0.8126688850716672\n",
      "train loss:0.8803102054770914\n",
      "train loss:0.8650181716330725\n",
      "train loss:0.8247951954045054\n",
      "train loss:0.8113257162206625\n",
      "train loss:0.9105613263806239\n",
      "train loss:0.929333735626713\n",
      "train loss:1.056875087315026\n",
      "train loss:0.9172268129272669\n",
      "train loss:0.9010229154665749\n",
      "train loss:0.9246842796522534\n",
      "train loss:0.7962722727690843\n",
      "train loss:1.1715341681554228\n",
      "train loss:1.0286374837528882\n",
      "train loss:0.9801229370476452\n",
      "train loss:0.785299047129396\n",
      "train loss:0.9463784109808441\n",
      "train loss:0.8257887084522114\n",
      "train loss:0.9101065162946195\n",
      "train loss:0.7547299544906179\n",
      "train loss:1.111893631542459\n",
      "train loss:0.8990975080882937\n",
      "train loss:0.9881377234328577\n",
      "train loss:1.0360276780634903\n",
      "train loss:0.8284514756712963\n",
      "train loss:1.0336297919482842\n",
      "train loss:0.8663588173690029\n",
      "train loss:0.965887790876835\n",
      "train loss:0.9078403622777325\n",
      "train loss:0.9815679056516288\n",
      "train loss:0.9448309857826364\n",
      "train loss:0.9816059753005303\n",
      "train loss:0.9726815465715133\n",
      "train loss:0.9419975178770862\n",
      "train loss:0.9926201582021572\n",
      "train loss:0.9020053673583027\n",
      "train loss:0.8645616701114143\n",
      "train loss:1.052666076349257\n",
      "train loss:0.7566770372848355\n",
      "train loss:0.8809382125550554\n",
      "train loss:0.9560318934007584\n",
      "train loss:0.8666586018917402\n",
      "train loss:1.07288829563867\n",
      "train loss:0.7115598572852815\n",
      "train loss:1.0299753675408234\n",
      "train loss:1.025728387071972\n",
      "train loss:0.8718366345035526\n",
      "train loss:0.9099690660668494\n",
      "train loss:0.8929192420909068\n",
      "train loss:0.9144652690903523\n",
      "train loss:0.9056351037220458\n",
      "train loss:0.9129330584795068\n",
      "train loss:0.8754028743486317\n",
      "train loss:0.8578331018533444\n",
      "train loss:0.852873447013889\n",
      "train loss:0.8952961210340266\n",
      "train loss:0.8643408614945711\n",
      "train loss:0.895696862322399\n",
      "train loss:0.828518829423551\n",
      "train loss:0.8216728647144366\n",
      "train loss:0.7692599956090134\n",
      "train loss:0.8607380927227029\n",
      "train loss:0.711886846807965\n",
      "train loss:0.8227268244406376\n",
      "train loss:0.796945121931345\n",
      "train loss:0.9215882905237861\n",
      "train loss:0.9044275129022737\n",
      "train loss:0.9200181530335414\n",
      "train loss:0.9309983978112827\n",
      "train loss:0.9020234475907623\n",
      "train loss:0.9749502603450307\n",
      "train loss:0.9643518222552193\n",
      "train loss:1.0127117811357806\n",
      "train loss:0.8510641568966925\n",
      "train loss:0.9380439962981362\n",
      "train loss:0.96757197328622\n",
      "train loss:0.9044551816909089\n",
      "train loss:0.8688833554933959\n",
      "train loss:0.8539140648012347\n",
      "train loss:1.047968650360196\n",
      "train loss:0.8473831491547675\n",
      "train loss:0.8807781384623057\n",
      "train loss:0.8599961711546332\n",
      "train loss:0.9034494092973011\n",
      "train loss:0.8617324396496265\n",
      "train loss:0.8844138986337119\n",
      "train loss:0.9577776558570045\n",
      "train loss:0.9771709336826307\n",
      "train loss:1.0057971571979636\n",
      "train loss:0.8107151273000943\n",
      "train loss:0.9259513726206201\n",
      "train loss:0.922125709977894\n",
      "train loss:0.8794026556943672\n",
      "train loss:0.8966950024634451\n",
      "train loss:0.8832540641741647\n",
      "train loss:0.9986862290260913\n",
      "train loss:1.0293563099976477\n",
      "train loss:0.9358993493031582\n",
      "train loss:0.7756720882129948\n",
      "train loss:0.997915900521273\n",
      "train loss:0.782468173491927\n",
      "train loss:0.877146921407825\n",
      "train loss:0.7370034129484212\n",
      "train loss:0.8926286281293007\n",
      "train loss:0.8877810161287356\n",
      "train loss:0.953710901607545\n",
      "train loss:0.8193797679944934\n",
      "train loss:0.9673236697196362\n",
      "train loss:0.8051045761132894\n",
      "train loss:0.8263656388349205\n",
      "train loss:0.8513764264057667\n",
      "train loss:0.9012819418065118\n",
      "train loss:0.8857487614940243\n",
      "train loss:0.8621337037648198\n",
      "train loss:0.8484499625048388\n",
      "train loss:0.8604123196655009\n",
      "train loss:0.8168248241804481\n",
      "train loss:0.9455381418473812\n",
      "train loss:0.9220243213654026\n",
      "train loss:0.796030017056366\n",
      "train loss:0.7045202582503194\n",
      "train loss:0.8252833910801088\n",
      "train loss:0.878223817731765\n",
      "train loss:0.8432653207629021\n",
      "train loss:0.9915535549557477\n",
      "train loss:0.9023136636610493\n",
      "train loss:0.7494816775331273\n",
      "train loss:0.9522597595384282\n",
      "train loss:0.976446761480371\n",
      "train loss:0.765869576889967\n",
      "train loss:0.7664495113214229\n",
      "train loss:1.0375746008283488\n",
      "train loss:1.0229634223641504\n",
      "train loss:0.8728898979092561\n",
      "train loss:0.7887922123003164\n",
      "train loss:0.8951521000375106\n",
      "train loss:0.833169821729255\n",
      "train loss:0.9475610139915469\n",
      "train loss:0.864802814150678\n",
      "train loss:0.7412887984917425\n",
      "train loss:0.8360046552879968\n",
      "train loss:0.7758644939943984\n",
      "train loss:0.8491422522506186\n",
      "train loss:0.8311960711371555\n",
      "train loss:0.7902493647306226\n",
      "train loss:0.8138486987736744\n",
      "train loss:0.8205149822205078\n",
      "train loss:0.897109150581832\n",
      "train loss:0.9262832074677783\n",
      "train loss:0.8689076446496756\n",
      "train loss:0.9130773713738572\n",
      "train loss:0.8915036968076354\n",
      "train loss:1.0481616206238011\n",
      "train loss:0.9194502186354583\n",
      "train loss:0.9817561521074974\n",
      "train loss:0.7340972153209075\n",
      "train loss:0.9041041935984648\n",
      "train loss:0.8737860175672307\n",
      "train loss:0.757869949337981\n",
      "train loss:0.8668072849889501\n",
      "train loss:0.8342313679488182\n",
      "train loss:0.9210976521653872\n",
      "train loss:0.8511038233647871\n",
      "train loss:0.9852734602509164\n",
      "train loss:1.0955617183708772\n",
      "train loss:0.8534255839036468\n",
      "train loss:0.7160321092886227\n",
      "train loss:0.796113008961536\n",
      "train loss:0.8546505342221965\n",
      "train loss:0.7213405112577594\n",
      "train loss:0.8157614316454904\n",
      "train loss:0.9131126581135811\n",
      "train loss:0.8338220621645087\n",
      "train loss:0.7988258318153934\n",
      "train loss:0.8426093323731729\n",
      "train loss:0.8773757102318785\n",
      "train loss:0.8018619253399223\n",
      "train loss:0.8793346626063141\n",
      "train loss:0.9070544157276895\n",
      "train loss:0.9049470962233381\n",
      "train loss:0.8591938901374889\n",
      "train loss:0.7858365546536069\n",
      "train loss:0.78275373147826\n",
      "train loss:0.7891768470414223\n",
      "train loss:0.8071387810128988\n",
      "train loss:0.8618791887655937\n",
      "train loss:0.8240894795866359\n",
      "train loss:0.7454699682196225\n",
      "train loss:0.7056018189050429\n",
      "train loss:0.8432067794804992\n",
      "train loss:0.7254254544255305\n",
      "train loss:0.8073241264208322\n",
      "train loss:0.9013067450382012\n",
      "train loss:0.6594758084437997\n",
      "train loss:0.7564192014617597\n",
      "train loss:0.7278899456233573\n",
      "train loss:0.85179691453253\n",
      "train loss:0.7635362003721056\n",
      "train loss:0.8854258072938755\n",
      "train loss:0.7706257830656871\n",
      "train loss:0.7213147231613781\n",
      "train loss:0.986649971520938\n",
      "train loss:0.7793710070207815\n",
      "train loss:0.6946899460839069\n",
      "train loss:0.8516587701050093\n",
      "train loss:0.8927740674448259\n",
      "train loss:0.8804856045436763\n",
      "train loss:0.8745963112206698\n",
      "train loss:0.854554942364479\n",
      "train loss:0.9115113065795256\n",
      "train loss:0.9610973958921528\n",
      "train loss:0.9438686475052975\n",
      "train loss:0.7212494536399273\n",
      "train loss:0.8251605294074712\n",
      "train loss:0.88339574230588\n",
      "train loss:0.7843787779772449\n",
      "train loss:0.893769203416626\n",
      "train loss:0.9113857805280565\n",
      "train loss:0.7649644506233\n",
      "train loss:0.7355806060034012\n",
      "train loss:0.907597824147159\n",
      "train loss:0.6329660477730328\n",
      "train loss:0.9085235501822695\n",
      "train loss:0.7068626697070437\n",
      "train loss:0.7416065667686813\n",
      "train loss:0.8128974953377001\n",
      "train loss:0.8502363682163796\n",
      "train loss:0.7571620408163249\n",
      "train loss:0.7398974111423214\n",
      "train loss:0.7625916497596735\n",
      "train loss:0.9622933482185114\n",
      "train loss:0.8441863485286352\n",
      "train loss:0.7499817840825829\n",
      "train loss:0.918351924513425\n",
      "train loss:0.7950234702714625\n",
      "train loss:0.8411167250416212\n",
      "train loss:1.0432550939511538\n",
      "train loss:0.9035458242309233\n",
      "train loss:0.846633314482633\n",
      "train loss:0.8175830396088948\n",
      "train loss:0.8327649546474288\n",
      "train loss:0.9240025766799326\n",
      "train loss:0.8893148491769145\n",
      "train loss:0.8604090306836867\n",
      "train loss:0.9764735346449511\n",
      "train loss:0.8951680999761124\n",
      "train loss:0.8897818223777679\n",
      "train loss:0.8954896625273411\n",
      "train loss:0.8952165417824078\n",
      "train loss:0.8802944713801243\n",
      "train loss:0.8408957891315277\n",
      "train loss:0.9904993579602275\n",
      "train loss:0.7986052389325792\n",
      "train loss:0.7655586748804534\n",
      "train loss:0.8357187054287148\n",
      "train loss:0.874471688552471\n",
      "train loss:0.8367061318823273\n",
      "train loss:0.9951636370590353\n",
      "train loss:0.7445258055345216\n",
      "train loss:0.8041605097347864\n",
      "train loss:0.7634680759055802\n",
      "train loss:0.7359715414292977\n",
      "train loss:0.6638410469351343\n",
      "train loss:0.8445512313024198\n",
      "train loss:0.8991678563628933\n",
      "train loss:0.7617270461057146\n",
      "train loss:0.8449306364864604\n",
      "train loss:0.740089299074029\n",
      "train loss:0.8378829274492305\n",
      "train loss:0.8192318837298309\n",
      "train loss:0.7606956136973697\n",
      "train loss:0.8064347195109802\n",
      "train loss:0.9530032013761067\n",
      "train loss:0.8486779796951063\n",
      "train loss:0.9310609081062818\n",
      "train loss:0.8020124305682428\n",
      "train loss:0.7130563605118955\n",
      "train loss:0.8093649693503354\n",
      "train loss:0.746563193605326\n",
      "train loss:0.8563695180440312\n",
      "train loss:0.8654748122047683\n",
      "train loss:0.8213248153814747\n",
      "train loss:0.7803563353296948\n",
      "train loss:0.8424547612412014\n",
      "train loss:0.7551189011253114\n",
      "train loss:0.8898539123616463\n",
      "train loss:0.7820942622393876\n",
      "train loss:0.7322796613053753\n",
      "train loss:0.8301360364584505\n",
      "train loss:0.6937367017874955\n",
      "train loss:0.6866139551178381\n",
      "train loss:0.8297108215410599\n",
      "train loss:0.8518578444747463\n",
      "train loss:0.847608873836489\n",
      "train loss:0.8003579834705526\n",
      "train loss:0.7229313981194284\n",
      "train loss:0.8066658798587527\n",
      "train loss:0.7624608408299761\n",
      "train loss:0.7952100380563151\n",
      "train loss:0.6992183131722582\n",
      "train loss:0.8017617391317927\n",
      "train loss:0.7822374656084192\n",
      "train loss:0.838967175780145\n",
      "train loss:0.7245710637713094\n",
      "train loss:0.8007426580395791\n",
      "train loss:0.8312750113697376\n",
      "train loss:0.8754851597312845\n",
      "train loss:0.8011354400271624\n",
      "train loss:0.8194808112947879\n",
      "train loss:0.7095757688464868\n",
      "train loss:0.6252733041903007\n",
      "train loss:0.8742381548351753\n",
      "train loss:0.7029491725324911\n",
      "train loss:0.7895761672089953\n",
      "train loss:1.030970494033499\n",
      "train loss:0.8861639824593679\n",
      "train loss:1.0246454175507211\n",
      "train loss:0.7373563120754987\n",
      "train loss:0.7533834195652435\n",
      "train loss:0.6749612244510139\n",
      "train loss:0.9045298697639573\n",
      "train loss:0.8601408638797102\n",
      "train loss:0.8759186179301378\n",
      "train loss:0.9523047811317776\n",
      "train loss:0.6691997347321571\n",
      "train loss:0.8808327888397349\n",
      "train loss:0.7515691866266552\n",
      "train loss:0.7880351387372386\n",
      "train loss:0.843193419932155\n",
      "train loss:0.7856145038499579\n",
      "train loss:0.8534979423853397\n",
      "train loss:0.7966486227984068\n",
      "train loss:0.8558631998397176\n",
      "train loss:0.8874819617848385\n",
      "train loss:0.6732967709904194\n",
      "train loss:0.8177667718614297\n",
      "train loss:0.8448590796441134\n",
      "train loss:0.8755708085024431\n",
      "train loss:0.9048166948399847\n",
      "train loss:0.8513106218825732\n",
      "train loss:0.8894906669257786\n",
      "=== epoch:4, train acc:0.68, test acc:0.676 ===\n",
      "train loss:0.9169786526019397\n",
      "train loss:0.6510450924790929\n",
      "train loss:0.6871714752235486\n",
      "train loss:0.9600647344965024\n",
      "train loss:0.7168858940639947\n",
      "train loss:0.8612383306471552\n",
      "train loss:0.8277061907528492\n",
      "train loss:0.9755102015769119\n",
      "train loss:0.7318470541302747\n",
      "train loss:0.7189372692975171\n",
      "train loss:0.823689833725298\n",
      "train loss:1.0149844790510436\n",
      "train loss:0.9104488386833448\n",
      "train loss:0.8051744745262498\n",
      "train loss:0.7985441217021301\n",
      "train loss:0.738612593269642\n",
      "train loss:1.000099610080414\n",
      "train loss:0.9021311090078237\n",
      "train loss:0.7378747311742518\n",
      "train loss:0.7392654305792054\n",
      "train loss:0.8732284037708026\n",
      "train loss:0.9530631173171465\n",
      "train loss:0.7702384955178403\n",
      "train loss:0.7998513225070367\n",
      "train loss:0.8003128645727342\n",
      "train loss:0.6999251298940231\n",
      "train loss:0.814026854218134\n",
      "train loss:0.80601886233076\n",
      "train loss:0.713862403318059\n",
      "train loss:0.6790909048772613\n",
      "train loss:0.7165007710659773\n",
      "train loss:0.7276773427036659\n",
      "train loss:0.8031107806818847\n",
      "train loss:0.7456199828431651\n",
      "train loss:0.7159908465852163\n",
      "train loss:0.8775068805982781\n",
      "train loss:0.718842335290181\n",
      "train loss:0.8394495635335639\n",
      "train loss:0.8622604804165355\n",
      "train loss:0.7606368825481056\n",
      "train loss:0.8064495727182694\n",
      "train loss:0.664740318659314\n",
      "train loss:0.6081678207182981\n",
      "train loss:0.7991777459728717\n",
      "train loss:0.7031724528282389\n",
      "train loss:0.7470935507034762\n",
      "train loss:0.7931827504892139\n",
      "train loss:0.8012774357042811\n",
      "train loss:0.7799703323350069\n",
      "train loss:0.7988975340947175\n",
      "train loss:0.8006882035677053\n",
      "train loss:0.8392555186679784\n",
      "train loss:0.9643287711536834\n",
      "train loss:0.8184043861165429\n",
      "train loss:0.7695153379700266\n",
      "train loss:0.7081229434315481\n",
      "train loss:0.6405631758933861\n",
      "train loss:0.8005543852588748\n",
      "train loss:0.7012679049949169\n",
      "train loss:0.7441014133389349\n",
      "train loss:0.7348824424817999\n",
      "train loss:0.8645799893071772\n",
      "train loss:0.8774072334318859\n",
      "train loss:0.7285005910030512\n",
      "train loss:0.6596620054055792\n",
      "train loss:0.7911566733909886\n",
      "train loss:0.748317869383932\n",
      "train loss:0.8150689471906628\n",
      "train loss:0.7539120368148662\n",
      "train loss:0.8034154318838042\n",
      "train loss:0.6993496042789907\n",
      "train loss:0.8453135113539265\n",
      "train loss:0.7144057879714744\n",
      "train loss:0.6811561060038386\n",
      "train loss:0.7788649921772068\n",
      "train loss:0.7198372417448321\n",
      "train loss:0.7307427489088272\n",
      "train loss:0.8144905055542583\n",
      "train loss:0.6149473757955006\n",
      "train loss:0.9291419449159161\n",
      "train loss:0.8315387812470633\n",
      "train loss:0.8173184219415127\n",
      "train loss:0.8923856130329537\n",
      "train loss:0.6646727472105184\n",
      "train loss:0.7986569483754578\n",
      "train loss:0.6656920091761819\n",
      "train loss:0.8233552373806425\n",
      "train loss:0.6574774891972666\n",
      "train loss:0.755586690444117\n",
      "train loss:0.7750667492035878\n",
      "train loss:0.827114061570947\n",
      "train loss:0.8194728584302376\n",
      "train loss:0.756302378643381\n",
      "train loss:0.7033384773618093\n",
      "train loss:0.8370712761470288\n",
      "train loss:0.6915187205517042\n",
      "train loss:0.8128216250206661\n",
      "train loss:0.7371929125934382\n",
      "train loss:0.6577658217642832\n",
      "train loss:1.080751227327026\n",
      "train loss:0.8131889108808269\n",
      "train loss:1.0516496958675043\n",
      "train loss:0.6526036480599429\n",
      "train loss:0.6553150748430093\n",
      "train loss:0.708596360429428\n",
      "train loss:0.796335927027046\n",
      "train loss:0.7864028584445993\n",
      "train loss:0.6735548346808359\n",
      "train loss:0.6976815664947299\n",
      "train loss:0.7728254787415065\n",
      "train loss:0.8726734740700809\n",
      "train loss:0.9423338531511702\n",
      "train loss:0.656190775674569\n",
      "train loss:0.7638868903716483\n",
      "train loss:0.7956020384268955\n",
      "train loss:0.718223399370419\n",
      "train loss:0.97936522078552\n",
      "train loss:0.7375060904058863\n",
      "train loss:0.8592870643021665\n",
      "train loss:0.8299080261669181\n",
      "train loss:0.7567821829696026\n",
      "train loss:0.7697042057492969\n",
      "train loss:0.9033228890642309\n",
      "train loss:0.9869107966400931\n",
      "train loss:0.7683567087023994\n",
      "train loss:0.7724429239780606\n",
      "train loss:0.7473166660348929\n",
      "train loss:0.7000344003915686\n",
      "train loss:0.6203034567785535\n",
      "train loss:0.8581858807993246\n",
      "train loss:0.7354011394248373\n",
      "train loss:0.8210909613802202\n",
      "train loss:0.6959646998849371\n",
      "train loss:0.7595182398380613\n",
      "train loss:0.7914562812061391\n",
      "train loss:0.7441320966720488\n",
      "train loss:0.9663821729591335\n",
      "train loss:0.6909563307811963\n",
      "train loss:0.8591289864617961\n",
      "train loss:0.7233571461434559\n",
      "train loss:0.5746321062992173\n",
      "train loss:0.7638492183797584\n",
      "train loss:0.6516959560026139\n",
      "train loss:0.7956540709281189\n",
      "train loss:0.786013503210522\n",
      "train loss:0.8283431240756495\n",
      "train loss:0.6506339185170471\n",
      "train loss:0.7590495632458226\n",
      "train loss:0.6902559816722167\n",
      "train loss:0.8542447349841717\n",
      "train loss:0.8870071319605941\n",
      "train loss:0.6591857032343285\n",
      "train loss:0.8012816328034954\n",
      "train loss:0.7883076726118547\n",
      "train loss:0.7050483908716508\n",
      "train loss:0.7472527021140679\n",
      "train loss:0.7779064311968924\n",
      "train loss:0.8274701205708527\n",
      "train loss:0.9694054667057103\n",
      "train loss:0.7569595449557793\n",
      "train loss:0.8520431808607433\n",
      "train loss:0.7220011227825209\n",
      "train loss:0.6756581144763365\n",
      "train loss:0.7425970780182315\n",
      "train loss:0.7337137658416695\n",
      "train loss:0.8246901254759587\n",
      "train loss:0.7642920256037778\n",
      "train loss:0.8600476080358358\n",
      "train loss:0.7261945271569737\n",
      "train loss:0.769307744977678\n",
      "train loss:0.7721247154606388\n",
      "train loss:0.7884298330732995\n",
      "train loss:0.7055249599115556\n",
      "train loss:0.6779805906883758\n",
      "train loss:0.7476129583864539\n",
      "train loss:0.7352456988031664\n",
      "train loss:0.8591735190144522\n",
      "train loss:0.746311628411085\n",
      "train loss:0.7976154924857513\n",
      "train loss:0.6830026827021735\n",
      "train loss:0.6098203923408008\n",
      "train loss:0.8372701501919415\n",
      "train loss:0.5823948052487672\n",
      "train loss:0.6159353726651087\n",
      "train loss:0.6626684959856419\n",
      "train loss:0.7869457250112376\n",
      "train loss:0.7422289795020676\n",
      "train loss:0.8258212496976064\n",
      "train loss:0.6902400852496661\n",
      "train loss:0.7086421591918889\n",
      "train loss:0.637190247825182\n",
      "train loss:0.7232658419361238\n",
      "train loss:0.6734769995346538\n",
      "train loss:0.7352517334992678\n",
      "train loss:0.7055770607741291\n",
      "train loss:0.7472715653931097\n",
      "train loss:0.8104830070644167\n",
      "train loss:0.7673359620007774\n",
      "train loss:0.5832156412236852\n",
      "train loss:0.6759162297322066\n",
      "train loss:0.6975265658602114\n",
      "train loss:0.6752215324787976\n",
      "train loss:0.6770826509947874\n",
      "train loss:0.7642834330574919\n",
      "train loss:0.7615652212825934\n",
      "train loss:0.7055925640591753\n",
      "train loss:0.7451735598652826\n",
      "train loss:0.749681652816689\n",
      "train loss:0.6944655922345562\n",
      "train loss:0.6828921378247074\n",
      "train loss:0.6854948832245726\n",
      "train loss:0.7618472381648815\n",
      "train loss:0.7544614864527495\n",
      "train loss:0.6833216433341885\n",
      "train loss:0.7110817685390978\n",
      "train loss:0.7125558126037943\n",
      "train loss:0.8609853715595416\n",
      "train loss:0.7607730671355037\n",
      "train loss:0.6750710157608271\n",
      "train loss:0.657102702993963\n",
      "train loss:0.7475449742840118\n",
      "train loss:0.6945715368641728\n",
      "train loss:0.819925742264369\n",
      "train loss:0.6662478814785697\n",
      "train loss:0.7397244264070072\n",
      "train loss:0.7317684473484333\n",
      "train loss:0.6663454250988577\n",
      "train loss:0.7583594524207544\n",
      "train loss:0.6314270022905035\n",
      "train loss:0.6476340975033757\n",
      "train loss:0.7436803033345908\n",
      "train loss:0.6549938655341887\n",
      "train loss:0.6919888604470088\n",
      "train loss:0.7628333817011425\n",
      "train loss:0.7952616540552827\n",
      "train loss:0.6588307269517988\n",
      "train loss:0.6032212544635138\n",
      "train loss:0.8117262154615125\n",
      "train loss:0.8656071763773484\n",
      "train loss:0.716712266161553\n",
      "train loss:0.6816410529867565\n",
      "train loss:0.7967712442995676\n",
      "train loss:0.6693450592148428\n",
      "train loss:0.789353380509422\n",
      "train loss:0.772714442070993\n",
      "train loss:0.77004229478462\n",
      "train loss:0.7353050600695481\n",
      "train loss:0.7142816892001607\n",
      "train loss:0.7227506460633312\n",
      "train loss:0.685675836595356\n",
      "train loss:0.820741651726738\n",
      "train loss:0.5850437515152412\n",
      "train loss:0.7747383180335957\n",
      "train loss:0.7798423999086012\n",
      "train loss:0.7611939117480034\n",
      "train loss:0.7357975814450113\n",
      "train loss:0.6287246681147398\n",
      "train loss:0.7716191375447906\n",
      "train loss:0.6949782178233045\n",
      "train loss:0.6172071848362133\n",
      "train loss:0.7327083394876404\n",
      "train loss:0.7179076857808013\n",
      "train loss:0.6426178830891327\n",
      "train loss:0.6612358062652166\n",
      "train loss:0.7796997281340368\n",
      "train loss:0.82812386132756\n",
      "train loss:0.7240211776985644\n",
      "train loss:0.6353106389212266\n",
      "train loss:0.6235572655893186\n",
      "train loss:0.7853149489964634\n",
      "train loss:0.5493840924068998\n",
      "train loss:0.7104469343155132\n",
      "train loss:0.7657660092235732\n",
      "train loss:0.8548867512773164\n",
      "train loss:0.6965432469685398\n",
      "train loss:0.644267739371432\n",
      "train loss:0.6614145089587155\n",
      "train loss:0.735351761558291\n",
      "train loss:0.8547878110760746\n",
      "train loss:0.6999465625175939\n",
      "train loss:0.663047941021576\n",
      "train loss:0.7177353230305592\n",
      "train loss:0.7352476406852726\n",
      "train loss:0.774323083132176\n",
      "train loss:0.8695110293115169\n",
      "train loss:0.6767010451486226\n",
      "train loss:0.8371309097141539\n",
      "train loss:0.6813962713504312\n",
      "train loss:0.6182081862291657\n",
      "train loss:0.6285950629143499\n",
      "train loss:0.6369210137686462\n",
      "train loss:0.6002163172267099\n",
      "train loss:0.713609372555479\n",
      "train loss:0.5619034717824068\n",
      "train loss:0.6885676043113793\n",
      "train loss:0.7962430350682598\n",
      "train loss:0.7032944387815298\n",
      "train loss:0.699444843461306\n",
      "train loss:0.6500918871383409\n",
      "train loss:0.6294817671439334\n",
      "train loss:0.7924078571520637\n",
      "train loss:0.83533441698956\n",
      "train loss:0.6556312852961206\n",
      "train loss:0.7457460222543291\n",
      "train loss:0.7133884516156348\n",
      "train loss:0.6351429882007195\n",
      "train loss:0.6212609135660903\n",
      "train loss:0.6670715455011017\n",
      "train loss:0.8036598183411477\n",
      "train loss:0.6851283716605319\n",
      "train loss:0.6689535117528537\n",
      "train loss:0.7293030283742781\n",
      "train loss:0.709613763473514\n",
      "train loss:0.6313521300951233\n",
      "train loss:0.8477657199834365\n",
      "train loss:0.80788379790043\n",
      "train loss:0.7224913896526148\n",
      "train loss:0.7234492612130323\n",
      "train loss:0.6577792097022312\n",
      "train loss:0.6547675342062875\n",
      "train loss:0.7504006448183609\n",
      "train loss:0.6306477413639764\n",
      "train loss:0.6577698385649617\n",
      "train loss:0.7023668737608786\n",
      "train loss:0.7228865311458506\n",
      "train loss:0.6637388754312893\n",
      "train loss:0.6157092540408248\n",
      "train loss:0.8492183510256709\n",
      "train loss:0.7525988991798872\n",
      "train loss:0.6601115928105272\n",
      "train loss:0.6387601897471553\n",
      "train loss:0.8012881369619009\n",
      "train loss:0.7068214669771468\n",
      "train loss:0.7364476276607981\n",
      "train loss:0.7845117026652981\n",
      "train loss:0.6409333069674733\n",
      "train loss:0.7122411403540997\n",
      "train loss:0.6723906223541768\n",
      "train loss:0.7086445039276321\n",
      "train loss:0.6791536404002705\n",
      "train loss:0.5630565370501306\n",
      "train loss:0.8670084551533555\n",
      "train loss:0.7133245620891151\n",
      "train loss:0.7724162469690125\n",
      "train loss:0.6915128375469297\n",
      "train loss:0.5968325458510044\n",
      "train loss:0.743612178714951\n",
      "train loss:0.6891072138240181\n",
      "train loss:0.7738928673403338\n",
      "train loss:0.7516479914198341\n",
      "train loss:0.7342108489948161\n",
      "train loss:0.6676594828482496\n",
      "train loss:0.6658533593854737\n",
      "train loss:0.7527545485257745\n",
      "train loss:0.5863450812671724\n",
      "train loss:0.7000092362904663\n",
      "train loss:0.8751713701663735\n",
      "train loss:0.7469374422356393\n",
      "train loss:0.6900007353308228\n",
      "train loss:0.7684468847626724\n",
      "train loss:0.7804861429950501\n",
      "train loss:0.6455106798117625\n",
      "train loss:0.6976328153542045\n",
      "train loss:0.6761959534773616\n",
      "train loss:0.6750879636457783\n",
      "train loss:0.7645255512117446\n",
      "train loss:0.7305253404845904\n",
      "train loss:0.6441362654992844\n",
      "train loss:0.6407210332501815\n",
      "train loss:0.625224010060664\n",
      "train loss:0.74282296769875\n",
      "train loss:0.5988668024191175\n",
      "train loss:0.6788994678793477\n",
      "train loss:0.6048559695719715\n",
      "train loss:0.8492656217796231\n",
      "train loss:0.5766759594335299\n",
      "train loss:0.7205474137024455\n",
      "train loss:0.7966131853030377\n",
      "train loss:0.7361442585851045\n",
      "train loss:0.7598891124185966\n",
      "train loss:0.6548934648075327\n",
      "train loss:0.6511877420099225\n",
      "train loss:0.681347840431851\n",
      "train loss:0.5876152028549094\n",
      "train loss:0.784343818125726\n",
      "train loss:0.7447872409253548\n",
      "train loss:0.673430518283014\n",
      "train loss:0.6540251703285379\n",
      "train loss:0.6362717606288957\n",
      "train loss:0.6089761621084424\n",
      "train loss:0.8284943343321425\n",
      "train loss:0.7351808760666003\n",
      "train loss:0.639638106362846\n",
      "train loss:0.7604190824109128\n",
      "train loss:0.6114814098343515\n",
      "train loss:0.5931201221804606\n",
      "train loss:0.6956321484056754\n",
      "train loss:0.7458888186675039\n",
      "train loss:0.6332935042231361\n",
      "train loss:0.6528293661339489\n",
      "train loss:0.6130965110933151\n",
      "train loss:0.6472452452736899\n",
      "train loss:0.5754940807721112\n",
      "train loss:0.7413324108448589\n",
      "train loss:0.7494785727268867\n",
      "train loss:0.6850061337369078\n",
      "train loss:0.6304697166038454\n",
      "train loss:0.7989004684456468\n",
      "train loss:0.7154105186222252\n",
      "train loss:0.5655383001722808\n",
      "train loss:0.5421464661414713\n",
      "train loss:0.7711010419500316\n",
      "train loss:0.8508920445095957\n",
      "train loss:0.6081948945092478\n",
      "train loss:0.9232548365518349\n",
      "train loss:0.6839624546701459\n",
      "train loss:0.6254185676074862\n",
      "train loss:0.7753993315934735\n",
      "train loss:0.6582054148267928\n",
      "train loss:0.7309090522300391\n",
      "train loss:0.7340567190301611\n",
      "train loss:0.6251035935647432\n",
      "train loss:0.9231152042053177\n",
      "train loss:0.5855953238711696\n",
      "train loss:0.6302709301058542\n",
      "train loss:0.8645267190310022\n",
      "train loss:0.6895035484352374\n",
      "train loss:0.755334914060605\n",
      "train loss:0.9717155736362915\n",
      "train loss:0.6233006325066648\n",
      "train loss:0.6524000385033092\n",
      "train loss:0.6464176282386235\n",
      "train loss:0.7812031831914841\n",
      "train loss:0.8175845789304286\n",
      "train loss:0.6585123932723868\n",
      "train loss:0.6025222621131627\n",
      "train loss:0.6416059286450302\n",
      "train loss:0.6195092215207969\n",
      "train loss:0.6838593726596414\n",
      "train loss:0.5569446710415338\n",
      "train loss:0.8109254044769046\n",
      "train loss:0.70108037024278\n",
      "train loss:0.6381042203706004\n",
      "train loss:0.7624625672207699\n",
      "train loss:0.5721664312736727\n",
      "train loss:0.795567142499167\n",
      "train loss:0.9070139705421988\n",
      "train loss:0.8327691921203326\n",
      "train loss:0.8435770972848837\n",
      "train loss:0.8053374655806275\n",
      "train loss:0.6826395567453661\n",
      "train loss:0.714978527450782\n",
      "train loss:0.7390673857842388\n",
      "train loss:0.8014824535013202\n",
      "train loss:0.7515401770910815\n",
      "train loss:0.7751525564645967\n",
      "train loss:0.7804896912316573\n",
      "train loss:0.6536479128757549\n",
      "train loss:0.6414288451308863\n",
      "train loss:0.6176828830166179\n",
      "train loss:0.7316231089765368\n",
      "train loss:0.7252980541704228\n",
      "train loss:0.7002701463488434\n",
      "train loss:0.6519080739814074\n",
      "train loss:0.6864315703344168\n",
      "train loss:0.5669544322799587\n",
      "train loss:0.5427705248859364\n",
      "train loss:0.6278965392309483\n",
      "train loss:0.5821762753897463\n",
      "train loss:0.6565040404695783\n",
      "train loss:0.6827883538493229\n",
      "train loss:0.8245203301154995\n",
      "train loss:0.7750169059997758\n",
      "train loss:0.8746614272333069\n",
      "train loss:0.7433216093712852\n",
      "train loss:0.7199908120821206\n",
      "train loss:0.7979804585811288\n",
      "train loss:0.669540208893702\n",
      "train loss:0.6822048617282634\n",
      "train loss:0.8034734059762972\n",
      "train loss:0.6100454362614474\n",
      "train loss:0.7548209166536434\n",
      "train loss:0.6581894734853555\n",
      "train loss:0.7764448535640366\n",
      "train loss:0.5878595365992035\n",
      "train loss:0.5519253251093966\n",
      "train loss:0.7085723423359569\n",
      "train loss:0.6529981099078221\n",
      "train loss:0.6455030633545868\n",
      "train loss:0.6962432788679859\n",
      "train loss:0.6969054694784076\n",
      "train loss:0.7327546446924066\n",
      "train loss:0.8275844572561921\n",
      "train loss:0.7742437798671956\n",
      "train loss:0.703260961467834\n",
      "train loss:0.6205196835665978\n",
      "train loss:0.7220830175109395\n",
      "train loss:0.6157688469085013\n",
      "train loss:0.6060874734425856\n",
      "train loss:0.7443446217643276\n",
      "train loss:0.7548634896427404\n",
      "train loss:0.5485836765185387\n",
      "train loss:0.5535941890517059\n",
      "train loss:0.6422248937450437\n",
      "train loss:0.5917209005987082\n",
      "train loss:0.5593180993463767\n",
      "train loss:0.8199001249551197\n",
      "train loss:0.5629930977521667\n",
      "train loss:0.5547058110099418\n",
      "train loss:0.884163176684647\n",
      "train loss:0.7633851754576121\n",
      "train loss:0.7143153183830242\n",
      "train loss:0.6228646249066524\n",
      "train loss:0.7293178729705506\n",
      "train loss:0.6624340522329071\n",
      "train loss:0.768043051499966\n",
      "train loss:1.0026309598299994\n",
      "train loss:0.7932837428596805\n",
      "train loss:0.584909428263988\n",
      "train loss:0.7618884152404024\n",
      "train loss:0.5543334290604705\n",
      "train loss:0.732952080795114\n",
      "train loss:0.7283922816179114\n",
      "train loss:0.6602612269157203\n",
      "train loss:0.7306737926781715\n",
      "train loss:0.6980519765659483\n",
      "train loss:0.6275918791095644\n",
      "train loss:0.5960325484224203\n",
      "train loss:0.7475246888506826\n",
      "train loss:0.8420346289284925\n",
      "train loss:0.7600735287686752\n",
      "train loss:0.6685225493387351\n",
      "train loss:0.722506424334751\n",
      "train loss:0.6919171216753933\n",
      "train loss:0.7716584357776405\n",
      "train loss:0.7276495601462466\n",
      "train loss:0.6776003396690977\n",
      "train loss:0.6883005609146573\n",
      "train loss:0.8552136997607894\n",
      "train loss:0.8550259410895137\n",
      "train loss:0.6758855769014203\n",
      "train loss:0.7269597109727214\n",
      "train loss:0.6327418370222806\n",
      "train loss:0.6746078284417707\n",
      "train loss:0.7515298683818572\n",
      "train loss:0.6807550745698133\n",
      "train loss:0.6251042325678658\n",
      "train loss:0.6623140486444796\n",
      "train loss:0.7699933681276424\n",
      "train loss:0.7127141035400137\n",
      "train loss:0.5686120418275386\n",
      "train loss:0.7217926213750505\n",
      "train loss:0.7109770126151492\n",
      "train loss:0.744188184694076\n",
      "train loss:0.687575356409702\n",
      "train loss:0.6311295739106576\n",
      "train loss:0.7336584720687314\n",
      "train loss:0.7361544466541736\n",
      "train loss:0.6601424097627855\n",
      "train loss:0.6392064282760915\n",
      "train loss:0.6908196248238184\n",
      "train loss:0.7259203196491407\n",
      "train loss:0.627605702013043\n",
      "train loss:0.6165530414174452\n",
      "train loss:0.6752867736678805\n",
      "train loss:0.741658003709325\n",
      "train loss:0.6860989564754931\n",
      "train loss:0.7608583428365779\n",
      "train loss:0.5614138582173634\n",
      "train loss:0.6366639952048114\n",
      "train loss:0.682905486825971\n",
      "train loss:0.7904535349129024\n",
      "train loss:0.7008816302676365\n",
      "train loss:0.5975595223473091\n",
      "train loss:0.741936379231386\n",
      "train loss:0.8205754975850287\n",
      "train loss:0.6320605206858608\n",
      "train loss:0.6764209724622741\n",
      "train loss:0.6442482619870722\n",
      "train loss:0.6041581659480603\n",
      "train loss:0.6086560475556009\n",
      "train loss:0.5926285468001026\n",
      "train loss:0.6463325443107526\n",
      "train loss:0.8107731517229538\n",
      "train loss:0.7492698020438044\n",
      "train loss:0.7608037543320353\n",
      "train loss:0.6871428306872701\n",
      "train loss:0.6988716811476927\n",
      "train loss:0.606611158439922\n",
      "train loss:0.7568781991447721\n",
      "train loss:0.6705916611631608\n",
      "train loss:0.7450709374902172\n",
      "train loss:0.621944813068939\n",
      "train loss:0.6576585363847731\n",
      "train loss:0.6196048771648908\n",
      "train loss:0.6281873652030585\n",
      "train loss:0.6613258622289887\n",
      "train loss:0.6160644139025147\n",
      "train loss:0.680397679211784\n",
      "train loss:0.5846423644210461\n",
      "=== epoch:5, train acc:0.705, test acc:0.708 ===\n",
      "train loss:0.6133638023791727\n",
      "train loss:0.6405848189566568\n",
      "train loss:0.7968503923392798\n",
      "train loss:0.6370796757264575\n",
      "train loss:0.7799699595328297\n",
      "train loss:0.8674115065493999\n",
      "train loss:0.7526526900589139\n",
      "train loss:0.787326664731253\n",
      "train loss:0.645948275277105\n",
      "train loss:0.6519054572658787\n",
      "train loss:0.5761963273347482\n",
      "train loss:0.7056219620081248\n",
      "train loss:0.8708732541892146\n",
      "train loss:0.6706813648122992\n",
      "train loss:0.6320639769634295\n",
      "train loss:0.6975141818157418\n",
      "train loss:0.6912538232810725\n",
      "train loss:0.7217286245864554\n",
      "train loss:0.7786945314753723\n",
      "train loss:0.6327605335882616\n",
      "train loss:0.7165669001738693\n",
      "train loss:0.7456689758543112\n",
      "train loss:0.6594106518856182\n",
      "train loss:0.6877929100455493\n",
      "train loss:0.600486971683121\n",
      "train loss:0.611121863828271\n",
      "train loss:0.6916732109262327\n",
      "train loss:0.6505469035782017\n",
      "train loss:0.8639042894545769\n",
      "train loss:0.7235253420426058\n",
      "train loss:0.6882982877228889\n",
      "train loss:0.6612808340613957\n",
      "train loss:0.6516725441017907\n",
      "train loss:0.6596602076820862\n",
      "train loss:0.6364685294157947\n",
      "train loss:0.6948621331751061\n",
      "train loss:0.5659685383327245\n",
      "train loss:0.686194949380162\n",
      "train loss:0.7173101412222741\n",
      "train loss:0.6205406496864475\n",
      "train loss:0.7544678564389907\n",
      "train loss:0.6776304072369611\n",
      "train loss:0.6104508611810371\n",
      "train loss:0.5885826411158721\n",
      "train loss:0.7020318236349015\n",
      "train loss:0.5201607826600232\n",
      "train loss:0.5009638643919802\n",
      "train loss:0.6242167040163771\n",
      "train loss:0.7405611932261154\n",
      "train loss:0.7026503869128241\n",
      "train loss:0.6071471215984426\n",
      "train loss:0.7743369482364817\n",
      "train loss:0.6453088658151316\n",
      "train loss:0.6543844342370164\n",
      "train loss:0.6346570631924026\n",
      "train loss:0.6704237660451936\n",
      "train loss:0.5439518047657944\n",
      "train loss:0.658289922073444\n",
      "train loss:0.6845160224139734\n",
      "train loss:0.5962788371553005\n",
      "train loss:0.5683934560978648\n",
      "train loss:0.688454319442923\n",
      "train loss:0.809085619670507\n",
      "train loss:0.6251836472419419\n",
      "train loss:0.6187755118762261\n",
      "train loss:0.7093078937741505\n",
      "train loss:0.6886753268714435\n",
      "train loss:0.7085882435892421\n",
      "train loss:0.6417963895249763\n",
      "train loss:0.5855552243548405\n",
      "train loss:0.7122505190576414\n",
      "train loss:0.6289130111384045\n",
      "train loss:0.5731336688408747\n",
      "train loss:0.6140492041559314\n",
      "train loss:0.7673989337400413\n",
      "train loss:0.7024569901093591\n",
      "train loss:0.5905503529306325\n",
      "train loss:0.7090531278271953\n",
      "train loss:0.5575517729964179\n",
      "train loss:0.6566253332983001\n",
      "train loss:0.5733451659255758\n",
      "train loss:0.697206668928133\n",
      "train loss:0.669715549546531\n",
      "train loss:0.5516129765469961\n",
      "train loss:0.7842664485242341\n",
      "train loss:0.6746968393596836\n",
      "train loss:0.6878242830232325\n",
      "train loss:0.6738874659536563\n",
      "train loss:0.754801126207044\n",
      "train loss:0.666588438327207\n",
      "train loss:0.7771144153995039\n",
      "train loss:0.8045347860137337\n",
      "train loss:0.8066444467814408\n",
      "train loss:0.6705147200693985\n",
      "train loss:0.6877741847901531\n",
      "train loss:0.6108077007794537\n",
      "train loss:0.5483638164908886\n",
      "train loss:0.6148550603064433\n",
      "train loss:0.6008987090506676\n",
      "train loss:0.6635697945674494\n",
      "train loss:0.5166731933661766\n",
      "train loss:0.7522306527114219\n",
      "train loss:0.6185098849630997\n",
      "train loss:0.6203938314996962\n",
      "train loss:0.6074980910311772\n",
      "train loss:0.5174754214383208\n",
      "train loss:0.6123593694656018\n",
      "train loss:0.765716951868349\n",
      "train loss:0.6227124671370013\n",
      "train loss:0.5679654970262411\n",
      "train loss:0.6764205794026468\n",
      "train loss:0.7095278701530515\n",
      "train loss:0.5891784342423174\n",
      "train loss:0.6379723623703132\n",
      "train loss:0.6051089683873176\n",
      "train loss:0.5958058832703257\n",
      "train loss:0.6179248748799033\n",
      "train loss:0.6608738726689384\n",
      "train loss:0.7954214979623389\n",
      "train loss:0.6134354512239518\n",
      "train loss:0.6863991397081048\n",
      "train loss:0.7494493150588248\n",
      "train loss:0.6292112947080799\n",
      "train loss:0.6885742290967066\n",
      "train loss:0.6176892934554816\n",
      "train loss:0.6301563462078998\n",
      "train loss:0.6494340524294381\n",
      "train loss:0.7950639431981348\n",
      "train loss:0.626979945395929\n",
      "train loss:0.6200868132291234\n",
      "train loss:0.6394058937999927\n",
      "train loss:0.7940719798334152\n",
      "train loss:0.698871343143706\n",
      "train loss:0.607929070732938\n",
      "train loss:0.6688816898811264\n",
      "train loss:0.8925119471724949\n",
      "train loss:0.7718467968718707\n",
      "train loss:0.6862996687639342\n",
      "train loss:0.6797419709212703\n",
      "train loss:0.5499490150862699\n",
      "train loss:0.6648104704430564\n",
      "train loss:0.6336745482248103\n",
      "train loss:0.6829177933011463\n",
      "train loss:0.7024674245638092\n",
      "train loss:0.5628469307111602\n",
      "train loss:0.5649758511125879\n",
      "train loss:0.7189352477684681\n",
      "train loss:0.7241011087789877\n",
      "train loss:0.7920163525896955\n",
      "train loss:0.5653148267684742\n",
      "train loss:0.6595719580201075\n",
      "train loss:0.6029456527328971\n",
      "train loss:0.6145982508894914\n",
      "train loss:0.6743933656454683\n",
      "train loss:0.6775899839604247\n",
      "train loss:0.6466799939624411\n",
      "train loss:0.524936257068115\n",
      "train loss:0.5046582827313129\n",
      "train loss:0.5639283514831126\n",
      "train loss:0.6794877751793004\n",
      "train loss:0.5546436033251517\n",
      "train loss:0.7009886377818557\n",
      "train loss:0.6557982507558022\n",
      "train loss:0.6664575135771357\n",
      "train loss:0.676466895196032\n",
      "train loss:0.6519257856839908\n",
      "train loss:0.7281384887295035\n",
      "train loss:0.5510007103679524\n",
      "train loss:0.5480177736222109\n",
      "train loss:0.8244986656150624\n",
      "train loss:0.6147283491112809\n",
      "train loss:0.6464588020109109\n",
      "train loss:0.5788031597178116\n",
      "train loss:0.6406209300862288\n",
      "train loss:0.7292900311662646\n",
      "train loss:0.6830078372716168\n",
      "train loss:0.5758724096221534\n",
      "train loss:0.8072431759962926\n",
      "train loss:0.6457138079222589\n",
      "train loss:0.4388918892296054\n",
      "train loss:0.5652062182320962\n",
      "train loss:0.7253718530668307\n",
      "train loss:0.6878686147188907\n",
      "train loss:0.6736462073020413\n",
      "train loss:0.781396085994585\n",
      "train loss:0.6715893136368853\n",
      "train loss:0.6571251293802313\n",
      "train loss:0.748385741895448\n",
      "train loss:0.5715223765343741\n",
      "train loss:0.8001087378670836\n",
      "train loss:0.6529385782089375\n",
      "train loss:0.5617256726953732\n",
      "train loss:0.6587106463145534\n",
      "train loss:0.7365626791324046\n",
      "train loss:0.6637228277182275\n",
      "train loss:0.6780346295409039\n",
      "train loss:0.5289469041069566\n",
      "train loss:0.627589655171856\n",
      "train loss:0.6848051191190513\n",
      "train loss:0.6967635947648684\n",
      "train loss:0.6568330780535864\n",
      "train loss:0.6300618178927844\n",
      "train loss:0.5623150681767488\n",
      "train loss:0.6631961045180055\n",
      "train loss:0.6828265046434687\n",
      "train loss:0.5848110610110849\n",
      "train loss:0.6569021155797873\n",
      "train loss:0.7226221126742602\n",
      "train loss:0.6551360876497182\n",
      "train loss:0.7212429435205727\n",
      "train loss:0.7135960107251528\n",
      "train loss:0.6609137268185208\n",
      "train loss:0.6292822641157015\n",
      "train loss:0.6024398787401997\n",
      "train loss:0.6032531715569357\n",
      "train loss:0.8135048713043149\n",
      "train loss:0.6041394660053953\n",
      "train loss:0.6222699565745294\n",
      "train loss:0.6398700785690328\n",
      "train loss:0.6196652389832066\n",
      "train loss:0.6130899310995145\n",
      "train loss:0.8495029534169573\n",
      "train loss:0.6084811261845674\n",
      "train loss:0.8079268929438335\n",
      "train loss:0.6499299310676468\n",
      "train loss:0.6289004180108394\n",
      "train loss:0.6966225089032057\n",
      "train loss:0.7310455591811222\n",
      "train loss:0.6614747191675413\n",
      "train loss:0.67164395445342\n",
      "train loss:0.5658161527035523\n",
      "train loss:0.8577381349892089\n",
      "train loss:0.7267797883083691\n",
      "train loss:0.6643618982712637\n",
      "train loss:0.7062881674075371\n",
      "train loss:0.6867362600023165\n",
      "train loss:0.6749638586203358\n",
      "train loss:0.6857863585225463\n",
      "train loss:0.6304776022805048\n",
      "train loss:0.6777683742415178\n",
      "train loss:0.7357638890640555\n",
      "train loss:0.5404822844339114\n",
      "train loss:0.6335069840250044\n",
      "train loss:0.6326210215376146\n",
      "train loss:0.7283106771568064\n",
      "train loss:0.6637334165197055\n",
      "train loss:0.5203962234974621\n",
      "train loss:0.6114963571843683\n",
      "train loss:0.6194547653967144\n",
      "train loss:0.7661919658281966\n",
      "train loss:0.619002609796858\n",
      "train loss:0.6251078094700775\n",
      "train loss:0.5888513245530849\n",
      "train loss:0.6051622291247474\n",
      "train loss:0.6109751429878224\n",
      "train loss:0.6545188290592445\n",
      "train loss:0.6063360152257451\n",
      "train loss:0.5456587301915641\n",
      "train loss:0.6616505889032418\n",
      "train loss:0.63989888039217\n",
      "train loss:0.5251510640126255\n",
      "train loss:0.6823214273835864\n",
      "train loss:0.7598023420268583\n",
      "train loss:0.6592504282034183\n",
      "train loss:0.5935192121303101\n",
      "train loss:0.5556974655250926\n",
      "train loss:0.7169768814963909\n",
      "train loss:0.6668125765557292\n",
      "train loss:0.5090114903407301\n",
      "train loss:0.5816823453931853\n",
      "train loss:0.5948363157803831\n",
      "train loss:0.6459760616565824\n",
      "train loss:0.6164799224970399\n",
      "train loss:0.7235602266012284\n",
      "train loss:0.6780574796399962\n",
      "train loss:0.8346960184962497\n",
      "train loss:0.5934153861748824\n",
      "train loss:0.5939341263005756\n",
      "train loss:0.6571910622449191\n",
      "train loss:0.5301774737585747\n",
      "train loss:0.5550231477138534\n",
      "train loss:0.7854375921646164\n",
      "train loss:0.5603933756972398\n",
      "train loss:0.6808121152029502\n",
      "train loss:0.8510738125928787\n",
      "train loss:0.6627097009863963\n",
      "train loss:0.6242706381865386\n",
      "train loss:0.5711304427741648\n",
      "train loss:0.6140866304416412\n",
      "train loss:0.6897367207637524\n",
      "train loss:0.6579033834773115\n",
      "train loss:0.7537385083174181\n",
      "train loss:0.6368591334977729\n",
      "train loss:0.7268033724115459\n",
      "train loss:0.6236084613725628\n",
      "train loss:0.6173155960238172\n",
      "train loss:0.5795433436023427\n",
      "train loss:0.6533764786414896\n",
      "train loss:0.6102811489530461\n",
      "train loss:0.5223436539938319\n",
      "train loss:0.7370586390986513\n",
      "train loss:0.5417773445513838\n",
      "train loss:0.624746221662034\n",
      "train loss:0.6802054975635434\n",
      "train loss:0.8101916257994639\n",
      "train loss:0.8005697290917072\n",
      "train loss:0.8224781076805161\n",
      "train loss:0.6005020263456815\n",
      "train loss:0.5751694058960093\n",
      "train loss:0.5109909522670424\n",
      "train loss:0.6666664289605241\n",
      "train loss:0.535866069978881\n",
      "train loss:0.6858105140157256\n",
      "train loss:0.713887618694479\n",
      "train loss:0.4928830847342514\n",
      "train loss:0.5678964365980141\n",
      "train loss:0.6041146036480367\n",
      "train loss:0.4657693032130625\n",
      "train loss:0.7395530507913594\n",
      "train loss:0.6763696613945693\n",
      "train loss:0.5459714693444007\n",
      "train loss:0.5383556195678783\n",
      "train loss:0.6654872449521123\n",
      "train loss:0.5822865695570498\n",
      "train loss:0.7269763870395329\n",
      "train loss:0.6788926820562047\n",
      "train loss:0.605918209443448\n",
      "train loss:0.6019307620051277\n",
      "train loss:0.7201946986246474\n",
      "train loss:0.6381163431239164\n",
      "train loss:0.5704665290507959\n",
      "train loss:0.6962532731202002\n",
      "train loss:0.7116733736623732\n",
      "train loss:0.6254486781021337\n",
      "train loss:0.7667662925052461\n",
      "train loss:0.7571303366718075\n",
      "train loss:0.6742087793641127\n",
      "train loss:0.6445865342885816\n",
      "train loss:0.6786145252849155\n",
      "train loss:0.6746508221492061\n",
      "train loss:0.6087630229014855\n",
      "train loss:0.6516873326314481\n",
      "train loss:0.8019159632570283\n",
      "train loss:0.6470950681105555\n",
      "train loss:0.6599939660500792\n",
      "train loss:0.5407019933291075\n",
      "train loss:0.802990356192261\n",
      "train loss:0.5827946239832683\n",
      "train loss:0.6014067867451621\n",
      "train loss:0.6235955727099933\n",
      "train loss:0.6091994457752518\n",
      "train loss:0.6235249666753124\n",
      "train loss:0.8627853717706943\n",
      "train loss:0.5434008753230973\n",
      "train loss:0.5981134643043426\n",
      "train loss:0.5530731067897087\n",
      "train loss:0.6392651329862733\n",
      "train loss:0.6053926816273316\n",
      "train loss:0.6605779311013383\n",
      "train loss:0.5983576690866177\n",
      "train loss:0.6214815092975157\n",
      "train loss:0.6061633971590557\n",
      "train loss:0.5829665292844866\n",
      "train loss:0.6496603967489206\n",
      "train loss:0.6206092207013392\n",
      "train loss:0.6611932428028134\n",
      "train loss:0.6852807256248903\n",
      "train loss:0.7105607573693407\n",
      "train loss:0.5597634010435968\n",
      "train loss:0.662125747770134\n",
      "train loss:0.5510510764589907\n",
      "train loss:0.7148834803640198\n",
      "train loss:0.5579448536194861\n",
      "train loss:0.6222232796389369\n",
      "train loss:0.5028949321399179\n",
      "train loss:0.6381416341221591\n",
      "train loss:0.5036048744133435\n",
      "train loss:0.5499837467291296\n",
      "train loss:0.6442506186280841\n",
      "train loss:0.637248189413943\n",
      "train loss:0.6252433346423723\n",
      "train loss:0.6149528636382766\n",
      "train loss:0.6832719462197572\n",
      "train loss:0.5687466701860902\n",
      "train loss:0.6598819598361126\n",
      "train loss:0.6479444328024729\n",
      "train loss:0.572112037699243\n",
      "train loss:0.5565261922589955\n",
      "train loss:0.5407506908122233\n",
      "train loss:0.7572729850435261\n",
      "train loss:0.5929903013298579\n",
      "train loss:0.6684530755700447\n",
      "train loss:0.7513572925977405\n",
      "train loss:0.5787978975921411\n",
      "train loss:0.6291334873550948\n",
      "train loss:0.5434967364348482\n",
      "train loss:0.7426884004248397\n",
      "train loss:0.6535598925172005\n",
      "train loss:0.5482725465357202\n",
      "train loss:0.5850442578756724\n",
      "train loss:0.6722078451534296\n",
      "train loss:0.6880495410484195\n",
      "train loss:0.7531146536158089\n",
      "train loss:0.6998316893241061\n",
      "train loss:0.7943280152707511\n",
      "train loss:0.4938574583421421\n",
      "train loss:0.4716785408894713\n",
      "train loss:0.5529513657876808\n",
      "train loss:0.578089033581752\n",
      "train loss:0.591521330083514\n",
      "train loss:0.6418953209774115\n",
      "train loss:0.7307754073496906\n",
      "train loss:0.6415861550080001\n",
      "train loss:0.5556887681531274\n",
      "train loss:0.5302934357815855\n",
      "train loss:0.5810286523078924\n",
      "train loss:0.8073247626831477\n",
      "train loss:0.5305181349809251\n",
      "train loss:0.49990096695015707\n",
      "train loss:0.7076067252098507\n",
      "train loss:0.5463496560891471\n",
      "train loss:0.6635890465984527\n",
      "train loss:0.7098231802669792\n",
      "train loss:0.6100661567044161\n",
      "train loss:0.6263764718941831\n",
      "train loss:0.5089227742527955\n",
      "train loss:0.6168260512294953\n",
      "train loss:0.5622344581934438\n",
      "train loss:0.59029257944095\n",
      "train loss:0.5792413033440065\n",
      "train loss:0.9124245097846252\n",
      "train loss:0.619845981908208\n",
      "train loss:0.6514154031256869\n",
      "train loss:0.6436477397424661\n",
      "train loss:0.676335829174039\n",
      "train loss:0.6168758861940054\n",
      "train loss:0.6465536969854777\n",
      "train loss:0.5683412850918559\n",
      "train loss:0.6455161658440689\n",
      "train loss:0.8246183182204608\n",
      "train loss:0.5299806047047168\n",
      "train loss:0.638372492614415\n",
      "train loss:0.4694718849066363\n",
      "train loss:0.514742166560561\n",
      "train loss:0.6628407487142661\n",
      "train loss:0.7543610099672736\n",
      "train loss:0.5709750232957457\n",
      "train loss:0.579991637596414\n",
      "train loss:0.6095296800344331\n",
      "train loss:0.4393486286905046\n",
      "train loss:0.6504074931817008\n",
      "train loss:0.5640741344462769\n",
      "train loss:0.59883634290996\n",
      "train loss:0.6811317072739824\n",
      "train loss:0.7420860464435409\n",
      "train loss:0.7060354689744577\n",
      "train loss:0.7872343352793126\n",
      "train loss:0.7394252378205808\n",
      "train loss:0.6567901159054286\n",
      "train loss:0.6693226174893024\n",
      "train loss:0.800398474850549\n",
      "train loss:0.5873318078473634\n",
      "train loss:0.5592723645839306\n",
      "train loss:0.6676733238240152\n",
      "train loss:0.6390087591566943\n",
      "train loss:0.7500990085363318\n",
      "train loss:0.9059380199438262\n",
      "train loss:0.5435116548173842\n",
      "train loss:0.7417933699342217\n",
      "train loss:0.7257636632115841\n",
      "train loss:0.553209907103415\n",
      "train loss:0.6073309736977017\n",
      "train loss:0.570672116965915\n",
      "train loss:0.5806720958099977\n",
      "train loss:0.6289160255931902\n",
      "train loss:0.7607296844263238\n",
      "train loss:0.6307637353096532\n",
      "train loss:0.603300126820431\n",
      "train loss:0.7444967280022317\n",
      "train loss:0.6405464863914194\n",
      "train loss:0.701765977063389\n",
      "train loss:0.6607721958806096\n",
      "train loss:0.5997975028338288\n",
      "train loss:0.6382531349692971\n",
      "train loss:0.580109999221578\n",
      "train loss:0.5849541473823081\n",
      "train loss:0.7543085388926554\n",
      "train loss:0.712844425326859\n",
      "train loss:0.6412531093734987\n",
      "train loss:0.6339690308773829\n",
      "train loss:0.6341839980866376\n",
      "train loss:0.6394403027065602\n",
      "train loss:0.575363852033823\n",
      "train loss:0.679953058919373\n",
      "train loss:0.7769824722522849\n",
      "train loss:0.592033442213406\n",
      "train loss:0.5113127261344418\n",
      "train loss:0.5556465616192505\n",
      "train loss:0.563904522484262\n",
      "train loss:0.5170199980203997\n",
      "train loss:0.5647047310107732\n",
      "train loss:0.7895421071013226\n",
      "train loss:0.5405916610882298\n",
      "train loss:0.6366720717012645\n",
      "train loss:0.627514512113261\n",
      "train loss:0.5547234454883424\n",
      "train loss:0.5718193269471324\n",
      "train loss:0.5653309842349582\n",
      "train loss:0.5828766247700995\n",
      "train loss:0.5472405772891568\n",
      "train loss:0.6339123761778069\n",
      "train loss:0.5857994094234932\n",
      "train loss:0.5992101985918264\n",
      "train loss:0.5368614823761834\n",
      "train loss:0.745770522345242\n",
      "train loss:0.5594247803681822\n",
      "train loss:0.5890143903493398\n",
      "train loss:0.561270679177015\n",
      "train loss:0.462070506196937\n",
      "train loss:0.5326271125434967\n",
      "train loss:0.6195188206628607\n",
      "train loss:0.5180980474384898\n",
      "train loss:0.5805171344441827\n",
      "train loss:0.6558316132094062\n",
      "train loss:0.6490367401627927\n",
      "train loss:0.5365421987463909\n",
      "train loss:0.5681689463651661\n",
      "train loss:0.4704048000081356\n",
      "train loss:0.6808554334795316\n",
      "train loss:0.6189193813531106\n",
      "train loss:0.575335855530107\n",
      "train loss:0.4826114455824008\n",
      "train loss:0.6519561436066723\n",
      "train loss:0.5109838841592648\n",
      "train loss:0.6200343315894145\n",
      "train loss:0.5142045605744638\n",
      "train loss:0.7546905495100401\n",
      "train loss:0.636625290444484\n",
      "train loss:0.5169308846515619\n",
      "train loss:0.5768313081302386\n",
      "train loss:0.6664806823844655\n",
      "train loss:0.6531464868507376\n",
      "train loss:0.6095011869057292\n",
      "train loss:0.5570491218816195\n",
      "train loss:0.5901855166995617\n",
      "train loss:0.7873676805617263\n",
      "train loss:0.6128451899739682\n",
      "train loss:0.7506791890308488\n",
      "train loss:0.5462040207414078\n",
      "train loss:0.6930423436962999\n",
      "train loss:0.8513046818334412\n",
      "train loss:0.6170135177663155\n",
      "train loss:0.6257159626310579\n",
      "train loss:0.6379292707164627\n",
      "train loss:0.6731742745037296\n",
      "train loss:0.6470946778031771\n",
      "train loss:0.6128508974631722\n",
      "train loss:0.5570056161365436\n",
      "train loss:0.5747407857795307\n",
      "train loss:0.6647484535017308\n",
      "train loss:0.7032686397477452\n",
      "train loss:0.6691765336167353\n",
      "train loss:0.7454048905317407\n",
      "train loss:0.6689463766354049\n",
      "train loss:0.6484458692277311\n",
      "train loss:0.5982850648860526\n",
      "train loss:0.5786978676738622\n",
      "train loss:0.5543929047180918\n",
      "train loss:0.7136612947552653\n",
      "train loss:0.5747011457185782\n",
      "train loss:0.7334849664478292\n",
      "train loss:0.5610052760015061\n",
      "train loss:0.59169940884792\n",
      "train loss:0.5787910184721873\n",
      "train loss:0.5780582038646638\n",
      "train loss:0.4540471080012203\n",
      "train loss:0.5688052316158605\n",
      "train loss:0.447866069730544\n",
      "train loss:0.5575541386657615\n",
      "train loss:0.6502854241994056\n",
      "train loss:0.6291500678105844\n",
      "train loss:0.646028570468927\n",
      "train loss:0.6288762214739484\n",
      "train loss:0.5397161388540214\n",
      "train loss:0.6260846539791872\n",
      "train loss:0.5671212451711487\n",
      "train loss:0.5654753363668452\n",
      "train loss:0.659064808898861\n",
      "train loss:0.6982624717207438\n",
      "train loss:0.7187469024600063\n",
      "train loss:0.6487504890947551\n",
      "train loss:0.6026956026319703\n",
      "train loss:0.6157093804562129\n",
      "train loss:0.5718528475277375\n",
      "train loss:0.6993379984794426\n",
      "train loss:0.5341532175219429\n",
      "train loss:0.5136096860032708\n",
      "train loss:0.5232675495910806\n",
      "train loss:0.5334687885716323\n",
      "train loss:0.6547181442290296\n",
      "=== epoch:6, train acc:0.726, test acc:0.733 ===\n",
      "train loss:0.8229860280826102\n",
      "train loss:0.5009057122008389\n",
      "train loss:0.6042883368413265\n",
      "train loss:0.6718080873977946\n",
      "train loss:0.706519925218163\n",
      "train loss:0.8924675696124724\n",
      "train loss:0.6077919590072397\n",
      "train loss:0.7150710252808526\n",
      "train loss:0.7643374250702502\n",
      "train loss:0.5468143278908493\n",
      "train loss:0.6217707789209603\n",
      "train loss:0.6210868246100236\n",
      "train loss:0.6355150616930572\n",
      "train loss:0.6895813665231855\n",
      "train loss:0.644770210975075\n",
      "train loss:0.6108979269506762\n",
      "train loss:0.7835992409936702\n",
      "train loss:0.5359457690502571\n",
      "train loss:0.5237521901664641\n",
      "train loss:0.517613210350548\n",
      "train loss:0.565901972376909\n",
      "train loss:0.6631018612862689\n",
      "train loss:0.6477715661316827\n",
      "train loss:0.5449346536937355\n",
      "train loss:0.5651144822903434\n",
      "train loss:0.5313965374182976\n",
      "train loss:0.6260152297566113\n",
      "train loss:0.6047250561417115\n",
      "train loss:0.6124801406944647\n",
      "train loss:0.7871549209277086\n",
      "train loss:0.5073258810920664\n",
      "train loss:0.6098958797568872\n",
      "train loss:0.5959106580636463\n",
      "train loss:0.5975948906610804\n",
      "train loss:0.7602521007915051\n",
      "train loss:0.6049940901719641\n",
      "train loss:0.6291216107702068\n",
      "train loss:0.6774971131171635\n",
      "train loss:0.6150258820189008\n",
      "train loss:0.5071133774436395\n",
      "train loss:0.6391443417281697\n",
      "train loss:0.6956948812070967\n",
      "train loss:0.5751489336387994\n",
      "train loss:0.5449378505031411\n",
      "train loss:0.5989359762932436\n",
      "train loss:0.5345924514486048\n",
      "train loss:0.8736126487017783\n",
      "train loss:0.5382555276658789\n",
      "train loss:0.532815246465433\n",
      "train loss:0.5789438559142145\n",
      "train loss:0.5059608645389909\n",
      "train loss:0.5905563179312238\n",
      "train loss:0.508390184290352\n",
      "train loss:0.5702424357473047\n",
      "train loss:0.7388299594914126\n",
      "train loss:0.5171181374626883\n",
      "train loss:0.5018945514581984\n",
      "train loss:0.6086734259159214\n",
      "train loss:0.582700838155813\n",
      "train loss:0.5825921856404465\n",
      "train loss:0.4915483799882503\n",
      "train loss:0.5284142775804969\n",
      "train loss:0.4301391858811033\n",
      "train loss:0.7887026932536826\n",
      "train loss:0.6451878967695667\n",
      "train loss:0.5811817406706318\n",
      "train loss:0.6599392424458209\n",
      "train loss:0.47509655081029406\n",
      "train loss:0.6443440987357693\n",
      "train loss:0.7098330648552801\n",
      "train loss:0.5535324305034502\n",
      "train loss:0.5175926826264861\n",
      "train loss:0.6392585151586162\n",
      "train loss:0.4990941168021044\n",
      "train loss:0.5409132631108358\n",
      "train loss:0.5898402672407852\n",
      "train loss:0.5843336358033788\n",
      "train loss:0.7323935918487123\n",
      "train loss:0.6793300872579231\n",
      "train loss:0.5599544622532154\n",
      "train loss:0.6314266391352461\n",
      "train loss:0.5404366267153151\n",
      "train loss:0.5759023188887681\n",
      "train loss:0.7054327312426575\n",
      "train loss:0.6760312138740708\n",
      "train loss:0.44113451483135285\n",
      "train loss:0.509399477266501\n",
      "train loss:0.7094733994750259\n",
      "train loss:0.6122941956157101\n",
      "train loss:0.6461356415509348\n",
      "train loss:0.6604094383485801\n",
      "train loss:0.5210405630601432\n",
      "train loss:0.6720945017082665\n",
      "train loss:0.636673299580166\n",
      "train loss:0.5706741891035995\n",
      "train loss:0.6766368843499722\n",
      "train loss:0.472450168033077\n",
      "train loss:0.5969605652793443\n",
      "train loss:0.6415873722452398\n",
      "train loss:0.5736696102647562\n",
      "train loss:0.6537863914557088\n",
      "train loss:0.6319147966241657\n",
      "train loss:0.48810705331992515\n",
      "train loss:0.5519565521389899\n",
      "train loss:0.6375577863942204\n",
      "train loss:0.6420738082818196\n",
      "train loss:0.6012385208764096\n",
      "train loss:0.6913552277094329\n",
      "train loss:0.5474526421719169\n",
      "train loss:0.76145541221655\n",
      "train loss:0.587653682018362\n",
      "train loss:0.546108674315056\n",
      "train loss:0.5736166515063065\n",
      "train loss:0.5024723691962434\n",
      "train loss:0.5208233559325212\n",
      "train loss:0.730513946566135\n",
      "train loss:0.5583512728470698\n",
      "train loss:0.5714577182162448\n",
      "train loss:0.6888441193776904\n",
      "train loss:0.6051288068094152\n",
      "train loss:0.561137098633603\n",
      "train loss:0.6165266062031439\n",
      "train loss:0.5500741330876323\n",
      "train loss:0.47002657829504246\n",
      "train loss:0.6293029674115956\n",
      "train loss:0.5055379286634832\n",
      "train loss:0.5768559835675222\n",
      "train loss:0.6772718730508409\n",
      "train loss:0.7331855388854552\n",
      "train loss:0.5286561496291075\n",
      "train loss:0.5223159949920897\n",
      "train loss:0.6729396286053638\n",
      "train loss:0.4875003524912036\n",
      "train loss:0.49383125022456126\n",
      "train loss:0.4581174958168323\n",
      "train loss:0.6854084637133597\n",
      "train loss:0.4741734757180809\n",
      "train loss:0.5392203115842261\n",
      "train loss:0.5990482076168744\n",
      "train loss:0.5562168178057241\n",
      "train loss:0.5172684340039622\n",
      "train loss:0.6570682060953904\n",
      "train loss:0.7786382037772998\n",
      "train loss:0.6055497307827488\n",
      "train loss:0.518943586277463\n",
      "train loss:0.5883449787662263\n",
      "train loss:0.7038099195713308\n",
      "train loss:0.6810350686806501\n",
      "train loss:0.45432243224266416\n",
      "train loss:0.4495661563468859\n",
      "train loss:0.49990526987130224\n",
      "train loss:0.554155775788383\n",
      "train loss:0.4666664453950095\n",
      "train loss:0.528169075819759\n",
      "train loss:0.5404451313386321\n",
      "train loss:0.608708549296566\n",
      "train loss:0.5914980774735146\n",
      "train loss:0.5644254744107332\n",
      "train loss:0.5365009793865864\n",
      "train loss:0.5777110368347638\n",
      "train loss:0.4903670892849399\n",
      "train loss:0.676784528570827\n",
      "train loss:0.5753453718626298\n",
      "train loss:0.7117726959261019\n",
      "train loss:0.6086841483775257\n",
      "train loss:0.5632431117050609\n",
      "train loss:0.5384117828993235\n",
      "train loss:0.7091636168078774\n",
      "train loss:0.614357280648511\n",
      "train loss:0.5323090213418176\n",
      "train loss:0.6452484859992599\n",
      "train loss:0.4925395633187005\n",
      "train loss:0.62036395584202\n",
      "train loss:0.656958728376222\n",
      "train loss:0.47057046372451466\n",
      "train loss:0.5662098338000333\n",
      "train loss:0.5214056107486654\n",
      "train loss:0.580599505897978\n",
      "train loss:0.4876585212063281\n",
      "train loss:0.6280394578826778\n",
      "train loss:0.6036259222721511\n",
      "train loss:0.7577795827255409\n",
      "train loss:0.5254723795537619\n",
      "train loss:0.4045745257638626\n",
      "train loss:0.5805610456332969\n",
      "train loss:0.6144793235703261\n",
      "train loss:0.5673880583646695\n",
      "train loss:0.46418950728568126\n",
      "train loss:0.6461853434705381\n",
      "train loss:0.5680490443883485\n",
      "train loss:0.6266237861421382\n",
      "train loss:0.5820821309931098\n",
      "train loss:0.5811518466250244\n",
      "train loss:0.652558570177066\n",
      "train loss:0.5346673277519416\n",
      "train loss:0.5288632258247434\n",
      "train loss:0.5850427821674689\n",
      "train loss:0.7163022446309764\n",
      "train loss:0.5899815054449145\n",
      "train loss:0.6572253153507538\n",
      "train loss:0.6676313478129471\n",
      "train loss:0.5898645842158946\n",
      "train loss:0.5127302064814899\n",
      "train loss:0.5694438867388416\n",
      "train loss:0.7781155559087278\n",
      "train loss:0.6650405368539745\n",
      "train loss:0.6983283268264358\n",
      "train loss:0.5433541654879184\n",
      "train loss:0.8432797685196879\n",
      "train loss:0.621191169295678\n",
      "train loss:0.6457677509701699\n",
      "train loss:0.6440783004965508\n",
      "train loss:0.6156188851630002\n",
      "train loss:0.5272312103352602\n",
      "train loss:0.5525737590528716\n",
      "train loss:0.6534396562712488\n",
      "train loss:0.628704652938262\n",
      "train loss:0.4104968428682345\n",
      "train loss:0.502481541800529\n",
      "train loss:0.5641253649296015\n",
      "train loss:0.6358724284566799\n",
      "train loss:0.5410088847094142\n",
      "train loss:0.6770036996571177\n",
      "train loss:0.6584990352137189\n",
      "train loss:0.6181713161974481\n",
      "train loss:0.87652103969688\n",
      "train loss:0.5234100430969266\n",
      "train loss:0.626581840929111\n",
      "train loss:0.6297760430683936\n",
      "train loss:0.5024512792834822\n",
      "train loss:0.5075984331171612\n",
      "train loss:0.687295179040339\n",
      "train loss:0.600372082117096\n",
      "train loss:0.604333865746806\n",
      "train loss:0.5714201452823299\n",
      "train loss:0.4415665479595873\n",
      "train loss:0.4054417374804513\n",
      "train loss:0.5634167299008004\n",
      "train loss:0.5564114125059557\n",
      "train loss:0.832070726698406\n",
      "train loss:0.5869097026145753\n",
      "train loss:0.6435391156813125\n",
      "train loss:0.5941374304855351\n",
      "train loss:0.599215584667554\n",
      "train loss:0.5404245676911399\n",
      "train loss:0.7658084477591224\n",
      "train loss:0.4112711419238701\n",
      "train loss:0.6023004071826719\n",
      "train loss:0.5261108787277637\n",
      "train loss:0.46551753364840825\n",
      "train loss:0.5412665456707622\n",
      "train loss:0.5856797489224399\n",
      "train loss:0.7180160008381752\n",
      "train loss:0.6368747317950088\n",
      "train loss:0.5605784320135289\n",
      "train loss:0.7877960651331445\n",
      "train loss:0.6216988924078545\n",
      "train loss:0.7493783428087617\n",
      "train loss:0.6013568840622865\n",
      "train loss:0.5544726101673461\n",
      "train loss:0.5045046053630293\n",
      "train loss:0.7694583067782039\n",
      "train loss:0.47572358678820764\n",
      "train loss:0.48333279568228454\n",
      "train loss:0.723853659696666\n",
      "train loss:0.6290830132958475\n",
      "train loss:0.6015322961591519\n",
      "train loss:0.5896951514238977\n",
      "train loss:0.5649516835494777\n",
      "train loss:0.5472194239258275\n",
      "train loss:0.6175750936269336\n",
      "train loss:0.7946863854897721\n",
      "train loss:0.5063368964367727\n",
      "train loss:0.5981394127212054\n",
      "train loss:0.6110165080175608\n",
      "train loss:0.49324061138097286\n",
      "train loss:0.6397589860896626\n",
      "train loss:0.5846899059364351\n",
      "train loss:0.5057760164653423\n",
      "train loss:0.585994771644648\n",
      "train loss:0.5278565649505996\n",
      "train loss:0.5736174814422625\n",
      "train loss:0.5564030391726827\n",
      "train loss:0.657467578719861\n",
      "train loss:0.5665202495551648\n",
      "train loss:0.6377172542673828\n",
      "train loss:0.48940800649226834\n",
      "train loss:0.6713015290901718\n",
      "train loss:0.6779819593890841\n",
      "train loss:0.5209437050740141\n",
      "train loss:0.47908272029375765\n",
      "train loss:0.5488880196157857\n",
      "train loss:0.5416491131847674\n",
      "train loss:0.5725491004002812\n",
      "train loss:0.6665350787911533\n",
      "train loss:0.6927804312260153\n",
      "train loss:0.5805849338319622\n",
      "train loss:0.5771438482841107\n",
      "train loss:0.494078128355245\n",
      "train loss:0.594006708338878\n",
      "train loss:0.6375177709944934\n",
      "train loss:0.5293320440946822\n",
      "train loss:0.5101393167599544\n",
      "train loss:0.6164699327978564\n",
      "train loss:0.4971729690916485\n",
      "train loss:0.5900934506123852\n",
      "train loss:0.5695862614818731\n",
      "train loss:0.5686667879976852\n",
      "train loss:0.5377474008052736\n",
      "train loss:0.5352515891355083\n",
      "train loss:0.6059078245072924\n",
      "train loss:0.558501017237489\n",
      "train loss:0.5867310934432528\n",
      "train loss:0.548284459908328\n",
      "train loss:0.5242307668943027\n",
      "train loss:0.5109775077720662\n",
      "train loss:0.6012129976514665\n",
      "train loss:0.7041086656796697\n",
      "train loss:0.5556922494441796\n",
      "train loss:0.6724899473900632\n",
      "train loss:0.5219751167730258\n",
      "train loss:0.7349477120562307\n",
      "train loss:0.49054545978376185\n",
      "train loss:0.6887021698451828\n",
      "train loss:0.7146347834185376\n",
      "train loss:0.45926687271640837\n",
      "train loss:0.7257324461514492\n",
      "train loss:0.6671369696948365\n",
      "train loss:0.5508961919508486\n",
      "train loss:0.6017185237355804\n",
      "train loss:0.6472403895585227\n",
      "train loss:0.514323279152009\n",
      "train loss:0.6685362275159801\n",
      "train loss:0.5471106616475436\n",
      "train loss:0.614615404499014\n",
      "train loss:0.531891266099666\n",
      "train loss:0.7004085405466519\n",
      "train loss:0.5271683109526596\n",
      "train loss:0.5674349857763695\n",
      "train loss:0.5540182368335348\n",
      "train loss:0.4321831222621279\n",
      "train loss:0.5705536416248522\n",
      "train loss:0.5283656035207396\n",
      "train loss:0.5353808936848472\n",
      "train loss:0.7554280768465409\n",
      "train loss:0.5839609364361268\n",
      "train loss:0.47227672887898975\n",
      "train loss:0.6573769261610326\n",
      "train loss:0.5616254452286147\n",
      "train loss:0.5629989029725442\n",
      "train loss:0.7149724375645363\n",
      "train loss:0.5055182887596523\n",
      "train loss:0.5992070886301343\n",
      "train loss:0.5752796239251304\n",
      "train loss:0.5498987051208103\n",
      "train loss:0.6537455281139395\n",
      "train loss:0.6619766862987353\n",
      "train loss:0.46181118866557824\n",
      "train loss:0.49524892649414576\n",
      "train loss:0.5283102600421816\n",
      "train loss:0.5506182668019902\n",
      "train loss:0.6993946958368141\n",
      "train loss:0.6278947310763183\n",
      "train loss:0.4930854901532324\n",
      "train loss:0.5807182581274603\n",
      "train loss:0.48351995422836347\n",
      "train loss:0.4978342310587865\n",
      "train loss:0.4131562341831719\n",
      "train loss:0.47434057025981397\n",
      "train loss:0.7047205810726702\n",
      "train loss:0.5095287584430929\n",
      "train loss:0.6156909451117996\n",
      "train loss:0.6816345866190394\n",
      "train loss:0.5971251571542103\n",
      "train loss:0.6579834243975172\n",
      "train loss:0.6388014535252834\n",
      "train loss:0.6098538362578473\n",
      "train loss:0.5621145940986496\n",
      "train loss:0.6020077178798444\n",
      "train loss:0.5103137119098817\n",
      "train loss:0.5579865019038172\n",
      "train loss:0.5763376298680766\n",
      "train loss:0.47676035576643067\n",
      "train loss:0.5733245575019449\n",
      "train loss:0.6118450938467649\n",
      "train loss:0.7251981286508556\n",
      "train loss:0.6271119836348502\n",
      "train loss:0.5364889361312964\n",
      "train loss:0.4934461741510496\n",
      "train loss:0.5328817676743666\n",
      "train loss:0.4761045317313811\n",
      "train loss:0.59298416214919\n",
      "train loss:0.5956539752656516\n",
      "train loss:0.5104691334490301\n",
      "train loss:0.5754079331021233\n",
      "train loss:0.5876173305239847\n",
      "train loss:0.5993876541427215\n",
      "train loss:0.6347562851401743\n",
      "train loss:0.5089642929537772\n",
      "train loss:0.5459199180967529\n",
      "train loss:0.5329056526664331\n",
      "train loss:0.5972541470414463\n",
      "train loss:0.518419525631101\n",
      "train loss:0.5579172066818417\n",
      "train loss:0.4755845840092405\n",
      "train loss:0.5373204009288216\n",
      "train loss:0.6194537063622524\n",
      "train loss:0.5577386604575237\n",
      "train loss:0.7008493176539479\n",
      "train loss:0.4705757813270588\n",
      "train loss:0.7887280541571902\n",
      "train loss:0.8381557035798677\n",
      "train loss:0.5132482867209291\n",
      "train loss:0.5618962899600384\n",
      "train loss:0.5682742285279855\n",
      "train loss:0.6526289128446576\n",
      "train loss:0.6842766464557897\n",
      "train loss:0.5361848759670447\n",
      "train loss:0.6090798116335288\n",
      "train loss:0.6168584524316394\n",
      "train loss:0.4673721073619836\n",
      "train loss:0.47592113842380995\n",
      "train loss:0.6690652058684026\n",
      "train loss:0.659726944371993\n",
      "train loss:0.7344452130855622\n",
      "train loss:0.5696993149539583\n",
      "train loss:0.7064866458740159\n",
      "train loss:0.5596253492174731\n",
      "train loss:0.6058836966020487\n",
      "train loss:0.574854288583495\n",
      "train loss:0.5695308415716002\n",
      "train loss:0.5106254618433758\n",
      "train loss:0.4855279693971488\n",
      "train loss:0.4767358718068909\n",
      "train loss:0.6210768364858137\n",
      "train loss:0.5886605611820507\n",
      "train loss:0.5460044361877572\n",
      "train loss:0.6519567462327305\n",
      "train loss:0.5185161559776773\n",
      "train loss:0.44532387480525676\n",
      "train loss:0.6151629443020465\n",
      "train loss:0.5852713219789293\n",
      "train loss:0.5168898088484001\n",
      "train loss:0.4548829627711012\n",
      "train loss:0.5695948541998935\n",
      "train loss:0.5069301904593617\n",
      "train loss:0.5064538744400712\n",
      "train loss:0.40480664029772645\n",
      "train loss:0.5391479222910561\n",
      "train loss:0.6461942316917362\n",
      "train loss:0.4439496161936707\n",
      "train loss:0.536119381204752\n",
      "train loss:0.6694473509401548\n",
      "train loss:0.5697183642393091\n",
      "train loss:0.5392560459055912\n",
      "train loss:0.527921607855362\n",
      "train loss:0.6240892587372442\n",
      "train loss:0.5574495081552039\n",
      "train loss:0.45175051577694864\n",
      "train loss:0.8278898669019238\n",
      "train loss:0.7444561579774494\n",
      "train loss:0.5461803757191903\n",
      "train loss:0.6194205728942633\n",
      "train loss:0.4998007517253271\n",
      "train loss:0.5368079565760354\n",
      "train loss:0.5848199663656294\n",
      "train loss:0.5166088413099879\n",
      "train loss:0.6051942364045979\n",
      "train loss:0.5771463101441575\n",
      "train loss:0.4593406592877377\n",
      "train loss:0.4673180241908167\n",
      "train loss:0.5077992903875633\n",
      "train loss:0.5014157131756029\n",
      "train loss:0.71595179744297\n",
      "train loss:0.5773020986638653\n",
      "train loss:0.4463917596656885\n",
      "train loss:0.5761137090372095\n",
      "train loss:0.5879230085642942\n",
      "train loss:0.5583228412811941\n",
      "train loss:0.4497810270663478\n",
      "train loss:0.5860286678924838\n",
      "train loss:0.6164206264851386\n",
      "train loss:0.4505076800347004\n",
      "train loss:0.4790334207943468\n",
      "train loss:0.542043469060005\n",
      "train loss:0.5320676360576977\n",
      "train loss:0.6287623819441671\n",
      "train loss:0.4533294098522657\n",
      "train loss:0.6478013898234507\n",
      "train loss:0.6452286975894254\n",
      "train loss:0.5811199570117976\n",
      "train loss:0.45323977831893\n",
      "train loss:0.5954636428668963\n",
      "train loss:0.5239185234138982\n",
      "train loss:0.5697036825073456\n",
      "train loss:0.624945202091698\n",
      "train loss:0.5800187259066507\n",
      "train loss:0.4877563445746075\n",
      "train loss:0.4969250437842272\n",
      "train loss:0.5435270551290757\n",
      "train loss:0.4723978629961075\n",
      "train loss:0.4754549405792183\n",
      "train loss:0.44296830070359183\n",
      "train loss:0.5390726848614047\n",
      "train loss:0.5121090056860897\n",
      "train loss:0.529109324907068\n",
      "train loss:0.4246779645765837\n",
      "train loss:0.5617844602542916\n",
      "train loss:0.5820774279480156\n",
      "train loss:0.6722293502728559\n",
      "train loss:0.6506298751405984\n",
      "train loss:0.6890864354492808\n",
      "train loss:0.6514215011470099\n",
      "train loss:0.3876971804622605\n",
      "train loss:0.5941350208963878\n",
      "train loss:0.4490986480520106\n",
      "train loss:0.4517111866771399\n",
      "train loss:0.6125154550488445\n",
      "train loss:0.5111443256875006\n",
      "train loss:0.40805868691816827\n",
      "train loss:0.525204926882975\n",
      "train loss:0.4823470852240299\n",
      "train loss:0.5124716086205457\n",
      "train loss:0.6416001042968423\n",
      "train loss:0.5383072114369375\n",
      "train loss:0.5199778167132209\n",
      "train loss:0.5132833621941365\n",
      "train loss:0.6197714127926767\n",
      "train loss:0.5353529846296667\n",
      "train loss:0.6058708669989815\n",
      "train loss:0.6895598755551936\n",
      "train loss:0.6014680200568912\n",
      "train loss:0.5334140458433773\n",
      "train loss:0.6091784873302383\n",
      "train loss:0.5643790544543521\n",
      "train loss:0.5758047207089882\n",
      "train loss:0.573102314451249\n",
      "train loss:0.5376384815084255\n",
      "train loss:0.551093579619064\n",
      "train loss:0.4915379044327803\n",
      "train loss:0.685837381430435\n",
      "train loss:0.5851479189942321\n",
      "train loss:0.4627571058178427\n",
      "train loss:0.5305548820158853\n",
      "train loss:0.5468168986768235\n",
      "train loss:0.5339486714360451\n",
      "train loss:0.5086522526201286\n",
      "train loss:0.5164308703624834\n",
      "train loss:0.6988457143909135\n",
      "train loss:0.5975606774766233\n",
      "train loss:0.5208895895067277\n",
      "train loss:0.6668126602027953\n",
      "train loss:0.5164154569853155\n",
      "train loss:0.6395093769608295\n",
      "train loss:0.4680285524955861\n",
      "train loss:0.5403751591727343\n",
      "train loss:0.42455110268985374\n",
      "train loss:0.5020435620390648\n",
      "train loss:0.7863041105074474\n",
      "train loss:0.6682142011244452\n",
      "train loss:0.43888920284536925\n",
      "train loss:0.48110022628331245\n",
      "train loss:0.6688833191401904\n",
      "train loss:0.570743970007921\n",
      "train loss:0.5879740766526246\n",
      "train loss:0.5668148678302469\n",
      "train loss:0.5653032416513637\n",
      "train loss:0.5569700959385561\n",
      "train loss:0.48606545755467884\n",
      "train loss:0.40870189016897784\n",
      "train loss:0.5253508733809489\n",
      "train loss:0.49033679262143903\n",
      "train loss:0.5039320307363587\n",
      "train loss:0.40302333886256636\n",
      "train loss:0.5555851009088671\n",
      "train loss:0.39425740147841437\n",
      "train loss:0.5065542738251613\n",
      "train loss:0.5089498730012161\n",
      "train loss:0.5677542287343245\n",
      "train loss:0.5020872541570511\n",
      "train loss:0.43856212972033404\n",
      "train loss:0.5738238725036688\n",
      "train loss:0.5889754031553405\n",
      "train loss:0.601034319979934\n",
      "train loss:0.44201673518456663\n",
      "train loss:0.5431894650312824\n",
      "train loss:0.665249364592149\n",
      "train loss:0.520067760483609\n",
      "train loss:0.5882767616189739\n",
      "train loss:0.5737995864367861\n",
      "train loss:0.626039014987948\n",
      "train loss:0.45687342337352155\n",
      "train loss:0.6245403892372522\n",
      "train loss:0.5197375173782786\n",
      "train loss:0.6098707050220614\n",
      "train loss:0.5335494748859815\n",
      "train loss:0.4863210720358462\n",
      "train loss:0.5707585315538879\n",
      "train loss:0.46870330570409535\n",
      "train loss:0.44730968308345664\n",
      "=== epoch:7, train acc:0.734, test acc:0.743 ===\n",
      "train loss:0.6655085597166756\n",
      "train loss:0.530944694359581\n",
      "train loss:0.575368311573907\n",
      "train loss:0.5509758323531605\n",
      "train loss:0.48009252428665855\n",
      "train loss:0.6403041140275463\n",
      "train loss:0.6459919173666847\n",
      "train loss:0.524122787787798\n",
      "train loss:0.7158598272078038\n",
      "train loss:0.42369684391406637\n",
      "train loss:0.5969810090764421\n",
      "train loss:0.4738849989121928\n",
      "train loss:0.7328477208062218\n",
      "train loss:0.7584686161276321\n",
      "train loss:0.6777459419370203\n",
      "train loss:0.4746446634510816\n",
      "train loss:0.5861688864031351\n",
      "train loss:0.5830785671316827\n",
      "train loss:0.4718223064051913\n",
      "train loss:0.5401167799136598\n",
      "train loss:0.4008617995641542\n",
      "train loss:0.44924965746539547\n",
      "train loss:0.6254734638501488\n",
      "train loss:0.5785106580100106\n",
      "train loss:0.4768804736307318\n",
      "train loss:0.630372198180082\n",
      "train loss:0.4877319725053113\n",
      "train loss:0.5790521975329779\n",
      "train loss:0.586802518507538\n",
      "train loss:0.5912563978658281\n",
      "train loss:0.4726226007124965\n",
      "train loss:0.5700840716126405\n",
      "train loss:0.5261684467821357\n",
      "train loss:0.4864002837671336\n",
      "train loss:0.6090200132780502\n",
      "train loss:0.585588955913469\n",
      "train loss:0.5644324135465504\n",
      "train loss:0.508174530196054\n",
      "train loss:0.5215381810094775\n",
      "train loss:0.44654757605668416\n",
      "train loss:0.6462723864240144\n",
      "train loss:0.5431050649318493\n",
      "train loss:0.5364723349448907\n",
      "train loss:0.5591026331687885\n",
      "train loss:0.49717586641010103\n",
      "train loss:0.5515042154080141\n",
      "train loss:0.4732893481525873\n",
      "train loss:0.5859817623772609\n",
      "train loss:0.45576526590042166\n",
      "train loss:0.5770570227012827\n",
      "train loss:0.7051572756411844\n",
      "train loss:0.49701847805466903\n",
      "train loss:0.4517526997516501\n",
      "train loss:0.5005384480361937\n",
      "train loss:0.4725670904357897\n",
      "train loss:0.514731245715097\n",
      "train loss:0.6222367511179339\n",
      "train loss:0.6257778461349455\n",
      "train loss:0.5949092726693676\n",
      "train loss:0.5571623054891163\n",
      "train loss:0.5580844063585965\n",
      "train loss:0.4211116772945116\n",
      "train loss:0.6026547776293613\n",
      "train loss:0.7326413358257099\n",
      "train loss:0.48533594734384494\n",
      "train loss:0.6056777197002482\n",
      "train loss:0.6546222069487965\n",
      "train loss:0.5487683840490486\n",
      "train loss:0.41573018916011556\n",
      "train loss:0.5091541810073544\n",
      "train loss:0.3720600988996554\n",
      "train loss:0.4967035679207872\n",
      "train loss:0.7627957009595073\n",
      "train loss:0.6055827382942437\n",
      "train loss:0.46611590984065354\n",
      "train loss:0.62167173489747\n",
      "train loss:0.7128570887611402\n",
      "train loss:0.4418695809431651\n",
      "train loss:0.5669286944807439\n",
      "train loss:0.5185495012175392\n",
      "train loss:0.5567215228477924\n",
      "train loss:0.552983160414333\n",
      "train loss:0.6465136016043672\n",
      "train loss:0.677435344199234\n",
      "train loss:0.5996914185538053\n",
      "train loss:0.5478887514028807\n",
      "train loss:0.532066196498792\n",
      "train loss:0.5405818745400373\n",
      "train loss:0.5210350845041158\n",
      "train loss:0.49405645573553053\n",
      "train loss:0.5506328674836065\n",
      "train loss:0.5380870200823694\n",
      "train loss:0.36711429442196675\n",
      "train loss:0.5965491873506208\n",
      "train loss:0.4646384249745236\n",
      "train loss:0.5059348982667461\n",
      "train loss:0.53419649033432\n",
      "train loss:0.5269694213529822\n",
      "train loss:0.4700039204248265\n",
      "train loss:0.5229645570406554\n",
      "train loss:0.5299933073467924\n",
      "train loss:0.5094378537777883\n",
      "train loss:0.5136335553957209\n",
      "train loss:0.589453016653333\n",
      "train loss:0.6399808590130122\n",
      "train loss:0.651943548312752\n",
      "train loss:0.6087818165629316\n",
      "train loss:0.46882029027287714\n",
      "train loss:0.46426794104612024\n",
      "train loss:0.5372354480540509\n",
      "train loss:0.6168731459221651\n",
      "train loss:0.660485299836679\n",
      "train loss:0.5993923353579926\n",
      "train loss:0.5744119845845134\n",
      "train loss:0.5648272990793296\n",
      "train loss:0.6194206283061582\n",
      "train loss:0.48149391782893536\n",
      "train loss:0.581013957484926\n",
      "train loss:0.5044065463795313\n",
      "train loss:0.5381076503320256\n",
      "train loss:0.5959806540542553\n",
      "train loss:0.4450217022963212\n",
      "train loss:0.4042607896904664\n",
      "train loss:0.5413051772292016\n",
      "train loss:0.565988421962712\n",
      "train loss:0.5564401759942643\n",
      "train loss:0.5743317444053588\n",
      "train loss:0.5717094054252532\n",
      "train loss:0.5685648074781702\n",
      "train loss:0.5589784862418383\n",
      "train loss:0.5736037872408476\n",
      "train loss:0.41416701320067856\n",
      "train loss:0.4859377000459248\n",
      "train loss:0.49116496379893376\n",
      "train loss:0.5692793674649437\n",
      "train loss:0.5607446407195614\n",
      "train loss:0.4456077794314017\n",
      "train loss:0.5594322708133934\n",
      "train loss:0.4917323389112573\n",
      "train loss:0.46621929836245635\n",
      "train loss:0.5126094504751273\n",
      "train loss:0.5066910504869178\n",
      "train loss:0.5205191468353673\n",
      "train loss:0.6278681789858751\n",
      "train loss:0.6191591092444314\n",
      "train loss:0.45043873514790567\n",
      "train loss:0.46648914595701074\n",
      "train loss:0.5950524078475293\n",
      "train loss:0.5280470234650622\n",
      "train loss:0.6056239544725495\n",
      "train loss:0.6465881362769273\n",
      "train loss:0.6379326586144144\n",
      "train loss:0.7441400808099998\n",
      "train loss:0.49145742405365406\n",
      "train loss:0.5258809304482718\n",
      "train loss:0.6550569077692883\n",
      "train loss:0.5742141778783233\n",
      "train loss:0.5460860277278264\n",
      "train loss:0.47971823086278886\n",
      "train loss:0.5596648037100789\n",
      "train loss:0.5613420700097165\n",
      "train loss:0.6269244863201128\n",
      "train loss:0.5473578698171636\n",
      "train loss:0.4772065006450555\n",
      "train loss:0.6125645440972706\n",
      "train loss:0.5676055604685406\n",
      "train loss:0.5353250553236449\n",
      "train loss:0.4808893502074488\n",
      "train loss:0.4536475801247186\n",
      "train loss:0.5906861172549217\n",
      "train loss:0.5045621567108783\n",
      "train loss:0.5487380244961199\n",
      "train loss:0.5752967052510707\n",
      "train loss:0.4836894780714043\n",
      "train loss:0.5407264287920891\n",
      "train loss:0.41808064288368113\n",
      "train loss:0.5793156090762569\n",
      "train loss:0.56318828349236\n",
      "train loss:0.43843342511536315\n",
      "train loss:0.6532931064559319\n",
      "train loss:0.6218590113359849\n",
      "train loss:0.4705685938449043\n",
      "train loss:0.526984302851051\n",
      "train loss:0.48749089560434905\n",
      "train loss:0.6129673451835952\n",
      "train loss:0.44521624746246397\n",
      "train loss:0.4951542200472371\n",
      "train loss:0.6071360109703626\n",
      "train loss:0.5091604768172615\n",
      "train loss:0.45117426912879594\n",
      "train loss:0.5286403747101344\n",
      "train loss:0.6915102774227353\n",
      "train loss:0.4820316927534767\n",
      "train loss:0.4333043945984874\n",
      "train loss:0.5091848707649049\n",
      "train loss:0.5791405770922158\n",
      "train loss:0.5302510600434284\n",
      "train loss:0.519759199844837\n",
      "train loss:0.5256382699408281\n",
      "train loss:0.6104442539459608\n",
      "train loss:0.46698587489892746\n",
      "train loss:0.4970264315816597\n",
      "train loss:0.46993573712057746\n",
      "train loss:0.5991786931433778\n",
      "train loss:0.479000939688856\n",
      "train loss:0.5353250107593869\n",
      "train loss:0.5180695265147136\n",
      "train loss:0.732318547828748\n",
      "train loss:0.5027664250639967\n",
      "train loss:0.6736586111487359\n",
      "train loss:0.6018483193270224\n",
      "train loss:0.5909268399027148\n",
      "train loss:0.5436211720371177\n",
      "train loss:0.7394446606778919\n",
      "train loss:0.4961270047483425\n",
      "train loss:0.5560571426166395\n",
      "train loss:0.6124977997828254\n",
      "train loss:0.5008069503172617\n",
      "train loss:0.508777845322727\n",
      "train loss:0.45214078199182495\n",
      "train loss:0.6414686221101582\n",
      "train loss:0.5511536931618857\n",
      "train loss:0.503002149690095\n",
      "train loss:0.6315519609385408\n",
      "train loss:0.4108387752596529\n",
      "train loss:0.5544043816359623\n",
      "train loss:0.5606532477758835\n",
      "train loss:0.44868647481482865\n",
      "train loss:0.5314436189574768\n",
      "train loss:0.5697990257764354\n",
      "train loss:0.5672208936726983\n",
      "train loss:0.4829264487303977\n",
      "train loss:0.5098175030949699\n",
      "train loss:0.5739937091157422\n",
      "train loss:0.4236126671752418\n",
      "train loss:0.5249462387877967\n",
      "train loss:0.5494317824535777\n",
      "train loss:0.7060928307147339\n",
      "train loss:0.5923390388084301\n",
      "train loss:0.33789824920145983\n",
      "train loss:0.4016090828089681\n",
      "train loss:0.6064928265061874\n",
      "train loss:0.5999649466103821\n",
      "train loss:0.5400847990223768\n",
      "train loss:0.5615707332449742\n",
      "train loss:0.5609532930528637\n",
      "train loss:0.4672649403826469\n",
      "train loss:0.5324787096762388\n",
      "train loss:0.4812167874730776\n",
      "train loss:0.5502602170165333\n",
      "train loss:0.7076067055133453\n",
      "train loss:0.5364909068067991\n",
      "train loss:0.42770370574581223\n",
      "train loss:0.3999279943779153\n",
      "train loss:0.48659532219792195\n",
      "train loss:0.4446284052953489\n",
      "train loss:0.5561222929847375\n",
      "train loss:0.4626662148292731\n",
      "train loss:0.7102732723037297\n",
      "train loss:0.536691591474707\n",
      "train loss:0.553754329071605\n",
      "train loss:0.5658561274197274\n",
      "train loss:0.5505258913110519\n",
      "train loss:0.6366854819976938\n",
      "train loss:0.5899843705694545\n",
      "train loss:0.6154701891574663\n",
      "train loss:0.5252740697050108\n",
      "train loss:0.7515505174194845\n",
      "train loss:0.6555006004038572\n",
      "train loss:0.5413560648167386\n",
      "train loss:0.5579770388953872\n",
      "train loss:0.6009629236208964\n",
      "train loss:0.5313960354247035\n",
      "train loss:0.44631798475544365\n",
      "train loss:0.4350946083594667\n",
      "train loss:0.5487731686821533\n",
      "train loss:0.4575022985077424\n",
      "train loss:0.6783157197392733\n",
      "train loss:0.5241346608736581\n",
      "train loss:0.5064311213850117\n",
      "train loss:0.45881143542962244\n",
      "train loss:0.4914450374849673\n",
      "train loss:0.519154487535092\n",
      "train loss:0.686308113366823\n",
      "train loss:0.53621155872947\n",
      "train loss:0.5234779275190324\n",
      "train loss:0.5410841629175027\n",
      "train loss:0.4449191883611849\n",
      "train loss:0.5645822779450449\n",
      "train loss:0.4799463871416754\n",
      "train loss:0.4459143832668398\n",
      "train loss:0.5119541448893876\n",
      "train loss:0.6334540927621681\n",
      "train loss:0.417449850751314\n",
      "train loss:0.44300377828652115\n",
      "train loss:0.39955502969759005\n",
      "train loss:0.6208214830347656\n",
      "train loss:0.45943151855383635\n",
      "train loss:0.45033106476782736\n",
      "train loss:0.7082716553113155\n",
      "train loss:0.5458997235468857\n",
      "train loss:0.5868628219222034\n",
      "train loss:0.567470674022969\n",
      "train loss:0.5242979711404895\n",
      "train loss:0.6681747827349243\n",
      "train loss:0.5136956175549643\n",
      "train loss:0.6615195839517889\n",
      "train loss:0.6250212073661071\n",
      "train loss:0.5052428787521902\n",
      "train loss:0.5400684830294241\n",
      "train loss:0.6018581423174727\n",
      "train loss:0.5003375353603683\n",
      "train loss:0.5469251937722663\n",
      "train loss:0.5278306516466226\n",
      "train loss:0.6633064950652232\n",
      "train loss:0.6088787316125615\n",
      "train loss:0.5344795436634782\n",
      "train loss:0.5059345071158957\n",
      "train loss:0.5055712944358083\n",
      "train loss:0.47207782579115753\n",
      "train loss:0.680526620062066\n",
      "train loss:0.48863064827729713\n",
      "train loss:0.6377485580808419\n",
      "train loss:0.4116920607391075\n",
      "train loss:0.40295796693749125\n",
      "train loss:0.6025893050867106\n",
      "train loss:0.6422024850183162\n",
      "train loss:0.7846162959845087\n",
      "train loss:0.5093490508298675\n",
      "train loss:0.6156193405159223\n",
      "train loss:0.5672346684278605\n",
      "train loss:0.5201540511401006\n",
      "train loss:0.3917160856747911\n",
      "train loss:0.5245024585545136\n",
      "train loss:0.6510650974625359\n",
      "train loss:0.4972738735325355\n",
      "train loss:0.45606471457579234\n",
      "train loss:0.49482252037978697\n",
      "train loss:0.4636866253415416\n",
      "train loss:0.6448284162582487\n",
      "train loss:0.6818250514929389\n",
      "train loss:0.6015954591165908\n",
      "train loss:0.4475598447138395\n",
      "train loss:0.48967018170658766\n",
      "train loss:0.5807586191746329\n",
      "train loss:0.594376365322842\n",
      "train loss:0.6155184015361574\n",
      "train loss:0.602724958075575\n",
      "train loss:0.5541253230652012\n",
      "train loss:0.49951331347505834\n",
      "train loss:0.4490468207957733\n",
      "train loss:0.6333106183265085\n",
      "train loss:0.4886344875549422\n",
      "train loss:0.4301898343684076\n",
      "train loss:0.5713560141132963\n",
      "train loss:0.5903521638353912\n",
      "train loss:0.6971135419945993\n",
      "train loss:0.67026418184199\n",
      "train loss:0.5369178384402463\n",
      "train loss:0.5576792098172987\n",
      "train loss:0.39447648640555194\n",
      "train loss:0.4190511691052655\n",
      "train loss:0.47527611403379055\n",
      "train loss:0.4686633532407327\n",
      "train loss:0.556731694917589\n",
      "train loss:0.5527186831056138\n",
      "train loss:0.6854863755380736\n",
      "train loss:0.5858508374876277\n",
      "train loss:0.6486838195497954\n",
      "train loss:0.6706334144676027\n",
      "train loss:0.5615481607715463\n",
      "train loss:0.51477269285626\n",
      "train loss:0.44993577361689774\n",
      "train loss:0.5062787562496347\n",
      "train loss:0.45922905119462404\n",
      "train loss:0.6754566433563005\n",
      "train loss:0.6261302589004313\n",
      "train loss:0.5237850386896502\n",
      "train loss:0.506521645753683\n",
      "train loss:0.5521232630918521\n",
      "train loss:0.6889292410389476\n",
      "train loss:0.550525296159876\n",
      "train loss:0.4405377151026075\n",
      "train loss:0.4745364353644461\n",
      "train loss:0.706816158565025\n",
      "train loss:0.6009273463217542\n",
      "train loss:0.3652409631590116\n",
      "train loss:0.5420667193957214\n",
      "train loss:0.43511495932939176\n",
      "train loss:0.5470175570510025\n",
      "train loss:0.4038332001173637\n",
      "train loss:0.43152928310901506\n",
      "train loss:0.3835175235444369\n",
      "train loss:0.6334037655671497\n",
      "train loss:0.5503176421663379\n",
      "train loss:0.5338505504519498\n",
      "train loss:0.481140038540789\n",
      "train loss:0.4768324892556057\n",
      "train loss:0.5147179378761223\n",
      "train loss:0.5379667347368012\n",
      "train loss:0.5138568681120109\n",
      "train loss:0.4526025046876134\n",
      "train loss:0.4824419401354454\n",
      "train loss:0.3989829218132296\n",
      "train loss:0.5005623583360957\n",
      "train loss:0.5805278441047924\n",
      "train loss:0.463226713926685\n",
      "train loss:0.6060681052613174\n",
      "train loss:0.6290926934336266\n",
      "train loss:0.661936026083827\n",
      "train loss:0.4054737294091331\n",
      "train loss:0.5021431947381818\n",
      "train loss:0.5603704776853832\n",
      "train loss:0.5449871122139299\n",
      "train loss:0.600541695193173\n",
      "train loss:0.5143018140965244\n",
      "train loss:0.5874677769007228\n",
      "train loss:0.5129748990556284\n",
      "train loss:0.5474423601702588\n",
      "train loss:0.6031699066805323\n",
      "train loss:0.6286183241186335\n",
      "train loss:0.611594891196028\n",
      "train loss:0.5071881873735005\n",
      "train loss:0.49109098325643075\n",
      "train loss:0.5000842121499076\n",
      "train loss:0.439137643281861\n",
      "train loss:0.4887589710168773\n",
      "train loss:0.4450140698720299\n",
      "train loss:0.5604287148653838\n",
      "train loss:0.5772897493408591\n",
      "train loss:0.46566850738000826\n",
      "train loss:0.4463254090511502\n",
      "train loss:0.5048317928582746\n",
      "train loss:0.5373377766490999\n",
      "train loss:0.6541137640408397\n",
      "train loss:0.5685038371138587\n",
      "train loss:0.46678755329840316\n",
      "train loss:0.6067659165612771\n",
      "train loss:0.49143125175535096\n",
      "train loss:0.5207276220921679\n",
      "train loss:0.6591907541621264\n",
      "train loss:0.4944900839661092\n",
      "train loss:0.5928420921964187\n",
      "train loss:0.5274071583085047\n",
      "train loss:0.6435586402347091\n",
      "train loss:0.49948066342947023\n",
      "train loss:0.5607708948672196\n",
      "train loss:0.42321991421796734\n",
      "train loss:0.507600450856868\n",
      "train loss:0.5179837052289361\n",
      "train loss:0.4811594944257327\n",
      "train loss:0.5285970794424457\n",
      "train loss:0.6640705395614078\n",
      "train loss:0.4888128939001277\n",
      "train loss:0.4789805303403295\n",
      "train loss:0.45137551130885634\n",
      "train loss:0.6221976284395015\n",
      "train loss:0.5475035852912894\n",
      "train loss:0.5155629756357887\n",
      "train loss:0.5304343835504572\n",
      "train loss:0.5635992389295051\n",
      "train loss:0.5975386144153529\n",
      "train loss:0.47667627155295117\n",
      "train loss:0.5224767359322172\n",
      "train loss:0.5304246734740758\n",
      "train loss:0.5072684277234606\n",
      "train loss:0.5700351433649545\n",
      "train loss:0.5352738433671569\n",
      "train loss:0.5379266368708356\n",
      "train loss:0.634013017137052\n",
      "train loss:0.5010464228649508\n",
      "train loss:0.5670924587464339\n",
      "train loss:0.6122595583814285\n",
      "train loss:0.6496384423938004\n",
      "train loss:0.4166689810971957\n",
      "train loss:0.5114693838392985\n",
      "train loss:0.4505236029856659\n",
      "train loss:0.6856902589610543\n",
      "train loss:0.542770109585315\n",
      "train loss:0.5506313342790335\n",
      "train loss:0.6372556055157694\n",
      "train loss:0.5510179078934925\n",
      "train loss:0.5161659043206911\n",
      "train loss:0.492784246266203\n",
      "train loss:0.5056104333789537\n",
      "train loss:0.5332613137741554\n",
      "train loss:0.5485291087628368\n",
      "train loss:0.5851548383640026\n",
      "train loss:0.4742601335371556\n",
      "train loss:0.514994685156365\n",
      "train loss:0.7339869180582896\n",
      "train loss:0.4972351706886926\n",
      "train loss:0.493659201555545\n",
      "train loss:0.4887410924740356\n",
      "train loss:0.607850372325619\n",
      "train loss:0.46940780464015314\n",
      "train loss:0.6410359083694211\n",
      "train loss:0.4939727215516465\n",
      "train loss:0.6246917107488222\n",
      "train loss:0.6199026170128856\n",
      "train loss:0.41639423491005245\n",
      "train loss:0.5438008517355745\n",
      "train loss:0.6732932823491592\n",
      "train loss:0.4722747904372199\n",
      "train loss:0.6411771196060078\n",
      "train loss:0.5608123356529419\n",
      "train loss:0.39284265819825764\n",
      "train loss:0.5895185716291775\n",
      "train loss:0.6795561659067143\n",
      "train loss:0.5570751070001196\n",
      "train loss:0.44391755281512085\n",
      "train loss:0.4465772531848692\n",
      "train loss:0.5550719585277415\n",
      "train loss:0.6389763211603171\n",
      "train loss:0.6032759164566255\n",
      "train loss:0.5859113572668083\n",
      "train loss:0.562248618877601\n",
      "train loss:0.5828249445220801\n",
      "train loss:0.49175502231451146\n",
      "train loss:0.5875195613200951\n",
      "train loss:0.6279572761620195\n",
      "train loss:0.5858675883936546\n",
      "train loss:0.6334643285561989\n",
      "train loss:0.682518664342815\n",
      "train loss:0.48734496051064125\n",
      "train loss:0.47983832862329967\n",
      "train loss:0.45386665244982133\n",
      "train loss:0.5083766960618419\n",
      "train loss:0.5717044735083798\n",
      "train loss:0.4688732492114185\n",
      "train loss:0.5113493818637592\n",
      "train loss:0.3921562317226029\n",
      "train loss:0.5293132020852551\n",
      "train loss:0.6739362619119815\n",
      "train loss:0.6176493901444631\n",
      "train loss:0.39876310082083427\n",
      "train loss:0.5267332389376764\n",
      "train loss:0.643115921235193\n",
      "train loss:0.6314190973393925\n",
      "train loss:0.5423691662192762\n",
      "train loss:0.725792518899289\n",
      "train loss:0.5831479923421642\n",
      "train loss:0.5530258745078495\n",
      "train loss:0.5240525480977146\n",
      "train loss:0.5336707002367791\n",
      "train loss:0.46557357434088886\n",
      "train loss:0.4306505292106161\n",
      "train loss:0.812204865618495\n",
      "train loss:0.5072621599113275\n",
      "train loss:0.5764503504683252\n",
      "train loss:0.4694995858051637\n",
      "train loss:0.5870696483870752\n",
      "train loss:0.525400636871156\n",
      "train loss:0.668803830777406\n",
      "train loss:0.6318093599772133\n",
      "train loss:0.49492156526472153\n",
      "train loss:0.6394743656253814\n",
      "train loss:0.5506149622773228\n",
      "train loss:0.5993466514721288\n",
      "train loss:0.43713852549859405\n",
      "train loss:0.5290110611577016\n",
      "train loss:0.6392155989023673\n",
      "train loss:0.5187945975463709\n",
      "train loss:0.3923738995284112\n",
      "train loss:0.5404925043327885\n",
      "train loss:0.7316524424595171\n",
      "train loss:0.7154031969844364\n",
      "train loss:0.5539175256464853\n",
      "train loss:0.6570651276907976\n",
      "train loss:0.6963161363220226\n",
      "train loss:0.5729894524155826\n",
      "train loss:0.49388542007466546\n",
      "train loss:0.49388613012625143\n",
      "train loss:0.5344682792771486\n",
      "train loss:0.6421573671626785\n",
      "train loss:0.5103202020715492\n",
      "train loss:0.6534267893519156\n",
      "train loss:0.4892629647911543\n",
      "train loss:0.5092973449962076\n",
      "train loss:0.4688242207819371\n",
      "train loss:0.6469018850907241\n",
      "train loss:0.6862232007390971\n",
      "train loss:0.51971930915419\n",
      "train loss:0.49585412464952516\n",
      "train loss:0.47815174309331554\n",
      "train loss:0.5182259818921043\n",
      "train loss:0.5297594361476169\n",
      "train loss:0.48002328540701705\n",
      "train loss:0.5931051352730713\n",
      "train loss:0.4627178204937248\n",
      "train loss:0.5767928334198598\n",
      "train loss:0.572168679147356\n",
      "train loss:0.47147910959641964\n",
      "train loss:0.5334214733469439\n",
      "train loss:0.44044297404414523\n",
      "train loss:0.47035568483304474\n",
      "train loss:0.5017404604955304\n",
      "train loss:0.39848538942020034\n",
      "train loss:0.432125909460556\n",
      "train loss:0.7761916532457456\n",
      "=== epoch:8, train acc:0.745, test acc:0.758 ===\n",
      "train loss:0.5783297004435012\n",
      "train loss:0.5363893495262101\n",
      "train loss:0.5145235346829586\n",
      "train loss:0.5097320322542672\n",
      "train loss:0.555844405231379\n",
      "train loss:0.5878433153369375\n",
      "train loss:0.6481899360633934\n",
      "train loss:0.5387176770145407\n",
      "train loss:0.6120136072348753\n",
      "train loss:0.5062180328918503\n",
      "train loss:0.5130309469892641\n",
      "train loss:0.4972771242707499\n",
      "train loss:0.5707392200342771\n",
      "train loss:0.46696934620303177\n",
      "train loss:0.7025129294141172\n",
      "train loss:0.47018947517007403\n",
      "train loss:0.44533576235119504\n",
      "train loss:0.4052567002560744\n",
      "train loss:0.5399745165719162\n",
      "train loss:0.559076990626894\n",
      "train loss:0.5756224646527328\n",
      "train loss:0.5270637963812955\n",
      "train loss:0.5273638773477152\n",
      "train loss:0.5621386031062443\n",
      "train loss:0.45927007670968434\n",
      "train loss:0.5763000298961122\n",
      "train loss:0.517027151909003\n",
      "train loss:0.42332061010769756\n",
      "train loss:0.570149211870055\n",
      "train loss:0.5340134498403319\n",
      "train loss:0.5347542815946451\n",
      "train loss:0.7933050747578492\n",
      "train loss:0.5121383131683791\n",
      "train loss:0.46603586889525866\n",
      "train loss:0.6025717411152286\n",
      "train loss:0.37724735833331224\n",
      "train loss:0.6197683130431728\n",
      "train loss:0.5762860542052447\n",
      "train loss:0.6131788514865119\n",
      "train loss:0.5051479856632421\n",
      "train loss:0.5934175809757828\n",
      "train loss:0.45039553035353225\n",
      "train loss:0.5650366871242813\n",
      "train loss:0.5074489802108004\n",
      "train loss:0.5586934945838578\n",
      "train loss:0.47252788415183145\n",
      "train loss:0.5380660222959212\n",
      "train loss:0.6780377399166622\n",
      "train loss:0.5271741790902694\n",
      "train loss:0.46946971597399473\n",
      "train loss:0.5294907389525725\n",
      "train loss:0.49904871263871764\n",
      "train loss:0.5343405779221948\n",
      "train loss:0.5483773526350906\n",
      "train loss:0.6366299674843905\n",
      "train loss:0.45460239818951337\n",
      "train loss:0.6566573609849935\n",
      "train loss:0.5194816377837762\n",
      "train loss:0.417919210970734\n",
      "train loss:0.5839571452102702\n",
      "train loss:0.4938041625350069\n",
      "train loss:0.5499663751744022\n",
      "train loss:0.5390487457887972\n",
      "train loss:0.3941216800413831\n",
      "train loss:0.4217476537668604\n",
      "train loss:0.5706218682953405\n",
      "train loss:0.45917999611524235\n",
      "train loss:0.5048013784298833\n",
      "train loss:0.44139927476100793\n",
      "train loss:0.41369015304309237\n",
      "train loss:0.5247343123528632\n",
      "train loss:0.5034244219798518\n",
      "train loss:0.44451787408703\n",
      "train loss:0.5242079443914547\n",
      "train loss:0.4926803873354874\n",
      "train loss:0.5750975693690649\n",
      "train loss:0.5187603474773913\n",
      "train loss:0.5989907482389504\n",
      "train loss:0.438097749419237\n",
      "train loss:0.5295281034186585\n",
      "train loss:0.4245040796480419\n",
      "train loss:0.5037022585583223\n",
      "train loss:0.4580485245776035\n",
      "train loss:0.48994808352996744\n",
      "train loss:0.627807664944076\n",
      "train loss:0.521842503398138\n",
      "train loss:0.575258119275352\n",
      "train loss:0.48561504279083406\n",
      "train loss:0.48353627794661025\n",
      "train loss:0.4846592999824385\n",
      "train loss:0.6687691533483487\n",
      "train loss:0.4660284953153714\n",
      "train loss:0.5684963365844681\n",
      "train loss:0.6139052377344253\n",
      "train loss:0.5147447549062086\n",
      "train loss:0.48026756143594973\n",
      "train loss:0.556860591936903\n",
      "train loss:0.46541412616825034\n",
      "train loss:0.6113619192084238\n",
      "train loss:0.6639775651378996\n",
      "train loss:0.5219386318926418\n",
      "train loss:0.682500306927893\n",
      "train loss:0.44483521018903205\n",
      "train loss:0.5953284126347106\n",
      "train loss:0.5961990860257607\n",
      "train loss:0.5281413202209808\n",
      "train loss:0.3915910713479471\n",
      "train loss:0.41339722491789416\n",
      "train loss:0.43606296358470453\n",
      "train loss:0.47518345763225867\n",
      "train loss:0.5044473997077095\n",
      "train loss:0.4044829039408615\n",
      "train loss:0.46014132569884125\n",
      "train loss:0.49392519262645157\n",
      "train loss:0.6424104023919684\n",
      "train loss:0.49507060200742686\n",
      "train loss:0.4879316169397913\n",
      "train loss:0.4858787057818537\n",
      "train loss:0.5001561935563698\n",
      "train loss:0.7227128182178265\n",
      "train loss:0.5349377912673183\n",
      "train loss:0.5085154617557748\n",
      "train loss:0.5219800156927683\n",
      "train loss:0.4050889132745092\n",
      "train loss:0.5829519368716286\n",
      "train loss:0.34612968280074186\n",
      "train loss:0.5914892895690788\n",
      "train loss:0.5567756182260282\n",
      "train loss:0.5864111813865343\n",
      "train loss:0.5295894422564388\n",
      "train loss:0.5197516195252236\n",
      "train loss:0.6263515763298378\n",
      "train loss:0.6428211751327741\n",
      "train loss:0.4112845387176253\n",
      "train loss:0.5508093210223232\n",
      "train loss:0.6919347860481386\n",
      "train loss:0.4957067473331365\n",
      "train loss:0.5224483994594676\n",
      "train loss:0.4771904614239859\n",
      "train loss:0.49191319901486225\n",
      "train loss:0.528304305642014\n",
      "train loss:0.5001404023815302\n",
      "train loss:0.6525174074483985\n",
      "train loss:0.4760417558578722\n",
      "train loss:0.5237233236814681\n",
      "train loss:0.5197750102900357\n",
      "train loss:0.5010588127366927\n",
      "train loss:0.5782497760426601\n",
      "train loss:0.634623389885872\n",
      "train loss:0.558503071164371\n",
      "train loss:0.4239278317685889\n",
      "train loss:0.5093214908476555\n",
      "train loss:0.5261647872826702\n",
      "train loss:0.44092096721352436\n",
      "train loss:0.5159546318600434\n",
      "train loss:0.6542044499124647\n",
      "train loss:0.5451923285351712\n",
      "train loss:0.5528867404540091\n",
      "train loss:0.4695658297495536\n",
      "train loss:0.5302422378119007\n",
      "train loss:0.4864412869657049\n",
      "train loss:0.5650712077244762\n",
      "train loss:0.5785478060081352\n",
      "train loss:0.5221006840920343\n",
      "train loss:0.5662664920697996\n",
      "train loss:0.48366061438987024\n",
      "train loss:0.5813180558471854\n",
      "train loss:0.7352457643306303\n",
      "train loss:0.45309542248667917\n",
      "train loss:0.5059839036197432\n",
      "train loss:0.6173917857374308\n",
      "train loss:0.4545638904551142\n",
      "train loss:0.5624408514585022\n",
      "train loss:0.5293815494546119\n",
      "train loss:0.43586928413466663\n",
      "train loss:0.5850812709533606\n",
      "train loss:0.49617800337877926\n",
      "train loss:0.4077409762524471\n",
      "train loss:0.457093171857754\n",
      "train loss:0.6173185212512395\n",
      "train loss:0.4079538389897251\n",
      "train loss:0.5374581262872213\n",
      "train loss:0.4361806932565979\n",
      "train loss:0.5425250336616473\n",
      "train loss:0.47595457473513997\n",
      "train loss:0.5264888814879957\n",
      "train loss:0.4892730303069653\n",
      "train loss:0.4394935301916529\n",
      "train loss:0.5665210966878209\n",
      "train loss:0.5430750072042937\n",
      "train loss:0.5466938019118784\n",
      "train loss:0.5123231471743364\n",
      "train loss:0.48663063037810694\n",
      "train loss:0.694574400294078\n",
      "train loss:0.5766340339201694\n",
      "train loss:0.48855771962103794\n",
      "train loss:0.519665044145464\n",
      "train loss:0.5333834708078858\n",
      "train loss:0.6783325411569582\n",
      "train loss:0.6021520754660448\n",
      "train loss:0.4390632234998691\n",
      "train loss:0.4934281895869904\n",
      "train loss:0.6487259318102987\n",
      "train loss:0.6758495558502353\n",
      "train loss:0.598972666047089\n",
      "train loss:0.5349705653464512\n",
      "train loss:0.5338081538364902\n",
      "train loss:0.5616436973289877\n",
      "train loss:0.6230529091610212\n",
      "train loss:0.31900355805707625\n",
      "train loss:0.5496669429272584\n",
      "train loss:0.5399373557100698\n",
      "train loss:0.5046848913332183\n",
      "train loss:0.6908082254209157\n",
      "train loss:0.5063718902281069\n",
      "train loss:0.46851620675008554\n",
      "train loss:0.4580016041284043\n",
      "train loss:0.5553682819466799\n",
      "train loss:0.3666384030387412\n",
      "train loss:0.6943024044904468\n",
      "train loss:0.5500836214149486\n",
      "train loss:0.5461415269203035\n",
      "train loss:0.573200339061419\n",
      "train loss:0.4774380471189415\n",
      "train loss:0.6142151588062066\n",
      "train loss:0.4570348556934535\n",
      "train loss:0.5823192635350237\n",
      "train loss:0.616240019662748\n",
      "train loss:0.31899262991256855\n",
      "train loss:0.4180802078970312\n",
      "train loss:0.5000260324141245\n",
      "train loss:0.4720830195769549\n",
      "train loss:0.4512562042864753\n",
      "train loss:0.4136678574783515\n",
      "train loss:0.5356006308784917\n",
      "train loss:0.5095734051423446\n",
      "train loss:0.6898122031300864\n",
      "train loss:0.5148225002656618\n",
      "train loss:0.4418091637878934\n",
      "train loss:0.5302984903719531\n",
      "train loss:0.43718274533348445\n",
      "train loss:0.49800130341166216\n",
      "train loss:0.5555516393214969\n",
      "train loss:0.6087652510417602\n",
      "train loss:0.5813069693508389\n",
      "train loss:0.6598469058675428\n",
      "train loss:0.6875577564639187\n",
      "train loss:0.5901758544648039\n",
      "train loss:0.5796101928225379\n",
      "train loss:0.5426897692648999\n",
      "train loss:0.5897910719647834\n",
      "train loss:0.4590911288316805\n",
      "train loss:0.5183936567598899\n",
      "train loss:0.5618985590990443\n",
      "train loss:0.4388490683589473\n",
      "train loss:0.6071632944958281\n",
      "train loss:0.5182618702843222\n",
      "train loss:0.5051855624172988\n",
      "train loss:0.518110728870063\n",
      "train loss:0.599713966662532\n",
      "train loss:0.6534912716299464\n",
      "train loss:0.43554474577594005\n",
      "train loss:0.468194922718142\n",
      "train loss:0.5072229220344248\n",
      "train loss:0.5883675790414734\n",
      "train loss:0.48195865946730565\n",
      "train loss:0.41012157236828833\n",
      "train loss:0.577111132832925\n",
      "train loss:0.43506500704359796\n",
      "train loss:0.4117532023711557\n",
      "train loss:0.6552619017619299\n",
      "train loss:0.47242257954712275\n",
      "train loss:0.5128515113931732\n",
      "train loss:0.5335830275307601\n",
      "train loss:0.4966422420543827\n",
      "train loss:0.411489671234085\n",
      "train loss:0.4589913487280207\n",
      "train loss:0.44773674969445165\n",
      "train loss:0.6472854230176643\n",
      "train loss:0.665879248916609\n",
      "train loss:0.49011605091716054\n",
      "train loss:0.6001226445629328\n",
      "train loss:0.36823956772473304\n",
      "train loss:0.43105223169642143\n",
      "train loss:0.6998664231191704\n",
      "train loss:0.41690021807467903\n",
      "train loss:0.48588100038374343\n",
      "train loss:0.5018943209883568\n",
      "train loss:0.398506620510097\n",
      "train loss:0.5766661976207391\n",
      "train loss:0.5476189314161513\n",
      "train loss:0.4356661839593274\n",
      "train loss:0.6724595484395657\n",
      "train loss:0.40852559231043933\n",
      "train loss:0.3856011941998671\n",
      "train loss:0.5025731064453323\n",
      "train loss:0.5027058282022168\n",
      "train loss:0.4656326786153654\n",
      "train loss:0.5575059632458504\n",
      "train loss:0.501017842272447\n",
      "train loss:0.45108681220336033\n",
      "train loss:0.5083956910790551\n",
      "train loss:0.47178249892691704\n",
      "train loss:0.5240732084384725\n",
      "train loss:0.4902092122417298\n",
      "train loss:0.5273505571017445\n",
      "train loss:0.5444962996518119\n",
      "train loss:0.6005763675089232\n",
      "train loss:0.552242052144442\n",
      "train loss:0.6332724138811957\n",
      "train loss:0.5176609436034295\n",
      "train loss:0.4783895570974241\n",
      "train loss:0.44611152836931356\n",
      "train loss:0.5081655562632357\n",
      "train loss:0.49666866347853555\n",
      "train loss:0.5028590975112499\n",
      "train loss:0.7554472914103871\n",
      "train loss:0.5012506111788347\n",
      "train loss:0.5663589482181063\n",
      "train loss:0.4854892063465676\n",
      "train loss:0.4123271351064834\n",
      "train loss:0.4942268189242337\n",
      "train loss:0.4033880061161138\n",
      "train loss:0.4638850581009215\n",
      "train loss:0.6728841626044431\n",
      "train loss:0.5006677511494297\n",
      "train loss:0.5626975427046874\n",
      "train loss:0.5370120013576659\n",
      "train loss:0.4284169806956485\n",
      "train loss:0.46358882076171226\n",
      "train loss:0.34620359443037446\n",
      "train loss:0.4958516994994286\n",
      "train loss:0.539887086545046\n",
      "train loss:0.4910547667749355\n",
      "train loss:0.6277474240427893\n",
      "train loss:0.46219660523434136\n",
      "train loss:0.5342375308405061\n",
      "train loss:0.48691811628087317\n",
      "train loss:0.5856371083384938\n",
      "train loss:0.5731782162267285\n",
      "train loss:0.5375167202367809\n",
      "train loss:0.5231493998098881\n",
      "train loss:0.5013202352775433\n",
      "train loss:0.5299583386152252\n",
      "train loss:0.4626750782002847\n",
      "train loss:0.44314692708964176\n",
      "train loss:0.5787191724722106\n",
      "train loss:0.4853695727129292\n",
      "train loss:0.4281467259721825\n",
      "train loss:0.49815993007849435\n",
      "train loss:0.47426990416678955\n",
      "train loss:0.6331310071941769\n",
      "train loss:0.5085558357354144\n",
      "train loss:0.4772535928067285\n",
      "train loss:0.5535720414571945\n",
      "train loss:0.6074974972865299\n",
      "train loss:0.5020678839441488\n",
      "train loss:0.48219703071614217\n",
      "train loss:0.4657004878569946\n",
      "train loss:0.42600102180034793\n",
      "train loss:0.45367133631661866\n",
      "train loss:0.3861142963974822\n",
      "train loss:0.5273902816039672\n",
      "train loss:0.5200286598918404\n",
      "train loss:0.5616677284552157\n",
      "train loss:0.4730176874755332\n",
      "train loss:0.49713416634537905\n",
      "train loss:0.38969588590848153\n",
      "train loss:0.7302132019733166\n",
      "train loss:0.479172312902277\n",
      "train loss:0.3912990053858789\n",
      "train loss:0.5402629350663049\n",
      "train loss:0.4118631799806893\n",
      "train loss:0.4248084175325662\n",
      "train loss:0.4379475467202974\n",
      "train loss:0.5833053318019935\n",
      "train loss:0.5141628878940485\n",
      "train loss:0.6900834460294921\n",
      "train loss:0.3987025697452431\n",
      "train loss:0.4598254617403146\n",
      "train loss:0.8046936639241186\n",
      "train loss:0.5081218541535703\n",
      "train loss:0.3746226599233405\n",
      "train loss:0.7019189785945079\n",
      "train loss:0.5886949836788152\n",
      "train loss:0.6937254658289683\n",
      "train loss:0.6175916061936663\n",
      "train loss:0.44800760844308646\n",
      "train loss:0.38477005810796183\n",
      "train loss:0.4754450505425814\n",
      "train loss:0.3584542455960046\n",
      "train loss:0.5429070916003509\n",
      "train loss:0.6572579537668806\n",
      "train loss:0.6222281872257343\n",
      "train loss:0.4054972979797871\n",
      "train loss:0.5363597365188345\n",
      "train loss:0.5000457386340832\n",
      "train loss:0.5412739700319367\n",
      "train loss:0.5044680704767486\n",
      "train loss:0.6849190250197223\n",
      "train loss:0.5453654049639747\n",
      "train loss:0.47186457159842954\n",
      "train loss:0.6072987522693417\n",
      "train loss:0.582969762777525\n",
      "train loss:0.5172594498737901\n",
      "train loss:0.41072069532353156\n",
      "train loss:0.3691729865155418\n",
      "train loss:0.4350267434353123\n",
      "train loss:0.4852052685030516\n",
      "train loss:0.5206297694695474\n",
      "train loss:0.5663629191599373\n",
      "train loss:0.4772494489926439\n",
      "train loss:0.4880145233433529\n",
      "train loss:0.5821633641169127\n",
      "train loss:0.37434779758389164\n",
      "train loss:0.5449982880324383\n",
      "train loss:0.41985573633174006\n",
      "train loss:0.3953434429168841\n",
      "train loss:0.509337567721731\n",
      "train loss:0.45050033677976414\n",
      "train loss:0.47125884088950015\n",
      "train loss:0.42869680646881797\n",
      "train loss:0.42331546485331295\n",
      "train loss:0.4091115154079414\n",
      "train loss:0.560107219466521\n",
      "train loss:0.4766812116335216\n",
      "train loss:0.6890066321333858\n",
      "train loss:0.5539408726926979\n",
      "train loss:0.4734588117228573\n",
      "train loss:0.6518239671466044\n",
      "train loss:0.5356492897745903\n",
      "train loss:0.6115389033225596\n",
      "train loss:0.35047931452963915\n",
      "train loss:0.4690318971328522\n",
      "train loss:0.5192644781622929\n",
      "train loss:0.45014636555430576\n",
      "train loss:0.49032027148247775\n",
      "train loss:0.49493466016811505\n",
      "train loss:0.46815059152367994\n",
      "train loss:0.4582649886993284\n",
      "train loss:0.5808288529617625\n",
      "train loss:0.5828001428318678\n",
      "train loss:0.4886815660322995\n",
      "train loss:0.413170099839124\n",
      "train loss:0.5314511399099893\n",
      "train loss:0.3406150064896729\n",
      "train loss:0.39640990939979226\n",
      "train loss:0.48240755372968347\n",
      "train loss:0.5755453827377548\n",
      "train loss:0.5683399932523547\n",
      "train loss:0.4125736305666115\n",
      "train loss:0.734062976124883\n",
      "train loss:0.41087501993321385\n",
      "train loss:0.5381416310250742\n",
      "train loss:0.519903262406784\n",
      "train loss:0.5392614556508394\n",
      "train loss:0.4640789097664818\n",
      "train loss:0.6441575117703222\n",
      "train loss:0.6443486703490872\n",
      "train loss:0.48006023466125725\n",
      "train loss:0.6725537338385618\n",
      "train loss:0.4517212298043436\n",
      "train loss:0.5186311938571737\n",
      "train loss:0.5373628760609055\n",
      "train loss:0.517197379895275\n",
      "train loss:0.5993937091931734\n",
      "train loss:0.4294213126932657\n",
      "train loss:0.5517066489324266\n",
      "train loss:0.5088682087980744\n",
      "train loss:0.4796026844400672\n",
      "train loss:0.6621454837157686\n",
      "train loss:0.4661120350450134\n",
      "train loss:0.5711997787745958\n",
      "train loss:0.5364917696951312\n",
      "train loss:0.4680332517206057\n",
      "train loss:0.4937345005833106\n",
      "train loss:0.5168538043016013\n",
      "train loss:0.6467196275201412\n",
      "train loss:0.49155009895990676\n",
      "train loss:0.5441730033401516\n",
      "train loss:0.5317230569935958\n",
      "train loss:0.5774737059257771\n",
      "train loss:0.5002207277900272\n",
      "train loss:0.4482658152903449\n",
      "train loss:0.41812631117527965\n",
      "train loss:0.5238733424632022\n",
      "train loss:0.4974587723780388\n",
      "train loss:0.7354756638733193\n",
      "train loss:0.4996497032663034\n",
      "train loss:0.4283258792238898\n",
      "train loss:0.5428137078347202\n",
      "train loss:0.4658167379447577\n",
      "train loss:0.47769501078970683\n",
      "train loss:0.3934329716326944\n",
      "train loss:0.45139388123185775\n",
      "train loss:0.6350907713712571\n",
      "train loss:0.4784125411860367\n",
      "train loss:0.5321854172944621\n",
      "train loss:0.6414052658694935\n",
      "train loss:0.35845420221006535\n",
      "train loss:0.44859637599008795\n",
      "train loss:0.5370825381442423\n",
      "train loss:0.5172587755771851\n",
      "train loss:0.47631878998757043\n",
      "train loss:0.5002803054449714\n",
      "train loss:0.43981708197427216\n",
      "train loss:0.3631242073294224\n",
      "train loss:0.47955575495418357\n",
      "train loss:0.5513861264851201\n",
      "train loss:0.481639390855239\n",
      "train loss:0.5863877513166842\n",
      "train loss:0.6056351670742165\n",
      "train loss:0.46172970153156584\n",
      "train loss:0.5257171088468819\n",
      "train loss:0.40062948784234825\n",
      "train loss:0.408699197512582\n",
      "train loss:0.5468817519413045\n",
      "train loss:0.5490597575059705\n",
      "train loss:0.728164741706213\n",
      "train loss:0.6053384620771963\n",
      "train loss:0.46059891601156944\n",
      "train loss:0.42880322211930944\n",
      "train loss:0.38826069961934523\n",
      "train loss:0.47148979838312277\n",
      "train loss:0.5735592101000273\n",
      "train loss:0.6335518403360464\n",
      "train loss:0.489446362252755\n",
      "train loss:0.6103207311141606\n",
      "train loss:0.5310024081570413\n",
      "train loss:0.37301781625337677\n",
      "train loss:0.5060140102263884\n",
      "train loss:0.4660693576214402\n",
      "train loss:0.47118668888437304\n",
      "train loss:0.5025741677416241\n",
      "train loss:0.5017460799733913\n",
      "train loss:0.40786199987523075\n",
      "train loss:0.4279913373916907\n",
      "train loss:0.4436990421093576\n",
      "train loss:0.4872431371785603\n",
      "train loss:0.4501193281196956\n",
      "train loss:0.5605476172131877\n",
      "train loss:0.5059337028124792\n",
      "train loss:0.45333286819785484\n",
      "train loss:0.4041098640094172\n",
      "train loss:0.4317985211787852\n",
      "train loss:0.47958154893219884\n",
      "train loss:0.5174144426699674\n",
      "train loss:0.6713806064571994\n",
      "train loss:0.66706947575315\n",
      "train loss:0.4449985492399233\n",
      "train loss:0.5320385787956746\n",
      "train loss:0.5247894135173654\n",
      "train loss:0.5658287212825617\n",
      "train loss:0.4853626483134505\n",
      "train loss:0.4964767013822729\n",
      "train loss:0.46000600030233907\n",
      "train loss:0.6045680527992788\n",
      "train loss:0.5138657534931625\n",
      "train loss:0.5309554659419287\n",
      "train loss:0.39723259087656443\n",
      "train loss:0.3625731962012303\n",
      "train loss:0.5408756832676653\n",
      "train loss:0.43997556242099173\n",
      "train loss:0.4860390314222041\n",
      "train loss:0.4078366432630541\n",
      "train loss:0.5539230148610972\n",
      "train loss:0.6433944824851097\n",
      "train loss:0.4543244743984484\n",
      "train loss:0.47952776351198517\n",
      "train loss:0.5762866063581449\n",
      "train loss:0.34834397704300707\n",
      "train loss:0.4132179197884102\n",
      "train loss:0.4698290123973136\n",
      "train loss:0.7241769221948363\n",
      "train loss:0.4521914795854636\n",
      "train loss:0.5303664980399302\n",
      "train loss:0.3901599884849508\n",
      "train loss:0.36183253230191537\n",
      "train loss:0.5566287347605718\n",
      "train loss:0.5143279380842851\n",
      "train loss:0.5747615957350853\n",
      "train loss:0.4977072595528096\n",
      "train loss:0.40876295200120416\n",
      "train loss:0.6266605638814713\n",
      "train loss:0.5260604760026079\n",
      "train loss:0.5172216443810993\n",
      "train loss:0.49554716250468883\n",
      "train loss:0.45258192437530026\n",
      "train loss:0.5762263280053356\n",
      "train loss:0.37560957185818133\n",
      "train loss:0.47888921166829385\n",
      "train loss:0.5473259434386984\n",
      "train loss:0.5303966712678436\n",
      "train loss:0.3685043786979189\n",
      "train loss:0.47837030636034084\n",
      "train loss:0.5613478742968366\n",
      "train loss:0.38085155013405336\n",
      "train loss:0.5241572262856496\n",
      "train loss:0.43966600814403606\n",
      "train loss:0.3797358921385265\n",
      "=== epoch:9, train acc:0.759, test acc:0.765 ===\n",
      "train loss:0.609826616614301\n",
      "train loss:0.502736193224878\n",
      "train loss:0.6277229773986027\n",
      "train loss:0.3994826815117863\n",
      "train loss:0.4626606850248127\n",
      "train loss:0.4758843991122598\n",
      "train loss:0.5231598454131775\n",
      "train loss:0.558596464745126\n",
      "train loss:0.49392686683415166\n",
      "train loss:0.6132938803335981\n",
      "train loss:0.4014764130310249\n",
      "train loss:0.4696470271053844\n",
      "train loss:0.3992261265575134\n",
      "train loss:0.48757886516764115\n",
      "train loss:0.546820332464706\n",
      "train loss:0.5791928875821268\n",
      "train loss:0.3810502820934802\n",
      "train loss:0.5585397191603187\n",
      "train loss:0.5024007843287568\n",
      "train loss:0.5784783876492938\n",
      "train loss:0.3678240561641929\n",
      "train loss:0.42734397631816934\n",
      "train loss:0.5208688834618896\n",
      "train loss:0.41797778519896445\n",
      "train loss:0.6244418839098295\n",
      "train loss:0.5586553527609194\n",
      "train loss:0.5590049550013135\n",
      "train loss:0.39723189552430205\n",
      "train loss:0.37055507908529106\n",
      "train loss:0.6045704333168885\n",
      "train loss:0.5050937100637752\n",
      "train loss:0.5565305405327267\n",
      "train loss:0.47288128110667643\n",
      "train loss:0.48733308974332756\n",
      "train loss:0.5111667637063784\n",
      "train loss:0.5964189685320234\n",
      "train loss:0.475599944328364\n",
      "train loss:0.5179771144964957\n",
      "train loss:0.5735716164505884\n",
      "train loss:0.527228236808583\n",
      "train loss:0.5192914954824179\n",
      "train loss:0.5720475105784468\n",
      "train loss:0.5908065239931569\n",
      "train loss:0.5890081396877319\n",
      "train loss:0.43601821420577047\n",
      "train loss:0.5881503506395399\n",
      "train loss:0.4218928554720494\n",
      "train loss:0.5918457391308448\n",
      "train loss:0.7832938130211012\n",
      "train loss:0.4032192902877311\n",
      "train loss:0.5268644158850918\n",
      "train loss:0.5705531851585954\n",
      "train loss:0.4380671275153602\n",
      "train loss:0.5804148886850033\n",
      "train loss:0.5656981534090769\n",
      "train loss:0.5007502862325205\n",
      "train loss:0.521182371581363\n",
      "train loss:0.46091269942540714\n",
      "train loss:0.47254143570869034\n",
      "train loss:0.5942103259998723\n",
      "train loss:0.45442336689038226\n",
      "train loss:0.43865909211911874\n",
      "train loss:0.5447555188392618\n",
      "train loss:0.6628594425411563\n",
      "train loss:0.5140282977632448\n",
      "train loss:0.4692094187292865\n",
      "train loss:0.44141153434594493\n",
      "train loss:0.59258763911748\n",
      "train loss:0.5100397958347278\n",
      "train loss:0.47904124354907274\n",
      "train loss:0.5577264322633563\n",
      "train loss:0.6025455471446748\n",
      "train loss:0.5308662246781262\n",
      "train loss:0.7015966822826334\n",
      "train loss:0.33489505516870816\n",
      "train loss:0.6089327320200928\n",
      "train loss:0.49082370165305844\n",
      "train loss:0.47538065068507757\n",
      "train loss:0.61770479704479\n",
      "train loss:0.4330874935807414\n",
      "train loss:0.5219775394592268\n",
      "train loss:0.4938711427503017\n",
      "train loss:0.44668291243402436\n",
      "train loss:0.5516042034677416\n",
      "train loss:0.6037093376836041\n",
      "train loss:0.60885553820522\n",
      "train loss:0.5419726224112754\n",
      "train loss:0.5208774268563847\n",
      "train loss:0.4702237057731976\n",
      "train loss:0.49801532340186566\n",
      "train loss:0.4825632026790816\n",
      "train loss:0.5244994248323281\n",
      "train loss:0.6105875928533195\n",
      "train loss:0.5468577079634956\n",
      "train loss:0.5976091755995562\n",
      "train loss:0.5563656824916935\n",
      "train loss:0.6142458954646074\n",
      "train loss:0.527348578858021\n",
      "train loss:0.3092627318187159\n",
      "train loss:0.5001477825526969\n",
      "train loss:0.5915462745821009\n",
      "train loss:0.3842802157373356\n",
      "train loss:0.40307082204519096\n",
      "train loss:0.6978382119807081\n",
      "train loss:0.4625252289326102\n",
      "train loss:0.5516449925548197\n",
      "train loss:0.43990509144125356\n",
      "train loss:0.5273849866748928\n",
      "train loss:0.5431765276999567\n",
      "train loss:0.3920794372466745\n",
      "train loss:0.4758789755948582\n",
      "train loss:0.5667949492620693\n",
      "train loss:0.4011758783913627\n",
      "train loss:0.5345337489728683\n",
      "train loss:0.5916723773127104\n",
      "train loss:0.5297661959335977\n",
      "train loss:0.3848068465708152\n",
      "train loss:0.37845453307022403\n",
      "train loss:0.6372357508748845\n",
      "train loss:0.3827870451968011\n",
      "train loss:0.4658798492763422\n",
      "train loss:0.4832131558469107\n",
      "train loss:0.5924452452259349\n",
      "train loss:0.45623783416455965\n",
      "train loss:0.5319895689280405\n",
      "train loss:0.691734104464041\n",
      "train loss:0.6746934232013478\n",
      "train loss:0.5360905429098756\n",
      "train loss:0.5534544247324592\n",
      "train loss:0.5607327111727963\n",
      "train loss:0.4696487321794864\n",
      "train loss:0.4193552287575153\n",
      "train loss:0.5103893725707255\n",
      "train loss:0.4108858432154834\n",
      "train loss:0.4921118008264358\n",
      "train loss:0.5267354065523533\n",
      "train loss:0.43944064364811963\n",
      "train loss:0.4258592518964123\n",
      "train loss:0.6128458630858772\n",
      "train loss:0.4715415172211045\n",
      "train loss:0.47587622887443565\n",
      "train loss:0.5723433474825426\n",
      "train loss:0.404923810785728\n",
      "train loss:0.5835318298746228\n",
      "train loss:0.40642172318777203\n",
      "train loss:0.39971720029408603\n",
      "train loss:0.5742735085896701\n",
      "train loss:0.3740967693319189\n",
      "train loss:0.4333109515120496\n",
      "train loss:0.5395382294771207\n",
      "train loss:0.3826139764583638\n",
      "train loss:0.4617242755180445\n",
      "train loss:0.5292959189117775\n",
      "train loss:0.593345107219944\n",
      "train loss:0.5061284283598163\n",
      "train loss:0.4982947597004101\n",
      "train loss:0.41029780924247794\n",
      "train loss:0.5086964394669624\n",
      "train loss:0.6232849739338591\n",
      "train loss:0.6675618721534734\n",
      "train loss:0.5182817700973851\n",
      "train loss:0.468388373137427\n",
      "train loss:0.5365520939446276\n",
      "train loss:0.4059942869937833\n",
      "train loss:0.5360822604069806\n",
      "train loss:0.48460160418960413\n",
      "train loss:0.5614883080479326\n",
      "train loss:0.4144897452402151\n",
      "train loss:0.5285282187710333\n",
      "train loss:0.4590405997595528\n",
      "train loss:0.5564360889796569\n",
      "train loss:0.5681305461369843\n",
      "train loss:0.4649609029256922\n",
      "train loss:0.6126872509160062\n",
      "train loss:0.5342942095094517\n",
      "train loss:0.3801269640254295\n",
      "train loss:0.44595822273345964\n",
      "train loss:0.5698112552676937\n",
      "train loss:0.5484219472514674\n",
      "train loss:0.4506049326482403\n",
      "train loss:0.6213129334825697\n",
      "train loss:0.43664213881921277\n",
      "train loss:0.5554629903775049\n",
      "train loss:0.4699477470788366\n",
      "train loss:0.4775420779643806\n",
      "train loss:0.6164944126396429\n",
      "train loss:0.5220912217713279\n",
      "train loss:0.4946274436484545\n",
      "train loss:0.46482155741325804\n",
      "train loss:0.5986950540180046\n",
      "train loss:0.6061494531442555\n",
      "train loss:0.5933029730860193\n",
      "train loss:0.35101561010782084\n",
      "train loss:0.49970789258310766\n",
      "train loss:0.5244137313795706\n",
      "train loss:0.4445854991376664\n",
      "train loss:0.525820869596827\n",
      "train loss:0.5543300530783102\n",
      "train loss:0.5660401855526775\n",
      "train loss:0.43940777171287676\n",
      "train loss:0.5312627268709765\n",
      "train loss:0.4032587874426233\n",
      "train loss:0.32028455871001876\n",
      "train loss:0.5765648453661367\n",
      "train loss:0.5443237881953751\n",
      "train loss:0.4921581817588118\n",
      "train loss:0.5930428397332813\n",
      "train loss:0.5320445390251128\n",
      "train loss:0.49281657752841945\n",
      "train loss:0.5459003710310536\n",
      "train loss:0.4740178101318652\n",
      "train loss:0.4466924648666212\n",
      "train loss:0.43255807659358825\n",
      "train loss:0.36165940788856105\n",
      "train loss:0.47937532287761336\n",
      "train loss:0.5750316070522562\n",
      "train loss:0.5666649638008794\n",
      "train loss:0.4884708204954324\n",
      "train loss:0.6301047085665192\n",
      "train loss:0.4527395692511454\n",
      "train loss:0.44771578462536\n",
      "train loss:0.4185394110260258\n",
      "train loss:0.4899591404878823\n",
      "train loss:0.3938296881944545\n",
      "train loss:0.3670476610040134\n",
      "train loss:0.3407811843723277\n",
      "train loss:0.46666225592987837\n",
      "train loss:0.3971596713849707\n",
      "train loss:0.5004367336122272\n",
      "train loss:0.5224106126556516\n",
      "train loss:0.5204844255114471\n",
      "train loss:0.4945997082747735\n",
      "train loss:0.5277605056360958\n",
      "train loss:0.6117926865817473\n",
      "train loss:0.44115610759143115\n",
      "train loss:0.4029201256014555\n",
      "train loss:0.5190053883155352\n",
      "train loss:0.5296486151334263\n",
      "train loss:0.632245746509538\n",
      "train loss:0.3885959186532524\n",
      "train loss:0.4193827401728877\n",
      "train loss:0.5251447465782201\n",
      "train loss:0.4977283269409289\n",
      "train loss:0.46504631995845086\n",
      "train loss:0.41961368245633685\n",
      "train loss:0.3605452797854344\n",
      "train loss:0.45427856811831296\n",
      "train loss:0.49580004487965296\n",
      "train loss:0.44632194259873004\n",
      "train loss:0.634290635628563\n",
      "train loss:0.5326674294803059\n",
      "train loss:0.438886483628821\n",
      "train loss:0.48972656410433246\n",
      "train loss:0.456890449370704\n",
      "train loss:0.49453633894732113\n",
      "train loss:0.4426944035074789\n",
      "train loss:0.5369111360001837\n",
      "train loss:0.5621485495234692\n",
      "train loss:0.38518242543743236\n",
      "train loss:0.5425141439204908\n",
      "train loss:0.6145490463103634\n",
      "train loss:0.5283042372027971\n",
      "train loss:0.6121354082048952\n",
      "train loss:0.45181084390590553\n",
      "train loss:0.39157950255545365\n",
      "train loss:0.5550773630484996\n",
      "train loss:0.5084905671181559\n",
      "train loss:0.43961253626375685\n",
      "train loss:0.46055789003172315\n",
      "train loss:0.45792315200645617\n",
      "train loss:0.5918461811088227\n",
      "train loss:0.39021320044295904\n",
      "train loss:0.41603968233292093\n",
      "train loss:0.3489243075646153\n",
      "train loss:0.4883473562974297\n",
      "train loss:0.4151281691594159\n",
      "train loss:0.5755124932610438\n",
      "train loss:0.6161189943225299\n",
      "train loss:0.514680194009965\n",
      "train loss:0.3879997121629263\n",
      "train loss:0.5602059892418203\n",
      "train loss:0.5166057122970118\n",
      "train loss:0.5323023278609554\n",
      "train loss:0.5126766395526612\n",
      "train loss:0.4987286868093386\n",
      "train loss:0.6074451197251832\n",
      "train loss:0.5852041586311683\n",
      "train loss:0.8249701933857599\n",
      "train loss:0.537836786052429\n",
      "train loss:0.5537110140479689\n",
      "train loss:0.5012596259884604\n",
      "train loss:0.4529290795916085\n",
      "train loss:0.5190912513639716\n",
      "train loss:0.7743618763923454\n",
      "train loss:0.4715628463410731\n",
      "train loss:0.581792274633775\n",
      "train loss:0.4303847655723244\n",
      "train loss:0.4334420083918867\n",
      "train loss:0.35220228825575284\n",
      "train loss:0.5322036451505651\n",
      "train loss:0.4793424917372221\n",
      "train loss:0.4781352375261345\n",
      "train loss:0.6158947653099268\n",
      "train loss:0.45825436398686725\n",
      "train loss:0.5329533721274202\n",
      "train loss:0.5380065540839073\n",
      "train loss:0.40321926447638634\n",
      "train loss:0.558248991025641\n",
      "train loss:0.5748600720177885\n",
      "train loss:0.5549965490069524\n",
      "train loss:0.5596408496329129\n",
      "train loss:0.48442991987725587\n",
      "train loss:0.40860854567907157\n",
      "train loss:0.48854028395284343\n",
      "train loss:0.546398074510041\n",
      "train loss:0.5594218528783572\n",
      "train loss:0.3721677000519221\n",
      "train loss:0.5904497811895538\n",
      "train loss:0.4663571312302202\n",
      "train loss:0.5544351937489919\n",
      "train loss:0.47311447661107986\n",
      "train loss:0.5541716566455613\n",
      "train loss:0.5484835985553367\n",
      "train loss:0.5330975589583383\n",
      "train loss:0.579078087521183\n",
      "train loss:0.5068704133102461\n",
      "train loss:0.4688795156449065\n",
      "train loss:0.5160362011674257\n",
      "train loss:0.5951478054601409\n",
      "train loss:0.6308364063492842\n",
      "train loss:0.426278669680128\n",
      "train loss:0.36343539900413757\n",
      "train loss:0.5465987801956504\n",
      "train loss:0.6098472097724986\n",
      "train loss:0.5363051604626056\n",
      "train loss:0.40665894124607893\n",
      "train loss:0.6366653330337091\n",
      "train loss:0.552202429310625\n",
      "train loss:0.5481568340306637\n",
      "train loss:0.622519107423026\n",
      "train loss:0.3891361796204265\n",
      "train loss:0.3836927688068382\n",
      "train loss:0.4299512532333302\n",
      "train loss:0.5298877225947954\n",
      "train loss:0.5043658268093199\n",
      "train loss:0.3017570632719404\n",
      "train loss:0.45518097618979647\n",
      "train loss:0.5219647494632665\n",
      "train loss:0.49609108296635546\n",
      "train loss:0.3882998459503586\n",
      "train loss:0.5931011119563944\n",
      "train loss:0.5666254269170458\n",
      "train loss:0.38818494628667494\n",
      "train loss:0.5103750386723059\n",
      "train loss:0.45843515070699914\n",
      "train loss:0.43575597042996655\n",
      "train loss:0.5200033769483556\n",
      "train loss:0.5293579098446223\n",
      "train loss:0.5194276294023864\n",
      "train loss:0.4536873219224107\n",
      "train loss:0.4218664267966781\n",
      "train loss:0.6060918386300923\n",
      "train loss:0.5692488431372044\n",
      "train loss:0.45086289948245684\n",
      "train loss:0.5084630079063339\n",
      "train loss:0.41954396306953584\n",
      "train loss:0.8871205519805998\n",
      "train loss:0.43045293670655516\n",
      "train loss:0.37401043841846365\n",
      "train loss:0.5156758504950956\n",
      "train loss:0.5288196222462411\n",
      "train loss:0.5317648585501246\n",
      "train loss:0.5107761041621789\n",
      "train loss:0.4643717920819328\n",
      "train loss:0.4647978036622781\n",
      "train loss:0.5302470084753172\n",
      "train loss:0.6504579609794783\n",
      "train loss:0.3466024661497265\n",
      "train loss:0.6322167725480119\n",
      "train loss:0.4101359714420777\n",
      "train loss:0.44775829339801454\n",
      "train loss:0.4984237474010168\n",
      "train loss:0.626447977097113\n",
      "train loss:0.4130558906719034\n",
      "train loss:0.36871982601473313\n",
      "train loss:0.45400840992021263\n",
      "train loss:0.4086659581155171\n",
      "train loss:0.530837351761636\n",
      "train loss:0.4071690990063865\n",
      "train loss:0.5217413267539628\n",
      "train loss:0.4547757124341375\n",
      "train loss:0.5158397913793855\n",
      "train loss:0.42435815516141157\n",
      "train loss:0.4861075437170114\n",
      "train loss:0.47437842757374676\n",
      "train loss:0.5628098616561352\n",
      "train loss:0.5508695925744407\n",
      "train loss:0.4333738961973066\n",
      "train loss:0.5538862508865356\n",
      "train loss:0.46649335215366894\n",
      "train loss:0.4115977567299708\n",
      "train loss:0.3951420556767607\n",
      "train loss:0.3869692784007242\n",
      "train loss:0.46198465099733127\n",
      "train loss:0.4189425761880045\n",
      "train loss:0.4625706730041308\n",
      "train loss:0.3493017746003014\n",
      "train loss:0.3911368908432577\n",
      "train loss:0.5230224537890098\n",
      "train loss:0.4985324699533238\n",
      "train loss:0.5169469716084192\n",
      "train loss:0.3987295602940612\n",
      "train loss:0.40018446523310736\n",
      "train loss:0.4604781441945443\n",
      "train loss:0.477492086604285\n",
      "train loss:0.56397708135609\n",
      "train loss:0.4708269974498963\n",
      "train loss:0.5215517467813074\n",
      "train loss:0.3802159181837465\n",
      "train loss:0.4885463567363377\n",
      "train loss:0.4072176883796913\n",
      "train loss:0.5871428380829692\n",
      "train loss:0.48311564929099887\n",
      "train loss:0.47768381035862156\n",
      "train loss:0.45410413835940344\n",
      "train loss:0.38404126687305296\n",
      "train loss:0.5154877161124173\n",
      "train loss:0.5174258057639914\n",
      "train loss:0.3460194495368233\n",
      "train loss:0.3988205643982877\n",
      "train loss:0.49422068097204397\n",
      "train loss:0.60127913396057\n",
      "train loss:0.6757853802608836\n",
      "train loss:0.5153388532682933\n",
      "train loss:0.5186211084283326\n",
      "train loss:0.44597762519945566\n",
      "train loss:0.5281945374944005\n",
      "train loss:0.5083016723582741\n",
      "train loss:0.39591976579462157\n",
      "train loss:0.4610796245156411\n",
      "train loss:0.40737036216031103\n",
      "train loss:0.43382707775103235\n",
      "train loss:0.5020573777228525\n",
      "train loss:0.35225274645565113\n",
      "train loss:0.5551886639110822\n",
      "train loss:0.6064148317253014\n",
      "train loss:0.4056194972553549\n",
      "train loss:0.4233307964174443\n",
      "train loss:0.50216923630672\n",
      "train loss:0.38500517588770733\n",
      "train loss:0.4839456762054234\n",
      "train loss:0.39982024781934944\n",
      "train loss:0.5316591791264134\n",
      "train loss:0.49787700121869927\n",
      "train loss:0.3478486858179568\n",
      "train loss:0.5699387754159482\n",
      "train loss:0.5822830198129797\n",
      "train loss:0.5132603811765659\n",
      "train loss:0.42454441785929886\n",
      "train loss:0.44908820299527064\n",
      "train loss:0.5419861438531663\n",
      "train loss:0.5794487773943778\n",
      "train loss:0.5198471949997939\n",
      "train loss:0.42768719448882186\n",
      "train loss:0.48609140007587404\n",
      "train loss:0.38435508767072035\n",
      "train loss:0.5318414297139761\n",
      "train loss:0.46227368298547994\n",
      "train loss:0.46433975654159737\n",
      "train loss:0.5172174513065615\n",
      "train loss:0.5823140275148443\n",
      "train loss:0.519431907854285\n",
      "train loss:0.5663964651242995\n",
      "train loss:0.6802050534012711\n",
      "train loss:0.37555807575047573\n",
      "train loss:0.6440120430097036\n",
      "train loss:0.44903334569997705\n",
      "train loss:0.576229528700343\n",
      "train loss:0.4529322051721057\n",
      "train loss:0.38696674035051304\n",
      "train loss:0.34832413960851305\n",
      "train loss:0.44058525090491796\n",
      "train loss:0.3619535430514127\n",
      "train loss:0.49553817730394534\n",
      "train loss:0.4278018436636222\n",
      "train loss:0.35278494717341574\n",
      "train loss:0.5416616981425457\n",
      "train loss:0.6938934988861637\n",
      "train loss:0.41022276900653437\n",
      "train loss:0.49543420529812804\n",
      "train loss:0.5284522618671381\n",
      "train loss:0.4535606527755464\n",
      "train loss:0.40772362325402745\n",
      "train loss:0.4139492133287915\n",
      "train loss:0.40201655219392096\n",
      "train loss:0.6310856591264795\n",
      "train loss:0.4263733334100401\n",
      "train loss:0.4922231892693684\n",
      "train loss:0.44674069260718513\n",
      "train loss:0.47031433183435056\n",
      "train loss:0.5427789627659334\n",
      "train loss:0.3498935019299975\n",
      "train loss:0.47521667265918993\n",
      "train loss:0.3985133096498168\n",
      "train loss:0.5200686419744078\n",
      "train loss:0.45846567868078814\n",
      "train loss:0.48049446592350653\n",
      "train loss:0.47233489981095794\n",
      "train loss:0.5249946362576078\n",
      "train loss:0.45142121108891364\n",
      "train loss:0.3826177853016269\n",
      "train loss:0.4624556241934263\n",
      "train loss:0.5135216634370129\n",
      "train loss:0.44969917577420015\n",
      "train loss:0.49730162373329356\n",
      "train loss:0.3870213615018937\n",
      "train loss:0.5482464313494417\n",
      "train loss:0.4194641172273981\n",
      "train loss:0.5124085915481199\n",
      "train loss:0.3916093663002173\n",
      "train loss:0.4804733606303691\n",
      "train loss:0.49122085755367495\n",
      "train loss:0.39771079955553934\n",
      "train loss:0.4404368360111535\n",
      "train loss:0.40074085623903927\n",
      "train loss:0.5612624293462907\n",
      "train loss:0.3875027769902266\n",
      "train loss:0.4572818087406311\n",
      "train loss:0.5247059928724219\n",
      "train loss:0.39466027567911316\n",
      "train loss:0.40842368350649116\n",
      "train loss:0.5773055917966982\n",
      "train loss:0.5451498469919707\n",
      "train loss:0.4647397591257952\n",
      "train loss:0.5000802537684027\n",
      "train loss:0.4907645299087382\n",
      "train loss:0.44060255483603844\n",
      "train loss:0.5498672894929395\n",
      "train loss:0.5693540823756679\n",
      "train loss:0.41074520095558087\n",
      "train loss:0.6163308849950124\n",
      "train loss:0.6687180170363652\n",
      "train loss:0.4364077856912031\n",
      "train loss:0.4946741976042099\n",
      "train loss:0.46200510913584336\n",
      "train loss:0.4690032023988169\n",
      "train loss:0.4071991077400242\n",
      "train loss:0.3394482759544875\n",
      "train loss:0.5931944864230168\n",
      "train loss:0.4369166335564647\n",
      "train loss:0.5493137577779318\n",
      "train loss:0.4552028275344229\n",
      "train loss:0.6046155965276974\n",
      "train loss:0.5889459934091598\n",
      "train loss:0.45790805029110493\n",
      "train loss:0.4922314154514862\n",
      "train loss:0.5123938370577548\n",
      "train loss:0.5557529216078637\n",
      "train loss:0.48114640102651474\n",
      "train loss:0.514963158378875\n",
      "train loss:0.4259978032397659\n",
      "train loss:0.3738028899782782\n",
      "train loss:0.453720323964767\n",
      "train loss:0.48216983379716155\n",
      "train loss:0.3477290844092898\n",
      "train loss:0.34831794951568346\n",
      "train loss:0.3773741329836532\n",
      "train loss:0.7176977691570456\n",
      "train loss:0.4766494478043647\n",
      "train loss:0.553528529637884\n",
      "train loss:0.49062980617388113\n",
      "train loss:0.47616764473244017\n",
      "train loss:0.4580026693383965\n",
      "train loss:0.5345289806642268\n",
      "train loss:0.4920375846994285\n",
      "train loss:0.46690356020309765\n",
      "train loss:0.49196152595925674\n",
      "train loss:0.5725835703184564\n",
      "train loss:0.4042211485657768\n",
      "train loss:0.3986252220134386\n",
      "train loss:0.44512426017634404\n",
      "train loss:0.4546492783773436\n",
      "train loss:0.47195929323715385\n",
      "train loss:0.4396807732391699\n",
      "train loss:0.5197785518777593\n",
      "train loss:0.4228013993536544\n",
      "train loss:0.3882406957223505\n",
      "train loss:0.4230836199284001\n",
      "train loss:0.4109135802928639\n",
      "train loss:0.42060619111280784\n",
      "train loss:0.36164560790476685\n",
      "train loss:0.40930313315776495\n",
      "train loss:0.4318736162366544\n",
      "train loss:0.41938116070384496\n",
      "train loss:0.5121583789971407\n",
      "train loss:0.5261561281011148\n",
      "train loss:0.5386752263576268\n",
      "train loss:0.5249620616331542\n",
      "train loss:0.37881928094533235\n",
      "train loss:0.5916959515522854\n",
      "=== epoch:10, train acc:0.765, test acc:0.769 ===\n",
      "train loss:0.3307140407548006\n",
      "train loss:0.49511956419440517\n",
      "train loss:0.3958758056853691\n",
      "train loss:0.4403266427013987\n",
      "train loss:0.4707757074396832\n",
      "train loss:0.4439343386677105\n",
      "train loss:0.4731047407447096\n",
      "train loss:0.5823021712979831\n",
      "train loss:0.3467679695373666\n",
      "train loss:0.5639679770518078\n",
      "train loss:0.5976680475851373\n",
      "train loss:0.5668388334684541\n",
      "train loss:0.46798132007739957\n",
      "train loss:0.5628271286075701\n",
      "train loss:0.5249533698961875\n",
      "train loss:0.5039848148445517\n",
      "train loss:0.5869896620851665\n",
      "train loss:0.603434844475135\n",
      "train loss:0.3905292583457856\n",
      "train loss:0.4967302091758027\n",
      "train loss:0.38677052498466813\n",
      "train loss:0.46727869834462793\n",
      "train loss:0.6182973930897837\n",
      "train loss:0.5457249637580858\n",
      "train loss:0.5263831679642316\n",
      "train loss:0.45699828435792\n",
      "train loss:0.3927387138092525\n",
      "train loss:0.38432545220680014\n",
      "train loss:0.5506149345031252\n",
      "train loss:0.4384175615954756\n",
      "train loss:0.6053384748641322\n",
      "train loss:0.38554605913068923\n",
      "train loss:0.3355846547028573\n",
      "train loss:0.4699845812362203\n",
      "train loss:0.4476924066496765\n",
      "train loss:0.47055407389927933\n",
      "train loss:0.49481278001259604\n",
      "train loss:0.4290561647429288\n",
      "train loss:0.6291696492616028\n",
      "train loss:0.4953640172986051\n",
      "train loss:0.49723929871188033\n",
      "train loss:0.6760117279657878\n",
      "train loss:0.4357638590744153\n",
      "train loss:0.4391940245400216\n",
      "train loss:0.5700235237534986\n",
      "train loss:0.3471391895555408\n",
      "train loss:0.3880807300225325\n",
      "train loss:0.46521598286178145\n",
      "train loss:0.43776134498765634\n",
      "train loss:0.4368845965100856\n",
      "train loss:0.5164216873364841\n",
      "train loss:0.46069234371292545\n",
      "train loss:0.5137684737621346\n",
      "train loss:0.5731657552804311\n",
      "train loss:0.5483738297741645\n",
      "train loss:0.575751732874273\n",
      "train loss:0.537593007507172\n",
      "train loss:0.45850372105047277\n",
      "train loss:0.4840562629357759\n",
      "train loss:0.46881231666435125\n",
      "train loss:0.4574773957143018\n",
      "train loss:0.4606721451262869\n",
      "train loss:0.3902390632027671\n",
      "train loss:0.5565929122464801\n",
      "train loss:0.50527575907609\n",
      "train loss:0.607037560278563\n",
      "train loss:0.47967621244601977\n",
      "train loss:0.55025040879984\n",
      "train loss:0.6343526378936654\n",
      "train loss:0.4884682223690925\n",
      "train loss:0.4974518010919834\n",
      "train loss:0.4107018445699432\n",
      "train loss:0.596607749708265\n",
      "train loss:0.5660463497600857\n",
      "train loss:0.5348291126003288\n",
      "train loss:0.4462021777775118\n",
      "train loss:0.3404086920164104\n",
      "train loss:0.6545056016980628\n",
      "train loss:0.4868073455178398\n",
      "train loss:0.4640007615006522\n",
      "train loss:0.6117370584413492\n",
      "train loss:0.4785265851680821\n",
      "train loss:0.5000673920786802\n",
      "train loss:0.4991360369636344\n",
      "train loss:0.6015552707469729\n",
      "train loss:0.501520531561434\n",
      "train loss:0.48825833598546103\n",
      "train loss:0.4485407673308099\n",
      "train loss:0.41510461772764473\n",
      "train loss:0.42205183157963433\n",
      "train loss:0.48349964318187694\n",
      "train loss:0.36475327565384574\n",
      "train loss:0.4659275566327457\n",
      "train loss:0.465477023806071\n",
      "train loss:0.4952848718958818\n",
      "train loss:0.46246525467905975\n",
      "train loss:0.34711945405934264\n",
      "train loss:0.47181545561677124\n",
      "train loss:0.6675138925845647\n",
      "train loss:0.5868871335960827\n",
      "train loss:0.4366089014277936\n",
      "train loss:0.4280687067691343\n",
      "train loss:0.48772421167948954\n",
      "train loss:0.4610810494800277\n",
      "train loss:0.3413469805908405\n",
      "train loss:0.47195577790946686\n",
      "train loss:0.46316062490313137\n",
      "train loss:0.5353380242146387\n",
      "train loss:0.4667687503947498\n",
      "train loss:0.46868789290928475\n",
      "train loss:0.6088252596771568\n",
      "train loss:0.5450238117263763\n",
      "train loss:0.4672085272396629\n",
      "train loss:0.4749984328222156\n",
      "train loss:0.6305423861989662\n",
      "train loss:0.47607526184520327\n",
      "train loss:0.4803022886331086\n",
      "train loss:0.4780291352351012\n",
      "train loss:0.39645378893326133\n",
      "train loss:0.4764506562036727\n",
      "train loss:0.4569480582175936\n",
      "train loss:0.5346497672095658\n",
      "train loss:0.6002926404718727\n",
      "train loss:0.4144306155304156\n",
      "train loss:0.5013578054310223\n",
      "train loss:0.5153610241403447\n",
      "train loss:0.42393930787523415\n",
      "train loss:0.5256737131641835\n",
      "train loss:0.6781397080827308\n",
      "train loss:0.6099047631521868\n",
      "train loss:0.4679153373091718\n",
      "train loss:0.3586985002792752\n",
      "train loss:0.49176924877906375\n",
      "train loss:0.46786962944219695\n",
      "train loss:0.38286323175761344\n",
      "train loss:0.4451769439655879\n",
      "train loss:0.3308956193778599\n",
      "train loss:0.42527766086900487\n",
      "train loss:0.3641739042437293\n",
      "train loss:0.5576757937178972\n",
      "train loss:0.46999888628304914\n",
      "train loss:0.4833655040355215\n",
      "train loss:0.46724125248467074\n",
      "train loss:0.4840420648062031\n",
      "train loss:0.7090459373820645\n",
      "train loss:0.4729941214955278\n",
      "train loss:0.5434054197434413\n",
      "train loss:0.34670329791320526\n",
      "train loss:0.5413353056934427\n",
      "train loss:0.5324205369273906\n",
      "train loss:0.5311283396097219\n",
      "train loss:0.620603328256464\n",
      "train loss:0.4697524144604581\n",
      "train loss:0.7753531232604567\n",
      "train loss:0.5222896538556676\n",
      "train loss:0.5573139621295107\n",
      "train loss:0.522495786623167\n",
      "train loss:0.5139474167399771\n",
      "train loss:0.4075236825634799\n",
      "train loss:0.6421994177454746\n",
      "train loss:0.4073281037200942\n",
      "train loss:0.37669290080325346\n",
      "train loss:0.440666111488955\n",
      "train loss:0.4515137911985748\n",
      "train loss:0.4585534182334189\n",
      "train loss:0.4634843950569468\n",
      "train loss:0.5680590111897544\n",
      "train loss:0.37954699811059334\n",
      "train loss:0.44857224500681037\n",
      "train loss:0.37752262205862125\n",
      "train loss:0.515390577553921\n",
      "train loss:0.519669488969811\n",
      "train loss:0.556724606143306\n",
      "train loss:0.5291637573818924\n",
      "train loss:0.39245923088653956\n",
      "train loss:0.4727833766192486\n",
      "train loss:0.4649716520848763\n",
      "train loss:0.5800769641274516\n",
      "train loss:0.40043849138310345\n",
      "train loss:0.5723858972915133\n",
      "train loss:0.4372039160828156\n",
      "train loss:0.4092872293922673\n",
      "train loss:0.5407662493636699\n",
      "train loss:0.38633865530762734\n",
      "train loss:0.3911063749010441\n",
      "train loss:0.48193422966771143\n",
      "train loss:0.4698950207402515\n",
      "train loss:0.4955420475145273\n",
      "train loss:0.44220392504091577\n",
      "train loss:0.6342601202416759\n",
      "train loss:0.40207714758373847\n",
      "train loss:0.4456277958329846\n",
      "train loss:0.49219192875053425\n",
      "train loss:0.6069292494514661\n",
      "train loss:0.47729927094356756\n",
      "train loss:0.49858109825889846\n",
      "train loss:0.45126885133943945\n",
      "train loss:0.5040113182894088\n",
      "train loss:0.48260487939440305\n",
      "train loss:0.5687906817299901\n",
      "train loss:0.39326955622806664\n",
      "train loss:0.5882566885394922\n",
      "train loss:0.3990325111956684\n",
      "train loss:0.47527533548304146\n",
      "train loss:0.6286470247229983\n",
      "train loss:0.4356292531916005\n",
      "train loss:0.4362987362521485\n",
      "train loss:0.36917704189626965\n",
      "train loss:0.5730464116561453\n",
      "train loss:0.5907598036142069\n",
      "train loss:0.4658270444466066\n",
      "train loss:0.4114932511380715\n",
      "train loss:0.5681265471555075\n",
      "train loss:0.5541767806068542\n",
      "train loss:0.3685384083840322\n",
      "train loss:0.5318290008453253\n",
      "train loss:0.5390669207377947\n",
      "train loss:0.6147075560950844\n",
      "train loss:0.49978619481821185\n",
      "train loss:0.5976753901570343\n",
      "train loss:0.44635144466292537\n",
      "train loss:0.4509224169328936\n",
      "train loss:0.49606098878648863\n",
      "train loss:0.34699793003175006\n",
      "train loss:0.5029885931075808\n",
      "train loss:0.47016706284678844\n",
      "train loss:0.43551401674005297\n",
      "train loss:0.3675605511804003\n",
      "train loss:0.46832608822578015\n",
      "train loss:0.5013651338597185\n",
      "train loss:0.5143365184257971\n",
      "train loss:0.531996587986296\n",
      "train loss:0.5229346017711641\n",
      "train loss:0.5410380280676056\n",
      "train loss:0.421830666079939\n",
      "train loss:0.4654885137010226\n",
      "train loss:0.42828717449857956\n",
      "train loss:0.5420053015821326\n",
      "train loss:0.46750700399895523\n",
      "train loss:0.4041923124655217\n",
      "train loss:0.5860884629910939\n",
      "train loss:0.5150691702969821\n",
      "train loss:0.44082514151304486\n",
      "train loss:0.6409744998781547\n",
      "train loss:0.5739140301549849\n",
      "train loss:0.4084615049140022\n",
      "train loss:0.3854500589259008\n",
      "train loss:0.5720155741426285\n",
      "train loss:0.46188274819373704\n",
      "train loss:0.6441338906768181\n",
      "train loss:0.545170884953922\n",
      "train loss:0.4462890378734098\n",
      "train loss:0.5006313421239246\n",
      "train loss:0.4791532423067709\n",
      "train loss:0.5601050175397494\n",
      "train loss:0.49995242648363714\n",
      "train loss:0.46500911303175263\n",
      "train loss:0.45713552364028354\n",
      "train loss:0.54471198427866\n",
      "train loss:0.5308986230741788\n",
      "train loss:0.33928190149235626\n",
      "train loss:0.464443698152441\n",
      "train loss:0.4542701405967886\n",
      "train loss:0.34077368955487913\n",
      "train loss:0.5510870699609945\n",
      "train loss:0.5258391150843459\n",
      "train loss:0.5476264785486201\n",
      "train loss:0.31397718778841555\n",
      "train loss:0.581838227655847\n",
      "train loss:0.541786088775745\n",
      "train loss:0.5392321748593016\n",
      "train loss:0.5544305329102758\n",
      "train loss:0.6018664747727737\n",
      "train loss:0.5194978045486409\n",
      "train loss:0.42646082700291915\n",
      "train loss:0.43494243930302756\n",
      "train loss:0.4626124272802683\n",
      "train loss:0.4360531581576515\n",
      "train loss:0.4415851479466194\n",
      "train loss:0.4091458531658729\n",
      "train loss:0.369542652670629\n",
      "train loss:0.4578670341959081\n",
      "train loss:0.41307811994997906\n",
      "train loss:0.4985792856038379\n",
      "train loss:0.48836018889436167\n",
      "train loss:0.49718442887347386\n",
      "train loss:0.48893256509322813\n",
      "train loss:0.45459505102703557\n",
      "train loss:0.40954874326485013\n",
      "train loss:0.3551370990921234\n",
      "train loss:0.44458756670634\n",
      "train loss:0.4975639116002044\n",
      "train loss:0.43416633972960794\n",
      "train loss:0.5130724804523388\n",
      "train loss:0.5181443748555137\n",
      "train loss:0.48219580085554986\n",
      "train loss:0.4542297965209935\n",
      "train loss:0.42830029145794646\n",
      "train loss:0.4207379486429528\n",
      "train loss:0.45541212841522966\n",
      "train loss:0.5371177764846362\n",
      "train loss:0.4890408122650609\n",
      "train loss:0.3574625767635233\n",
      "train loss:0.32777536697543685\n",
      "train loss:0.6198996238110626\n",
      "train loss:0.4035432015681489\n",
      "train loss:0.6766805579979\n",
      "train loss:0.41652934168516126\n",
      "train loss:0.400641161792091\n",
      "train loss:0.35830059122040386\n",
      "train loss:0.4316197944544375\n",
      "train loss:0.451551067295888\n",
      "train loss:0.4473936948731192\n",
      "train loss:0.5730651396754584\n",
      "train loss:0.39681513456003903\n",
      "train loss:0.5004354832259544\n",
      "train loss:0.35564435719675197\n",
      "train loss:0.4246040269030065\n",
      "train loss:0.42845839069592495\n",
      "train loss:0.4337679511529234\n",
      "train loss:0.7793128251839779\n",
      "train loss:0.43360436581110806\n",
      "train loss:0.42871016145862584\n",
      "train loss:0.6436920853399687\n",
      "train loss:0.4632649649946207\n",
      "train loss:0.3507745738975486\n",
      "train loss:0.4792341967071729\n",
      "train loss:0.5428494184003657\n",
      "train loss:0.40131917359987573\n",
      "train loss:0.485843515696095\n",
      "train loss:0.34641440899166265\n",
      "train loss:0.4577869654648947\n",
      "train loss:0.5945700115501169\n",
      "train loss:0.41701815681672194\n",
      "train loss:0.6312631764972122\n",
      "train loss:0.33194762320153587\n",
      "train loss:0.4754466711589614\n",
      "train loss:0.7150046682833556\n",
      "train loss:0.5414127173948422\n",
      "train loss:0.42178841107589876\n",
      "train loss:0.4834811295981813\n",
      "train loss:0.5908734201880073\n",
      "train loss:0.5468354142284321\n",
      "train loss:0.4768980378129386\n",
      "train loss:0.3561865563077387\n",
      "train loss:0.615065838195298\n",
      "train loss:0.5923450964254826\n",
      "train loss:0.3080414829755419\n",
      "train loss:0.4021254648459046\n",
      "train loss:0.648065074643106\n",
      "train loss:0.45597231998068305\n",
      "train loss:0.4395437126583684\n",
      "train loss:0.4875121684644644\n",
      "train loss:0.4876206029771788\n",
      "train loss:0.4548854253571218\n",
      "train loss:0.4215162824928224\n",
      "train loss:0.47305788611878624\n",
      "train loss:0.43335639763906797\n",
      "train loss:0.38479878399106754\n",
      "train loss:0.43473811340816154\n",
      "train loss:0.456436762025259\n",
      "train loss:0.49850102362329257\n",
      "train loss:0.5101132144101352\n",
      "train loss:0.3439949128462838\n",
      "train loss:0.39597670533967944\n",
      "train loss:0.4312623009157606\n",
      "train loss:0.5109879172325452\n",
      "train loss:0.5385838766667775\n",
      "train loss:0.49423269354554944\n",
      "train loss:0.5394168814968281\n",
      "train loss:0.4752697338340699\n",
      "train loss:0.645695972327881\n",
      "train loss:0.36709676897797733\n",
      "train loss:0.48859762008923013\n",
      "train loss:0.39261553261007814\n",
      "train loss:0.45738297544034556\n",
      "train loss:0.41230247903606343\n",
      "train loss:0.5134286426755393\n",
      "train loss:0.42071952201357943\n",
      "train loss:0.5598272256557888\n",
      "train loss:0.37822744500367184\n",
      "train loss:0.44799785112521434\n",
      "train loss:0.4452570680040349\n",
      "train loss:0.4503201357556105\n",
      "train loss:0.8571265928923637\n",
      "train loss:0.5965696064213669\n",
      "train loss:0.40877396089322354\n",
      "train loss:0.5790447852840485\n",
      "train loss:0.48309539663336326\n",
      "train loss:0.5143937473766811\n",
      "train loss:0.44382059465595025\n",
      "train loss:0.39844272408841597\n",
      "train loss:0.6322783774285362\n",
      "train loss:0.6721796471143283\n",
      "train loss:0.35597031837407345\n",
      "train loss:0.42392468727881794\n",
      "train loss:0.3913017001435828\n",
      "train loss:0.4598267937117175\n",
      "train loss:0.39338454239100995\n",
      "train loss:0.351715449747333\n",
      "train loss:0.54886639484403\n",
      "train loss:0.3426435600263379\n",
      "train loss:0.4418725164282439\n",
      "train loss:0.374951352480831\n",
      "train loss:0.4802973229771102\n",
      "train loss:0.5552490863965714\n",
      "train loss:0.37334385809628556\n",
      "train loss:0.4543717316041713\n",
      "train loss:0.4661155586629904\n",
      "train loss:0.4990203333688509\n",
      "train loss:0.40189151728383493\n",
      "train loss:0.41608410504436577\n",
      "train loss:0.6553713497589823\n",
      "train loss:0.5262532029695631\n",
      "train loss:0.49106844573561304\n",
      "train loss:0.533401213126107\n",
      "train loss:0.5368573571287554\n",
      "train loss:0.46850027530629595\n",
      "train loss:0.4140668037699588\n",
      "train loss:0.47982066109992777\n",
      "train loss:0.5010326390764888\n",
      "train loss:0.5062649179748727\n",
      "train loss:0.4088171597074971\n",
      "train loss:0.5852629602707541\n",
      "train loss:0.4761127103517768\n",
      "train loss:0.5163837539855733\n",
      "train loss:0.5018611302077204\n",
      "train loss:0.4773774570992502\n",
      "train loss:0.5474242188109801\n",
      "train loss:0.42649970051577024\n",
      "train loss:0.5346224459084878\n",
      "train loss:0.3100212996086962\n",
      "train loss:0.5058541052658394\n",
      "train loss:0.34594876420305604\n",
      "train loss:0.6289316091398515\n",
      "train loss:0.597018629688005\n",
      "train loss:0.5335215315709961\n",
      "train loss:0.4328230889910572\n",
      "train loss:0.5492303741865605\n",
      "train loss:0.558536456954284\n",
      "train loss:0.5370413745836333\n",
      "train loss:0.5723006271852281\n",
      "train loss:0.4694692325885585\n",
      "train loss:0.43711034160012363\n",
      "train loss:0.6107482582228544\n",
      "train loss:0.45810965427897443\n",
      "train loss:0.6303009072887064\n",
      "train loss:0.6572325874313694\n",
      "train loss:0.5376273263433431\n",
      "train loss:0.5089068819261336\n",
      "train loss:0.4605472369225561\n",
      "train loss:0.4932611600816566\n",
      "train loss:0.5008255259491428\n",
      "train loss:0.501133052801064\n",
      "train loss:0.42770706299975153\n",
      "train loss:0.5365422609511583\n",
      "train loss:0.5165731219759752\n",
      "train loss:0.5867697147431246\n",
      "train loss:0.4838328894858811\n",
      "train loss:0.47980878697919577\n",
      "train loss:0.5670713176847352\n",
      "train loss:0.5358228607562693\n",
      "train loss:0.45567469780797576\n",
      "train loss:0.39289481761383016\n",
      "train loss:0.5335168889002211\n",
      "train loss:0.4993272080838611\n",
      "train loss:0.38079092374098805\n",
      "train loss:0.5032120083924343\n",
      "train loss:0.5967267966344799\n",
      "train loss:0.47826145075741067\n",
      "train loss:0.3879749775591744\n",
      "train loss:0.42521264577255147\n",
      "train loss:0.32498993685513555\n",
      "train loss:0.4760231463246022\n",
      "train loss:0.47548943318878295\n",
      "train loss:0.5469241400163499\n",
      "train loss:0.5447308245659761\n",
      "train loss:0.49498467892501125\n",
      "train loss:0.5616172563342473\n",
      "train loss:0.44399928763800245\n",
      "train loss:0.5808910264347673\n",
      "train loss:0.421250336975196\n",
      "train loss:0.46287073915049404\n",
      "train loss:0.5429147559620813\n",
      "train loss:0.4302940851566736\n",
      "train loss:0.5384335565886059\n",
      "train loss:0.5029667069639061\n",
      "train loss:0.528026371676238\n",
      "train loss:0.4806961510184171\n",
      "train loss:0.48942336936942205\n",
      "train loss:0.5040703090958902\n",
      "train loss:0.3183000395540593\n",
      "train loss:0.3700795366780595\n",
      "train loss:0.4768546255110367\n",
      "train loss:0.4738893340236877\n",
      "train loss:0.47793924294477824\n",
      "train loss:0.46496820270311695\n",
      "train loss:0.3709143767821306\n",
      "train loss:0.4375371868629944\n",
      "train loss:0.49041837352237194\n",
      "train loss:0.545240412583576\n",
      "train loss:0.5257410942925036\n",
      "train loss:0.4173453121997132\n",
      "train loss:0.4047644187857698\n",
      "train loss:0.4602470952533942\n",
      "train loss:0.4386984346148071\n",
      "train loss:0.42245969466850075\n",
      "train loss:0.5125386923194812\n",
      "train loss:0.49521403114918655\n",
      "train loss:0.45033707293540914\n",
      "train loss:0.5381232499748846\n",
      "train loss:0.4187972013507698\n",
      "train loss:0.4920709703399795\n",
      "train loss:0.6613950423068209\n",
      "train loss:0.5281677434988081\n",
      "train loss:0.5488147338470566\n",
      "train loss:0.5216106226850211\n",
      "train loss:0.3761539294401049\n",
      "train loss:0.5021017686270245\n",
      "train loss:0.44854770082914847\n",
      "train loss:0.47010197643512713\n",
      "train loss:0.37256860381724927\n",
      "train loss:0.4760426791478741\n",
      "train loss:0.4604847024318692\n",
      "train loss:0.5078984395834599\n",
      "train loss:0.38537591021518336\n",
      "train loss:0.41157854513298886\n",
      "train loss:0.3795596662767658\n",
      "train loss:0.37812034994770033\n",
      "train loss:0.41542581029236464\n",
      "train loss:0.49678586265553265\n",
      "train loss:0.40894643056281127\n",
      "train loss:0.41707591417292084\n",
      "train loss:0.3761516682841484\n",
      "train loss:0.6174392511038146\n",
      "train loss:0.49848207758014706\n",
      "train loss:0.40611648360835545\n",
      "train loss:0.45843042767098313\n",
      "train loss:0.4854643563411013\n",
      "train loss:0.497472218352091\n",
      "train loss:0.49992380787366497\n",
      "train loss:0.45320644512424657\n",
      "train loss:0.3546389159319394\n",
      "train loss:0.44309649132096907\n",
      "train loss:0.4341071355580323\n",
      "train loss:0.3512636327228087\n",
      "train loss:0.44443131108636896\n",
      "train loss:0.43325826551108504\n",
      "train loss:0.6687323296981343\n",
      "train loss:0.4858330336709074\n",
      "train loss:0.37569284658535546\n",
      "train loss:0.5489075995592824\n",
      "train loss:0.39319239334594697\n",
      "train loss:0.41565187379630275\n",
      "train loss:0.6293228153859757\n",
      "train loss:0.502405113492511\n",
      "train loss:0.6063392202077084\n",
      "train loss:0.46447369346081474\n",
      "train loss:0.5052786285488551\n",
      "train loss:0.6764354766705226\n",
      "train loss:0.3986279333214886\n",
      "train loss:0.5953355863384848\n",
      "train loss:0.6637603539914657\n",
      "train loss:0.465333522984977\n",
      "train loss:0.4338887065861149\n",
      "train loss:0.4159500977555637\n",
      "train loss:0.4661711584170978\n",
      "train loss:0.407686254421901\n",
      "train loss:0.5996821107415785\n",
      "train loss:0.39943111511050206\n",
      "train loss:0.45990391310943124\n",
      "train loss:0.4998311206587689\n",
      "train loss:0.5088159352397333\n",
      "train loss:0.4851539716432237\n",
      "train loss:0.42107379962798147\n",
      "train loss:0.3648919313288566\n",
      "train loss:0.7982475518054524\n",
      "train loss:0.5178434604323301\n",
      "train loss:0.36425071921308855\n",
      "train loss:0.5694842383191893\n",
      "train loss:0.5796259461075486\n",
      "train loss:0.47482180617250935\n",
      "train loss:0.454511906987543\n",
      "train loss:0.3993960366211073\n",
      "train loss:0.3655986267574953\n",
      "train loss:0.3245919056454559\n",
      "train loss:0.47679629763537323\n",
      "train loss:0.3568475365113597\n",
      "train loss:0.3895152851524751\n",
      "train loss:0.33151379562369115\n",
      "train loss:0.36642338968190624\n",
      "train loss:0.5389869741615299\n",
      "train loss:0.47226907278296415\n",
      "train loss:0.48763846366591856\n",
      "train loss:0.31054699805025543\n",
      "train loss:0.40498627447201424\n",
      "train loss:0.4367558649563202\n",
      "train loss:0.4880885066626264\n",
      "train loss:0.5419216312575197\n",
      "train loss:0.5083952412286622\n",
      "=== epoch:11, train acc:0.767, test acc:0.779 ===\n",
      "train loss:0.41808667025257323\n",
      "train loss:0.5583734991071171\n",
      "train loss:0.44427533226076654\n",
      "train loss:0.5067164510358514\n",
      "train loss:0.400048444422579\n",
      "train loss:0.39249865642794995\n",
      "train loss:0.3752737118937951\n",
      "train loss:0.47818318578791164\n",
      "train loss:0.45760303517592293\n",
      "train loss:0.3807747107477904\n",
      "train loss:0.3436217577140735\n",
      "train loss:0.4077174949248114\n",
      "train loss:0.30911462830900777\n",
      "train loss:0.4702489000964655\n",
      "train loss:0.5060433279829948\n",
      "train loss:0.47022247635732894\n",
      "train loss:0.41010236888127294\n",
      "train loss:0.4194019244135951\n",
      "train loss:0.4589798931000255\n",
      "train loss:0.5223961747248554\n",
      "train loss:0.5247176101357419\n",
      "train loss:0.531101038129298\n",
      "train loss:0.4126128488650908\n",
      "train loss:0.4566203421596039\n",
      "train loss:0.5353889427684091\n",
      "train loss:0.42179786972768196\n",
      "train loss:0.5305458622386239\n",
      "train loss:0.39101172322478683\n",
      "train loss:0.4587301063697195\n",
      "train loss:0.4173781240987173\n",
      "train loss:0.4799142026171552\n",
      "train loss:0.5383850495029026\n",
      "train loss:0.5625385310171489\n",
      "train loss:0.5013955541446107\n",
      "train loss:0.5667392531879818\n",
      "train loss:0.48927635181042495\n",
      "train loss:0.60029236582918\n",
      "train loss:0.5058710713078739\n",
      "train loss:0.5571101570816058\n",
      "train loss:0.46644789023795447\n",
      "train loss:0.5708030126342962\n",
      "train loss:0.5699783616681138\n",
      "train loss:0.44462737847371414\n",
      "train loss:0.41195776960836417\n",
      "train loss:0.47797013695726975\n",
      "train loss:0.4628473212378265\n",
      "train loss:0.4214318433627977\n",
      "train loss:0.5714079362729997\n",
      "train loss:0.39233619327873503\n",
      "train loss:0.46350061534916537\n",
      "train loss:0.6002778909369108\n",
      "train loss:0.6391637818914178\n",
      "train loss:0.4976253685945894\n",
      "train loss:0.48223698618694916\n",
      "train loss:0.5314863511872913\n",
      "train loss:0.4704723669223588\n",
      "train loss:0.5432747387823781\n",
      "train loss:0.5040365295201756\n",
      "train loss:0.43820116578573853\n",
      "train loss:0.6323211915221949\n",
      "train loss:0.45454913048876117\n",
      "train loss:0.4942459922728073\n",
      "train loss:0.5387137728945234\n",
      "train loss:0.5793269622333651\n",
      "train loss:0.43858026413380924\n",
      "train loss:0.679470119781306\n",
      "train loss:0.42202687711303194\n",
      "train loss:0.40845964984815\n",
      "train loss:0.40719922549064\n",
      "train loss:0.553519072359464\n",
      "train loss:0.4854997418419278\n",
      "train loss:0.3864434908316435\n",
      "train loss:0.45565585617314275\n",
      "train loss:0.5823527613384829\n",
      "train loss:0.5486855289539465\n",
      "train loss:0.43558219004730786\n",
      "train loss:0.4010710524213961\n",
      "train loss:0.47807657844779905\n",
      "train loss:0.4380537057192813\n",
      "train loss:0.39801362850851063\n",
      "train loss:0.41873767080496177\n",
      "train loss:0.44777170259522014\n",
      "train loss:0.5792186964432681\n",
      "train loss:0.4661063765364142\n",
      "train loss:0.35050118519305273\n",
      "train loss:0.460284723267407\n",
      "train loss:0.5268712935060197\n",
      "train loss:0.4804155063009798\n",
      "train loss:0.5949357733637065\n",
      "train loss:0.48478638432701404\n",
      "train loss:0.5862017449590281\n",
      "train loss:0.4158882944425539\n",
      "train loss:0.43626009308621677\n",
      "train loss:0.45101333743712635\n",
      "train loss:0.5961374992524746\n",
      "train loss:0.41636542835427837\n",
      "train loss:0.49133550754324096\n",
      "train loss:0.4956879920588508\n",
      "train loss:0.663688331245336\n",
      "train loss:0.501610234492541\n",
      "train loss:0.5220945520180983\n",
      "train loss:0.4283553918899781\n",
      "train loss:0.5491100029584338\n",
      "train loss:0.5091975926861809\n",
      "train loss:0.524935277593595\n",
      "train loss:0.41116819923396164\n",
      "train loss:0.4303097687720786\n",
      "train loss:0.33930998906486953\n",
      "train loss:0.2898391230611841\n",
      "train loss:0.37712554480771127\n",
      "train loss:0.47624007994234147\n",
      "train loss:0.4168848563210903\n",
      "train loss:0.48642732257384375\n",
      "train loss:0.44643760574323\n",
      "train loss:0.4437034531765409\n",
      "train loss:0.4369038214849396\n",
      "train loss:0.34866772923471934\n",
      "train loss:0.4986761526212863\n",
      "train loss:0.4176389703712655\n",
      "train loss:0.48712102486397746\n",
      "train loss:0.46323280535776395\n",
      "train loss:0.45107919317164513\n",
      "train loss:0.35794700910837723\n",
      "train loss:0.5266995011718336\n",
      "train loss:0.4528520757806861\n",
      "train loss:0.66542190235092\n",
      "train loss:0.3747313258319667\n",
      "train loss:0.3714943154834836\n",
      "train loss:0.4923352730587593\n",
      "train loss:0.4253752718028197\n",
      "train loss:0.45973061256799824\n",
      "train loss:0.4814222097438897\n",
      "train loss:0.3454820259236759\n",
      "train loss:0.40820038425618754\n",
      "train loss:0.3657488791546237\n",
      "train loss:0.5732098582498263\n",
      "train loss:0.496135990529243\n",
      "train loss:0.4591910323175912\n",
      "train loss:0.3643412150223127\n",
      "train loss:0.3893228920592467\n",
      "train loss:0.4378862697428926\n",
      "train loss:0.5364591321499207\n",
      "train loss:0.5955242860526768\n",
      "train loss:0.48374206667314323\n",
      "train loss:0.5201199079580826\n",
      "train loss:0.6961237863664808\n",
      "train loss:0.6322928414064074\n",
      "train loss:0.46865835436029507\n",
      "train loss:0.5702754163376421\n",
      "train loss:0.4250252184999022\n",
      "train loss:0.3473442425133036\n",
      "train loss:0.6036862267824918\n",
      "train loss:0.38591908964733024\n",
      "train loss:0.6060667440832229\n",
      "train loss:0.4290676804341986\n",
      "train loss:0.506272187420618\n",
      "train loss:0.5109611069536644\n",
      "train loss:0.3779795149382001\n",
      "train loss:0.3702227108051322\n",
      "train loss:0.5223412377094373\n",
      "train loss:0.5827435295826531\n",
      "train loss:0.4560024490843586\n",
      "train loss:0.5050376848541791\n",
      "train loss:0.5205835890593221\n",
      "train loss:0.3851453691533557\n",
      "train loss:0.43575903571008917\n",
      "train loss:0.5486864216214106\n",
      "train loss:0.46291024154421917\n",
      "train loss:0.517684320012196\n",
      "train loss:0.6218760275255099\n",
      "train loss:0.5016248180877568\n",
      "train loss:0.46340860287134944\n",
      "train loss:0.4828541294548635\n",
      "train loss:0.43899313799296846\n",
      "train loss:0.5411901758011177\n",
      "train loss:0.4245887968908738\n",
      "train loss:0.4927618510257967\n",
      "train loss:0.4695813912602139\n",
      "train loss:0.4533889996338928\n",
      "train loss:0.43178005089947186\n",
      "train loss:0.49891102296096074\n",
      "train loss:0.42715246159223114\n",
      "train loss:0.30858026072938477\n",
      "train loss:0.5493075409619071\n",
      "train loss:0.5212415681243712\n",
      "train loss:0.5064715647269132\n",
      "train loss:0.4245065460266267\n",
      "train loss:0.47032331556982354\n",
      "train loss:0.5395662715725742\n",
      "train loss:0.5836477513403059\n",
      "train loss:0.386786370482771\n",
      "train loss:0.3676581901246123\n",
      "train loss:0.5240783770559586\n",
      "train loss:0.5194866900492007\n",
      "train loss:0.42341077662779386\n",
      "train loss:0.457518184898477\n",
      "train loss:0.5558588406555631\n",
      "train loss:0.5785460787234791\n",
      "train loss:0.5196789030852408\n",
      "train loss:0.4609757956847792\n",
      "train loss:0.47450888988131673\n",
      "train loss:0.5492628826497171\n",
      "train loss:0.33853262687906494\n",
      "train loss:0.4569543525366406\n",
      "train loss:0.679362424956096\n",
      "train loss:0.5244644310693746\n",
      "train loss:0.3684994699200543\n",
      "train loss:0.4426017305981469\n",
      "train loss:0.5429181875133628\n",
      "train loss:0.5640345581712397\n",
      "train loss:0.4412530315899154\n",
      "train loss:0.36672296091873974\n",
      "train loss:0.36947175632253876\n",
      "train loss:0.5739080965079121\n",
      "train loss:0.5304653179162471\n",
      "train loss:0.37886229362420254\n",
      "train loss:0.5130495231164032\n",
      "train loss:0.36468941084059464\n",
      "train loss:0.5168965986498678\n",
      "train loss:0.5544289008126241\n",
      "train loss:0.48484891373446287\n",
      "train loss:0.7371422455250766\n",
      "train loss:0.5504713042570607\n",
      "train loss:0.5701078674597722\n",
      "train loss:0.4251851878230767\n",
      "train loss:0.513498633227875\n",
      "train loss:0.6399124791269338\n",
      "train loss:0.49144835592478503\n",
      "train loss:0.6042461129253426\n",
      "train loss:0.42814169785338313\n",
      "train loss:0.5132351315603246\n",
      "train loss:0.5691440865592959\n",
      "train loss:0.4632839933523804\n",
      "train loss:0.424731737832217\n",
      "train loss:0.5246513273010573\n",
      "train loss:0.3882537912123\n",
      "train loss:0.4837130140996237\n",
      "train loss:0.4939491238397843\n",
      "train loss:0.4489663440356943\n",
      "train loss:0.49371446557456466\n",
      "train loss:0.42283064408006354\n",
      "train loss:0.48276524083567834\n",
      "train loss:0.4744369487726728\n",
      "train loss:0.47485783273448284\n",
      "train loss:0.3777098909027098\n",
      "train loss:0.5584702418082387\n",
      "train loss:0.5975416598562028\n",
      "train loss:0.582962291507587\n",
      "train loss:0.43319427724277576\n",
      "train loss:0.34122288810440154\n",
      "train loss:0.5053422346932169\n",
      "train loss:0.4603227652023705\n",
      "train loss:0.3930591474292681\n",
      "train loss:0.35526997110950903\n",
      "train loss:0.4721410483517737\n",
      "train loss:0.4647235068140305\n",
      "train loss:0.4995567063935188\n",
      "train loss:0.56471702067358\n",
      "train loss:0.3964349998303134\n",
      "train loss:0.4657727091739512\n",
      "train loss:0.37869431394723535\n",
      "train loss:0.5290346720837575\n",
      "train loss:0.4761174928287017\n",
      "train loss:0.4311469067850784\n",
      "train loss:0.7006340102829219\n",
      "train loss:0.4426007040686233\n",
      "train loss:0.40283383542997453\n",
      "train loss:0.45499483418636716\n",
      "train loss:0.5365441347579354\n",
      "train loss:0.4749735400016392\n",
      "train loss:0.5949887488647175\n",
      "train loss:0.523791924564301\n",
      "train loss:0.5024502482885495\n",
      "train loss:0.3936392417868144\n",
      "train loss:0.5371544669095794\n",
      "train loss:0.391158637498027\n",
      "train loss:0.4070567730222052\n",
      "train loss:0.530157011886399\n",
      "train loss:0.5115165034155651\n",
      "train loss:0.39808171606190884\n",
      "train loss:0.46930173946606574\n",
      "train loss:0.4410876091973515\n",
      "train loss:0.6129490705770447\n",
      "train loss:0.5504124876062924\n",
      "train loss:0.5598904893362748\n",
      "train loss:0.41604138746924624\n",
      "train loss:0.4859103361668407\n",
      "train loss:0.504625445914054\n",
      "train loss:0.48907443988125593\n",
      "train loss:0.36617277031653017\n",
      "train loss:0.3745803065152423\n",
      "train loss:0.44345673323565377\n",
      "train loss:0.4724214780358381\n",
      "train loss:0.39905661690837496\n",
      "train loss:0.6135419763618589\n",
      "train loss:0.548444381590027\n",
      "train loss:0.5048263409050664\n",
      "train loss:0.38827673817985653\n",
      "train loss:0.4624062621968583\n",
      "train loss:0.4158469077352459\n",
      "train loss:0.34697737658117733\n",
      "train loss:0.37905734329168994\n",
      "train loss:0.4070657560408009\n",
      "train loss:0.5144155151845863\n",
      "train loss:0.6030832320332382\n",
      "train loss:0.42858839943995725\n",
      "train loss:0.3719418277121856\n",
      "train loss:0.35483071559099244\n",
      "train loss:0.40576471402806297\n",
      "train loss:0.6135975684076027\n",
      "train loss:0.40059225733826354\n",
      "train loss:0.5609209554357681\n",
      "train loss:0.5564611294027371\n",
      "train loss:0.3781768888555156\n",
      "train loss:0.36622651348471374\n",
      "train loss:0.3789424407088787\n",
      "train loss:0.42374678208498706\n",
      "train loss:0.47113014607733694\n",
      "train loss:0.4661042717336087\n",
      "train loss:0.617447045431614\n",
      "train loss:0.5077237403197777\n",
      "train loss:0.4339715216768474\n",
      "train loss:0.5711822317294701\n",
      "train loss:0.41846396738096286\n",
      "train loss:0.34501007332223166\n",
      "train loss:0.48057848286817373\n",
      "train loss:0.40397659267419633\n",
      "train loss:0.42345743956333337\n",
      "train loss:0.622837764659639\n",
      "train loss:0.4245842632327619\n",
      "train loss:0.4719248445067407\n",
      "train loss:0.44574304989931635\n",
      "train loss:0.3174508162203881\n",
      "train loss:0.3682575924577105\n",
      "train loss:0.543403731476145\n",
      "train loss:0.3770250546952414\n",
      "train loss:0.5349377320601888\n",
      "train loss:0.5088203014456583\n",
      "train loss:0.5228912659110514\n",
      "train loss:0.4956402471177851\n",
      "train loss:0.7254828477240868\n",
      "train loss:0.37715945123396755\n",
      "train loss:0.4967383460234131\n",
      "train loss:0.430952067859389\n",
      "train loss:0.5512972488342366\n",
      "train loss:0.45406374638283553\n",
      "train loss:0.33470543046270385\n",
      "train loss:0.4393798122269753\n",
      "train loss:0.49354586633583497\n",
      "train loss:0.45903901663226543\n",
      "train loss:0.3602196615454485\n",
      "train loss:0.5662840743760372\n",
      "train loss:0.3910088334886911\n",
      "train loss:0.43351593084011664\n",
      "train loss:0.4834507893838154\n",
      "train loss:0.44940719734675355\n",
      "train loss:0.42073950018561024\n",
      "train loss:0.46104912263756115\n",
      "train loss:0.4797948687144096\n",
      "train loss:0.38219695850463103\n",
      "train loss:0.49588819139963236\n",
      "train loss:0.48466685610445137\n",
      "train loss:0.4204139720759411\n",
      "train loss:0.4097096270577987\n",
      "train loss:0.5124092718315326\n",
      "train loss:0.3235880121244225\n",
      "train loss:0.42677447729227713\n",
      "train loss:0.36521727637041557\n",
      "train loss:0.44680150267749985\n",
      "train loss:0.4785112402389637\n",
      "train loss:0.47338622570983196\n",
      "train loss:0.3604948479617597\n",
      "train loss:0.4531129798477895\n",
      "train loss:0.49537512125380806\n",
      "train loss:0.3854025979485216\n",
      "train loss:0.44876112594307566\n",
      "train loss:0.43324920248922383\n",
      "train loss:0.4811761438744871\n",
      "train loss:0.4099019268518814\n",
      "train loss:0.4005230987329395\n",
      "train loss:0.4645988455543007\n",
      "train loss:0.46177911660427684\n",
      "train loss:0.41198955613752\n",
      "train loss:0.4806438926709828\n",
      "train loss:0.4515270916426696\n",
      "train loss:0.4114974931552353\n",
      "train loss:0.46940826674781505\n",
      "train loss:0.47038164353342254\n",
      "train loss:0.4059596008455739\n",
      "train loss:0.5414592669816037\n",
      "train loss:0.44475822952890065\n",
      "train loss:0.38539584891969797\n",
      "train loss:0.476318217363483\n",
      "train loss:0.41122281536147887\n",
      "train loss:0.3412669882540032\n",
      "train loss:0.5082401201078534\n",
      "train loss:0.3822089479181031\n",
      "train loss:0.45938498562140895\n",
      "train loss:0.49692695316107716\n",
      "train loss:0.5072847415650185\n",
      "train loss:0.4999877785378133\n",
      "train loss:0.3947276620806876\n",
      "train loss:0.484461247143896\n",
      "train loss:0.5016834102466329\n",
      "train loss:0.4743055082636408\n",
      "train loss:0.503108026739097\n",
      "train loss:0.5239278442866577\n",
      "train loss:0.6299797417064156\n",
      "train loss:0.4373898564722054\n",
      "train loss:0.506307744740594\n",
      "train loss:0.4319697067085566\n",
      "train loss:0.32033473165707155\n",
      "train loss:0.33601274036227674\n",
      "train loss:0.41245625275373143\n",
      "train loss:0.4607076714070004\n",
      "train loss:0.47121636424014307\n",
      "train loss:0.5710299040365239\n",
      "train loss:0.429767196077306\n",
      "train loss:0.40921142953468886\n",
      "train loss:0.4724714691825922\n",
      "train loss:0.4762119301721108\n",
      "train loss:0.40705938099872496\n",
      "train loss:0.5053757331227101\n",
      "train loss:0.42244313054355187\n",
      "train loss:0.5153572279276866\n",
      "train loss:0.44697958310036895\n",
      "train loss:0.39292358610788247\n",
      "train loss:0.5350364701113329\n",
      "train loss:0.42320362093098995\n",
      "train loss:0.47253521061433207\n",
      "train loss:0.4649368900921772\n",
      "train loss:0.5535693651595726\n",
      "train loss:0.48189562925834645\n",
      "train loss:0.4679433475997768\n",
      "train loss:0.3793851822740488\n",
      "train loss:0.41096444733244764\n",
      "train loss:0.5339817349935888\n",
      "train loss:0.40517535859516884\n",
      "train loss:0.5120223972447571\n",
      "train loss:0.4419668055252629\n",
      "train loss:0.36713100325074544\n",
      "train loss:0.4918504932490492\n",
      "train loss:0.42092056561756513\n",
      "train loss:0.4998142270157288\n",
      "train loss:0.4684810553167152\n",
      "train loss:0.4609807157994688\n",
      "train loss:0.35963010704482995\n",
      "train loss:0.36348482011497985\n",
      "train loss:0.39137610128245615\n",
      "train loss:0.5298193578053979\n",
      "train loss:0.5222227957346404\n",
      "train loss:0.40881579937205165\n",
      "train loss:0.47195566358766267\n",
      "train loss:0.39039572232720104\n",
      "train loss:0.47213936949176616\n",
      "train loss:0.31140021537177315\n",
      "train loss:0.4438749134866016\n",
      "train loss:0.6096063492386438\n",
      "train loss:0.49524627186707354\n",
      "train loss:0.3994111539958899\n",
      "train loss:0.5390143203473516\n",
      "train loss:0.5100144286408796\n",
      "train loss:0.511229189634467\n",
      "train loss:0.5860365043343174\n",
      "train loss:0.4106527582684853\n",
      "train loss:0.4219021910838771\n",
      "train loss:0.4164636485350512\n",
      "train loss:0.35005397177255676\n",
      "train loss:0.4345709654751368\n",
      "train loss:0.4119419316919721\n",
      "train loss:0.3796294173855217\n",
      "train loss:0.5118359919887738\n",
      "train loss:0.3966103594087001\n",
      "train loss:0.4882509991451158\n",
      "train loss:0.4343636418274407\n",
      "train loss:0.4282142101253496\n",
      "train loss:0.4013878052446067\n",
      "train loss:0.47895850381431404\n",
      "train loss:0.4384690876679286\n",
      "train loss:0.38584563351551115\n",
      "train loss:0.5355680847515486\n",
      "train loss:0.46858441140559115\n",
      "train loss:0.612001704173967\n",
      "train loss:0.5192074257704994\n",
      "train loss:0.4382396921389472\n",
      "train loss:0.49103568001008874\n",
      "train loss:0.4693125224464451\n",
      "train loss:0.5039975654713611\n",
      "train loss:0.4733659142816405\n",
      "train loss:0.42976866363510524\n",
      "train loss:0.41640756123796485\n",
      "train loss:0.47661529246466455\n",
      "train loss:0.3677328486111176\n",
      "train loss:0.5437225137069108\n",
      "train loss:0.44271294644345\n",
      "train loss:0.4315554995919889\n",
      "train loss:0.5419662110260148\n",
      "train loss:0.6255799947486421\n",
      "train loss:0.5013774020533149\n",
      "train loss:0.40963987870092955\n",
      "train loss:0.34273316501857315\n",
      "train loss:0.44170812955777145\n",
      "train loss:0.49738906129710636\n",
      "train loss:0.4114828784980978\n",
      "train loss:0.45716701295893203\n",
      "train loss:0.5274750292740343\n",
      "train loss:0.4193375409383007\n",
      "train loss:0.4200903925137339\n",
      "train loss:0.42097192915617876\n",
      "train loss:0.5315440543567422\n",
      "train loss:0.3608024516019511\n",
      "train loss:0.443564133704944\n",
      "train loss:0.4875186202419339\n",
      "train loss:0.36844257401993014\n",
      "train loss:0.40947589626138814\n",
      "train loss:0.5070607968020937\n",
      "train loss:0.46291757342846346\n",
      "train loss:0.4408272583670245\n",
      "train loss:0.3700886957242413\n",
      "train loss:0.3949110477601183\n",
      "train loss:0.3684471187599614\n",
      "train loss:0.3016815569716488\n",
      "train loss:0.47062707655002617\n",
      "train loss:0.3507368893495039\n",
      "train loss:0.5740335483252397\n",
      "train loss:0.49488383241481515\n",
      "train loss:0.46063930285387833\n",
      "train loss:0.4454479201501053\n",
      "train loss:0.5486139074413537\n",
      "train loss:0.35131974008746375\n",
      "train loss:0.4086618281738296\n",
      "train loss:0.5963675077191959\n",
      "train loss:0.4460166892295115\n",
      "train loss:0.5209775359163316\n",
      "train loss:0.4566076815665931\n",
      "train loss:0.4216798750991952\n",
      "train loss:0.33371672241793654\n",
      "train loss:0.364037297330432\n",
      "train loss:0.32617489457985693\n",
      "train loss:0.4561716259178626\n",
      "train loss:0.45781025630652955\n",
      "train loss:0.4888456862189164\n",
      "train loss:0.5187631490446509\n",
      "train loss:0.4536652251815804\n",
      "train loss:0.6884061720438183\n",
      "train loss:0.5126496259246274\n",
      "train loss:0.40214439899709187\n",
      "train loss:0.4766142220582995\n",
      "train loss:0.4165766413629002\n",
      "train loss:0.4355123392897626\n",
      "train loss:0.40096962422347376\n",
      "train loss:0.3776348744998785\n",
      "train loss:0.5286901571513527\n",
      "train loss:0.4516471626092587\n",
      "train loss:0.5106375124599812\n",
      "train loss:0.41683003041808064\n",
      "train loss:0.5324721281164907\n",
      "train loss:0.39363695079082406\n",
      "train loss:0.4762248242246969\n",
      "train loss:0.5381464597633758\n",
      "train loss:0.5165812347672574\n",
      "train loss:0.5880221589873197\n",
      "train loss:0.44725308224667804\n",
      "train loss:0.4152944198076432\n",
      "train loss:0.5144421222859731\n",
      "train loss:0.4697537472162333\n",
      "train loss:0.42827481476092677\n",
      "train loss:0.33562315895755773\n",
      "train loss:0.5815108027357415\n",
      "train loss:0.32150935327859465\n",
      "train loss:0.5089479809773332\n",
      "train loss:0.3871060939994695\n",
      "train loss:0.40522307206878644\n",
      "train loss:0.4630478954830076\n",
      "train loss:0.6166767905432438\n",
      "train loss:0.33377150644656445\n",
      "train loss:0.4131729310950258\n",
      "train loss:0.40739174461845595\n",
      "train loss:0.3688172952971233\n",
      "train loss:0.4006750253128718\n",
      "train loss:0.41255271659353804\n",
      "train loss:0.49132772721696727\n",
      "train loss:0.519834089178817\n",
      "train loss:0.5532252747367731\n",
      "train loss:0.45335551388734224\n",
      "train loss:0.3673698888416464\n",
      "train loss:0.476965886137922\n",
      "train loss:0.3726185476661295\n",
      "train loss:0.383515896112381\n",
      "train loss:0.473470463099822\n",
      "train loss:0.3317954055862666\n",
      "train loss:0.39617418881471456\n",
      "train loss:0.35377541802484375\n",
      "train loss:0.5313641408706506\n",
      "train loss:0.31509421765064005\n",
      "train loss:0.5096987369023399\n",
      "train loss:0.4926465992068231\n",
      "train loss:0.42526747420889793\n",
      "train loss:0.4737304469450717\n",
      "train loss:0.44805372062424126\n",
      "=== epoch:12, train acc:0.8, test acc:0.807 ===\n",
      "train loss:0.43819796323110016\n",
      "train loss:0.4747220763492562\n",
      "train loss:0.39504287127497373\n",
      "train loss:0.3541307349315238\n",
      "train loss:0.4980171076470284\n",
      "train loss:0.44570628491040226\n",
      "train loss:0.4743605573987983\n",
      "train loss:0.4030757688278612\n",
      "train loss:0.4051088690365199\n",
      "train loss:0.47837297434286863\n",
      "train loss:0.5677575003673909\n",
      "train loss:0.41124054852087805\n",
      "train loss:0.3052567031360861\n",
      "train loss:0.3812594093623224\n",
      "train loss:0.6130407831416944\n",
      "train loss:0.4629287228193349\n",
      "train loss:0.40210651810262166\n",
      "train loss:0.3737289829066414\n",
      "train loss:0.44728719447177734\n",
      "train loss:0.36834489368357076\n",
      "train loss:0.39910069447756974\n",
      "train loss:0.5150622493889511\n",
      "train loss:0.39878186531491605\n",
      "train loss:0.38973000813186526\n",
      "train loss:0.4450518130275789\n",
      "train loss:0.5666984287259573\n",
      "train loss:0.39006320179540593\n",
      "train loss:0.5327075452160791\n",
      "train loss:0.4075128461563582\n",
      "train loss:0.5255397252226173\n",
      "train loss:0.4293811111298199\n",
      "train loss:0.45365307296995183\n",
      "train loss:0.40239769902606576\n",
      "train loss:0.47311083709409274\n",
      "train loss:0.4739726235058964\n",
      "train loss:0.37983969814042273\n",
      "train loss:0.3455892085644087\n",
      "train loss:0.4541011062639416\n",
      "train loss:0.5185421945172717\n",
      "train loss:0.5621641321887207\n",
      "train loss:0.3879671417459079\n",
      "train loss:0.43107445258599947\n",
      "train loss:0.49440324486665005\n",
      "train loss:0.4393348545170759\n",
      "train loss:0.3963845218035831\n",
      "train loss:0.3983157566520543\n",
      "train loss:0.533692893964216\n",
      "train loss:0.5414099483561987\n",
      "train loss:0.3952575880912657\n",
      "train loss:0.5287703138887497\n",
      "train loss:0.3477770905581593\n",
      "train loss:0.47221437675473077\n",
      "train loss:0.3862108919823193\n",
      "train loss:0.4270908110934014\n",
      "train loss:0.5144849134574829\n",
      "train loss:0.5249670553478859\n",
      "train loss:0.36579184443882035\n",
      "train loss:0.3828976384247681\n",
      "train loss:0.4141594477333184\n",
      "train loss:0.3419795572952429\n",
      "train loss:0.4304904279714174\n",
      "train loss:0.4515834594809773\n",
      "train loss:0.35153212112553783\n",
      "train loss:0.523442301238747\n",
      "train loss:0.48609182103224086\n",
      "train loss:0.4863062364821473\n",
      "train loss:0.43383110178765505\n",
      "train loss:0.518917359608824\n",
      "train loss:0.5250893465159033\n",
      "train loss:0.4455956265527354\n",
      "train loss:0.4672881162262827\n",
      "train loss:0.471215644942822\n",
      "train loss:0.5750850667094674\n",
      "train loss:0.40600014503851944\n",
      "train loss:0.4924462547588105\n",
      "train loss:0.4109035615096805\n",
      "train loss:0.3843615319286511\n",
      "train loss:0.6295116865569524\n",
      "train loss:0.6204515945128193\n",
      "train loss:0.49984017567549804\n",
      "train loss:0.39643211364293074\n",
      "train loss:0.5823927206019299\n",
      "train loss:0.4904316994195454\n",
      "train loss:0.3638189388774342\n",
      "train loss:0.6499588161515514\n",
      "train loss:0.528341847990945\n",
      "train loss:0.5010641671916052\n",
      "train loss:0.39167195935844307\n",
      "train loss:0.35319270846874423\n",
      "train loss:0.48204665299170235\n",
      "train loss:0.42040625416263727\n",
      "train loss:0.4258469498924173\n",
      "train loss:0.31630629990909614\n",
      "train loss:0.452467246082998\n",
      "train loss:0.4915431986056857\n",
      "train loss:0.3839134848818505\n",
      "train loss:0.542913790323887\n",
      "train loss:0.5009495548827069\n",
      "train loss:0.4006252467136217\n",
      "train loss:0.3895949176030848\n",
      "train loss:0.2583015309286517\n",
      "train loss:0.378070861364555\n",
      "train loss:0.5713436917313552\n",
      "train loss:0.406089373050888\n",
      "train loss:0.45935202011187476\n",
      "train loss:0.36136117543731217\n",
      "train loss:0.3929270976137093\n",
      "train loss:0.5130469327152801\n",
      "train loss:0.47142362820794514\n",
      "train loss:0.4664166878228071\n",
      "train loss:0.3889541630439309\n",
      "train loss:0.48185160349973993\n",
      "train loss:0.5163283204619645\n",
      "train loss:0.423382800162465\n",
      "train loss:0.4387396929233001\n",
      "train loss:0.4263890944566922\n",
      "train loss:0.344014774414542\n",
      "train loss:0.43143288601418467\n",
      "train loss:0.4542207247911345\n",
      "train loss:0.45066564849314283\n",
      "train loss:0.43327005855809003\n",
      "train loss:0.5284299432309982\n",
      "train loss:0.4945811989855955\n",
      "train loss:0.5140797556803054\n",
      "train loss:0.4606165539115765\n",
      "train loss:0.4501442775170681\n",
      "train loss:0.47352172715640395\n",
      "train loss:0.4992651203783535\n",
      "train loss:0.39697109832957594\n",
      "train loss:0.38143923995524615\n",
      "train loss:0.4738206178520663\n",
      "train loss:0.46671691453748737\n",
      "train loss:0.44134061188895785\n",
      "train loss:0.4892673907304052\n",
      "train loss:0.3868024694730202\n",
      "train loss:0.5618588640834424\n",
      "train loss:0.5913495916878828\n",
      "train loss:0.5951787434970938\n",
      "train loss:0.47692969290180115\n",
      "train loss:0.5188162560624436\n",
      "train loss:0.42087514562392087\n",
      "train loss:0.39976736074951136\n",
      "train loss:0.37312186164152833\n",
      "train loss:0.4756779770859038\n",
      "train loss:0.4844952604549351\n",
      "train loss:0.39745336457390373\n",
      "train loss:0.4230383647493062\n",
      "train loss:0.3906239885551095\n",
      "train loss:0.3534023637035952\n",
      "train loss:0.5235946636842763\n",
      "train loss:0.5557408455420813\n",
      "train loss:0.4233043456738283\n",
      "train loss:0.44841097974918825\n",
      "train loss:0.29839156389285526\n",
      "train loss:0.5188909651321745\n",
      "train loss:0.4155793554599347\n",
      "train loss:0.48261217613248625\n",
      "train loss:0.6565224670716031\n",
      "train loss:0.30748052277982807\n",
      "train loss:0.42240177008137764\n",
      "train loss:0.39322663504960526\n",
      "train loss:0.413393915919063\n",
      "train loss:0.4458733773163989\n",
      "train loss:0.42483547157684515\n",
      "train loss:0.36493213348647024\n",
      "train loss:0.46276742400092546\n",
      "train loss:0.4003193251737619\n",
      "train loss:0.49381314412911403\n",
      "train loss:0.3707154061634758\n",
      "train loss:0.49160208409405975\n",
      "train loss:0.5714039886565819\n",
      "train loss:0.34945488228332616\n",
      "train loss:0.5856060452005019\n",
      "train loss:0.5279941814874227\n",
      "train loss:0.3628004123113962\n",
      "train loss:0.517650056288497\n",
      "train loss:0.46827936079990445\n",
      "train loss:0.29748960021677123\n",
      "train loss:0.2582601559808963\n",
      "train loss:0.480564169994944\n",
      "train loss:0.47949863457921815\n",
      "train loss:0.4594528565404379\n",
      "train loss:0.3674080452805799\n",
      "train loss:0.5167454354397856\n",
      "train loss:0.6387498402219807\n",
      "train loss:0.6117645818018219\n",
      "train loss:0.32196466138819474\n",
      "train loss:0.3947529104432654\n",
      "train loss:0.47985125978386056\n",
      "train loss:0.539412375842468\n",
      "train loss:0.3795919709945288\n",
      "train loss:0.4683590790893465\n",
      "train loss:0.46007458030001175\n",
      "train loss:0.36749700741913244\n",
      "train loss:0.47438561870986473\n",
      "train loss:0.5101621809047627\n",
      "train loss:0.3203187913132745\n",
      "train loss:0.31529922471177796\n",
      "train loss:0.40203517807873923\n",
      "train loss:0.4932020794703048\n",
      "train loss:0.4006933589181651\n",
      "train loss:0.2717635757788357\n",
      "train loss:0.4139711309686821\n",
      "train loss:0.40189069260468757\n",
      "train loss:0.39962007845783437\n",
      "train loss:0.5701368593374736\n",
      "train loss:0.5399928669266431\n",
      "train loss:0.4012693286653197\n",
      "train loss:0.3165879195543072\n",
      "train loss:0.39953072429033587\n",
      "train loss:0.43053559424698884\n",
      "train loss:0.45625753823142673\n",
      "train loss:0.3343385929337965\n",
      "train loss:0.41543768486940225\n",
      "train loss:0.5886804010196218\n",
      "train loss:0.45549502681517345\n",
      "train loss:0.3401626413390961\n",
      "train loss:0.433324738355624\n",
      "train loss:0.406679283331013\n",
      "train loss:0.4374644342061063\n",
      "train loss:0.40142958850680793\n",
      "train loss:0.47915860371769364\n",
      "train loss:0.4902527775008691\n",
      "train loss:0.5904543780503642\n",
      "train loss:0.5238138923616658\n",
      "train loss:0.35239489562184273\n",
      "train loss:0.7341890329542298\n",
      "train loss:0.49388548134506927\n",
      "train loss:0.41696440905645105\n",
      "train loss:0.4203369076039954\n",
      "train loss:0.3509234902442204\n",
      "train loss:0.45809105637118447\n",
      "train loss:0.4036664564439515\n",
      "train loss:0.3416347309806156\n",
      "train loss:0.39423080463862087\n",
      "train loss:0.4777672946802071\n",
      "train loss:0.42642136356020877\n",
      "train loss:0.44831229379083853\n",
      "train loss:0.4618682409354147\n",
      "train loss:0.5785572215282486\n",
      "train loss:0.345944508365222\n",
      "train loss:0.3952529724633762\n",
      "train loss:0.42528178603628375\n",
      "train loss:0.40060829741792475\n",
      "train loss:0.4896517172849532\n",
      "train loss:0.4114682307466723\n",
      "train loss:0.4752489237881627\n",
      "train loss:0.4956604154838087\n",
      "train loss:0.5229513627314826\n",
      "train loss:0.36815351918708056\n",
      "train loss:0.5772329652468161\n",
      "train loss:0.4684654377789424\n",
      "train loss:0.47529799166421755\n",
      "train loss:0.35538387417195877\n",
      "train loss:0.3730900102932248\n",
      "train loss:0.34818567517394894\n",
      "train loss:0.5982482679015536\n",
      "train loss:0.5792024367110112\n",
      "train loss:0.43453403295450654\n",
      "train loss:0.5412484642581568\n",
      "train loss:0.45623991572217343\n",
      "train loss:0.40370717551447305\n",
      "train loss:0.4979125629433076\n",
      "train loss:0.5557250320508895\n",
      "train loss:0.4219230817279949\n",
      "train loss:0.40778274478680415\n",
      "train loss:0.48473989019128255\n",
      "train loss:0.49635408132689596\n",
      "train loss:0.46238182018185264\n",
      "train loss:0.4608426127184827\n",
      "train loss:0.5062673592831901\n",
      "train loss:0.5349366604779979\n",
      "train loss:0.5149388618590128\n",
      "train loss:0.44624654772543304\n",
      "train loss:0.6529112931772727\n",
      "train loss:0.38695062999940655\n",
      "train loss:0.39827128228193637\n",
      "train loss:0.5221694281961473\n",
      "train loss:0.40218159491619865\n",
      "train loss:0.34382633751141456\n",
      "train loss:0.48480046837111224\n",
      "train loss:0.44604801356337087\n",
      "train loss:0.4169605546250168\n",
      "train loss:0.4095391676550989\n",
      "train loss:0.5555991972586575\n",
      "train loss:0.41430351532173476\n",
      "train loss:0.3863688266379876\n",
      "train loss:0.4041139952586995\n",
      "train loss:0.3830439483489353\n",
      "train loss:0.43526589666104276\n",
      "train loss:0.39442467751276067\n",
      "train loss:0.49291516525054435\n",
      "train loss:0.6141774095643051\n",
      "train loss:0.5127733327007404\n",
      "train loss:0.49057715730528473\n",
      "train loss:0.45398435066475673\n",
      "train loss:0.5068724424904199\n",
      "train loss:0.4974889334105869\n",
      "train loss:0.4280180634420851\n",
      "train loss:0.5257555667407998\n",
      "train loss:0.511699428794694\n",
      "train loss:0.5030969641631678\n",
      "train loss:0.3846104661702421\n",
      "train loss:0.4365663545858729\n",
      "train loss:0.679596851630895\n",
      "train loss:0.522306654118405\n",
      "train loss:0.4348620978480081\n",
      "train loss:0.3939365438065052\n",
      "train loss:0.40329765036768916\n",
      "train loss:0.40071253102762505\n",
      "train loss:0.37270414236168486\n",
      "train loss:0.4474859056922647\n",
      "train loss:0.4221363940974544\n",
      "train loss:0.48004149399393475\n",
      "train loss:0.5452466975725437\n",
      "train loss:0.45397469878536134\n",
      "train loss:0.44653851147626233\n",
      "train loss:0.3821837937515094\n",
      "train loss:0.5023892159045669\n",
      "train loss:0.35956365630833964\n",
      "train loss:0.46248570300713226\n",
      "train loss:0.44311981284729896\n",
      "train loss:0.4435052092247698\n",
      "train loss:0.38503478453621903\n",
      "train loss:0.3685281010585119\n",
      "train loss:0.5118993575382297\n",
      "train loss:0.44358434593839535\n",
      "train loss:0.4548034103783337\n",
      "train loss:0.43469204548480705\n",
      "train loss:0.424324654068073\n",
      "train loss:0.5571693528797691\n",
      "train loss:0.3562790336996112\n",
      "train loss:0.45902003545188336\n",
      "train loss:0.38125718485706345\n",
      "train loss:0.4682915690999285\n",
      "train loss:0.5753516103563089\n",
      "train loss:0.4545043032085415\n",
      "train loss:0.47964594145431677\n",
      "train loss:0.4267233788593661\n",
      "train loss:0.5009619752224187\n",
      "train loss:0.3821092628755526\n",
      "train loss:0.41639554095980275\n",
      "train loss:0.48706465128062787\n",
      "train loss:0.37488536054145855\n",
      "train loss:0.44002781754062953\n",
      "train loss:0.4966015871189565\n",
      "train loss:0.5003972390111869\n",
      "train loss:0.36307040931907686\n",
      "train loss:0.5120277931996442\n",
      "train loss:0.46369264693154233\n",
      "train loss:0.42086724570093814\n",
      "train loss:0.53069486969262\n",
      "train loss:0.41360264532068297\n",
      "train loss:0.4792817502158394\n",
      "train loss:0.39078589004704006\n",
      "train loss:0.416881847008787\n",
      "train loss:0.4306518908023208\n",
      "train loss:0.5238943379931223\n",
      "train loss:0.5938771731093165\n",
      "train loss:0.5081467660046972\n",
      "train loss:0.413687786882442\n",
      "train loss:0.5101646884991742\n",
      "train loss:0.4555891030719749\n",
      "train loss:0.44677586471045194\n",
      "train loss:0.5423824518569519\n",
      "train loss:0.4739538262623774\n",
      "train loss:0.5386686368647965\n",
      "train loss:0.48154811481849774\n",
      "train loss:0.4611244154576814\n",
      "train loss:0.4628409632886599\n",
      "train loss:0.47887852680513743\n",
      "train loss:0.436250947172873\n",
      "train loss:0.5699578196965199\n",
      "train loss:0.538889035618296\n",
      "train loss:0.4425599509269481\n",
      "train loss:0.4187699410871906\n",
      "train loss:0.45627379414944885\n",
      "train loss:0.5292508263423327\n",
      "train loss:0.42005277246024336\n",
      "train loss:0.3749796885801176\n",
      "train loss:0.35828335962745506\n",
      "train loss:0.40857714894888053\n",
      "train loss:0.4060326746437397\n",
      "train loss:0.3916367837350458\n",
      "train loss:0.4755135444111273\n",
      "train loss:0.3389547677615795\n",
      "train loss:0.35152132787504753\n",
      "train loss:0.4214105204968568\n",
      "train loss:0.36614997762789353\n",
      "train loss:0.36160621007152005\n",
      "train loss:0.4674506925670081\n",
      "train loss:0.41818891800110447\n",
      "train loss:0.41956588557681473\n",
      "train loss:0.3952361363344521\n",
      "train loss:0.46448220756038666\n",
      "train loss:0.4137430793832849\n",
      "train loss:0.43062118902955\n",
      "train loss:0.4413742748532702\n",
      "train loss:0.4680109184447718\n",
      "train loss:0.6783983667384547\n",
      "train loss:0.4913607588335111\n",
      "train loss:0.5215819716303007\n",
      "train loss:0.4685073447473707\n",
      "train loss:0.3618712398605467\n",
      "train loss:0.30929908988367266\n",
      "train loss:0.45711224478698353\n",
      "train loss:0.4690952854982312\n",
      "train loss:0.40534247745970303\n",
      "train loss:0.4668954763114981\n",
      "train loss:0.4381974391820709\n",
      "train loss:0.6147430990738194\n",
      "train loss:0.390237947927138\n",
      "train loss:0.43766705608922024\n",
      "train loss:0.42351561220539063\n",
      "train loss:0.4627140506097988\n",
      "train loss:0.38895699646292287\n",
      "train loss:0.5552150751025762\n",
      "train loss:0.5457659806882406\n",
      "train loss:0.40135040355307267\n",
      "train loss:0.47204006881937405\n",
      "train loss:0.43123279187594865\n",
      "train loss:0.3714065526730067\n",
      "train loss:0.4220886646259549\n",
      "train loss:0.4084394910111355\n",
      "train loss:0.48978626554666677\n",
      "train loss:0.5626831923573775\n",
      "train loss:0.4392182926560608\n",
      "train loss:0.40974917878685224\n",
      "train loss:0.4196892153093247\n",
      "train loss:0.5535071859239774\n",
      "train loss:0.5468263286305095\n",
      "train loss:0.6081058440964314\n",
      "train loss:0.6163402612469079\n",
      "train loss:0.3571267259481637\n",
      "train loss:0.45080537557752015\n",
      "train loss:0.4848307009054563\n",
      "train loss:0.45161173809771027\n",
      "train loss:0.367241146393501\n",
      "train loss:0.4541800725870827\n",
      "train loss:0.46399531135566974\n",
      "train loss:0.5343957903668417\n",
      "train loss:0.4317830237961935\n",
      "train loss:0.3406790412163651\n",
      "train loss:0.4478898903053758\n",
      "train loss:0.3369556775931464\n",
      "train loss:0.577055451013388\n",
      "train loss:0.4517358782349266\n",
      "train loss:0.3413733629602192\n",
      "train loss:0.5679134852698065\n",
      "train loss:0.5020975949217275\n",
      "train loss:0.38949895465762485\n",
      "train loss:0.49588648743105784\n",
      "train loss:0.41182573848637605\n",
      "train loss:0.42865104381023017\n",
      "train loss:0.49172984289148225\n",
      "train loss:0.4515131480547743\n",
      "train loss:0.40008343971916815\n",
      "train loss:0.2978151362734091\n",
      "train loss:0.4484004422608068\n",
      "train loss:0.3513495869814186\n",
      "train loss:0.5728397940073677\n",
      "train loss:0.2714704099641093\n",
      "train loss:0.2987106839978444\n",
      "train loss:0.5838989237751031\n",
      "train loss:0.35120834493097475\n",
      "train loss:0.33483129632214187\n",
      "train loss:0.5426908542523566\n",
      "train loss:0.4694334112683181\n",
      "train loss:0.4207466873940694\n",
      "train loss:0.5290719084861673\n",
      "train loss:0.4405706698805419\n",
      "train loss:0.5382317716026213\n",
      "train loss:0.41779126661286675\n",
      "train loss:0.5229047124101684\n",
      "train loss:0.3724373489833734\n",
      "train loss:0.42593400835212625\n",
      "train loss:0.4715913573101523\n",
      "train loss:0.41214824510925424\n",
      "train loss:0.38939381858902533\n",
      "train loss:0.3071533572590425\n",
      "train loss:0.32595784934936056\n",
      "train loss:0.38546184990993865\n",
      "train loss:0.3404589664976016\n",
      "train loss:0.3719015097364601\n",
      "train loss:0.4878958003839735\n",
      "train loss:0.3480050431612781\n",
      "train loss:0.3799569115998615\n",
      "train loss:0.49999074638206037\n",
      "train loss:0.4546770777701083\n",
      "train loss:0.3901963841359715\n",
      "train loss:0.326318165464767\n",
      "train loss:0.448326662927224\n",
      "train loss:0.29486504346767045\n",
      "train loss:0.4763359688124717\n",
      "train loss:0.45902471502987036\n",
      "train loss:0.45569606364126714\n",
      "train loss:0.40249993883296065\n",
      "train loss:0.35136537297273684\n",
      "train loss:0.5381591676948847\n",
      "train loss:0.3870416333149572\n",
      "train loss:0.4120111360533084\n",
      "train loss:0.4019560366088742\n",
      "train loss:0.5095517678724438\n",
      "train loss:0.35968206503405203\n",
      "train loss:0.3301266962445426\n",
      "train loss:0.4185279374590963\n",
      "train loss:0.38776183481909954\n",
      "train loss:0.31349556963239117\n",
      "train loss:0.40094239617421\n",
      "train loss:0.3921956304200208\n",
      "train loss:0.3851543008410752\n",
      "train loss:0.5283583063982489\n",
      "train loss:0.3014033799742029\n",
      "train loss:0.4186448520005229\n",
      "train loss:0.4144560484825393\n",
      "train loss:0.40090800566454876\n",
      "train loss:0.38369350255962104\n",
      "train loss:0.3743238403596574\n",
      "train loss:0.591233999753618\n",
      "train loss:0.49624578203304537\n",
      "train loss:0.5294386350595653\n",
      "train loss:0.3453550198479634\n",
      "train loss:0.5180014688628273\n",
      "train loss:0.42298571125825385\n",
      "train loss:0.3585311177972988\n",
      "train loss:0.5878607215714977\n",
      "train loss:0.5401686357109162\n",
      "train loss:0.4221702723705741\n",
      "train loss:0.31877767944201646\n",
      "train loss:0.5719031936199537\n",
      "train loss:0.3785875981138594\n",
      "train loss:0.40918159495135265\n",
      "train loss:0.4642639720803524\n",
      "train loss:0.4558494663244463\n",
      "train loss:0.5052932462899957\n",
      "train loss:0.3736678962757308\n",
      "train loss:0.40639683738755683\n",
      "train loss:0.39395145230442813\n",
      "train loss:0.5656490904774851\n",
      "train loss:0.33281849222209275\n",
      "train loss:0.4234001428368803\n",
      "train loss:0.39539033965460385\n",
      "train loss:0.36533415135338937\n",
      "train loss:0.3302549671603641\n",
      "train loss:0.4045256181638247\n",
      "train loss:0.3673106077801058\n",
      "train loss:0.3095019361025968\n",
      "train loss:0.4129409873011689\n",
      "train loss:0.3593784917017231\n",
      "train loss:0.5198082956745962\n",
      "train loss:0.44630608424737084\n",
      "train loss:0.40526109909819064\n",
      "train loss:0.3658590097452339\n",
      "train loss:0.40273343967284186\n",
      "train loss:0.5010312315839246\n",
      "train loss:0.3772735679710466\n",
      "train loss:0.6126013778105209\n",
      "train loss:0.3517126681095897\n",
      "train loss:0.3978333777544931\n",
      "train loss:0.39659171199210924\n",
      "train loss:0.3625293924510748\n",
      "train loss:0.3987207717559012\n",
      "train loss:0.3894661985187765\n",
      "train loss:0.36075344058979736\n",
      "train loss:0.6154502952105767\n",
      "train loss:0.5001445552518423\n",
      "train loss:0.42853353297408725\n",
      "train loss:0.41639293547138223\n",
      "train loss:0.42875553154671137\n",
      "train loss:0.3287772643688294\n",
      "train loss:0.384929587781802\n",
      "train loss:0.5157643276280469\n",
      "train loss:0.6304150916255034\n",
      "train loss:0.43325129315613675\n",
      "train loss:0.3951195311651292\n",
      "train loss:0.4304828486775134\n",
      "train loss:0.5996646687763427\n",
      "train loss:0.4192406572953072\n",
      "train loss:0.4043951432964053\n",
      "train loss:0.3190737981720797\n",
      "train loss:0.4354071219365608\n",
      "train loss:0.35456078996022306\n",
      "train loss:0.447718906264221\n",
      "train loss:0.4535701556442408\n",
      "train loss:0.32753534428053277\n",
      "train loss:0.5926349543578872\n",
      "train loss:0.4458730111402497\n",
      "train loss:0.5349655695268083\n",
      "train loss:0.4336323953475299\n",
      "train loss:0.4403461979256963\n",
      "train loss:0.36549188686067724\n",
      "train loss:0.47955978835862484\n",
      "train loss:0.4755467958294086\n",
      "train loss:0.47827365603475064\n",
      "train loss:0.4437701689528803\n",
      "train loss:0.6669010811321228\n",
      "train loss:0.4849116662422276\n",
      "train loss:0.41061462489199185\n",
      "train loss:0.5660125906592376\n",
      "train loss:0.4466317368236306\n",
      "=== epoch:13, train acc:0.811, test acc:0.812 ===\n",
      "train loss:0.410851677976047\n",
      "train loss:0.49278875997758226\n",
      "train loss:0.5222088085945662\n",
      "train loss:0.46568383463971513\n",
      "train loss:0.44381697786099544\n",
      "train loss:0.4249139561261719\n",
      "train loss:0.5006380215689454\n",
      "train loss:0.4380688945498498\n",
      "train loss:0.42963291511107554\n",
      "train loss:0.47785373867689557\n",
      "train loss:0.43086210222316834\n",
      "train loss:0.44053057532078554\n",
      "train loss:0.3978369962449696\n",
      "train loss:0.49132464881757576\n",
      "train loss:0.5339454710940088\n",
      "train loss:0.4244739232798934\n",
      "train loss:0.3694777524033984\n",
      "train loss:0.6084510959490569\n",
      "train loss:0.48578973975682355\n",
      "train loss:0.3677631923037016\n",
      "train loss:0.5310938082018993\n",
      "train loss:0.30056376823536624\n",
      "train loss:0.3886565645305656\n",
      "train loss:0.5751959298933058\n",
      "train loss:0.5237330052734521\n",
      "train loss:0.45216958139778574\n",
      "train loss:0.49266631563104274\n",
      "train loss:0.4186243251370524\n",
      "train loss:0.5245204190015208\n",
      "train loss:0.5835807954510088\n",
      "train loss:0.560158530303428\n",
      "train loss:0.6044425406567132\n",
      "train loss:0.38174485227307575\n",
      "train loss:0.45130988797064553\n",
      "train loss:0.4221142445477221\n",
      "train loss:0.358899460700072\n",
      "train loss:0.4534824655848457\n",
      "train loss:0.5023256159445935\n",
      "train loss:0.38252084755700544\n",
      "train loss:0.3510577610362916\n",
      "train loss:0.498366820646794\n",
      "train loss:0.4182261293313195\n",
      "train loss:0.39263990965968737\n",
      "train loss:0.43707130427524027\n",
      "train loss:0.41022367268151627\n",
      "train loss:0.3103927257985075\n",
      "train loss:0.4578432901407499\n",
      "train loss:0.5358214063100033\n",
      "train loss:0.3943482870478273\n",
      "train loss:0.3608380758805761\n",
      "train loss:0.4815944057683141\n",
      "train loss:0.49184114889454433\n",
      "train loss:0.3842705582332508\n",
      "train loss:0.5885057060230358\n",
      "train loss:0.39032314305794713\n",
      "train loss:0.4595701729906709\n",
      "train loss:0.43799275954067146\n",
      "train loss:0.47175780606197065\n",
      "train loss:0.37035785569345675\n",
      "train loss:0.2963657324230658\n",
      "train loss:0.36687227775667935\n",
      "train loss:0.5335986708349182\n",
      "train loss:0.4983382135231897\n",
      "train loss:0.45762515049874836\n",
      "train loss:0.36421696811639903\n",
      "train loss:0.49357007037716794\n",
      "train loss:0.3824051165847349\n",
      "train loss:0.3332342972250843\n",
      "train loss:0.40564863010878377\n",
      "train loss:0.3673122500642124\n",
      "train loss:0.3480056071447129\n",
      "train loss:0.3295441004408476\n",
      "train loss:0.46466970124198875\n",
      "train loss:0.5284851605920343\n",
      "train loss:0.46344734499154844\n",
      "train loss:0.3628394782891904\n",
      "train loss:0.4745450769164243\n",
      "train loss:0.4103120410875294\n",
      "train loss:0.4473505080383773\n",
      "train loss:0.2658192371430406\n",
      "train loss:0.35133294257053677\n",
      "train loss:0.3350447207326106\n",
      "train loss:0.40862797214102264\n",
      "train loss:0.6137923669992622\n",
      "train loss:0.2848315109626839\n",
      "train loss:0.4264806763617863\n",
      "train loss:0.3340366664377155\n",
      "train loss:0.3606022006033334\n",
      "train loss:0.40387016093163974\n",
      "train loss:0.3317659896847643\n",
      "train loss:0.5025742481238076\n",
      "train loss:0.3538596309422271\n",
      "train loss:0.5910683594397281\n",
      "train loss:0.42914837098717024\n",
      "train loss:0.35005959698181544\n",
      "train loss:0.6284305757733107\n",
      "train loss:0.5448468981619734\n",
      "train loss:0.43906594366479623\n",
      "train loss:0.32589776199523357\n",
      "train loss:0.39245021814717307\n",
      "train loss:0.39400305139115877\n",
      "train loss:0.395018196954697\n",
      "train loss:0.43823778785722584\n",
      "train loss:0.32579207499564494\n",
      "train loss:0.4660563967708551\n",
      "train loss:0.44939157775052346\n",
      "train loss:0.39846640987399573\n",
      "train loss:0.44293582071557674\n",
      "train loss:0.5210758529243512\n",
      "train loss:0.27371247012082367\n",
      "train loss:0.48503620561265853\n",
      "train loss:0.6967443801478542\n",
      "train loss:0.3693777498436707\n",
      "train loss:0.42235847678129024\n",
      "train loss:0.44187358170289803\n",
      "train loss:0.38558937084902195\n",
      "train loss:0.365104512317683\n",
      "train loss:0.3355697112657176\n",
      "train loss:0.36457575380674045\n",
      "train loss:0.40386528695115137\n",
      "train loss:0.39215509497373036\n",
      "train loss:0.4159141909842367\n",
      "train loss:0.4729875866705161\n",
      "train loss:0.41486384127271564\n",
      "train loss:0.40159674250220534\n",
      "train loss:0.4244975322919961\n",
      "train loss:0.3983569869961292\n",
      "train loss:0.35546759827812663\n",
      "train loss:0.4518084759506311\n",
      "train loss:0.48981801321539126\n",
      "train loss:0.4187314310069173\n",
      "train loss:0.35973803813153105\n",
      "train loss:0.31662398117142465\n",
      "train loss:0.5143308686610214\n",
      "train loss:0.4235121747291929\n",
      "train loss:0.38954052596075867\n",
      "train loss:0.3793143115073043\n",
      "train loss:0.3044447936674115\n",
      "train loss:0.3368385968373752\n",
      "train loss:0.48049020040990825\n",
      "train loss:0.37258783273549617\n",
      "train loss:0.41114306136939605\n",
      "train loss:0.540037286479561\n",
      "train loss:0.4147458057080747\n",
      "train loss:0.3622014317326204\n",
      "train loss:0.4911650196972085\n",
      "train loss:0.4386573465303269\n",
      "train loss:0.49848510405201407\n",
      "train loss:0.39037463559085234\n",
      "train loss:0.36681365303479974\n",
      "train loss:0.3427857809813147\n",
      "train loss:0.34892894870370705\n",
      "train loss:0.45390760194409085\n",
      "train loss:0.46780105473782885\n",
      "train loss:0.4075018056987519\n",
      "train loss:0.3133014682143296\n",
      "train loss:0.33813443532828463\n",
      "train loss:0.4046597473078964\n",
      "train loss:0.3993101649241717\n",
      "train loss:0.3944256682137533\n",
      "train loss:0.47034546338904726\n",
      "train loss:0.3532479246466679\n",
      "train loss:0.5195242610087054\n",
      "train loss:0.2954955445929469\n",
      "train loss:0.4072206151013693\n",
      "train loss:0.30667482844439176\n",
      "train loss:0.41789473250666986\n",
      "train loss:0.3557638110238227\n",
      "train loss:0.42697246529548294\n",
      "train loss:0.7264359338062601\n",
      "train loss:0.4851003070800507\n",
      "train loss:0.5675790684987063\n",
      "train loss:0.5119153842125117\n",
      "train loss:0.28102142093834426\n",
      "train loss:0.3652347101697419\n",
      "train loss:0.4735783340381727\n",
      "train loss:0.4369939941715045\n",
      "train loss:0.3362362487844895\n",
      "train loss:0.2775599119682664\n",
      "train loss:0.4861111345193577\n",
      "train loss:0.39211462610453607\n",
      "train loss:0.45922067182845266\n",
      "train loss:0.3970009249230204\n",
      "train loss:0.43547331652390237\n",
      "train loss:0.3842215131404558\n",
      "train loss:0.6216093243637814\n",
      "train loss:0.4566392015093832\n",
      "train loss:0.4840376699422763\n",
      "train loss:0.3493213071946728\n",
      "train loss:0.4308121987591301\n",
      "train loss:0.47669815981454616\n",
      "train loss:0.3469886919434488\n",
      "train loss:0.459405194615911\n",
      "train loss:0.5347707798443598\n",
      "train loss:0.5185085251679428\n",
      "train loss:0.38002039553016403\n",
      "train loss:0.3116402268128733\n",
      "train loss:0.5296028436040877\n",
      "train loss:0.2733478406159346\n",
      "train loss:0.5432893234632923\n",
      "train loss:0.46952966638426785\n",
      "train loss:0.41955854026000433\n",
      "train loss:0.5000830322142131\n",
      "train loss:0.39346999499576696\n",
      "train loss:0.4920058048901918\n",
      "train loss:0.371993899976839\n",
      "train loss:0.39889103026347306\n",
      "train loss:0.3250457458301528\n",
      "train loss:0.3854224948216764\n",
      "train loss:0.41192722955465366\n",
      "train loss:0.48746090663615044\n",
      "train loss:0.31807045643888104\n",
      "train loss:0.5194692909974249\n",
      "train loss:0.39880966395276923\n",
      "train loss:0.3978164556166632\n",
      "train loss:0.43952489478915596\n",
      "train loss:0.36463223944846973\n",
      "train loss:0.38897063786096175\n",
      "train loss:0.5450961762868809\n",
      "train loss:0.39096032346385234\n",
      "train loss:0.43281218786184517\n",
      "train loss:0.3613649993178118\n",
      "train loss:0.3434163990067079\n",
      "train loss:0.4051424863116189\n",
      "train loss:0.38825272003726047\n",
      "train loss:0.4849356264494821\n",
      "train loss:0.4910239667393818\n",
      "train loss:0.3795229068570958\n",
      "train loss:0.41487968912333495\n",
      "train loss:0.38448115827751045\n",
      "train loss:0.3673950472799956\n",
      "train loss:0.37773738161349546\n",
      "train loss:0.28343193711729225\n",
      "train loss:0.45198384306724365\n",
      "train loss:0.4530795451144676\n",
      "train loss:0.3580334478551939\n",
      "train loss:0.3393882325967589\n",
      "train loss:0.40284723052875576\n",
      "train loss:0.36421937661079135\n",
      "train loss:0.36352713285924915\n",
      "train loss:0.45117266133695716\n",
      "train loss:0.48323536896431923\n",
      "train loss:0.37026642310877717\n",
      "train loss:0.5248619070485988\n",
      "train loss:0.3200257387089057\n",
      "train loss:0.46131839127980356\n",
      "train loss:0.5153275087556474\n",
      "train loss:0.3172662042075441\n",
      "train loss:0.4723697576569744\n",
      "train loss:0.36378452270123346\n",
      "train loss:0.39150876037374893\n",
      "train loss:0.3869553668991926\n",
      "train loss:0.502552171364658\n",
      "train loss:0.394250754041514\n",
      "train loss:0.37867622580346\n",
      "train loss:0.43050631103286086\n",
      "train loss:0.34115403428376867\n",
      "train loss:0.35999778971558166\n",
      "train loss:0.3946761654123746\n",
      "train loss:0.3830199524343561\n",
      "train loss:0.4884136264212195\n",
      "train loss:0.45610488852842307\n",
      "train loss:0.5000734183473594\n",
      "train loss:0.39650804625447655\n",
      "train loss:0.3639827282215303\n",
      "train loss:0.41985282869438095\n",
      "train loss:0.45720880454305\n",
      "train loss:0.39404242852571225\n",
      "train loss:0.29491891892152233\n",
      "train loss:0.3833874720938512\n",
      "train loss:0.601482119304949\n",
      "train loss:0.5242284160147919\n",
      "train loss:0.5340080184166603\n",
      "train loss:0.38681437936206975\n",
      "train loss:0.37878874562798204\n",
      "train loss:0.32383105362729475\n",
      "train loss:0.4319082089942398\n",
      "train loss:0.44497535015531364\n",
      "train loss:0.4429382183898156\n",
      "train loss:0.5685304691671026\n",
      "train loss:0.38531063246685177\n",
      "train loss:0.28137388288304593\n",
      "train loss:0.38336811425421863\n",
      "train loss:0.5672346499256239\n",
      "train loss:0.4560729547795631\n",
      "train loss:0.5727523641249178\n",
      "train loss:0.4089315526684373\n",
      "train loss:0.5008852761609419\n",
      "train loss:0.49919415777632936\n",
      "train loss:0.39719915649348414\n",
      "train loss:0.260476616988235\n",
      "train loss:0.3590600636847137\n",
      "train loss:0.4104002635777787\n",
      "train loss:0.27911839485440637\n",
      "train loss:0.36748202329490104\n",
      "train loss:0.3630176821141466\n",
      "train loss:0.34701168263706395\n",
      "train loss:0.31320868922862266\n",
      "train loss:0.4367388001075432\n",
      "train loss:0.41997960434171383\n",
      "train loss:0.3893036351904244\n",
      "train loss:0.37109866277519105\n",
      "train loss:0.44796699992178657\n",
      "train loss:0.3334719065690998\n",
      "train loss:0.36382959154645755\n",
      "train loss:0.39726345686771786\n",
      "train loss:0.27712407840781383\n",
      "train loss:0.3887543571316728\n",
      "train loss:0.23569251228851398\n",
      "train loss:0.4165724069843997\n",
      "train loss:0.5884915944883689\n",
      "train loss:0.30270433483326337\n",
      "train loss:0.44275629191386146\n",
      "train loss:0.3441096732358374\n",
      "train loss:0.35703995742670047\n",
      "train loss:0.43734836925756454\n",
      "train loss:0.47332280810132354\n",
      "train loss:0.3480396980466718\n",
      "train loss:0.3134022984245203\n",
      "train loss:0.3538810011398034\n",
      "train loss:0.32150085657772237\n",
      "train loss:0.4893978956737844\n",
      "train loss:0.26457308096908283\n",
      "train loss:0.45496581224283494\n",
      "train loss:0.36095662005827983\n",
      "train loss:0.4962072799016851\n",
      "train loss:0.3650130875469767\n",
      "train loss:0.40232582988043075\n",
      "train loss:0.3724758193367256\n",
      "train loss:0.3621668208403683\n",
      "train loss:0.42147758304380517\n",
      "train loss:0.41170627980289565\n",
      "train loss:0.2971137031784571\n",
      "train loss:0.32332338630517776\n",
      "train loss:0.37113559050908557\n",
      "train loss:0.4321366363134011\n",
      "train loss:0.35996008637189125\n",
      "train loss:0.3162467929761646\n",
      "train loss:0.333224367143826\n",
      "train loss:0.37420493666472554\n",
      "train loss:0.36102350878933737\n",
      "train loss:0.5673189706745339\n",
      "train loss:0.38224493942071974\n",
      "train loss:0.3383186954511984\n",
      "train loss:0.301752999690043\n",
      "train loss:0.3490209199764353\n",
      "train loss:0.415272192812433\n",
      "train loss:0.3393795329759879\n",
      "train loss:0.32814342329487917\n",
      "train loss:0.7114529513616279\n",
      "train loss:0.32577134221490206\n",
      "train loss:0.39047900199452534\n",
      "train loss:0.4239954065235099\n",
      "train loss:0.2779074210984243\n",
      "train loss:0.2932403922150457\n",
      "train loss:0.44985547497542017\n",
      "train loss:0.49710509223943267\n",
      "train loss:0.34600178471382065\n",
      "train loss:0.3662291312227249\n",
      "train loss:0.550685988084804\n",
      "train loss:0.3271422430906146\n",
      "train loss:0.4385903798343196\n",
      "train loss:0.4316316401229484\n",
      "train loss:0.31326305759134\n",
      "train loss:0.43448297104333866\n",
      "train loss:0.267427287829081\n",
      "train loss:0.28965188312278706\n",
      "train loss:0.44250601707175297\n",
      "train loss:0.46229572585703393\n",
      "train loss:0.5011279698443921\n",
      "train loss:0.28127567159394423\n",
      "train loss:0.2954438384740517\n",
      "train loss:0.3580199205005071\n",
      "train loss:0.3333563979597217\n",
      "train loss:0.46497444877201843\n",
      "train loss:0.3890906296988133\n",
      "train loss:0.43708181469379154\n",
      "train loss:0.31709432541345356\n",
      "train loss:0.37441822253042184\n",
      "train loss:0.3556642639369921\n",
      "train loss:0.4617142021387338\n",
      "train loss:0.41549970499847994\n",
      "train loss:0.44168878920830834\n",
      "train loss:0.4345536236001405\n",
      "train loss:0.5234223748357826\n",
      "train loss:0.5529600067036865\n",
      "train loss:0.4076471507473303\n",
      "train loss:0.42871533548746543\n",
      "train loss:0.37383932650109963\n",
      "train loss:0.3689397318317377\n",
      "train loss:0.37078673330214096\n",
      "train loss:0.35209280234160156\n",
      "train loss:0.586871924034839\n",
      "train loss:0.38781067204743846\n",
      "train loss:0.30805948173628606\n",
      "train loss:0.39685442278077365\n",
      "train loss:0.4994021195628578\n",
      "train loss:0.40024073778851305\n",
      "train loss:0.48987234829335363\n",
      "train loss:0.5152303286525232\n",
      "train loss:0.3726829341104251\n",
      "train loss:0.3525006026783077\n",
      "train loss:0.3093730527592358\n",
      "train loss:0.38507848105356607\n",
      "train loss:0.5209627926204363\n",
      "train loss:0.32016381014526857\n",
      "train loss:0.32751721425714614\n",
      "train loss:0.41763578784197875\n",
      "train loss:0.44260835647935504\n",
      "train loss:0.4758970335085877\n",
      "train loss:0.3999837880246398\n",
      "train loss:0.3660823867837356\n",
      "train loss:0.3494755305402377\n",
      "train loss:0.47760899375089444\n",
      "train loss:0.3478170073903533\n",
      "train loss:0.3726038732588631\n",
      "train loss:0.36271895459296977\n",
      "train loss:0.3202971103617249\n",
      "train loss:0.379456543855041\n",
      "train loss:0.2865489903682183\n",
      "train loss:0.2789231042282724\n",
      "train loss:0.4546256687048579\n",
      "train loss:0.5321084849736415\n",
      "train loss:0.4067006716159012\n",
      "train loss:0.3121675629084969\n",
      "train loss:0.38263942484278923\n",
      "train loss:0.607300129883836\n",
      "train loss:0.37278817725223534\n",
      "train loss:0.41056273089241685\n",
      "train loss:0.32198312643446314\n",
      "train loss:0.3018822090072994\n",
      "train loss:0.2988477720898404\n",
      "train loss:0.3105275217494671\n",
      "train loss:0.3900729360130248\n",
      "train loss:0.4278518896898334\n",
      "train loss:0.46506349545137743\n",
      "train loss:0.46076807395775377\n",
      "train loss:0.4012680366065899\n",
      "train loss:0.30711211747899164\n",
      "train loss:0.34561604956462155\n",
      "train loss:0.39473955160540974\n",
      "train loss:0.35491482541110264\n",
      "train loss:0.40461363564204966\n",
      "train loss:0.38565652224274755\n",
      "train loss:0.46336043689301293\n",
      "train loss:0.3360026146494458\n",
      "train loss:0.2728522742776321\n",
      "train loss:0.38777519275984346\n",
      "train loss:0.3027026252327739\n",
      "train loss:0.32520961184049424\n",
      "train loss:0.3012668445557756\n",
      "train loss:0.32031808460478517\n",
      "train loss:0.40521148733131157\n",
      "train loss:0.3000081002172344\n",
      "train loss:0.43004453175039337\n",
      "train loss:0.2988219919164495\n",
      "train loss:0.36619897695110454\n",
      "train loss:0.4623660528966698\n",
      "train loss:0.336651940479897\n",
      "train loss:0.5296594422195031\n",
      "train loss:0.34405014329235073\n",
      "train loss:0.3312751598018199\n",
      "train loss:0.3679060100233318\n",
      "train loss:0.4203535398980724\n",
      "train loss:0.225472549039302\n",
      "train loss:0.506785713916662\n",
      "train loss:0.2965166740888821\n",
      "train loss:0.3282189807315341\n",
      "train loss:0.4024012126150664\n",
      "train loss:0.39626781930888455\n",
      "train loss:0.37803474427631273\n",
      "train loss:0.47932161907016374\n",
      "train loss:0.3330866706274346\n",
      "train loss:0.2569233229963723\n",
      "train loss:0.257629546240703\n",
      "train loss:0.5115535860925061\n",
      "train loss:0.35950278265955\n",
      "train loss:0.31585182448048793\n",
      "train loss:0.476262545582284\n",
      "train loss:0.3804632632479171\n",
      "train loss:0.27008401875739013\n",
      "train loss:0.4247715020087932\n",
      "train loss:0.4803148130092753\n",
      "train loss:0.30063010886903696\n",
      "train loss:0.3713293416830791\n",
      "train loss:0.3351248146455789\n",
      "train loss:0.34695163415524505\n",
      "train loss:0.44322207185700163\n",
      "train loss:0.3527590627388395\n",
      "train loss:0.3927042206825941\n",
      "train loss:0.3959611166433137\n",
      "train loss:0.4490098685586532\n",
      "train loss:0.2774060111463207\n",
      "train loss:0.43266614424760064\n",
      "train loss:0.2744361379795454\n",
      "train loss:0.4337123929809487\n",
      "train loss:0.28085748281232414\n",
      "train loss:0.295108671108636\n",
      "train loss:0.3090872449195841\n",
      "train loss:0.37646575719896747\n",
      "train loss:0.44254386214433744\n",
      "train loss:0.4033085445557603\n",
      "train loss:0.39094940475335904\n",
      "train loss:0.2885809192784953\n",
      "train loss:0.3942099547446379\n",
      "train loss:0.47813604306049584\n",
      "train loss:0.4159383411597809\n",
      "train loss:0.43661039039893773\n",
      "train loss:0.3505367196907575\n",
      "train loss:0.35770737298064664\n",
      "train loss:0.5351667524288605\n",
      "train loss:0.2776749338162893\n",
      "train loss:0.3972784277070018\n",
      "train loss:0.3014443192127859\n",
      "train loss:0.433678088494476\n",
      "train loss:0.43062004364532463\n",
      "train loss:0.35539214201744007\n",
      "train loss:0.40493833805684515\n",
      "train loss:0.39290542374065723\n",
      "train loss:0.4553337895634939\n",
      "train loss:0.36359670744558253\n",
      "train loss:0.40426898336540795\n",
      "train loss:0.4174529489212251\n",
      "train loss:0.3322669157270347\n",
      "train loss:0.4734190864935264\n",
      "train loss:0.41281092508359024\n",
      "train loss:0.2937560871500069\n",
      "train loss:0.421096274353909\n",
      "train loss:0.48562893781226385\n",
      "train loss:0.49804657751707077\n",
      "train loss:0.4730250324608061\n",
      "train loss:0.3827408141181114\n",
      "train loss:0.4177042425292181\n",
      "train loss:0.4533971234601342\n",
      "train loss:0.4786798432081929\n",
      "train loss:0.45808380665446213\n",
      "train loss:0.376584593987122\n",
      "train loss:0.47948310427265844\n",
      "train loss:0.349980002244589\n",
      "train loss:0.3565625088485232\n",
      "train loss:0.4338294458843329\n",
      "train loss:0.37650755956122844\n",
      "train loss:0.5039850925890306\n",
      "train loss:0.397940527208477\n",
      "train loss:0.49037842789725306\n",
      "train loss:0.48636447124886945\n",
      "train loss:0.31789921784279235\n",
      "train loss:0.31893564179617445\n",
      "train loss:0.2777318588289035\n",
      "train loss:0.3221543545531512\n",
      "train loss:0.33993748658462314\n",
      "train loss:0.4383258909446697\n",
      "train loss:0.40647889547807475\n",
      "train loss:0.5477585240297025\n",
      "train loss:0.24643851115789317\n",
      "train loss:0.4481816720971773\n",
      "train loss:0.4854701807643123\n",
      "train loss:0.30821844984561997\n",
      "train loss:0.3442050510236612\n",
      "train loss:0.3048433728458202\n",
      "train loss:0.31552640697173273\n",
      "train loss:0.3710920624686217\n",
      "train loss:0.4240661646252652\n",
      "train loss:0.2740358334395027\n",
      "train loss:0.32222222248477306\n",
      "train loss:0.2309817096358523\n",
      "train loss:0.38853236299818145\n",
      "train loss:0.26343183409875276\n",
      "train loss:0.38477795086375105\n",
      "train loss:0.3303261386828455\n",
      "train loss:0.35370584177689307\n",
      "train loss:0.45253545790776856\n",
      "train loss:0.3328202216742467\n",
      "train loss:0.39817679084456836\n",
      "train loss:0.3746915058890613\n",
      "train loss:0.3746497433606548\n",
      "train loss:0.3903796176119904\n",
      "train loss:0.5368263444011319\n",
      "train loss:0.28210601169981947\n",
      "train loss:0.3985897918609825\n",
      "train loss:0.36944090107762617\n",
      "train loss:0.41354917913479006\n",
      "train loss:0.39768389521381964\n",
      "train loss:0.41103976836129247\n",
      "train loss:0.2984205095308382\n",
      "train loss:0.33377740945388595\n",
      "train loss:0.40546554853221045\n",
      "train loss:0.31015136114016084\n",
      "train loss:0.35632452482976035\n",
      "train loss:0.2879049705927008\n",
      "train loss:0.5290150660105737\n",
      "train loss:0.29406137129500265\n",
      "train loss:0.4123612101711745\n",
      "train loss:0.3718233836991307\n",
      "train loss:0.47913817675777604\n",
      "train loss:0.45250621964993953\n",
      "train loss:0.27925881582352985\n",
      "train loss:0.3750402166210804\n",
      "train loss:0.3541602094136453\n",
      "train loss:0.3863091583081802\n",
      "=== epoch:14, train acc:0.825, test acc:0.843 ===\n",
      "train loss:0.4348785608680881\n",
      "train loss:0.39862782322336093\n",
      "train loss:0.45357648547076623\n",
      "train loss:0.403319748329245\n",
      "train loss:0.35393337654943813\n",
      "train loss:0.38195528179344057\n",
      "train loss:0.2783629756446509\n",
      "train loss:0.48564517132344165\n",
      "train loss:0.42517737063868777\n",
      "train loss:0.37574106631543047\n",
      "train loss:0.3056790717812282\n",
      "train loss:0.3080598231364166\n",
      "train loss:0.4793177408412659\n",
      "train loss:0.40799905698460215\n",
      "train loss:0.4318676267076782\n",
      "train loss:0.5221914330322261\n",
      "train loss:0.4874353376533838\n",
      "train loss:0.3699957542403671\n",
      "train loss:0.35197214575382596\n",
      "train loss:0.38250399825619935\n",
      "train loss:0.43379712490658384\n",
      "train loss:0.46485772690269234\n",
      "train loss:0.30280694473832265\n",
      "train loss:0.5582322518701638\n",
      "train loss:0.41929946638635207\n",
      "train loss:0.323816541103225\n",
      "train loss:0.3200490283413351\n",
      "train loss:0.29194449651029847\n",
      "train loss:0.36684400189991506\n",
      "train loss:0.4129060360276693\n",
      "train loss:0.41162860787526817\n",
      "train loss:0.4131886049133394\n",
      "train loss:0.2974978602909862\n",
      "train loss:0.2856000091945158\n",
      "train loss:0.3235449135586292\n",
      "train loss:0.3984770398913365\n",
      "train loss:0.37596867307390525\n",
      "train loss:0.4326133911222433\n",
      "train loss:0.3878589737117227\n",
      "train loss:0.3850083345631474\n",
      "train loss:0.5821670254098955\n",
      "train loss:0.3417251386990958\n",
      "train loss:0.4083424896060378\n",
      "train loss:0.3878428893676664\n",
      "train loss:0.43382512754581737\n",
      "train loss:0.3717945507040185\n",
      "train loss:0.33629555581187004\n",
      "train loss:0.35111399094574947\n",
      "train loss:0.37432045009983644\n",
      "train loss:0.49887492129656136\n",
      "train loss:0.4573502705238933\n",
      "train loss:0.42097623809544826\n",
      "train loss:0.44485651407081905\n",
      "train loss:0.412987902671131\n",
      "train loss:0.3671261783091691\n",
      "train loss:0.3159972170910966\n",
      "train loss:0.4115281435020431\n",
      "train loss:0.400104667517409\n",
      "train loss:0.30277715994120413\n",
      "train loss:0.2755101352528782\n",
      "train loss:0.2909801512103096\n",
      "train loss:0.37547075343336295\n",
      "train loss:0.46588462980143847\n",
      "train loss:0.3596833609942031\n",
      "train loss:0.2731530197804294\n",
      "train loss:0.36991540466636386\n",
      "train loss:0.3819678505908183\n",
      "train loss:0.48932215618948915\n",
      "train loss:0.2841934617977228\n",
      "train loss:0.4685706670354611\n",
      "train loss:0.270709768446052\n",
      "train loss:0.3046517129364177\n",
      "train loss:0.4046222421361778\n",
      "train loss:0.31452550112789357\n",
      "train loss:0.37628054294545643\n",
      "train loss:0.39400910764112645\n",
      "train loss:0.5013995519118731\n",
      "train loss:0.35585014006749544\n",
      "train loss:0.5211778739455338\n",
      "train loss:0.366897880666963\n",
      "train loss:0.4472266493961147\n",
      "train loss:0.4929359088440713\n",
      "train loss:0.3567095020979081\n",
      "train loss:0.435330322522582\n",
      "train loss:0.45979259950739\n",
      "train loss:0.36320707274138747\n",
      "train loss:0.4246861002857569\n",
      "train loss:0.5871425126568036\n",
      "train loss:0.3267983563757734\n",
      "train loss:0.45647955608887547\n",
      "train loss:0.3538259992461958\n",
      "train loss:0.412588258415372\n",
      "train loss:0.33798416049715563\n",
      "train loss:0.34563100265803903\n",
      "train loss:0.3225736650032435\n",
      "train loss:0.5212260604190129\n",
      "train loss:0.4095797875892385\n",
      "train loss:0.4484116330279359\n",
      "train loss:0.4150017533000427\n",
      "train loss:0.4674731951849617\n",
      "train loss:0.3754258299117751\n",
      "train loss:0.47708741317639664\n",
      "train loss:0.36289030751115253\n",
      "train loss:0.4041630065377184\n",
      "train loss:0.3619798726109078\n",
      "train loss:0.41513100150058674\n",
      "train loss:0.43341033278306573\n",
      "train loss:0.4712819624078264\n",
      "train loss:0.338726727165019\n",
      "train loss:0.4023793546307941\n",
      "train loss:0.5683076429383902\n",
      "train loss:0.44665935170636983\n",
      "train loss:0.41038813133713425\n",
      "train loss:0.5216496465550695\n",
      "train loss:0.337561110316489\n",
      "train loss:0.33821592882205737\n",
      "train loss:0.2499506698792157\n",
      "train loss:0.3476013020535151\n",
      "train loss:0.34609681248065643\n",
      "train loss:0.38349842713200016\n",
      "train loss:0.3097171992442438\n",
      "train loss:0.3298753254478519\n",
      "train loss:0.5009941843978662\n",
      "train loss:0.4286844120543586\n",
      "train loss:0.48128019346153345\n",
      "train loss:0.3501031893729135\n",
      "train loss:0.4238090263521933\n",
      "train loss:0.33222769022084725\n",
      "train loss:0.4442060886003852\n",
      "train loss:0.38751936386776814\n",
      "train loss:0.4666864585802778\n",
      "train loss:0.3824566265870201\n",
      "train loss:0.33451208988430037\n",
      "train loss:0.5167657935870114\n",
      "train loss:0.6532509925565789\n",
      "train loss:0.30601368791622063\n",
      "train loss:0.3440441575053523\n",
      "train loss:0.3901674267512705\n",
      "train loss:0.25580456270339014\n",
      "train loss:0.41241518426934626\n",
      "train loss:0.4662095047178099\n",
      "train loss:0.32662799644544677\n",
      "train loss:0.3943228545522654\n",
      "train loss:0.3787750939930662\n",
      "train loss:0.34372715421855005\n",
      "train loss:0.4480557154816349\n",
      "train loss:0.47339098967382376\n",
      "train loss:0.3114544818989639\n",
      "train loss:0.3332344103495889\n",
      "train loss:0.41214874930279577\n",
      "train loss:0.36081685825135695\n",
      "train loss:0.3493734184611534\n",
      "train loss:0.27519947342674267\n",
      "train loss:0.40483992810785246\n",
      "train loss:0.38348570080033156\n",
      "train loss:0.4800867514145974\n",
      "train loss:0.49801824344588236\n",
      "train loss:0.39535735446576226\n",
      "train loss:0.3756834779342423\n",
      "train loss:0.42245743568999344\n",
      "train loss:0.27317463845015916\n",
      "train loss:0.35316045001032803\n",
      "train loss:0.4386433516600951\n",
      "train loss:0.37373954815396154\n",
      "train loss:0.3601887422805002\n",
      "train loss:0.25874259145412404\n",
      "train loss:0.26114750985258284\n",
      "train loss:0.29400969339247\n",
      "train loss:0.27956935653839254\n",
      "train loss:0.23002084626290525\n",
      "train loss:0.35246921794096947\n",
      "train loss:0.2879723082016388\n",
      "train loss:0.329629767284516\n",
      "train loss:0.4102670810614885\n",
      "train loss:0.5145397374744286\n",
      "train loss:0.3032767497824749\n",
      "train loss:0.5599424332273122\n",
      "train loss:0.5133441638017859\n",
      "train loss:0.34119245144875193\n",
      "train loss:0.40926309954354956\n",
      "train loss:0.5007178031650653\n",
      "train loss:0.4999640316641378\n",
      "train loss:0.3059032762261469\n",
      "train loss:0.3870029209243775\n",
      "train loss:0.30777935266420964\n",
      "train loss:0.3725781911016454\n",
      "train loss:0.416370262876597\n",
      "train loss:0.43186163053833715\n",
      "train loss:0.3520188124504475\n",
      "train loss:0.5029828426521323\n",
      "train loss:0.2791057377573281\n",
      "train loss:0.33887115024436226\n",
      "train loss:0.4223790942776494\n",
      "train loss:0.4932731383492458\n",
      "train loss:0.4314157858030954\n",
      "train loss:0.3562455249713304\n",
      "train loss:0.4483099110087768\n",
      "train loss:0.29903531820361584\n",
      "train loss:0.34409044088798013\n",
      "train loss:0.3193689562744738\n",
      "train loss:0.43724997178028124\n",
      "train loss:0.4243081356519294\n",
      "train loss:0.3416328689181597\n",
      "train loss:0.3843653138508524\n",
      "train loss:0.291824186721281\n",
      "train loss:0.335687232393264\n",
      "train loss:0.44021465499175716\n",
      "train loss:0.26861900827940827\n",
      "train loss:0.5548015107273971\n",
      "train loss:0.40784272031282554\n",
      "train loss:0.5447648110269366\n",
      "train loss:0.31909654622965183\n",
      "train loss:0.40252464128730686\n",
      "train loss:0.3430793557136241\n",
      "train loss:0.35090172745181214\n",
      "train loss:0.389423579896146\n",
      "train loss:0.41205555096109914\n",
      "train loss:0.37483910990512653\n",
      "train loss:0.3151396132741913\n",
      "train loss:0.26993829941201314\n",
      "train loss:0.43202923913965674\n",
      "train loss:0.38578627991080794\n",
      "train loss:0.40202343244168054\n",
      "train loss:0.3206983610780162\n",
      "train loss:0.3359244518153154\n",
      "train loss:0.4108443654985065\n",
      "train loss:0.39946052664737375\n",
      "train loss:0.26679464900417177\n",
      "train loss:0.30008783072135553\n",
      "train loss:0.3629395665188971\n",
      "train loss:0.40927048441894115\n",
      "train loss:0.41369774964791006\n",
      "train loss:0.4811564313235981\n",
      "train loss:0.22844565097361605\n",
      "train loss:0.5170042515120401\n",
      "train loss:0.531949599308495\n",
      "train loss:0.3631911091843423\n",
      "train loss:0.31421910296714145\n",
      "train loss:0.5165312723020175\n",
      "train loss:0.3247462053368026\n",
      "train loss:0.3959130113091715\n",
      "train loss:0.32200119702574503\n",
      "train loss:0.3174394858975253\n",
      "train loss:0.4691570230602743\n",
      "train loss:0.418360274146379\n",
      "train loss:0.32740500829672164\n",
      "train loss:0.40481953790336533\n",
      "train loss:0.43157855327429145\n",
      "train loss:0.36724953278695854\n",
      "train loss:0.5790734242792387\n",
      "train loss:0.3447982224392664\n",
      "train loss:0.2845224844305366\n",
      "train loss:0.2984764265053433\n",
      "train loss:0.4458474374421547\n",
      "train loss:0.40697479754310656\n",
      "train loss:0.3355576860849981\n",
      "train loss:0.5075072055301391\n",
      "train loss:0.3841618356675611\n",
      "train loss:0.4118985170417377\n",
      "train loss:0.4379438514102151\n",
      "train loss:0.455424248441195\n",
      "train loss:0.3985236042509886\n",
      "train loss:0.4344997001424595\n",
      "train loss:0.3891504802803611\n",
      "train loss:0.3711331648023704\n",
      "train loss:0.46203628016115156\n",
      "train loss:0.35897922906065005\n",
      "train loss:0.3099109433660489\n",
      "train loss:0.30395309955427663\n",
      "train loss:0.31715548876190414\n",
      "train loss:0.4078854185383313\n",
      "train loss:0.2931553145507739\n",
      "train loss:0.3690826374887877\n",
      "train loss:0.2688707673486152\n",
      "train loss:0.4891273056884466\n",
      "train loss:0.31616253065175226\n",
      "train loss:0.38760132384904106\n",
      "train loss:0.2629121417760616\n",
      "train loss:0.4045939443560867\n",
      "train loss:0.459377022038553\n",
      "train loss:0.40453896836071496\n",
      "train loss:0.3817930984377113\n",
      "train loss:0.397283413604736\n",
      "train loss:0.3667795964925993\n",
      "train loss:0.21983853560254127\n",
      "train loss:0.34196327787378733\n",
      "train loss:0.2057817628833519\n",
      "train loss:0.34779078813962483\n",
      "train loss:0.3675600649286853\n",
      "train loss:0.37056532716063645\n",
      "train loss:0.39365277195005943\n",
      "train loss:0.49540034780106057\n",
      "train loss:0.2971147544765222\n",
      "train loss:0.4156381910853655\n",
      "train loss:0.34900733339522155\n",
      "train loss:0.4000926936014983\n",
      "train loss:0.3267786552789092\n",
      "train loss:0.4066363732171807\n",
      "train loss:0.4277013718408996\n",
      "train loss:0.3290827458086487\n",
      "train loss:0.2770337536819984\n",
      "train loss:0.4677724637294722\n",
      "train loss:0.4310714973077492\n",
      "train loss:0.25646192382611604\n",
      "train loss:0.29052850360183613\n",
      "train loss:0.36306000148250306\n",
      "train loss:0.2911510975550219\n",
      "train loss:0.33709477435615104\n",
      "train loss:0.3249085906990027\n",
      "train loss:0.38800360854996524\n",
      "train loss:0.40893201394569423\n",
      "train loss:0.36056095513184266\n",
      "train loss:0.26924896664702136\n",
      "train loss:0.3700838625191389\n",
      "train loss:0.37576189499925305\n",
      "train loss:0.2735213354989022\n",
      "train loss:0.5703453019909276\n",
      "train loss:0.324591108608304\n",
      "train loss:0.3649457406307146\n",
      "train loss:0.42299974197016127\n",
      "train loss:0.3505015752020537\n",
      "train loss:0.36290743665337083\n",
      "train loss:0.18126293188101614\n",
      "train loss:0.4023123304869378\n",
      "train loss:0.40883589892095373\n",
      "train loss:0.26385902256030713\n",
      "train loss:0.5852247878244704\n",
      "train loss:0.4734539887603587\n",
      "train loss:0.26211001067940903\n",
      "train loss:0.3376470723319342\n",
      "train loss:0.29736233017395597\n",
      "train loss:0.23520665634130047\n",
      "train loss:0.4562034488536469\n",
      "train loss:0.3607164991176881\n",
      "train loss:0.16425419321679122\n",
      "train loss:0.28872952340018515\n",
      "train loss:0.3175429659493068\n",
      "train loss:0.3436711043118175\n",
      "train loss:0.3556826380736448\n",
      "train loss:0.29133647821254854\n",
      "train loss:0.46039399711490225\n",
      "train loss:0.32203817559463643\n",
      "train loss:0.26722423074650037\n",
      "train loss:0.4138956342643463\n",
      "train loss:0.2273862710927627\n",
      "train loss:0.3537388251653762\n",
      "train loss:0.3849802121801411\n",
      "train loss:0.346328605644497\n",
      "train loss:0.24169766375805557\n",
      "train loss:0.41166344952421113\n",
      "train loss:0.40787632247423905\n",
      "train loss:0.39224360773810185\n",
      "train loss:0.34114828520979884\n",
      "train loss:0.3320933939768163\n",
      "train loss:0.44051489313626896\n",
      "train loss:0.41369489865494347\n",
      "train loss:0.502596545554458\n",
      "train loss:0.4268646775660509\n",
      "train loss:0.42603080007984595\n",
      "train loss:0.30353327779720446\n",
      "train loss:0.39928677865294915\n",
      "train loss:0.2977156886587208\n",
      "train loss:0.27245521877325274\n",
      "train loss:0.33553160499172763\n",
      "train loss:0.42441331098992985\n",
      "train loss:0.370771754205786\n",
      "train loss:0.2870331293248151\n",
      "train loss:0.42428531154134097\n",
      "train loss:0.39298669193172964\n",
      "train loss:0.33647170403034354\n",
      "train loss:0.41095946770494685\n",
      "train loss:0.4225392822879405\n",
      "train loss:0.32362800663523417\n",
      "train loss:0.3955538936525636\n",
      "train loss:0.3120933958819484\n",
      "train loss:0.5075289742181168\n",
      "train loss:0.46809613609528017\n",
      "train loss:0.19288460435781313\n",
      "train loss:0.45035198393378795\n",
      "train loss:0.3401488524907479\n",
      "train loss:0.6060122933812727\n",
      "train loss:0.4572167946588116\n",
      "train loss:0.33880670617234054\n",
      "train loss:0.2532587817166692\n",
      "train loss:0.4417703482964405\n",
      "train loss:0.3173440018527714\n",
      "train loss:0.28066634165821713\n",
      "train loss:0.38688285153666463\n",
      "train loss:0.3996599651926819\n",
      "train loss:0.42854356776647834\n",
      "train loss:0.5150673563866399\n",
      "train loss:0.4193227470640577\n",
      "train loss:0.5121291019376865\n",
      "train loss:0.3173465129376199\n",
      "train loss:0.40649224644736587\n",
      "train loss:0.3759908751428569\n",
      "train loss:0.36052050911567257\n",
      "train loss:0.3813491596435605\n",
      "train loss:0.26592058710542643\n",
      "train loss:0.42980234245181703\n",
      "train loss:0.33909528439957676\n",
      "train loss:0.39320295666191674\n",
      "train loss:0.3280636778813681\n",
      "train loss:0.3820640008063878\n",
      "train loss:0.3245526298553627\n",
      "train loss:0.33261151857969196\n",
      "train loss:0.2730588574608243\n",
      "train loss:0.3192242532854142\n",
      "train loss:0.33407455821627685\n",
      "train loss:0.34314620977899174\n",
      "train loss:0.3341928924551503\n",
      "train loss:0.3533539066907027\n",
      "train loss:0.3076508803102161\n",
      "train loss:0.29760374323601224\n",
      "train loss:0.35715878559450426\n",
      "train loss:0.2807089512361283\n",
      "train loss:0.4993506301453033\n",
      "train loss:0.3882451265845075\n",
      "train loss:0.35197238345918985\n",
      "train loss:0.4395032098274836\n",
      "train loss:0.3460274303855514\n",
      "train loss:0.4716994966019767\n",
      "train loss:0.28376964701616364\n",
      "train loss:0.37315344982040805\n",
      "train loss:0.2926710862225605\n",
      "train loss:0.3969060418690148\n",
      "train loss:0.342970595481688\n",
      "train loss:0.38576518467609533\n",
      "train loss:0.4863579198143925\n",
      "train loss:0.4414909503591302\n",
      "train loss:0.35104569644744515\n",
      "train loss:0.20778471734656123\n",
      "train loss:0.5019430130879075\n",
      "train loss:0.3408569013725664\n",
      "train loss:0.3476558668346798\n",
      "train loss:0.3193902680761751\n",
      "train loss:0.3839144540883015\n",
      "train loss:0.3688261924608205\n",
      "train loss:0.3714452412117483\n",
      "train loss:0.3753711247380396\n",
      "train loss:0.27284107721588424\n",
      "train loss:0.49274114514142364\n",
      "train loss:0.36778303866233\n",
      "train loss:0.3668768793419713\n",
      "train loss:0.32507879941129636\n",
      "train loss:0.5288800000990388\n",
      "train loss:0.4886605335412896\n",
      "train loss:0.3694246187497792\n",
      "train loss:0.4790412155759984\n",
      "train loss:0.3776229609781142\n",
      "train loss:0.3044086305026036\n",
      "train loss:0.4722823077694297\n",
      "train loss:0.31951268238574954\n",
      "train loss:0.2556697392205694\n",
      "train loss:0.36997006461399096\n",
      "train loss:0.3170428174525098\n",
      "train loss:0.396836534160461\n",
      "train loss:0.4432188658990323\n",
      "train loss:0.30684071827061304\n",
      "train loss:0.35345801563495316\n",
      "train loss:0.2921320329118045\n",
      "train loss:0.4166184362132158\n",
      "train loss:0.33460874526391765\n",
      "train loss:0.33625600968097463\n",
      "train loss:0.2789201379927626\n",
      "train loss:0.40953718353263413\n",
      "train loss:0.4181102616277441\n",
      "train loss:0.40807914912465826\n",
      "train loss:0.41781738934824864\n",
      "train loss:0.43085840456629904\n",
      "train loss:0.3293638962324584\n",
      "train loss:0.306147177690953\n",
      "train loss:0.29086088472802707\n",
      "train loss:0.3531407676668363\n",
      "train loss:0.3211629592635595\n",
      "train loss:0.3816873092271608\n",
      "train loss:0.4014632014247674\n",
      "train loss:0.3853759542898096\n",
      "train loss:0.26617135100588596\n",
      "train loss:0.27321526404294444\n",
      "train loss:0.31579952717596393\n",
      "train loss:0.312786290082809\n",
      "train loss:0.38179634374855603\n",
      "train loss:0.4289119718970571\n",
      "train loss:0.3149920990054551\n",
      "train loss:0.4577157730835477\n",
      "train loss:0.45685486928864755\n",
      "train loss:0.4472320328817752\n",
      "train loss:0.4009427010678991\n",
      "train loss:0.2833088908415024\n",
      "train loss:0.4508260469135562\n",
      "train loss:0.3458129899545937\n",
      "train loss:0.3194190145302869\n",
      "train loss:0.46193593766209423\n",
      "train loss:0.46854211179388744\n",
      "train loss:0.2852099008684155\n",
      "train loss:0.36278144991901895\n",
      "train loss:0.4582924217909603\n",
      "train loss:0.306536064141442\n",
      "train loss:0.40841851464305085\n",
      "train loss:0.364392798528559\n",
      "train loss:0.32986238320824074\n",
      "train loss:0.2971174028228468\n",
      "train loss:0.4602882042190947\n",
      "train loss:0.3423076792625232\n",
      "train loss:0.2633796481352062\n",
      "train loss:0.31745518069973067\n",
      "train loss:0.3892391239991385\n",
      "train loss:0.4295180509022064\n",
      "train loss:0.5042667985302259\n",
      "train loss:0.4136139435747244\n",
      "train loss:0.4044632657991808\n",
      "train loss:0.30362620896406123\n",
      "train loss:0.4426849141966778\n",
      "train loss:0.27334717311545953\n",
      "train loss:0.22935790884078358\n",
      "train loss:0.5021113065155663\n",
      "train loss:0.3903441062617858\n",
      "train loss:0.34701397943932283\n",
      "train loss:0.41666540750224795\n",
      "train loss:0.4501535735998112\n",
      "train loss:0.34260758126017243\n",
      "train loss:0.3306909791839626\n",
      "train loss:0.26888375924798047\n",
      "train loss:0.35369186326868357\n",
      "train loss:0.43467054879152217\n",
      "train loss:0.34259476615764994\n",
      "train loss:0.33143384071725046\n",
      "train loss:0.46548517672655876\n",
      "train loss:0.37677299900108097\n",
      "train loss:0.3194291936165731\n",
      "train loss:0.34040659303548165\n",
      "train loss:0.33930163597880414\n",
      "train loss:0.3876908595435273\n",
      "train loss:0.37895468811233635\n",
      "train loss:0.564194840581914\n",
      "train loss:0.26494866629273583\n",
      "train loss:0.3824834356992184\n",
      "train loss:0.3994113745553412\n",
      "train loss:0.5176955040129073\n",
      "train loss:0.25325485213924664\n",
      "train loss:0.2853287909352031\n",
      "train loss:0.37847647516164984\n",
      "train loss:0.28810976662385307\n",
      "train loss:0.21757519831316646\n",
      "train loss:0.38873770907375665\n",
      "train loss:0.3952476945410153\n",
      "train loss:0.40129714191157206\n",
      "train loss:0.27036886948909133\n",
      "train loss:0.5055986747921224\n",
      "train loss:0.3395379818452461\n",
      "train loss:0.35112847823187665\n",
      "train loss:0.4195802549494027\n",
      "train loss:0.44311156848160765\n",
      "train loss:0.4292043342446272\n",
      "train loss:0.42758132322086767\n",
      "train loss:0.28329106400117166\n",
      "train loss:0.22057634852265784\n",
      "train loss:0.3503317782033857\n",
      "train loss:0.4374379166547153\n",
      "train loss:0.38151023540450846\n",
      "train loss:0.33581962529919873\n",
      "train loss:0.3907324209052998\n",
      "train loss:0.26759965478457415\n",
      "train loss:0.42422996070947067\n",
      "train loss:0.4157741447889475\n",
      "train loss:0.40647693414319513\n",
      "train loss:0.22832794265657305\n",
      "train loss:0.3224193543198325\n",
      "train loss:0.39555738202092505\n",
      "train loss:0.33906108138565755\n",
      "train loss:0.3385049265652158\n",
      "train loss:0.3384431594596177\n",
      "train loss:0.389895581136103\n",
      "train loss:0.3058400438714382\n",
      "train loss:0.26363543513825105\n",
      "train loss:0.48110844482305937\n",
      "train loss:0.33010086349030615\n",
      "train loss:0.3872555928598006\n",
      "train loss:0.36733755258890916\n",
      "train loss:0.3316079273014019\n",
      "train loss:0.44935334113065273\n",
      "train loss:0.26843838927140234\n",
      "train loss:0.3923399460946246\n",
      "train loss:0.3116917620597838\n",
      "train loss:0.6107979367382942\n",
      "train loss:0.3444578360448707\n",
      "train loss:0.1671749290136362\n",
      "train loss:0.34713700482484205\n",
      "train loss:0.30304953536542567\n",
      "train loss:0.27524471709431786\n",
      "train loss:0.4638376545529325\n",
      "train loss:0.353882743690998\n",
      "train loss:0.5070554433394652\n",
      "train loss:0.35645093502041014\n",
      "train loss:0.4638577150458086\n",
      "train loss:0.4505752910675988\n",
      "train loss:0.33070042971422464\n",
      "train loss:0.377607392999437\n",
      "train loss:0.35748324693941824\n",
      "=== epoch:15, train acc:0.833, test acc:0.848 ===\n",
      "train loss:0.30952089417332423\n",
      "train loss:0.29943244520396867\n",
      "train loss:0.3749336217119867\n",
      "train loss:0.30541162362265695\n",
      "train loss:0.42737832631125855\n",
      "train loss:0.3749123931720995\n",
      "train loss:0.37259344360723745\n",
      "train loss:0.2977682731532352\n",
      "train loss:0.1569619365885022\n",
      "train loss:0.44829689401820283\n",
      "train loss:0.3370591089232749\n",
      "train loss:0.3918254184761021\n",
      "train loss:0.34367270723248167\n",
      "train loss:0.4097169543690714\n",
      "train loss:0.4115138446332629\n",
      "train loss:0.3603941513178699\n",
      "train loss:0.28071010379125577\n",
      "train loss:0.2506529951799092\n",
      "train loss:0.34547182999025383\n",
      "train loss:0.4022296127999597\n",
      "train loss:0.43293690337778223\n",
      "train loss:0.45406105975984606\n",
      "train loss:0.3595862831217391\n",
      "train loss:0.27894745179939795\n",
      "train loss:0.35658580011593477\n",
      "train loss:0.32595927883831544\n",
      "train loss:0.41899852117581554\n",
      "train loss:0.42219007011561216\n",
      "train loss:0.31908393211415403\n",
      "train loss:0.4067980027096008\n",
      "train loss:0.26694799045145046\n",
      "train loss:0.2811628177079742\n",
      "train loss:0.35469109778799174\n",
      "train loss:0.4483717819479213\n",
      "train loss:0.3879916757483119\n",
      "train loss:0.4409046561367487\n",
      "train loss:0.3249056806649138\n",
      "train loss:0.48482500108581517\n",
      "train loss:0.4469215222172551\n",
      "train loss:0.3829904490336209\n",
      "train loss:0.2644986590893724\n",
      "train loss:0.3079838029119415\n",
      "train loss:0.3321413212701011\n",
      "train loss:0.3990991586363141\n",
      "train loss:0.35527727376432766\n",
      "train loss:0.33620340262144055\n",
      "train loss:0.2778091158592148\n",
      "train loss:0.34044780250669215\n",
      "train loss:0.23042042088748244\n",
      "train loss:0.30833920350186966\n",
      "train loss:0.3150213934209807\n",
      "train loss:0.45660526346918073\n",
      "train loss:0.3440688476129201\n",
      "train loss:0.2591781154128378\n",
      "train loss:0.28600971623608307\n",
      "train loss:0.34553942830675605\n",
      "train loss:0.3043457252107372\n",
      "train loss:0.5638719170427758\n",
      "train loss:0.33022769718384787\n",
      "train loss:0.2774173840663135\n",
      "train loss:0.33716515010492626\n",
      "train loss:0.2991883960718796\n",
      "train loss:0.36385854713996124\n",
      "train loss:0.34430160080007466\n",
      "train loss:0.4296141569369747\n",
      "train loss:0.346564013108956\n",
      "train loss:0.20941605660312568\n",
      "train loss:0.373912651283434\n",
      "train loss:0.36466198217607615\n",
      "train loss:0.31171714759995983\n",
      "train loss:0.37893043553600336\n",
      "train loss:0.3682925247864057\n",
      "train loss:0.392445320085865\n",
      "train loss:0.4481750293695591\n",
      "train loss:0.3310205884592669\n",
      "train loss:0.3689515567927708\n",
      "train loss:0.40989333676331896\n",
      "train loss:0.24509920808155386\n",
      "train loss:0.3575393717529649\n",
      "train loss:0.2380009741799537\n",
      "train loss:0.2802749825771536\n",
      "train loss:0.28149880429691737\n",
      "train loss:0.3472774488743454\n",
      "train loss:0.32799360044577425\n",
      "train loss:0.34371584370636776\n",
      "train loss:0.29761260377070964\n",
      "train loss:0.3333235310958031\n",
      "train loss:0.38981873869532613\n",
      "train loss:0.38954422907401215\n",
      "train loss:0.2400831452589839\n",
      "train loss:0.4301108545957242\n",
      "train loss:0.46058525916666787\n",
      "train loss:0.34724725307252896\n",
      "train loss:0.3703230321626239\n",
      "train loss:0.37629050055361873\n",
      "train loss:0.35080766298243604\n",
      "train loss:0.26786133085212194\n",
      "train loss:0.3950162986922588\n",
      "train loss:0.4123210738878825\n",
      "train loss:0.35729052436557096\n",
      "train loss:0.3699799131238819\n",
      "train loss:0.32635293199176596\n",
      "train loss:0.36412818958644144\n",
      "train loss:0.3402550945648278\n",
      "train loss:0.36242007377903124\n",
      "train loss:0.4254734539498342\n",
      "train loss:0.3341871118241817\n",
      "train loss:0.28081891224687505\n",
      "train loss:0.38160978942250146\n",
      "train loss:0.34495733268488205\n",
      "train loss:0.34367883008300815\n",
      "train loss:0.2990889521174214\n",
      "train loss:0.3702850381228588\n",
      "train loss:0.43237847990211903\n",
      "train loss:0.21981185557519844\n",
      "train loss:0.23817751380920935\n",
      "train loss:0.4040018032467493\n",
      "train loss:0.3954635564528811\n",
      "train loss:0.3174265015825451\n",
      "train loss:0.3869851438938203\n",
      "train loss:0.36374761266060596\n",
      "train loss:0.3743010477540098\n",
      "train loss:0.22157587865178358\n",
      "train loss:0.3326083399135122\n",
      "train loss:0.3478339837605924\n",
      "train loss:0.31363181019364017\n",
      "train loss:0.4390036821130311\n",
      "train loss:0.3896880982820579\n",
      "train loss:0.3562513709472671\n",
      "train loss:0.39054145979541\n",
      "train loss:0.39345964034627284\n",
      "train loss:0.233197702893704\n",
      "train loss:0.4059706776149752\n",
      "train loss:0.3899134852866704\n",
      "train loss:0.232830883755059\n",
      "train loss:0.3680729786418814\n",
      "train loss:0.2727570141207736\n",
      "train loss:0.4780898671337359\n",
      "train loss:0.29577531624871556\n",
      "train loss:0.37976794919891793\n",
      "train loss:0.3395584706290484\n",
      "train loss:0.4595387688154936\n",
      "train loss:0.3360784681804618\n",
      "train loss:0.415701575290871\n",
      "train loss:0.31061480628874677\n",
      "train loss:0.41064024416706874\n",
      "train loss:0.36863467977571196\n",
      "train loss:0.3523607746583822\n",
      "train loss:0.3458034113087764\n",
      "train loss:0.23391659708904744\n",
      "train loss:0.36714734724844383\n",
      "train loss:0.4440959805301168\n",
      "train loss:0.4060150141750169\n",
      "train loss:0.38347289872517765\n",
      "train loss:0.37441856780309885\n",
      "train loss:0.32993909463455656\n",
      "train loss:0.2622952777276836\n",
      "train loss:0.37314359750923126\n",
      "train loss:0.4862389132919418\n",
      "train loss:0.3966424838527762\n",
      "train loss:0.3490445984782109\n",
      "train loss:0.4181542958395522\n",
      "train loss:0.3986014099530079\n",
      "train loss:0.37711124054747147\n",
      "train loss:0.4489580784807824\n",
      "train loss:0.3020267050934768\n",
      "train loss:0.30836642149879473\n",
      "train loss:0.24858391850227368\n",
      "train loss:0.27924540947281856\n",
      "train loss:0.4848347761539287\n",
      "train loss:0.3691294154584619\n",
      "train loss:0.37115598486372664\n",
      "train loss:0.33204100347548393\n",
      "train loss:0.26173649203227956\n",
      "train loss:0.21131687857859086\n",
      "train loss:0.34133625837975246\n",
      "train loss:0.4024697110797363\n",
      "train loss:0.4112899616333096\n",
      "train loss:0.29744035549130887\n",
      "train loss:0.3154946049637147\n",
      "train loss:0.2799695783852983\n",
      "train loss:0.33797848037830946\n",
      "train loss:0.3843959838852765\n",
      "train loss:0.4743069021382816\n",
      "train loss:0.295355775111682\n",
      "train loss:0.5021449914303231\n",
      "train loss:0.24806379016024163\n",
      "train loss:0.4018094846246777\n",
      "train loss:0.39823501586558896\n",
      "train loss:0.25349977773238636\n",
      "train loss:0.30461031813017575\n",
      "train loss:0.30192450086860734\n",
      "train loss:0.5090424112530976\n",
      "train loss:0.2729568789802145\n",
      "train loss:0.5308974812067493\n",
      "train loss:0.2738516716947662\n",
      "train loss:0.35620298878700213\n",
      "train loss:0.5128764064984417\n",
      "train loss:0.31561358810328427\n",
      "train loss:0.4916329082820104\n",
      "train loss:0.3403343687954801\n",
      "train loss:0.34906755383794563\n",
      "train loss:0.3559049286490694\n",
      "train loss:0.5397448679810201\n",
      "train loss:0.3053101255995902\n",
      "train loss:0.3735066553880078\n",
      "train loss:0.33944503669705606\n",
      "train loss:0.5106546734082282\n",
      "train loss:0.4567304803041956\n",
      "train loss:0.4724679846678025\n",
      "train loss:0.3924857690026354\n",
      "train loss:0.3669845618918771\n",
      "train loss:0.30397961085154385\n",
      "train loss:0.32407337633264777\n",
      "train loss:0.35621923903189134\n",
      "train loss:0.44733387737368235\n",
      "train loss:0.3333709312921302\n",
      "train loss:0.38891976267565376\n",
      "train loss:0.30327937380257747\n",
      "train loss:0.2870619818550826\n",
      "train loss:0.33384103177279334\n",
      "train loss:0.29188783647163075\n",
      "train loss:0.35528786886157976\n",
      "train loss:0.34348202842300346\n",
      "train loss:0.36969141714786163\n",
      "train loss:0.3861402040723489\n",
      "train loss:0.29427097346443887\n",
      "train loss:0.22333628319748533\n",
      "train loss:0.2496980213484704\n",
      "train loss:0.29918537103145343\n",
      "train loss:0.34567183630623516\n",
      "train loss:0.33391448107923494\n",
      "train loss:0.3806422767145376\n",
      "train loss:0.2885804159504996\n",
      "train loss:0.2899803260745667\n",
      "train loss:0.3000825117504051\n",
      "train loss:0.39525448357477266\n",
      "train loss:0.5109795482459717\n",
      "train loss:0.3333387627188014\n",
      "train loss:0.23357221030211522\n",
      "train loss:0.21286299347933535\n",
      "train loss:0.27857884472085603\n",
      "train loss:0.339279025820907\n",
      "train loss:0.252249191095835\n",
      "train loss:0.26890581683968573\n",
      "train loss:0.42526926009651034\n",
      "train loss:0.3324245967040929\n",
      "train loss:0.40833464141855985\n",
      "train loss:0.3004400349983549\n",
      "train loss:0.40535496712071356\n",
      "train loss:0.41562872049625005\n",
      "train loss:0.5078301742899046\n",
      "train loss:0.22137548607748317\n",
      "train loss:0.5487008805350728\n",
      "train loss:0.3421884916688401\n",
      "train loss:0.3110074607709063\n",
      "train loss:0.45380377015789014\n",
      "train loss:0.2412244139625753\n",
      "train loss:0.5559800452165683\n",
      "train loss:0.2523293161213237\n",
      "train loss:0.4146193695810264\n",
      "train loss:0.3341856803214563\n",
      "train loss:0.4068582987903659\n",
      "train loss:0.3100512119068253\n",
      "train loss:0.3632488251174408\n",
      "train loss:0.4547045401242187\n",
      "train loss:0.5313621324627729\n",
      "train loss:0.3419409230839346\n",
      "train loss:0.3681903294660348\n",
      "train loss:0.3350864742811317\n",
      "train loss:0.30302768048321205\n",
      "train loss:0.3472014056239616\n",
      "train loss:0.2751213310155339\n",
      "train loss:0.30574297828726865\n",
      "train loss:0.33989643392075897\n",
      "train loss:0.26025626176293015\n",
      "train loss:0.4036767714100739\n",
      "train loss:0.4311341623658628\n",
      "train loss:0.3617748447577456\n",
      "train loss:0.2350435235674179\n",
      "train loss:0.3911014604410549\n",
      "train loss:0.3477105547049632\n",
      "train loss:0.35788859222129865\n",
      "train loss:0.26043856984582336\n",
      "train loss:0.34316299611786766\n",
      "train loss:0.20026969998313457\n",
      "train loss:0.3568835889680657\n",
      "train loss:0.36157237786043417\n",
      "train loss:0.39713057432102106\n",
      "train loss:0.4183570083275226\n",
      "train loss:0.37577339163263085\n",
      "train loss:0.3942910575761342\n",
      "train loss:0.4639323966276639\n",
      "train loss:0.38706860328424303\n",
      "train loss:0.3488326770790344\n",
      "train loss:0.3945191032790043\n",
      "train loss:0.4683724557617947\n",
      "train loss:0.2807232603183937\n",
      "train loss:0.36075210677517916\n",
      "train loss:0.40969941672987803\n",
      "train loss:0.27373747335497334\n",
      "train loss:0.4403224131311452\n",
      "train loss:0.4341922747355051\n",
      "train loss:0.266990758823578\n",
      "train loss:0.31396765188667985\n",
      "train loss:0.44502258951931656\n",
      "train loss:0.44300617819202787\n",
      "train loss:0.42106769167335095\n",
      "train loss:0.41570351462787997\n",
      "train loss:0.3624333486567454\n",
      "train loss:0.2291202695073548\n",
      "train loss:0.39478808846669866\n",
      "train loss:0.2903830401839629\n",
      "train loss:0.552579636876862\n",
      "train loss:0.4221572307905769\n",
      "train loss:0.36226866223848037\n",
      "train loss:0.38082097550643534\n",
      "train loss:0.45599492239052053\n",
      "train loss:0.2429240296420409\n",
      "train loss:0.41053880761709893\n",
      "train loss:0.37273853194599627\n",
      "train loss:0.2689169729921373\n",
      "train loss:0.33336800050324233\n",
      "train loss:0.2842121717064756\n",
      "train loss:0.28496895403797906\n",
      "train loss:0.3183520982885658\n",
      "train loss:0.27610730215660095\n",
      "train loss:0.2811349330483457\n",
      "train loss:0.25403435917385925\n",
      "train loss:0.42997236270180417\n",
      "train loss:0.42097201514748744\n",
      "train loss:0.24179273144984903\n",
      "train loss:0.37270434661765756\n",
      "train loss:0.3320723236405753\n",
      "train loss:0.349863010463268\n",
      "train loss:0.3164772463931242\n",
      "train loss:0.28729059710068844\n",
      "train loss:0.3992734875752501\n",
      "train loss:0.39113537963150835\n",
      "train loss:0.3258076007688489\n",
      "train loss:0.3113931850458646\n",
      "train loss:0.2662334157241258\n",
      "train loss:0.5971251535195348\n",
      "train loss:0.3630327325776914\n",
      "train loss:0.4121822239075986\n",
      "train loss:0.46727607252666814\n",
      "train loss:0.28655819953877437\n",
      "train loss:0.2645415486737819\n",
      "train loss:0.4334632659061851\n",
      "train loss:0.3624040968887282\n",
      "train loss:0.3342871981161237\n",
      "train loss:0.3786217994548253\n",
      "train loss:0.5593996693426889\n",
      "train loss:0.4850687462436509\n",
      "train loss:0.43071812116592717\n",
      "train loss:0.31281249403955025\n",
      "train loss:0.3591106972888827\n",
      "train loss:0.44761200821446195\n",
      "train loss:0.4971713751273104\n",
      "train loss:0.3870800553934072\n",
      "train loss:0.36614333526641885\n",
      "train loss:0.3005819355898442\n",
      "train loss:0.35374141372054796\n",
      "train loss:0.2761687936002065\n",
      "train loss:0.30398589561058403\n",
      "train loss:0.250037809485335\n",
      "train loss:0.42849169338397525\n",
      "train loss:0.30936064525023727\n",
      "train loss:0.4571588711002055\n",
      "train loss:0.45307221075302273\n",
      "train loss:0.28061681146559736\n",
      "train loss:0.33955174071900596\n",
      "train loss:0.2945597975971095\n",
      "train loss:0.4218490875727335\n",
      "train loss:0.28166985981409004\n",
      "train loss:0.4294717042940923\n",
      "train loss:0.35205674915062457\n",
      "train loss:0.4138358663356363\n",
      "train loss:0.3380526475217938\n",
      "train loss:0.3307819362336737\n",
      "train loss:0.45932083248152966\n",
      "train loss:0.37486941945020386\n",
      "train loss:0.40846841348990504\n",
      "train loss:0.47123093790694165\n",
      "train loss:0.5940216597861417\n",
      "train loss:0.384458331253655\n",
      "train loss:0.38581214067827246\n",
      "train loss:0.3324228847529135\n",
      "train loss:0.29822355791173444\n",
      "train loss:0.28517207613327683\n",
      "train loss:0.4348538121904674\n",
      "train loss:0.31052766670128484\n",
      "train loss:0.4356625006700887\n",
      "train loss:0.3271507600919212\n",
      "train loss:0.37068435260373256\n",
      "train loss:0.36815104331610776\n",
      "train loss:0.2920956311838328\n",
      "train loss:0.47105864222557065\n",
      "train loss:0.32376591484904954\n",
      "train loss:0.3916756793456148\n",
      "train loss:0.2671036609269726\n",
      "train loss:0.3518359608123828\n",
      "train loss:0.2923057832886029\n",
      "train loss:0.3027523315754244\n",
      "train loss:0.3252429065077997\n",
      "train loss:0.36152223764262\n",
      "train loss:0.3612566258804323\n",
      "train loss:0.31926796084991155\n",
      "train loss:0.30891742311771614\n",
      "train loss:0.2555584955086293\n",
      "train loss:0.34258094801978195\n",
      "train loss:0.36684185296972577\n",
      "train loss:0.25389510469438703\n",
      "train loss:0.5046741526529503\n",
      "train loss:0.3821769021373477\n",
      "train loss:0.361738929784008\n",
      "train loss:0.3151971138713084\n",
      "train loss:0.35378130850881534\n",
      "train loss:0.2649155066196443\n",
      "train loss:0.24824540501282638\n",
      "train loss:0.277034538119354\n",
      "train loss:0.33356534274437233\n",
      "train loss:0.38337411281091627\n",
      "train loss:0.3712227400604389\n",
      "train loss:0.5833442251581599\n",
      "train loss:0.3620004432482301\n",
      "train loss:0.47828406305074994\n",
      "train loss:0.3848891390207993\n",
      "train loss:0.27291227992181954\n",
      "train loss:0.30419656373751613\n",
      "train loss:0.47090695999850846\n",
      "train loss:0.3840262912291284\n",
      "train loss:0.3998294392612782\n",
      "train loss:0.3033351984706254\n",
      "train loss:0.5719111169199738\n",
      "train loss:0.3453877492884432\n",
      "train loss:0.391903460175936\n",
      "train loss:0.46717814095148463\n",
      "train loss:0.30418158040327403\n",
      "train loss:0.370404593202435\n",
      "train loss:0.38019670227734603\n",
      "train loss:0.35574736017281544\n",
      "train loss:0.2783857246360225\n",
      "train loss:0.35851192998662257\n",
      "train loss:0.22413031083481708\n",
      "train loss:0.408202179451223\n",
      "train loss:0.2209802346822828\n",
      "train loss:0.3247457999151425\n",
      "train loss:0.3444982005007208\n",
      "train loss:0.4596964366831296\n",
      "train loss:0.4020146704550293\n",
      "train loss:0.5394990760500692\n",
      "train loss:0.3996201754043891\n",
      "train loss:0.3450430000407504\n",
      "train loss:0.4134026231328766\n",
      "train loss:0.21111064636746882\n",
      "train loss:0.26114351176967515\n",
      "train loss:0.3992175953819909\n",
      "train loss:0.31081797750814266\n",
      "train loss:0.49991412660342355\n",
      "train loss:0.24933780071448974\n",
      "train loss:0.4025996986070354\n",
      "train loss:0.36351144915300887\n",
      "train loss:0.5261603720449609\n",
      "train loss:0.2835101668756842\n",
      "train loss:0.31771996435614297\n",
      "train loss:0.3871245804111629\n",
      "train loss:0.40944398075559685\n",
      "train loss:0.4062572207300967\n",
      "train loss:0.25908141082786085\n",
      "train loss:0.4983956618080842\n",
      "train loss:0.30675977685372974\n",
      "train loss:0.3288165704096125\n",
      "train loss:0.28657248906278315\n",
      "train loss:0.5584259120747574\n",
      "train loss:0.3786871358581003\n",
      "train loss:0.2779186809730435\n",
      "train loss:0.24183769696951984\n",
      "train loss:0.3314684222606836\n",
      "train loss:0.3454586433009474\n",
      "train loss:0.42988034979243567\n",
      "train loss:0.3611243596673777\n",
      "train loss:0.31747289048050126\n",
      "train loss:0.36638722434575505\n",
      "train loss:0.3515098977257183\n",
      "train loss:0.28467406113476995\n",
      "train loss:0.22267562733558105\n",
      "train loss:0.29464372723851767\n",
      "train loss:0.35533386450439686\n",
      "train loss:0.2905111729064553\n",
      "train loss:0.2923875408838246\n",
      "train loss:0.28520623330403483\n",
      "train loss:0.3538666577117512\n",
      "train loss:0.5003539752205954\n",
      "train loss:0.22160883340054988\n",
      "train loss:0.515437936941881\n",
      "train loss:0.39659535944175106\n",
      "train loss:0.37439159213614326\n",
      "train loss:0.34401204348150294\n",
      "train loss:0.5034083951949542\n",
      "train loss:0.2767797811722926\n",
      "train loss:0.47424868883916027\n",
      "train loss:0.5059414253861657\n",
      "train loss:0.30544269048764844\n",
      "train loss:0.3892487668432532\n",
      "train loss:0.28937289814157774\n",
      "train loss:0.44872841677611675\n",
      "train loss:0.3529421059722435\n",
      "train loss:0.44693230318359467\n",
      "train loss:0.2934045035678394\n",
      "train loss:0.2950662199267565\n",
      "train loss:0.31533478911483553\n",
      "train loss:0.34513888400659387\n",
      "train loss:0.35694264094089445\n",
      "train loss:0.3082847926851945\n",
      "train loss:0.3709615470088125\n",
      "train loss:0.48761833500331386\n",
      "train loss:0.389443001075744\n",
      "train loss:0.3542293411465481\n",
      "train loss:0.46668159624610395\n",
      "train loss:0.36559099686631263\n",
      "train loss:0.3037245986093211\n",
      "train loss:0.2105261437119249\n",
      "train loss:0.2865715804382618\n",
      "train loss:0.4598805809626771\n",
      "train loss:0.4022715879989123\n",
      "train loss:0.37846566798591796\n",
      "train loss:0.29534243633662305\n",
      "train loss:0.3423854211563557\n",
      "train loss:0.24631794491773273\n",
      "train loss:0.2503401216516191\n",
      "train loss:0.27518879181944483\n",
      "train loss:0.42645378625478664\n",
      "train loss:0.45522272860726576\n",
      "train loss:0.26076000674525784\n",
      "train loss:0.32528295090827547\n",
      "train loss:0.26915311918776047\n",
      "train loss:0.42122906839780727\n",
      "train loss:0.3582085273952243\n",
      "train loss:0.5887786726329058\n",
      "train loss:0.35690388446528487\n",
      "train loss:0.23036185362245007\n",
      "train loss:0.31535206917709757\n",
      "train loss:0.2432599248270964\n",
      "train loss:0.3849022882544261\n",
      "train loss:0.5184202006512696\n",
      "train loss:0.46353667762763834\n",
      "train loss:0.4988209715286355\n",
      "train loss:0.3807292695311801\n",
      "train loss:0.32488595817907945\n",
      "train loss:0.43671127799310233\n",
      "train loss:0.44153343249617477\n",
      "train loss:0.288405770677268\n",
      "train loss:0.408966012283939\n",
      "train loss:0.3647638509644628\n",
      "train loss:0.2695631200287671\n",
      "train loss:0.41595736379242676\n",
      "train loss:0.29507934067196123\n",
      "train loss:0.34284565364962377\n",
      "train loss:0.2284139223654577\n",
      "train loss:0.30675250891784483\n",
      "train loss:0.36227359868252196\n",
      "train loss:0.3932333765039593\n",
      "train loss:0.43291732644112413\n",
      "train loss:0.2891214570950823\n",
      "train loss:0.35047467400234333\n",
      "train loss:0.3565114881844354\n",
      "train loss:0.3999925263536025\n",
      "train loss:0.3603753003580878\n",
      "train loss:0.40331158690619984\n",
      "train loss:0.391932601022653\n",
      "train loss:0.3482517683667778\n",
      "train loss:0.5004369481111107\n",
      "train loss:0.2731191241318291\n",
      "train loss:0.268252823863342\n",
      "train loss:0.26446174125503413\n",
      "train loss:0.21416368124827778\n",
      "train loss:0.47147868098693574\n",
      "train loss:0.3176697221624554\n",
      "train loss:0.29688032115287655\n",
      "train loss:0.33112621021721905\n",
      "train loss:0.32484490376510955\n",
      "train loss:0.2729619085308563\n",
      "train loss:0.31114507266843905\n",
      "train loss:0.3527838536233105\n",
      "train loss:0.329067106726926\n",
      "train loss:0.3892093424907218\n",
      "train loss:0.32997812536909926\n",
      "train loss:0.3405570928455767\n",
      "train loss:0.36252344944926407\n",
      "train loss:0.3763066087111025\n",
      "train loss:0.23670721558584656\n",
      "train loss:0.3451833292963334\n",
      "train loss:0.3019155459516649\n",
      "train loss:0.2906099527467261\n",
      "train loss:0.47181564695152983\n",
      "train loss:0.2947748578237253\n",
      "train loss:0.26835128564545413\n",
      "train loss:0.3574256170015941\n",
      "train loss:0.2545263824413421\n",
      "=== epoch:16, train acc:0.845, test acc:0.853 ===\n",
      "train loss:0.41791889827085216\n",
      "train loss:0.2631631525169193\n",
      "train loss:0.3823685788113525\n",
      "train loss:0.4842319031828754\n",
      "train loss:0.35507732590572433\n",
      "train loss:0.49480303455349256\n",
      "train loss:0.3294541866800178\n",
      "train loss:0.3093604010734829\n",
      "train loss:0.3352648240523729\n",
      "train loss:0.383666346562299\n",
      "train loss:0.25939668129645593\n",
      "train loss:0.30226056303369836\n",
      "train loss:0.33205912732822845\n",
      "train loss:0.4829228671129583\n",
      "train loss:0.337629001046692\n",
      "train loss:0.21796488787007323\n",
      "train loss:0.35413198230981857\n",
      "train loss:0.37096061764203975\n",
      "train loss:0.29083940217831067\n",
      "train loss:0.31619596882201917\n",
      "train loss:0.2647816265819024\n",
      "train loss:0.384326475649456\n",
      "train loss:0.3332293203250088\n",
      "train loss:0.33170875588844956\n",
      "train loss:0.5413205871338415\n",
      "train loss:0.24784816942743115\n",
      "train loss:0.39645751725958533\n",
      "train loss:0.26589986612479727\n",
      "train loss:0.42026499813891405\n",
      "train loss:0.400919839505863\n",
      "train loss:0.32716984233391444\n",
      "train loss:0.3736934732758946\n",
      "train loss:0.3690489326792408\n",
      "train loss:0.3302259055914337\n",
      "train loss:0.37125835724030687\n",
      "train loss:0.3010542989580549\n",
      "train loss:0.39749673467716007\n",
      "train loss:0.3287152720681302\n",
      "train loss:0.670698605608749\n",
      "train loss:0.45462467602063383\n",
      "train loss:0.3746958200356476\n",
      "train loss:0.24890979114272022\n",
      "train loss:0.22862907273084157\n",
      "train loss:0.37901558699215143\n",
      "train loss:0.324395019543912\n",
      "train loss:0.3798801292117722\n",
      "train loss:0.2848174211706491\n",
      "train loss:0.27664470471166225\n",
      "train loss:0.27843443475349855\n",
      "train loss:0.4541033464520538\n",
      "train loss:0.3780347836634612\n",
      "train loss:0.3633953423457512\n",
      "train loss:0.27012201292708743\n",
      "train loss:0.37990178699345406\n",
      "train loss:0.4063121290322411\n",
      "train loss:0.33326483949620966\n",
      "train loss:0.3090268398722385\n",
      "train loss:0.3371874267265688\n",
      "train loss:0.28664825949547607\n",
      "train loss:0.28576159188080036\n",
      "train loss:0.28429772313157536\n",
      "train loss:0.3177227348221011\n",
      "train loss:0.4340441648367724\n",
      "train loss:0.34512213654504975\n",
      "train loss:0.2938760294830606\n",
      "train loss:0.2721021962447885\n",
      "train loss:0.42327819181042287\n",
      "train loss:0.32770261310217075\n",
      "train loss:0.30340509911184704\n",
      "train loss:0.22220001322712193\n",
      "train loss:0.24363239236717799\n",
      "train loss:0.35395325963874774\n",
      "train loss:0.5072346601461941\n",
      "train loss:0.3866599796854132\n",
      "train loss:0.3231987282346631\n",
      "train loss:0.33340308127224155\n",
      "train loss:0.32704545925473794\n",
      "train loss:0.439622132237055\n",
      "train loss:0.2410738466150211\n",
      "train loss:0.3237204644953017\n",
      "train loss:0.35315540648171656\n",
      "train loss:0.3806069346080628\n",
      "train loss:0.274505399779741\n",
      "train loss:0.3302732597532354\n",
      "train loss:0.2739774109435611\n",
      "train loss:0.3145223116811682\n",
      "train loss:0.3708958703340284\n",
      "train loss:0.3389365910185667\n",
      "train loss:0.3563889414355559\n",
      "train loss:0.3605715183701818\n",
      "train loss:0.33893117815216306\n",
      "train loss:0.4263762189350189\n",
      "train loss:0.25418255910247106\n",
      "train loss:0.3254656584962718\n",
      "train loss:0.3618283832615727\n",
      "train loss:0.2927901425900435\n",
      "train loss:0.24914698535563154\n",
      "train loss:0.2841641999044995\n",
      "train loss:0.24106325966534217\n",
      "train loss:0.25750711018587086\n",
      "train loss:0.3477025451938177\n",
      "train loss:0.2136648502421556\n",
      "train loss:0.2666316158748331\n",
      "train loss:0.4813899547868128\n",
      "train loss:0.3275938241933974\n",
      "train loss:0.4763815329586393\n",
      "train loss:0.5036487478848816\n",
      "train loss:0.4835512524532751\n",
      "train loss:0.22792941338721068\n",
      "train loss:0.22957962127366102\n",
      "train loss:0.4203723528435512\n",
      "train loss:0.2683562658002934\n",
      "train loss:0.22648547409573674\n",
      "train loss:0.3137487552134006\n",
      "train loss:0.33775130311809526\n",
      "train loss:0.4538331057682259\n",
      "train loss:0.34814622550228264\n",
      "train loss:0.26318287448598116\n",
      "train loss:0.24636468890836516\n",
      "train loss:0.242610481614103\n",
      "train loss:0.32437864513466463\n",
      "train loss:0.27858244271873234\n",
      "train loss:0.36180542295560836\n",
      "train loss:0.41254265860775874\n",
      "train loss:0.26120192605453735\n",
      "train loss:0.33587335383589106\n",
      "train loss:0.40395176795675736\n",
      "train loss:0.2966315611385191\n",
      "train loss:0.25238726643185855\n",
      "train loss:0.3240569638228431\n",
      "train loss:0.5566986296034124\n",
      "train loss:0.23177900607198995\n",
      "train loss:0.41263779645629944\n",
      "train loss:0.32246131769852027\n",
      "train loss:0.22896666416912606\n",
      "train loss:0.24377020038362937\n",
      "train loss:0.3555453982549801\n",
      "train loss:0.3055389025608876\n",
      "train loss:0.43203127765702787\n",
      "train loss:0.23793343994374572\n",
      "train loss:0.25542934410615453\n",
      "train loss:0.32580051570180474\n",
      "train loss:0.2516544200360195\n",
      "train loss:0.41621275175165307\n",
      "train loss:0.45430238496305025\n",
      "train loss:0.3636365171497811\n",
      "train loss:0.3388228732434765\n",
      "train loss:0.22659613872890108\n",
      "train loss:0.41764127400446865\n",
      "train loss:0.27107579792626824\n",
      "train loss:0.2214407455727727\n",
      "train loss:0.4534496709561892\n",
      "train loss:0.29442926990434914\n",
      "train loss:0.4283741375475494\n",
      "train loss:0.37866242244531373\n",
      "train loss:0.3020069684416457\n",
      "train loss:0.4398091804183768\n",
      "train loss:0.5584978916164078\n",
      "train loss:0.3972991507169455\n",
      "train loss:0.261476682645781\n",
      "train loss:0.3480304633218199\n",
      "train loss:0.34005961870070694\n",
      "train loss:0.34281419741163544\n",
      "train loss:0.4610552354272627\n",
      "train loss:0.311425390727439\n",
      "train loss:0.5086878334571374\n",
      "train loss:0.4218353283521368\n",
      "train loss:0.35647992232754355\n",
      "train loss:0.28820160263826755\n",
      "train loss:0.38366113280322645\n",
      "train loss:0.26817617668852856\n",
      "train loss:0.23824270823891547\n",
      "train loss:0.2686202296239378\n",
      "train loss:0.31530491360007573\n",
      "train loss:0.25607123761835654\n",
      "train loss:0.39672542965167573\n",
      "train loss:0.32110431074643947\n",
      "train loss:0.25638044187645814\n",
      "train loss:0.36956720336006266\n",
      "train loss:0.28117401461441166\n",
      "train loss:0.318504879823043\n",
      "train loss:0.27327473541580594\n",
      "train loss:0.2779937489459466\n",
      "train loss:0.3237802279338704\n",
      "train loss:0.41334416122917383\n",
      "train loss:0.3195774375585252\n",
      "train loss:0.3454369887088201\n",
      "train loss:0.4071364069477885\n",
      "train loss:0.1931268967415158\n",
      "train loss:0.328912727675784\n",
      "train loss:0.4006791896736175\n",
      "train loss:0.5276193106217992\n",
      "train loss:0.27958233945381833\n",
      "train loss:0.4063426760638022\n",
      "train loss:0.36332274325313435\n",
      "train loss:0.3552161787500681\n",
      "train loss:0.3848956727721245\n",
      "train loss:0.3681141117770256\n",
      "train loss:0.37597268794222205\n",
      "train loss:0.3550397203199824\n",
      "train loss:0.3441621648628873\n",
      "train loss:0.41254200716291156\n",
      "train loss:0.32133148066446277\n",
      "train loss:0.28977932670452394\n",
      "train loss:0.40533223997249657\n",
      "train loss:0.44322516957401265\n",
      "train loss:0.3490252461187248\n",
      "train loss:0.36127993074841525\n",
      "train loss:0.3878757301925688\n",
      "train loss:0.4303090006080719\n",
      "train loss:0.6972005645201566\n",
      "train loss:0.31490118249344207\n",
      "train loss:0.2778414644351202\n",
      "train loss:0.3561136866032894\n",
      "train loss:0.45836467438304396\n",
      "train loss:0.3049461917650704\n",
      "train loss:0.41835540634867435\n",
      "train loss:0.33725634953063677\n",
      "train loss:0.2981952736206121\n",
      "train loss:0.3007203603752041\n",
      "train loss:0.4403706430215744\n",
      "train loss:0.20999353722430034\n",
      "train loss:0.6184465034780754\n",
      "train loss:0.23609984193679381\n",
      "train loss:0.46028705769258693\n",
      "train loss:0.33758327710593294\n",
      "train loss:0.3310241017868896\n",
      "train loss:0.4692334718461349\n",
      "train loss:0.2699667220105539\n",
      "train loss:0.29395555514109317\n",
      "train loss:0.27939668006559104\n",
      "train loss:0.3903044108795619\n",
      "train loss:0.23370069604038293\n",
      "train loss:0.40709628316897\n",
      "train loss:0.41228379572371343\n",
      "train loss:0.38211167872454105\n",
      "train loss:0.37699973830352157\n",
      "train loss:0.2590801160471465\n",
      "train loss:0.3989230310413674\n",
      "train loss:0.25510367803601003\n",
      "train loss:0.31739514378500666\n",
      "train loss:0.2582041915080911\n",
      "train loss:0.26708779282418915\n",
      "train loss:0.3336036619971117\n",
      "train loss:0.4189803366020067\n",
      "train loss:0.2077513889970964\n",
      "train loss:0.35165191490729397\n",
      "train loss:0.40043235099833185\n",
      "train loss:0.31132627667478546\n",
      "train loss:0.32678676265334644\n",
      "train loss:0.39107752299696186\n",
      "train loss:0.3838554926926498\n",
      "train loss:0.525772404964533\n",
      "train loss:0.41691176837478117\n",
      "train loss:0.3036555772676789\n",
      "train loss:0.2909305210640939\n",
      "train loss:0.3645165303834258\n",
      "train loss:0.34096173634664917\n",
      "train loss:0.3474282437852855\n",
      "train loss:0.3622739020008426\n",
      "train loss:0.3290283308334363\n",
      "train loss:0.3712331615326023\n",
      "train loss:0.429700199614986\n",
      "train loss:0.37629197245003715\n",
      "train loss:0.2784309155419574\n",
      "train loss:0.45079468737655576\n",
      "train loss:0.28544107822164017\n",
      "train loss:0.3981043356302896\n",
      "train loss:0.32328861352908866\n",
      "train loss:0.3463277925896254\n",
      "train loss:0.32123305429211174\n",
      "train loss:0.28640813684978256\n",
      "train loss:0.2816563391668458\n",
      "train loss:0.2211129146708263\n",
      "train loss:0.3400461236026179\n",
      "train loss:0.3156254316427544\n",
      "train loss:0.40075094524979016\n",
      "train loss:0.350664611586588\n",
      "train loss:0.30710423532319475\n",
      "train loss:0.5333708414843116\n",
      "train loss:0.24020896376166398\n",
      "train loss:0.2774479794212079\n",
      "train loss:0.5070594452542887\n",
      "train loss:0.46558945985299316\n",
      "train loss:0.28720162691485834\n",
      "train loss:0.33451785796691413\n",
      "train loss:0.43170179007890425\n",
      "train loss:0.40550826241509424\n",
      "train loss:0.3445644802143957\n",
      "train loss:0.2192245135599081\n",
      "train loss:0.3434413348744473\n",
      "train loss:0.2841888800733628\n",
      "train loss:0.4938069292197419\n",
      "train loss:0.30242927678668885\n",
      "train loss:0.21293085035410658\n",
      "train loss:0.4894339739844202\n",
      "train loss:0.2020412664000016\n",
      "train loss:0.26768623589060564\n",
      "train loss:0.3557165569236181\n",
      "train loss:0.38104734847132365\n",
      "train loss:0.4043661536305456\n",
      "train loss:0.43009856450265427\n",
      "train loss:0.3474930253538681\n",
      "train loss:0.37597624400642815\n",
      "train loss:0.269218773607353\n",
      "train loss:0.2649314052597765\n",
      "train loss:0.3793162698488314\n",
      "train loss:0.29047552177100766\n",
      "train loss:0.5769545151507343\n",
      "train loss:0.5309987037381907\n",
      "train loss:0.2907192091428091\n",
      "train loss:0.3217668211684963\n",
      "train loss:0.4050697635047824\n",
      "train loss:0.41691597741235575\n",
      "train loss:0.3052509388187615\n",
      "train loss:0.30505391256909653\n",
      "train loss:0.41182536714386375\n",
      "train loss:0.3507763522981995\n",
      "train loss:0.3561180333794617\n",
      "train loss:0.4405821645212217\n",
      "train loss:0.21417705170045628\n",
      "train loss:0.25184222667704037\n",
      "train loss:0.3890974523801991\n",
      "train loss:0.24265516210496313\n",
      "train loss:0.39260653814137597\n",
      "train loss:0.1961765891425028\n",
      "train loss:0.2669320029323957\n",
      "train loss:0.3768200161608917\n",
      "train loss:0.530146684343427\n",
      "train loss:0.30209841006709853\n",
      "train loss:0.20347609554264814\n",
      "train loss:0.37276638714146015\n",
      "train loss:0.32980651809407546\n",
      "train loss:0.3566719176417358\n",
      "train loss:0.3664855207499261\n",
      "train loss:0.31511184713648754\n",
      "train loss:0.3995773101945256\n",
      "train loss:0.44085321816388245\n",
      "train loss:0.3569153219396371\n",
      "train loss:0.2946610505257097\n",
      "train loss:0.26556280672787463\n",
      "train loss:0.3648629568349482\n",
      "train loss:0.2734765689603542\n",
      "train loss:0.36513880679569793\n",
      "train loss:0.37630088232805536\n",
      "train loss:0.23330330697638452\n",
      "train loss:0.3671447240387702\n",
      "train loss:0.335094365289267\n",
      "train loss:0.3481677726796459\n",
      "train loss:0.4158335557502821\n",
      "train loss:0.31641573459644623\n",
      "train loss:0.32214810133314115\n",
      "train loss:0.22816765721032883\n",
      "train loss:0.36284299416888366\n",
      "train loss:0.3166941737631151\n",
      "train loss:0.29788631653648423\n",
      "train loss:0.453838899244085\n",
      "train loss:0.35494395143433627\n",
      "train loss:0.3122979360037458\n",
      "train loss:0.2473262386673791\n",
      "train loss:0.3484805637320169\n",
      "train loss:0.30534532037725304\n",
      "train loss:0.4121236381696506\n",
      "train loss:0.36829546281306164\n",
      "train loss:0.3232258959386507\n",
      "train loss:0.44107190453348444\n",
      "train loss:0.4002430519688625\n",
      "train loss:0.3547592997025735\n",
      "train loss:0.2817270084028126\n",
      "train loss:0.2529228458504383\n",
      "train loss:0.3484478998793692\n",
      "train loss:0.3064827175299365\n",
      "train loss:0.2284457726646384\n",
      "train loss:0.47862712333996343\n",
      "train loss:0.2414879247726281\n",
      "train loss:0.3452267862438548\n",
      "train loss:0.29571881031444947\n",
      "train loss:0.47390861941543927\n",
      "train loss:0.5583325165690065\n",
      "train loss:0.2819017935587067\n",
      "train loss:0.37904614108920937\n",
      "train loss:0.29812247312622825\n",
      "train loss:0.47766814411604697\n",
      "train loss:0.3800972868858915\n",
      "train loss:0.4687506914694948\n",
      "train loss:0.3752234051926229\n",
      "train loss:0.3559794096421451\n",
      "train loss:0.5086709110470775\n",
      "train loss:0.25033907310427606\n",
      "train loss:0.4681470307462078\n",
      "train loss:0.4222163106858571\n",
      "train loss:0.2601666876220512\n",
      "train loss:0.3440469830035643\n",
      "train loss:0.3367494788564986\n",
      "train loss:0.3294277488052087\n",
      "train loss:0.45361383506005354\n",
      "train loss:0.32191321715136034\n",
      "train loss:0.39336952877610853\n",
      "train loss:0.2988673155355291\n",
      "train loss:0.5038380124405815\n",
      "train loss:0.39077909784141995\n",
      "train loss:0.3334367409980697\n",
      "train loss:0.34465574694004447\n",
      "train loss:0.3212614222479397\n",
      "train loss:0.42500672908957304\n",
      "train loss:0.2579272328862181\n",
      "train loss:0.46141133400921236\n",
      "train loss:0.2823289929232952\n",
      "train loss:0.4981640989980552\n",
      "train loss:0.3713054042513969\n",
      "train loss:0.35058197494280596\n",
      "train loss:0.21384187430618973\n",
      "train loss:0.3627211810220625\n",
      "train loss:0.4446003457119631\n",
      "train loss:0.3526622008524572\n",
      "train loss:0.2496209574254228\n",
      "train loss:0.3679218761971399\n",
      "train loss:0.28061695409339504\n",
      "train loss:0.38789806558367834\n",
      "train loss:0.3408604788512233\n",
      "train loss:0.286713901047669\n",
      "train loss:0.34658299898749556\n",
      "train loss:0.4124326071566122\n",
      "train loss:0.25374877315247985\n",
      "train loss:0.3544851076962429\n",
      "train loss:0.3196892369885643\n",
      "train loss:0.32680434749472326\n",
      "train loss:0.30932459163339093\n",
      "train loss:0.35805651385810977\n",
      "train loss:0.4085628288877836\n",
      "train loss:0.29817261243573806\n",
      "train loss:0.24646361624840055\n",
      "train loss:0.39641709962800625\n",
      "train loss:0.37854272723729854\n",
      "train loss:0.3028002477564427\n",
      "train loss:0.3221464104871245\n",
      "train loss:0.3347264102540266\n",
      "train loss:0.2591840139373936\n",
      "train loss:0.2716817802113492\n",
      "train loss:0.30333299373839706\n",
      "train loss:0.39085107234118643\n",
      "train loss:0.40317661329084714\n",
      "train loss:0.25822998470407066\n",
      "train loss:0.3537428314655017\n",
      "train loss:0.24901661570341146\n",
      "train loss:0.26213406768441094\n",
      "train loss:0.3448306629061604\n",
      "train loss:0.24907874541720246\n",
      "train loss:0.30077396670789525\n",
      "train loss:0.3081832163432497\n",
      "train loss:0.3529680853656983\n",
      "train loss:0.430338407463635\n",
      "train loss:0.2755839771164439\n",
      "train loss:0.2904526124592156\n",
      "train loss:0.32308226570253323\n",
      "train loss:0.351866295782675\n",
      "train loss:0.389289792316881\n",
      "train loss:0.3372179553687741\n",
      "train loss:0.2484823500141113\n",
      "train loss:0.23812697026630208\n",
      "train loss:0.2362523831002636\n",
      "train loss:0.2740121033381067\n",
      "train loss:0.41717457578479417\n",
      "train loss:0.44220037037599724\n",
      "train loss:0.41437146655601564\n",
      "train loss:0.43195700846932317\n",
      "train loss:0.3245793249803836\n",
      "train loss:0.3068514307601458\n",
      "train loss:0.4593748144920612\n",
      "train loss:0.42593279560166075\n",
      "train loss:0.23680871514964413\n",
      "train loss:0.3065023824288932\n",
      "train loss:0.27093095310625737\n",
      "train loss:0.30678140603179416\n",
      "train loss:0.3455599936315348\n",
      "train loss:0.38537085007677696\n",
      "train loss:0.32269578465860005\n",
      "train loss:0.29100054898120076\n",
      "train loss:0.31153796478604084\n",
      "train loss:0.23146660670214186\n",
      "train loss:0.4597795605120851\n",
      "train loss:0.4175743651044977\n",
      "train loss:0.31934457320095544\n",
      "train loss:0.3936932812572385\n",
      "train loss:0.2788258884138234\n",
      "train loss:0.24781675445524398\n",
      "train loss:0.33394239287306854\n",
      "train loss:0.45194207608583753\n",
      "train loss:0.45899839816992133\n",
      "train loss:0.38970948872080624\n",
      "train loss:0.3000656681087976\n",
      "train loss:0.2601711916889029\n",
      "train loss:0.33680369733704046\n",
      "train loss:0.22022790695792266\n",
      "train loss:0.3013329034771971\n",
      "train loss:0.2924908864187346\n",
      "train loss:0.28318682792096206\n",
      "train loss:0.46847943422668764\n",
      "train loss:0.3680872068981023\n",
      "train loss:0.39818227300153664\n",
      "train loss:0.35294081761093027\n",
      "train loss:0.39916367463701424\n",
      "train loss:0.3585878058422092\n",
      "train loss:0.3536949176438117\n",
      "train loss:0.41225294269109314\n",
      "train loss:0.38230832869739456\n",
      "train loss:0.39058188981717484\n",
      "train loss:0.31434842463003926\n",
      "train loss:0.30563534224927563\n",
      "train loss:0.43383214474143855\n",
      "train loss:0.34051175313820875\n",
      "train loss:0.31024483828396937\n",
      "train loss:0.4563511815811938\n",
      "train loss:0.34808283929212147\n",
      "train loss:0.4107220078503996\n",
      "train loss:0.27622165227231343\n",
      "train loss:0.399001052541961\n",
      "train loss:0.3185025154898129\n",
      "train loss:0.2828025695014365\n",
      "train loss:0.38850344372384027\n",
      "train loss:0.27682182912891623\n",
      "train loss:0.3490498783967935\n",
      "train loss:0.20214272815820614\n",
      "train loss:0.40460640942460663\n",
      "train loss:0.21683706016611576\n",
      "train loss:0.42974971077747154\n",
      "train loss:0.3344800319024534\n",
      "train loss:0.3176424540923357\n",
      "train loss:0.5027170888560973\n",
      "train loss:0.3655291730987603\n",
      "train loss:0.3196627688400404\n",
      "train loss:0.3281428154593283\n",
      "train loss:0.3705500509219217\n",
      "train loss:0.3651572449537768\n",
      "train loss:0.3113740770418427\n",
      "train loss:0.2621831953259556\n",
      "train loss:0.40216926768019234\n",
      "train loss:0.4842811689201968\n",
      "train loss:0.22670449655922145\n",
      "train loss:0.16646782029228582\n",
      "train loss:0.4287788742532922\n",
      "train loss:0.3831592902223799\n",
      "train loss:0.3017454504207463\n",
      "train loss:0.3288403707232474\n",
      "train loss:0.28637775079731126\n",
      "train loss:0.2710048192427503\n",
      "train loss:0.38473382650685845\n",
      "train loss:0.22159873578222666\n",
      "train loss:0.37293509743024594\n",
      "train loss:0.30608846387703303\n",
      "train loss:0.2952760552354633\n",
      "train loss:0.35944529921698737\n",
      "train loss:0.36044095479618876\n",
      "train loss:0.48796799772171423\n",
      "train loss:0.3810188203134067\n",
      "train loss:0.5731426604701703\n",
      "train loss:0.2796686232569203\n",
      "train loss:0.402458186995443\n",
      "train loss:0.20792068586092388\n",
      "train loss:0.4098150807222619\n",
      "train loss:0.35144132148220614\n",
      "train loss:0.532704797162048\n",
      "train loss:0.4253041941058154\n",
      "train loss:0.39105076427780594\n",
      "train loss:0.36347762235288295\n",
      "train loss:0.34967809050845383\n",
      "train loss:0.2423127872916703\n",
      "train loss:0.37198527504679857\n",
      "train loss:0.21664338169014602\n",
      "train loss:0.51707100883714\n",
      "train loss:0.3015722495006043\n",
      "train loss:0.4108324779820371\n",
      "train loss:0.32097680405866014\n",
      "train loss:0.3424807090510063\n",
      "train loss:0.4235561943788562\n",
      "train loss:0.3093485309721101\n",
      "train loss:0.3629836543680199\n",
      "train loss:0.2589898567763096\n",
      "train loss:0.3837489742862714\n",
      "train loss:0.348534289428772\n",
      "train loss:0.27524815964692745\n",
      "train loss:0.23088916070662951\n",
      "train loss:0.2800694927592562\n",
      "train loss:0.3836000463209514\n",
      "train loss:0.45376349992237386\n",
      "train loss:0.24609864176415286\n",
      "train loss:0.3438763548569305\n",
      "train loss:0.39104508335679017\n",
      "train loss:0.24947910453368136\n",
      "train loss:0.4948230068050236\n",
      "train loss:0.23807324514180933\n",
      "train loss:0.46013175614570606\n",
      "train loss:0.33862319340501096\n",
      "train loss:0.2720923475003042\n",
      "train loss:0.279677686154414\n",
      "train loss:0.39762180881226045\n",
      "train loss:0.30139791127597065\n",
      "train loss:0.26074293829894235\n",
      "train loss:0.5434142761721312\n",
      "train loss:0.40236206821543485\n",
      "=== epoch:17, train acc:0.877, test acc:0.863 ===\n",
      "train loss:0.2833036096741555\n",
      "train loss:0.26577024691658147\n",
      "train loss:0.4454938197085535\n",
      "train loss:0.32409031266387067\n",
      "train loss:0.27067010172238787\n",
      "train loss:0.2968687040396895\n",
      "train loss:0.3859230868869387\n",
      "train loss:0.29171701193483257\n",
      "train loss:0.27695170577036543\n",
      "train loss:0.3887172123471651\n",
      "train loss:0.3056194710370485\n",
      "train loss:0.3381053754606767\n",
      "train loss:0.4247886012460325\n",
      "train loss:0.306554525231361\n",
      "train loss:0.34533060663546694\n",
      "train loss:0.39616831331632796\n",
      "train loss:0.29586104304364597\n",
      "train loss:0.39570645314640884\n",
      "train loss:0.264325469493366\n",
      "train loss:0.3944114404385573\n",
      "train loss:0.21985683491640767\n",
      "train loss:0.32170718085155026\n",
      "train loss:0.28889836128327007\n",
      "train loss:0.269234569324341\n",
      "train loss:0.43315233222891764\n",
      "train loss:0.2708300301948798\n",
      "train loss:0.4169794808450555\n",
      "train loss:0.3394306044032821\n",
      "train loss:0.37811575224336685\n",
      "train loss:0.29299501469529354\n",
      "train loss:0.27411814236400706\n",
      "train loss:0.3309377497924503\n",
      "train loss:0.5090418767826181\n",
      "train loss:0.4676735771242312\n",
      "train loss:0.4440613943872247\n",
      "train loss:0.3761935563229875\n",
      "train loss:0.4671422669159586\n",
      "train loss:0.26009078376011874\n",
      "train loss:0.35107102498206444\n",
      "train loss:0.273704038171485\n",
      "train loss:0.43249033907837414\n",
      "train loss:0.2699802289347909\n",
      "train loss:0.307535993919218\n",
      "train loss:0.3952377107535834\n",
      "train loss:0.3070735119785495\n",
      "train loss:0.326178872268688\n",
      "train loss:0.41157133310837124\n",
      "train loss:0.31367145201694635\n",
      "train loss:0.3490161115669706\n",
      "train loss:0.284185074841619\n",
      "train loss:0.3051060762798879\n",
      "train loss:0.3078690111067139\n",
      "train loss:0.275379352639405\n",
      "train loss:0.40767377893999573\n",
      "train loss:0.2866568784239169\n",
      "train loss:0.3472485805645287\n",
      "train loss:0.28379186425699765\n",
      "train loss:0.3160587824691057\n",
      "train loss:0.439809461647817\n",
      "train loss:0.2976739891909032\n",
      "train loss:0.26689021399798923\n",
      "train loss:0.32960019210694663\n",
      "train loss:0.36280832628839677\n",
      "train loss:0.34372042754743115\n",
      "train loss:0.288990409710999\n",
      "train loss:0.26616138866472494\n",
      "train loss:0.5123431163644624\n",
      "train loss:0.26735581530992525\n",
      "train loss:0.33869385266840796\n",
      "train loss:0.3766335400221667\n",
      "train loss:0.2587532619083183\n",
      "train loss:0.4012703609276946\n",
      "train loss:0.4782368309099653\n",
      "train loss:0.40479797991484767\n",
      "train loss:0.28143680399061505\n",
      "train loss:0.2850935853836372\n",
      "train loss:0.23893694101183274\n",
      "train loss:0.35439228976547804\n",
      "train loss:0.36231618644032465\n",
      "train loss:0.2519806602527363\n",
      "train loss:0.42283070804011325\n",
      "train loss:0.3060198246925514\n",
      "train loss:0.4249543736457448\n",
      "train loss:0.4607111071192626\n",
      "train loss:0.3138082972362378\n",
      "train loss:0.28920958798592267\n",
      "train loss:0.3924350748055152\n",
      "train loss:0.4207552089094975\n",
      "train loss:0.28895446086768456\n",
      "train loss:0.397444202901173\n",
      "train loss:0.34900706895593636\n",
      "train loss:0.29762381923384224\n",
      "train loss:0.45134248582770503\n",
      "train loss:0.3272931902340048\n",
      "train loss:0.32453456498052013\n",
      "train loss:0.1601212515820472\n",
      "train loss:0.463996910770613\n",
      "train loss:0.2507150579845144\n",
      "train loss:0.316404477856224\n",
      "train loss:0.4759816579527586\n",
      "train loss:0.5147654592240481\n",
      "train loss:0.24289792164446136\n",
      "train loss:0.3233223282121302\n",
      "train loss:0.3688134271560447\n",
      "train loss:0.37180477959158886\n",
      "train loss:0.4614226190256373\n",
      "train loss:0.29951893044619965\n",
      "train loss:0.40816550171782906\n",
      "train loss:0.2542697089909885\n",
      "train loss:0.2755977435281493\n",
      "train loss:0.29323573578913925\n",
      "train loss:0.3022433020774468\n",
      "train loss:0.2885175029339651\n",
      "train loss:0.31701973048667137\n",
      "train loss:0.34847547405842627\n",
      "train loss:0.3874640194498922\n",
      "train loss:0.3030521251318658\n",
      "train loss:0.22473509358848087\n",
      "train loss:0.5656088606470129\n",
      "train loss:0.3825755709638816\n",
      "train loss:0.3086971328208556\n",
      "train loss:0.22914171111317264\n",
      "train loss:0.42422622159670764\n",
      "train loss:0.4043077114078036\n",
      "train loss:0.4464391651786024\n",
      "train loss:0.34708296827413343\n",
      "train loss:0.3345192859948293\n",
      "train loss:0.30448121358416047\n",
      "train loss:0.4161286110329058\n",
      "train loss:0.41656579041859515\n",
      "train loss:0.2772189327092427\n",
      "train loss:0.3634861193256145\n",
      "train loss:0.48418967975946464\n",
      "train loss:0.22691669292020555\n",
      "train loss:0.2816265990054865\n",
      "train loss:0.41162628248097766\n",
      "train loss:0.3249317010010123\n",
      "train loss:0.39270808496334075\n",
      "train loss:0.341662022384342\n",
      "train loss:0.3224371776552652\n",
      "train loss:0.4288119942863125\n",
      "train loss:0.4513466485741653\n",
      "train loss:0.22530994337940563\n",
      "train loss:0.3424666591114762\n",
      "train loss:0.33322271596650394\n",
      "train loss:0.312537159111591\n",
      "train loss:0.2837905982139482\n",
      "train loss:0.579645782725487\n",
      "train loss:0.42228056630970784\n",
      "train loss:0.3451562295401851\n",
      "train loss:0.35235724894893017\n",
      "train loss:0.3710660537097038\n",
      "train loss:0.3656716249638645\n",
      "train loss:0.26047364335469714\n",
      "train loss:0.3250530235584474\n",
      "train loss:0.3869849918547238\n",
      "train loss:0.36139230076236645\n",
      "train loss:0.3845231184622571\n",
      "train loss:0.4268055068107084\n",
      "train loss:0.2541961299369186\n",
      "train loss:0.32221627068252295\n",
      "train loss:0.37044787766915055\n",
      "train loss:0.32811835341975315\n",
      "train loss:0.29836113136381986\n",
      "train loss:0.378129472495903\n",
      "train loss:0.21941655519380443\n",
      "train loss:0.3831992747728739\n",
      "train loss:0.41409599570765243\n",
      "train loss:0.34521192322861005\n",
      "train loss:0.3858857055309153\n",
      "train loss:0.3545315216803801\n",
      "train loss:0.429644517377315\n",
      "train loss:0.27780950382871034\n",
      "train loss:0.2678109386851296\n",
      "train loss:0.3675901745149689\n",
      "train loss:0.4067046942523272\n",
      "train loss:0.2690960076753607\n",
      "train loss:0.26875344024901887\n",
      "train loss:0.31920400078372724\n",
      "train loss:0.34788981589581913\n",
      "train loss:0.3102625748835544\n",
      "train loss:0.33774064707172174\n",
      "train loss:0.3144175499905642\n",
      "train loss:0.42315112160173485\n",
      "train loss:0.3322853023614198\n",
      "train loss:0.3768459522153275\n",
      "train loss:0.32071142034158256\n",
      "train loss:0.2684017579614803\n",
      "train loss:0.5113555580470376\n",
      "train loss:0.42067121496659793\n",
      "train loss:0.3247193372497618\n",
      "train loss:0.2954978301045821\n",
      "train loss:0.4259587453107199\n",
      "train loss:0.220442746334669\n",
      "train loss:0.30727728740086385\n",
      "train loss:0.4475007605512389\n",
      "train loss:0.44073419926899277\n",
      "train loss:0.4399244566102156\n",
      "train loss:0.39609599396587236\n",
      "train loss:0.2882141526213342\n",
      "train loss:0.3093339426047205\n",
      "train loss:0.4415895458695218\n",
      "train loss:0.3464264703957015\n",
      "train loss:0.33769058142648284\n",
      "train loss:0.2779393511770891\n",
      "train loss:0.2669688277306653\n",
      "train loss:0.41186489097544543\n",
      "train loss:0.3729241766965526\n",
      "train loss:0.24857508260419137\n",
      "train loss:0.35361542055301404\n",
      "train loss:0.2843846739657264\n",
      "train loss:0.3227776621278133\n",
      "train loss:0.3033574100826175\n",
      "train loss:0.2811477630739296\n",
      "train loss:0.2466561549070896\n",
      "train loss:0.34809138779100857\n",
      "train loss:0.2209705561345626\n",
      "train loss:0.3252838057357579\n",
      "train loss:0.36072041064340543\n",
      "train loss:0.3462685674187471\n",
      "train loss:0.40129752155304826\n",
      "train loss:0.2924629904320769\n",
      "train loss:0.3108946763616908\n",
      "train loss:0.41676421089135984\n",
      "train loss:0.4041646052327322\n",
      "train loss:0.2778004152472934\n",
      "train loss:0.25976076934545117\n",
      "train loss:0.33177354852265856\n",
      "train loss:0.4463361365359846\n",
      "train loss:0.36021480619174506\n",
      "train loss:0.3473660102880668\n",
      "train loss:0.3219486287214805\n",
      "train loss:0.4250350773115624\n",
      "train loss:0.40352578127919786\n",
      "train loss:0.3237948045604268\n",
      "train loss:0.294227852211976\n",
      "train loss:0.446977405088229\n",
      "train loss:0.504223796482592\n",
      "train loss:0.34696329155183697\n",
      "train loss:0.3129853345425372\n",
      "train loss:0.33251274548050114\n",
      "train loss:0.2903326597544043\n",
      "train loss:0.3276229757540739\n",
      "train loss:0.24633379007451192\n",
      "train loss:0.31157099802777904\n",
      "train loss:0.34906545084115836\n",
      "train loss:0.3673024285388207\n",
      "train loss:0.32251731668455547\n",
      "train loss:0.3852640089060512\n",
      "train loss:0.3596502623523463\n",
      "train loss:0.5461784709623579\n",
      "train loss:0.3019859919076769\n",
      "train loss:0.3284382573307887\n",
      "train loss:0.2747737403783511\n",
      "train loss:0.45830279164344695\n",
      "train loss:0.44607164052759124\n",
      "train loss:0.21356715445192928\n",
      "train loss:0.3941524456332735\n",
      "train loss:0.4209770515008342\n",
      "train loss:0.42859868771589105\n",
      "train loss:0.41735331438676626\n",
      "train loss:0.3333030173235458\n",
      "train loss:0.2610868868119048\n",
      "train loss:0.3157029143354249\n",
      "train loss:0.23614300400097435\n",
      "train loss:0.3479008546001216\n",
      "train loss:0.3282614327254268\n",
      "train loss:0.27269906842921604\n",
      "train loss:0.33850978208790594\n",
      "train loss:0.2727661235890652\n",
      "train loss:0.1783484776808778\n",
      "train loss:0.25436428070765915\n",
      "train loss:0.30756299637625456\n",
      "train loss:0.46768173985609257\n",
      "train loss:0.28862300898328236\n",
      "train loss:0.39158763557052034\n",
      "train loss:0.3927554221574082\n",
      "train loss:0.3045488132491982\n",
      "train loss:0.3857184330831437\n",
      "train loss:0.45541296889720984\n",
      "train loss:0.28188512052323134\n",
      "train loss:0.333140411598043\n",
      "train loss:0.31066331175998846\n",
      "train loss:0.4207963864959543\n",
      "train loss:0.2972077766135072\n",
      "train loss:0.42299620281349254\n",
      "train loss:0.3611128733441197\n",
      "train loss:0.35827911688887915\n",
      "train loss:0.48300699238944816\n",
      "train loss:0.35009783063397315\n",
      "train loss:0.4366275548256164\n",
      "train loss:0.2774719909999687\n",
      "train loss:0.3065957074474149\n",
      "train loss:0.29185441321119365\n",
      "train loss:0.38082534971416915\n",
      "train loss:0.3392237952731618\n",
      "train loss:0.37995055440578795\n",
      "train loss:0.22869606144304405\n",
      "train loss:0.3758329874576992\n",
      "train loss:0.28266668415654883\n",
      "train loss:0.31440760425203224\n",
      "train loss:0.3835340767991323\n",
      "train loss:0.2717657186563719\n",
      "train loss:0.305871495079386\n",
      "train loss:0.39011973917140336\n",
      "train loss:0.4160854039401121\n",
      "train loss:0.31066104709433423\n",
      "train loss:0.2621250728967171\n",
      "train loss:0.34038729105981425\n",
      "train loss:0.2568886596185667\n",
      "train loss:0.30460418793240934\n",
      "train loss:0.3990339565377119\n",
      "train loss:0.3266385118516845\n",
      "train loss:0.34450261667980336\n",
      "train loss:0.2184049027743318\n",
      "train loss:0.3255012682028483\n",
      "train loss:0.2941389995896002\n",
      "train loss:0.32461753665448134\n",
      "train loss:0.3767721940884902\n",
      "train loss:0.39798975077037846\n",
      "train loss:0.3330484729725699\n",
      "train loss:0.344733830468345\n",
      "train loss:0.3015000666355969\n",
      "train loss:0.37542691348330637\n",
      "train loss:0.32116798878789515\n",
      "train loss:0.3838321852405634\n",
      "train loss:0.3294192131609182\n",
      "train loss:0.19417072289181644\n",
      "train loss:0.3773333563582216\n",
      "train loss:0.3475743231332746\n",
      "train loss:0.2577581351038838\n",
      "train loss:0.3325461526557079\n",
      "train loss:0.2524909879066152\n",
      "train loss:0.38131668051708845\n",
      "train loss:0.3278318285669347\n",
      "train loss:0.2519186003505143\n",
      "train loss:0.3490020903703386\n",
      "train loss:0.45147300276700525\n",
      "train loss:0.38808677956850807\n",
      "train loss:0.28396747735703587\n",
      "train loss:0.37485651398533226\n",
      "train loss:0.39922314273896636\n",
      "train loss:0.3170879292139113\n",
      "train loss:0.44333649486828314\n",
      "train loss:0.2186038657627466\n",
      "train loss:0.4248135204538193\n",
      "train loss:0.2803501678279824\n",
      "train loss:0.28319111642776046\n",
      "train loss:0.3055481814954064\n",
      "train loss:0.37896566865853415\n",
      "train loss:0.27521821690345355\n",
      "train loss:0.2809139672429153\n",
      "train loss:0.2613494537712008\n",
      "train loss:0.2738209332950118\n",
      "train loss:0.4411711338447924\n",
      "train loss:0.2036317508181762\n",
      "train loss:0.2593831579141251\n",
      "train loss:0.4306130748583958\n",
      "train loss:0.37465919756945565\n",
      "train loss:0.3814404899908607\n",
      "train loss:0.3798913040481818\n",
      "train loss:0.2640034390138801\n",
      "train loss:0.29684649358636783\n",
      "train loss:0.31718200497232973\n",
      "train loss:0.3986351240053397\n",
      "train loss:0.28998335274800574\n",
      "train loss:0.3211800027258559\n",
      "train loss:0.2972092806646969\n",
      "train loss:0.29486758914415073\n",
      "train loss:0.30821590338536536\n",
      "train loss:0.41617923238273863\n",
      "train loss:0.258750495908592\n",
      "train loss:0.3202196574018433\n",
      "train loss:0.3510611251444945\n",
      "train loss:0.3220397170807855\n",
      "train loss:0.3864496248086639\n",
      "train loss:0.2740414202651858\n",
      "train loss:0.39062057025326646\n",
      "train loss:0.349973944570734\n",
      "train loss:0.33942774679165877\n",
      "train loss:0.3201526385585142\n",
      "train loss:0.2959284691397845\n",
      "train loss:0.33777207514748986\n",
      "train loss:0.4165915209676673\n",
      "train loss:0.44776981406877525\n",
      "train loss:0.3244308560290007\n",
      "train loss:0.2814162603533976\n",
      "train loss:0.23518509839899748\n",
      "train loss:0.28804442082728177\n",
      "train loss:0.32715443407636774\n",
      "train loss:0.36822249049086314\n",
      "train loss:0.23237935497451176\n",
      "train loss:0.3487106165761739\n",
      "train loss:0.30190029121579526\n",
      "train loss:0.43377047644613503\n",
      "train loss:0.42243867753647885\n",
      "train loss:0.3593218303790253\n",
      "train loss:0.35219710970684076\n",
      "train loss:0.29132521578281756\n",
      "train loss:0.34862770591780234\n",
      "train loss:0.3343774747672886\n",
      "train loss:0.4250034106500924\n",
      "train loss:0.3547312660745833\n",
      "train loss:0.22816074914747464\n",
      "train loss:0.18957612014388375\n",
      "train loss:0.3708761272465751\n",
      "train loss:0.26007631627237315\n",
      "train loss:0.3201067686037319\n",
      "train loss:0.3400068932145385\n",
      "train loss:0.25690970406287145\n",
      "train loss:0.31936524691866364\n",
      "train loss:0.2545293687421011\n",
      "train loss:0.2969958417168673\n",
      "train loss:0.29610399178398267\n",
      "train loss:0.2836671282281633\n",
      "train loss:0.31425623159410565\n",
      "train loss:0.4066964781711041\n",
      "train loss:0.2602621965389538\n",
      "train loss:0.2870195026608258\n",
      "train loss:0.316148513484372\n",
      "train loss:0.31059516000455206\n",
      "train loss:0.34407608203903\n",
      "train loss:0.31218448630688234\n",
      "train loss:0.4900238432891072\n",
      "train loss:0.258188207281036\n",
      "train loss:0.3430878151561213\n",
      "train loss:0.25594861420618237\n",
      "train loss:0.2062882518094803\n",
      "train loss:0.30548530081753056\n",
      "train loss:0.3935670108346542\n",
      "train loss:0.2238434470942762\n",
      "train loss:0.3222484835194439\n",
      "train loss:0.2874003976124922\n",
      "train loss:0.3310647634104837\n",
      "train loss:0.282610608722151\n",
      "train loss:0.41592372112759873\n",
      "train loss:0.23300937253245582\n",
      "train loss:0.2945922873532597\n",
      "train loss:0.4210812035886544\n",
      "train loss:0.2151328546388188\n",
      "train loss:0.2745102587665289\n",
      "train loss:0.26764037920974804\n",
      "train loss:0.24200463826702912\n",
      "train loss:0.2559353744237155\n",
      "train loss:0.4040198797700692\n",
      "train loss:0.3650098303914648\n",
      "train loss:0.38216424053017917\n",
      "train loss:0.2576751914667993\n",
      "train loss:0.33019902394115547\n",
      "train loss:0.4390224822334523\n",
      "train loss:0.4299987594949581\n",
      "train loss:0.2410166118731939\n",
      "train loss:0.31381856317116935\n",
      "train loss:0.24602853559768953\n",
      "train loss:0.4367395326761864\n",
      "train loss:0.3086643218836815\n",
      "train loss:0.3749300542911134\n",
      "train loss:0.19419626987920516\n",
      "train loss:0.30694539319089176\n",
      "train loss:0.3892330593636256\n",
      "train loss:0.35617771830286765\n",
      "train loss:0.2661490106067516\n",
      "train loss:0.3609478344046409\n",
      "train loss:0.23310238891873422\n",
      "train loss:0.2787047982524278\n",
      "train loss:0.21553877563065402\n",
      "train loss:0.35137771779101706\n",
      "train loss:0.3611924222468143\n",
      "train loss:0.3187621391390835\n",
      "train loss:0.2555415827252293\n",
      "train loss:0.29926244894048926\n",
      "train loss:0.2724519204908705\n",
      "train loss:0.3446308659302737\n",
      "train loss:0.19242285405144305\n",
      "train loss:0.32669794085213577\n",
      "train loss:0.4848441047095149\n",
      "train loss:0.2637437311834436\n",
      "train loss:0.280262783346872\n",
      "train loss:0.37959813830166256\n",
      "train loss:0.3712851672785758\n",
      "train loss:0.3927202053018724\n",
      "train loss:0.24532045327124005\n",
      "train loss:0.3103888830172836\n",
      "train loss:0.36117840749346264\n",
      "train loss:0.22570405213961503\n",
      "train loss:0.5028132924114117\n",
      "train loss:0.27160908693140123\n",
      "train loss:0.27743429250157875\n",
      "train loss:0.32543448463033664\n",
      "train loss:0.3594894685083628\n",
      "train loss:0.34078705012200816\n",
      "train loss:0.4246493931029836\n",
      "train loss:0.3281154337170058\n",
      "train loss:0.30312081838825167\n",
      "train loss:0.4215885881112622\n",
      "train loss:0.3140334691494071\n",
      "train loss:0.27065214979930685\n",
      "train loss:0.3052995200288007\n",
      "train loss:0.24348620905420557\n",
      "train loss:0.3024316860644553\n",
      "train loss:0.5937244410837759\n",
      "train loss:0.21977059016757217\n",
      "train loss:0.3993783407280038\n",
      "train loss:0.36055420021021656\n",
      "train loss:0.3218655436284417\n",
      "train loss:0.33284544682405803\n",
      "train loss:0.2993900365648473\n",
      "train loss:0.3319379335600071\n",
      "train loss:0.32682093557410286\n",
      "train loss:0.41406014487987436\n",
      "train loss:0.23364915430327163\n",
      "train loss:0.26803914522554406\n",
      "train loss:0.21976378864962653\n",
      "train loss:0.3635300142676536\n",
      "train loss:0.27319606428822235\n",
      "train loss:0.24520184905669012\n",
      "train loss:0.30581765247474857\n",
      "train loss:0.3726689270986998\n",
      "train loss:0.1888891636755861\n",
      "train loss:0.41631948559846793\n",
      "train loss:0.6200936487364292\n",
      "train loss:0.27943905352287024\n",
      "train loss:0.2758244365836638\n",
      "train loss:0.3458671235099047\n",
      "train loss:0.26554753992620744\n",
      "train loss:0.28399066251146726\n",
      "train loss:0.29250329199920727\n",
      "train loss:0.491608846260108\n",
      "train loss:0.28984261602019373\n",
      "train loss:0.29341118797996424\n",
      "train loss:0.3733840078127506\n",
      "train loss:0.4017177433329415\n",
      "train loss:0.5551830174706543\n",
      "train loss:0.35065947161793437\n",
      "train loss:0.2421837354116759\n",
      "train loss:0.38842095922106135\n",
      "train loss:0.2679626736940388\n",
      "train loss:0.43884837635763707\n",
      "train loss:0.24565885937052442\n",
      "train loss:0.3730425678077908\n",
      "train loss:0.4669731049432159\n",
      "train loss:0.3237817758532017\n",
      "train loss:0.2888916428739215\n",
      "train loss:0.4100953235511345\n",
      "train loss:0.33575565473461033\n",
      "train loss:0.23117284621153739\n",
      "train loss:0.34215366931896113\n",
      "train loss:0.24512102839689462\n",
      "train loss:0.2908256101703024\n",
      "train loss:0.39485723241816717\n",
      "train loss:0.27652138792125674\n",
      "train loss:0.35768873862160044\n",
      "train loss:0.24390789771176175\n",
      "train loss:0.3953796112315859\n",
      "train loss:0.44346692906025675\n",
      "train loss:0.25391253667792896\n",
      "train loss:0.2902955552606206\n",
      "train loss:0.29749196663648525\n",
      "train loss:0.2768833963852173\n",
      "train loss:0.3295418750328007\n",
      "train loss:0.43040614754960493\n",
      "train loss:0.3128862640918168\n",
      "train loss:0.3029078619492681\n",
      "train loss:0.28749523069425936\n",
      "train loss:0.4513462972402103\n",
      "train loss:0.4491439160035945\n",
      "train loss:0.2526415574968285\n",
      "train loss:0.339398648231236\n",
      "train loss:0.2851289563306192\n",
      "train loss:0.19820790439390937\n",
      "train loss:0.3351403953486482\n",
      "train loss:0.3687617779937737\n",
      "train loss:0.5263669205342095\n",
      "train loss:0.32535897172302564\n",
      "train loss:0.23165509004512266\n",
      "train loss:0.24727111452697328\n",
      "train loss:0.2279831345553216\n",
      "train loss:0.5404569845037925\n",
      "train loss:0.42349056897958703\n",
      "train loss:0.349235867390491\n",
      "train loss:0.29951949486487656\n",
      "train loss:0.2730150264673441\n",
      "train loss:0.37821514363490655\n",
      "train loss:0.30720627852759774\n",
      "train loss:0.27389130610795204\n",
      "train loss:0.3499629269577032\n",
      "train loss:0.4414763901143761\n",
      "train loss:0.3191071595574985\n",
      "train loss:0.33165536309925886\n",
      "train loss:0.20524390653680422\n",
      "train loss:0.38224968567085055\n",
      "train loss:0.3068119466770398\n",
      "train loss:0.21707976873109136\n",
      "train loss:0.39205973184006054\n",
      "train loss:0.2360420294427495\n",
      "train loss:0.3197436129293073\n",
      "train loss:0.2779349400896465\n",
      "train loss:0.41182941167688425\n",
      "train loss:0.3551044332050369\n",
      "train loss:0.324911363209203\n",
      "=== epoch:18, train acc:0.853, test acc:0.853 ===\n",
      "train loss:0.30962346701481025\n",
      "train loss:0.5054334389258769\n",
      "train loss:0.2852989151217004\n",
      "train loss:0.21626457388465437\n",
      "train loss:0.24768799348054032\n",
      "train loss:0.20670889072245494\n",
      "train loss:0.20269051756433737\n",
      "train loss:0.26710449219490745\n",
      "train loss:0.32205829665398594\n",
      "train loss:0.4363231678698668\n",
      "train loss:0.22355593330483908\n",
      "train loss:0.27342690528005387\n",
      "train loss:0.42800587748321156\n",
      "train loss:0.359490286579357\n",
      "train loss:0.2824699988968096\n",
      "train loss:0.30994337951195106\n",
      "train loss:0.2939865080694245\n",
      "train loss:0.2053940787381071\n",
      "train loss:0.27523883610901817\n",
      "train loss:0.46728916137397614\n",
      "train loss:0.38053068736739426\n",
      "train loss:0.28167130442035054\n",
      "train loss:0.3743196876519871\n",
      "train loss:0.39072432001352736\n",
      "train loss:0.2686505796924571\n",
      "train loss:0.34919144434469\n",
      "train loss:0.19377248687597529\n",
      "train loss:0.26941263712435265\n",
      "train loss:0.4063636288539837\n",
      "train loss:0.24135607111200807\n",
      "train loss:0.2325446151925786\n",
      "train loss:0.3123638791707773\n",
      "train loss:0.3962075497669453\n",
      "train loss:0.2474757788910974\n",
      "train loss:0.4046909593105423\n",
      "train loss:0.3382303165469056\n",
      "train loss:0.26138234339922567\n",
      "train loss:0.2845458567003112\n",
      "train loss:0.17899495942055754\n",
      "train loss:0.3091043006832071\n",
      "train loss:0.4888872565463905\n",
      "train loss:0.30515288493414494\n",
      "train loss:0.33131228420910325\n",
      "train loss:0.2596406491334833\n",
      "train loss:0.39302146542443156\n",
      "train loss:0.42182574342447743\n",
      "train loss:0.28458776733545077\n",
      "train loss:0.25421982700171136\n",
      "train loss:0.29578090613897245\n",
      "train loss:0.37531697378633133\n",
      "train loss:0.2548929785216098\n",
      "train loss:0.3664815988898603\n",
      "train loss:0.31694632375392623\n",
      "train loss:0.21759097901297098\n",
      "train loss:0.44089827907097784\n",
      "train loss:0.3127370254722973\n",
      "train loss:0.27670154129429025\n",
      "train loss:0.35639453687673417\n",
      "train loss:0.3969212380302227\n",
      "train loss:0.3709018618539854\n",
      "train loss:0.31663676594000684\n",
      "train loss:0.3007415128043665\n",
      "train loss:0.5061609768811164\n",
      "train loss:0.38986193027786414\n",
      "train loss:0.35092664716437033\n",
      "train loss:0.4068798292413053\n",
      "train loss:0.37851706771287347\n",
      "train loss:0.25951776708358776\n",
      "train loss:0.32643892718043593\n",
      "train loss:0.3787726653413035\n",
      "train loss:0.32826673216213237\n",
      "train loss:0.40168635638647104\n",
      "train loss:0.2960421304010023\n",
      "train loss:0.34222435048346367\n",
      "train loss:0.42550852918903365\n",
      "train loss:0.2536908265322472\n",
      "train loss:0.32786731471161623\n",
      "train loss:0.3687883950448663\n",
      "train loss:0.17211025630281993\n",
      "train loss:0.385148676609475\n",
      "train loss:0.28355677908645033\n",
      "train loss:0.31729590240418104\n",
      "train loss:0.34506712134943307\n",
      "train loss:0.3336806460992597\n",
      "train loss:0.31255271900923476\n",
      "train loss:0.2962761185796807\n",
      "train loss:0.3651710665033203\n",
      "train loss:0.33564765347573877\n",
      "train loss:0.3295265398713791\n",
      "train loss:0.3062042651001009\n",
      "train loss:0.34423043208499615\n",
      "train loss:0.32737900627606636\n",
      "train loss:0.35025128770940256\n",
      "train loss:0.2744698363394885\n",
      "train loss:0.27264125332997025\n",
      "train loss:0.38240502999892373\n",
      "train loss:0.31848871038624843\n",
      "train loss:0.5074669120197476\n",
      "train loss:0.2530273581116698\n",
      "train loss:0.3064753765155846\n",
      "train loss:0.27479879190769446\n",
      "train loss:0.3635776304311523\n",
      "train loss:0.3949376948787955\n",
      "train loss:0.23608684426851326\n",
      "train loss:0.3812301641170738\n",
      "train loss:0.4647502783018798\n",
      "train loss:0.3580661386744871\n",
      "train loss:0.2593415636841126\n",
      "train loss:0.281899817077295\n",
      "train loss:0.431540900271122\n",
      "train loss:0.40955339537698643\n",
      "train loss:0.47308798228688054\n",
      "train loss:0.2800418138254262\n",
      "train loss:0.2635525170798564\n",
      "train loss:0.4241841908172708\n",
      "train loss:0.1773432582321239\n",
      "train loss:0.19778088131883295\n",
      "train loss:0.20477350972419295\n",
      "train loss:0.3381593329384174\n",
      "train loss:0.22511203918922645\n",
      "train loss:0.24884642780503186\n",
      "train loss:0.46853495656947375\n",
      "train loss:0.4450100778148254\n",
      "train loss:0.26717935889055733\n",
      "train loss:0.3818035945540621\n",
      "train loss:0.44345238421329275\n",
      "train loss:0.3674920781863257\n",
      "train loss:0.34876728363280884\n",
      "train loss:0.24125891863942037\n",
      "train loss:0.29389881461200607\n",
      "train loss:0.36475934196387805\n",
      "train loss:0.40555332066685496\n",
      "train loss:0.5598593676310282\n",
      "train loss:0.2713793499285952\n",
      "train loss:0.4195033119583379\n",
      "train loss:0.2616432223118326\n",
      "train loss:0.2089044652911711\n",
      "train loss:0.43617095093633174\n",
      "train loss:0.307344650841011\n",
      "train loss:0.3583895099873943\n",
      "train loss:0.26702444697862915\n",
      "train loss:0.4181024161477191\n",
      "train loss:0.4197721295935055\n",
      "train loss:0.4340357958860356\n",
      "train loss:0.4128861823996741\n",
      "train loss:0.38573438616955613\n",
      "train loss:0.41105000189468577\n",
      "train loss:0.26919632502407065\n",
      "train loss:0.31351517371783405\n",
      "train loss:0.4010570568192607\n",
      "train loss:0.31695588567013405\n",
      "train loss:0.35428037708029675\n",
      "train loss:0.4743455802478252\n",
      "train loss:0.2891031634086037\n",
      "train loss:0.37643361213138143\n",
      "train loss:0.23202974636003212\n",
      "train loss:0.293186867429882\n",
      "train loss:0.3660131949227941\n",
      "train loss:0.2558023959469436\n",
      "train loss:0.3923232019063337\n",
      "train loss:0.3731804662523461\n",
      "train loss:0.31128848601921794\n",
      "train loss:0.25966531734772624\n",
      "train loss:0.28212684595639925\n",
      "train loss:0.5468057152941601\n",
      "train loss:0.2502755193158496\n",
      "train loss:0.3132821909419436\n",
      "train loss:0.2639944398781255\n",
      "train loss:0.4504797109747918\n",
      "train loss:0.2941292073043627\n",
      "train loss:0.2913138895776775\n",
      "train loss:0.39207923427218583\n",
      "train loss:0.38681350862994246\n",
      "train loss:0.28095070631816144\n",
      "train loss:0.40424604700855293\n",
      "train loss:0.28518042575758895\n",
      "train loss:0.37382570561312967\n",
      "train loss:0.3439500949679537\n",
      "train loss:0.25343123471192003\n",
      "train loss:0.49757403049332494\n",
      "train loss:0.3375543006251847\n",
      "train loss:0.27903654449952536\n",
      "train loss:0.25593430271250184\n",
      "train loss:0.23657483112281444\n",
      "train loss:0.29249445813901925\n",
      "train loss:0.23263125312897037\n",
      "train loss:0.3425506636345495\n",
      "train loss:0.31935978118180275\n",
      "train loss:0.2626858445974695\n",
      "train loss:0.4608339525144502\n",
      "train loss:0.34632009822410287\n",
      "train loss:0.24439092019391342\n",
      "train loss:0.5156011064527091\n",
      "train loss:0.36066157818995803\n",
      "train loss:0.4484443270202341\n",
      "train loss:0.3581728474883307\n",
      "train loss:0.24402809702718137\n",
      "train loss:0.30317280883843134\n",
      "train loss:0.4627285330873564\n",
      "train loss:0.3229437049543185\n",
      "train loss:0.28070003422232787\n",
      "train loss:0.4950405205978177\n",
      "train loss:0.44330452575341106\n",
      "train loss:0.27075306729238696\n",
      "train loss:0.2126062522772102\n",
      "train loss:0.3949163743029554\n",
      "train loss:0.2785550423764117\n",
      "train loss:0.2570959395614104\n",
      "train loss:0.3410498964635743\n",
      "train loss:0.33623838095707803\n",
      "train loss:0.251245323100189\n",
      "train loss:0.2780440118632209\n",
      "train loss:0.32110144988138467\n",
      "train loss:0.34365205704852253\n",
      "train loss:0.3633023670578596\n",
      "train loss:0.3640458205929781\n",
      "train loss:0.3705925533703373\n",
      "train loss:0.2972652999510813\n",
      "train loss:0.2899907022967996\n",
      "train loss:0.3135494895603763\n",
      "train loss:0.4719846423758256\n",
      "train loss:0.4718226564163243\n",
      "train loss:0.48144349196969793\n",
      "train loss:0.4848291164557741\n",
      "train loss:0.24980347776061085\n",
      "train loss:0.30886579791549074\n",
      "train loss:0.27090256306903177\n",
      "train loss:0.3503996633295018\n",
      "train loss:0.2729314598647526\n",
      "train loss:0.2620256213636954\n",
      "train loss:0.36806126301577907\n",
      "train loss:0.3369050251956542\n",
      "train loss:0.4224449451755135\n",
      "train loss:0.26317945785220687\n",
      "train loss:0.4359142445478416\n",
      "train loss:0.3164925375936431\n",
      "train loss:0.2957579508360252\n",
      "train loss:0.3635806705973185\n",
      "train loss:0.2532717357562331\n",
      "train loss:0.3888015289584129\n",
      "train loss:0.2696824894847429\n",
      "train loss:0.4326518548624471\n",
      "train loss:0.2550971912399659\n",
      "train loss:0.3440811786165197\n",
      "train loss:0.37786823958081556\n",
      "train loss:0.3292196725487662\n",
      "train loss:0.23809541944814966\n",
      "train loss:0.28365478838408426\n",
      "train loss:0.3090608241798739\n",
      "train loss:0.2932106181711735\n",
      "train loss:0.29387147168658057\n",
      "train loss:0.2711063109060201\n",
      "train loss:0.3887130218180376\n",
      "train loss:0.33016149606204515\n",
      "train loss:0.2123878905514343\n",
      "train loss:0.22119278046932553\n",
      "train loss:0.36843846634044064\n",
      "train loss:0.32943474612243245\n",
      "train loss:0.40270009953830427\n",
      "train loss:0.25072975883347093\n",
      "train loss:0.335036039730424\n",
      "train loss:0.33762208837975116\n",
      "train loss:0.38136260799696653\n",
      "train loss:0.4099267080985257\n",
      "train loss:0.31745410964836596\n",
      "train loss:0.3885340424106659\n",
      "train loss:0.2930486259641367\n",
      "train loss:0.24820368509527824\n",
      "train loss:0.21735812044765632\n",
      "train loss:0.36524358297305526\n",
      "train loss:0.4137327960760673\n",
      "train loss:0.25079204938372235\n",
      "train loss:0.1508083391237601\n",
      "train loss:0.3200447654911004\n",
      "train loss:0.32301815568369785\n",
      "train loss:0.26051564131113847\n",
      "train loss:0.26364805490899984\n",
      "train loss:0.23229704354698005\n",
      "train loss:0.3526578421884885\n",
      "train loss:0.32607439819728484\n",
      "train loss:0.31002003525340155\n",
      "train loss:0.408199703634397\n",
      "train loss:0.2837684316336015\n",
      "train loss:0.21521344627132347\n",
      "train loss:0.27221785461928627\n",
      "train loss:0.3382389198291267\n",
      "train loss:0.4044931414322667\n",
      "train loss:0.21061326370032554\n",
      "train loss:0.38339112718862794\n",
      "train loss:0.3082388284903064\n",
      "train loss:0.47215601581523553\n",
      "train loss:0.30046700327235576\n",
      "train loss:0.40442575771045386\n",
      "train loss:0.35939455772894163\n",
      "train loss:0.303003758823208\n",
      "train loss:0.2916323622982886\n",
      "train loss:0.28238823455165585\n",
      "train loss:0.3260555714720453\n",
      "train loss:0.4210383632070264\n",
      "train loss:0.2645519564713339\n",
      "train loss:0.3758957734553853\n",
      "train loss:0.34888152713057186\n",
      "train loss:0.3928402536251451\n",
      "train loss:0.3237438043730585\n",
      "train loss:0.18900396818275972\n",
      "train loss:0.2614466802550836\n",
      "train loss:0.3572717553377121\n",
      "train loss:0.31355737458545335\n",
      "train loss:0.28576216679243055\n",
      "train loss:0.48978051936051387\n",
      "train loss:0.28098783168396524\n",
      "train loss:0.4250187946084957\n",
      "train loss:0.31152380926913403\n",
      "train loss:0.37702980390037505\n",
      "train loss:0.2563800382523079\n",
      "train loss:0.2423429850833769\n",
      "train loss:0.24140699504252208\n",
      "train loss:0.2706897053931908\n",
      "train loss:0.2604161562773895\n",
      "train loss:0.34365238520188107\n",
      "train loss:0.2758102918403813\n",
      "train loss:0.2806899209940598\n",
      "train loss:0.3811693585892156\n",
      "train loss:0.29597993861563476\n",
      "train loss:0.4069435345988747\n",
      "train loss:0.3657831061373774\n",
      "train loss:0.2868639742158747\n",
      "train loss:0.3556647540101983\n",
      "train loss:0.36377850592228794\n",
      "train loss:0.19907546824795108\n",
      "train loss:0.25215360313210444\n",
      "train loss:0.3523666262757245\n",
      "train loss:0.22406249000603393\n",
      "train loss:0.3688762690017017\n",
      "train loss:0.42216048987464816\n",
      "train loss:0.3321617347512631\n",
      "train loss:0.20641136611812683\n",
      "train loss:0.33565715138418967\n",
      "train loss:0.3984016806899168\n",
      "train loss:0.3011062174658183\n",
      "train loss:0.42079920026938233\n",
      "train loss:0.29325902900025985\n",
      "train loss:0.1800692074213696\n",
      "train loss:0.2643331598043513\n",
      "train loss:0.33897005041168526\n",
      "train loss:0.30607197539787123\n",
      "train loss:0.2671890085888922\n",
      "train loss:0.5525207205921591\n",
      "train loss:0.1928748331769211\n",
      "train loss:0.35891719781866416\n",
      "train loss:0.3874115187658537\n",
      "train loss:0.40156481245501746\n",
      "train loss:0.3980557955937\n",
      "train loss:0.2807016648894499\n",
      "train loss:0.3714039287462982\n",
      "train loss:0.334263759473098\n",
      "train loss:0.3836283732096467\n",
      "train loss:0.4706075258133303\n",
      "train loss:0.35818800440813986\n",
      "train loss:0.43293279444427346\n",
      "train loss:0.21828934390493637\n",
      "train loss:0.24918281523079222\n",
      "train loss:0.3653896789711494\n",
      "train loss:0.2819262671190961\n",
      "train loss:0.48405638663534745\n",
      "train loss:0.2200948535166582\n",
      "train loss:0.29187892659475256\n",
      "train loss:0.3554982531496131\n",
      "train loss:0.20508341477782863\n",
      "train loss:0.3700562035125286\n",
      "train loss:0.6295488088915013\n",
      "train loss:0.28700206116345767\n",
      "train loss:0.19804055964179315\n",
      "train loss:0.3973416395826119\n",
      "train loss:0.321225411689716\n",
      "train loss:0.43002043052810096\n",
      "train loss:0.29079239487168024\n",
      "train loss:0.5465499436630834\n",
      "train loss:0.39190090673432176\n",
      "train loss:0.21345412174194564\n",
      "train loss:0.37581814255979723\n",
      "train loss:0.33514257191754154\n",
      "train loss:0.32576639466436974\n",
      "train loss:0.25228098265256205\n",
      "train loss:0.3438558095635896\n",
      "train loss:0.40084970913554363\n",
      "train loss:0.28463346915246523\n",
      "train loss:0.3640853644637886\n",
      "train loss:0.3631756083565142\n",
      "train loss:0.29243592694352616\n",
      "train loss:0.2918288152075536\n",
      "train loss:0.40807843129601684\n",
      "train loss:0.3224047604185289\n",
      "train loss:0.25618170168268917\n",
      "train loss:0.372620671868603\n",
      "train loss:0.34186083246513727\n",
      "train loss:0.2801099929444439\n",
      "train loss:0.2360956914589739\n",
      "train loss:0.4564221131206181\n",
      "train loss:0.4735048027688144\n",
      "train loss:0.30565671775495973\n",
      "train loss:0.3193711334984709\n",
      "train loss:0.42448844245596523\n",
      "train loss:0.2772367509469779\n",
      "train loss:0.21273751035524224\n",
      "train loss:0.23404799885088823\n",
      "train loss:0.25110241877955913\n",
      "train loss:0.4651604696512149\n",
      "train loss:0.6410179055085854\n",
      "train loss:0.2858703657959996\n",
      "train loss:0.23896114428476095\n",
      "train loss:0.28475906507455223\n",
      "train loss:0.30441305849623623\n",
      "train loss:0.3310318935462089\n",
      "train loss:0.5025706416514064\n",
      "train loss:0.35341457072809634\n",
      "train loss:0.3333823915767352\n",
      "train loss:0.3635416168677875\n",
      "train loss:0.2975347398444734\n",
      "train loss:0.256928266842267\n",
      "train loss:0.28653926992270784\n",
      "train loss:0.3623753392556972\n",
      "train loss:0.3499398853601171\n",
      "train loss:0.20859438254332965\n",
      "train loss:0.39887057581540747\n",
      "train loss:0.35556136954812445\n",
      "train loss:0.391679783379568\n",
      "train loss:0.3386893812021489\n",
      "train loss:0.37127351646608564\n",
      "train loss:0.36115392848575534\n",
      "train loss:0.34289851495202617\n",
      "train loss:0.39111149292567843\n",
      "train loss:0.3589072177502057\n",
      "train loss:0.3254909330477522\n",
      "train loss:0.48305692425904034\n",
      "train loss:0.36867870013853554\n",
      "train loss:0.3281549447435107\n",
      "train loss:0.4935599031484889\n",
      "train loss:0.3165860260483636\n",
      "train loss:0.3947764867003501\n",
      "train loss:0.5985860878908459\n",
      "train loss:0.2492050177441591\n",
      "train loss:0.38324618635529356\n",
      "train loss:0.4937381889052911\n",
      "train loss:0.2698821802063797\n",
      "train loss:0.2685340376437789\n",
      "train loss:0.41843532867206945\n",
      "train loss:0.45785504042645153\n",
      "train loss:0.3178215527586035\n",
      "train loss:0.46247607370584504\n",
      "train loss:0.2980796887709707\n",
      "train loss:0.44968953956945973\n",
      "train loss:0.2809277872263575\n",
      "train loss:0.2514297351507537\n",
      "train loss:0.30534332411735754\n",
      "train loss:0.32271497065849175\n",
      "train loss:0.4147540532479694\n",
      "train loss:0.3889450833687657\n",
      "train loss:0.3076605352136267\n",
      "train loss:0.3131991732027769\n",
      "train loss:0.30233485555536327\n",
      "train loss:0.3742266179323596\n",
      "train loss:0.3248573880468924\n",
      "train loss:0.25420521714494004\n",
      "train loss:0.42996098852101744\n",
      "train loss:0.3014879392467139\n",
      "train loss:0.3191869924204237\n",
      "train loss:0.3158162392785563\n",
      "train loss:0.3042744881501978\n",
      "train loss:0.35196583058253905\n",
      "train loss:0.4087073110492524\n",
      "train loss:0.2980947066699751\n",
      "train loss:0.3162257244625304\n",
      "train loss:0.43215448289558545\n",
      "train loss:0.34377081850137636\n",
      "train loss:0.40002288706594413\n",
      "train loss:0.28177180395484025\n",
      "train loss:0.28376007323071234\n",
      "train loss:0.3403892706245759\n",
      "train loss:0.31278022533228494\n",
      "train loss:0.35048843172050176\n",
      "train loss:0.23341709720906942\n",
      "train loss:0.3553019452859055\n",
      "train loss:0.2312338443089579\n",
      "train loss:0.2635419911047336\n",
      "train loss:0.3093707442424531\n",
      "train loss:0.4126320959442171\n",
      "train loss:0.36344931877197334\n",
      "train loss:0.4098100073369712\n",
      "train loss:0.3676866803977012\n",
      "train loss:0.3331421174005467\n",
      "train loss:0.38324541653402583\n",
      "train loss:0.22406464196843756\n",
      "train loss:0.22399330944397758\n",
      "train loss:0.4226921824695811\n",
      "train loss:0.3315714629261077\n",
      "train loss:0.23404302091298468\n",
      "train loss:0.23274386012649212\n",
      "train loss:0.33757731140239433\n",
      "train loss:0.35915088396018985\n",
      "train loss:0.23292402365708603\n",
      "train loss:0.33768048323103955\n",
      "train loss:0.30207232856964755\n",
      "train loss:0.23478287152805813\n",
      "train loss:0.318592884911534\n",
      "train loss:0.44435458833123925\n",
      "train loss:0.26621805111047636\n",
      "train loss:0.23062205199168417\n",
      "train loss:0.3655598374761276\n",
      "train loss:0.2732640354214197\n",
      "train loss:0.24656569550125113\n",
      "train loss:0.25566961803293836\n",
      "train loss:0.3559678073435085\n",
      "train loss:0.23707724034915945\n",
      "train loss:0.4325495715479499\n",
      "train loss:0.244487723569973\n",
      "train loss:0.328262597726458\n",
      "train loss:0.3688424004627952\n",
      "train loss:0.22978615746210262\n",
      "train loss:0.30704480206583595\n",
      "train loss:0.2172382994880125\n",
      "train loss:0.26274027204795525\n",
      "train loss:0.2777704972219995\n",
      "train loss:0.2538883837680943\n",
      "train loss:0.24802283091213684\n",
      "train loss:0.41496379163371316\n",
      "train loss:0.4121451652204047\n",
      "train loss:0.3080567702112598\n",
      "train loss:0.25811161724942155\n",
      "train loss:0.3080158006817287\n",
      "train loss:0.5172668391458428\n",
      "train loss:0.17285986452204127\n",
      "train loss:0.24405972523856811\n",
      "train loss:0.3218850924497591\n",
      "train loss:0.278285789769147\n",
      "train loss:0.4117284084771191\n",
      "train loss:0.23377520499277288\n",
      "train loss:0.4442014607573612\n",
      "train loss:0.432956913259643\n",
      "train loss:0.3304002553388464\n",
      "train loss:0.29379952746650073\n",
      "train loss:0.3630365280344142\n",
      "train loss:0.38215827645816236\n",
      "train loss:0.4001736663205156\n",
      "train loss:0.3391447997282217\n",
      "train loss:0.2920748672639762\n",
      "train loss:0.3020228945174776\n",
      "train loss:0.23946908582760185\n",
      "train loss:0.31556809168414834\n",
      "train loss:0.31236746993599185\n",
      "train loss:0.4665262547651991\n",
      "train loss:0.4643601251297503\n",
      "train loss:0.4030692787652033\n",
      "train loss:0.4119563206552027\n",
      "train loss:0.26900978709631984\n",
      "train loss:0.28694112495889107\n",
      "train loss:0.4090317053286949\n",
      "train loss:0.41497032036471615\n",
      "train loss:0.39158405107719296\n",
      "train loss:0.34427807694744295\n",
      "train loss:0.5045781906323763\n",
      "train loss:0.4670354430870548\n",
      "train loss:0.4074491446903839\n",
      "train loss:0.34664859494201855\n",
      "train loss:0.34721888854317773\n",
      "train loss:0.24372887899081777\n",
      "train loss:0.31698101651187066\n",
      "train loss:0.3268407827444635\n",
      "train loss:0.26722551540931716\n",
      "train loss:0.39024115440016194\n",
      "train loss:0.21614096028709945\n",
      "train loss:0.3374844166504865\n",
      "train loss:0.32896011583208645\n",
      "train loss:0.27594243868532276\n",
      "train loss:0.34730929708910824\n",
      "train loss:0.32135439488962414\n",
      "train loss:0.3731184688049327\n",
      "train loss:0.3044044739324023\n",
      "train loss:0.229133339386063\n",
      "train loss:0.2975702582966243\n",
      "train loss:0.30026355355928547\n",
      "train loss:0.17602347584310912\n",
      "train loss:0.40941641718701954\n",
      "train loss:0.2694971485604025\n",
      "train loss:0.2781670917590369\n",
      "train loss:0.26620465762415985\n",
      "train loss:0.28096711083875087\n",
      "train loss:0.3412628550608642\n",
      "train loss:0.37625735471275584\n",
      "train loss:0.5679992213991204\n",
      "train loss:0.2785342045962554\n",
      "train loss:0.30143324704154045\n",
      "train loss:0.2959965180430302\n",
      "train loss:0.24422041810764517\n",
      "train loss:0.41974246605142135\n",
      "train loss:0.2132815830159449\n",
      "train loss:0.19834707129688442\n",
      "train loss:0.37856013608597405\n",
      "train loss:0.39666377757417115\n",
      "train loss:0.3647081430896754\n",
      "=== epoch:19, train acc:0.853, test acc:0.86 ===\n",
      "train loss:0.38829565699854135\n",
      "train loss:0.45927126993166\n",
      "train loss:0.34561849448609366\n",
      "train loss:0.20308102199696734\n",
      "train loss:0.27928709401409607\n",
      "train loss:0.2633379960346672\n",
      "train loss:0.25047458627213154\n",
      "train loss:0.48619944804504756\n",
      "train loss:0.269379061169195\n",
      "train loss:0.2984701406670272\n",
      "train loss:0.2751444809520288\n",
      "train loss:0.19313289782481188\n",
      "train loss:0.3523252948197412\n",
      "train loss:0.36195213711761914\n",
      "train loss:0.37244839153114806\n",
      "train loss:0.31251343146771304\n",
      "train loss:0.23418852956336184\n",
      "train loss:0.249496643791434\n",
      "train loss:0.28354559543982627\n",
      "train loss:0.21903501860967275\n",
      "train loss:0.22574723624980197\n",
      "train loss:0.28508020559735864\n",
      "train loss:0.37188772935776204\n",
      "train loss:0.38432203479736493\n",
      "train loss:0.3257301027060779\n",
      "train loss:0.3149469390671664\n",
      "train loss:0.34002761432490913\n",
      "train loss:0.2431690940548109\n",
      "train loss:0.23574789367594245\n",
      "train loss:0.4213941834639683\n",
      "train loss:0.20491172313918948\n",
      "train loss:0.28621347846472195\n",
      "train loss:0.38599889443235297\n",
      "train loss:0.21384504792468398\n",
      "train loss:0.2847916377801235\n",
      "train loss:0.331145834384643\n",
      "train loss:0.4023086056621538\n",
      "train loss:0.29103238840005885\n",
      "train loss:0.48096311254623925\n",
      "train loss:0.3549498871713385\n",
      "train loss:0.33327316641747506\n",
      "train loss:0.2677357331356318\n",
      "train loss:0.41902473829795317\n",
      "train loss:0.42268509862875975\n",
      "train loss:0.3346325840408935\n",
      "train loss:0.39209002560992856\n",
      "train loss:0.3889787194993406\n",
      "train loss:0.27095525726981995\n",
      "train loss:0.3615896438689596\n",
      "train loss:0.39003536867059063\n",
      "train loss:0.48447871754848826\n",
      "train loss:0.44688416851015417\n",
      "train loss:0.2569415980116982\n",
      "train loss:0.34819613988756243\n",
      "train loss:0.32995914049465347\n",
      "train loss:0.30292104202307707\n",
      "train loss:0.2798997720490625\n",
      "train loss:0.4065676010966346\n",
      "train loss:0.33386951825858324\n",
      "train loss:0.3012010999909435\n",
      "train loss:0.3045905566936897\n",
      "train loss:0.1885854079267196\n",
      "train loss:0.23367622031664217\n",
      "train loss:0.28846781799010307\n",
      "train loss:0.3213149925555222\n",
      "train loss:0.24806671776102507\n",
      "train loss:0.27677861483829785\n",
      "train loss:0.4052698405582763\n",
      "train loss:0.37316885912109576\n",
      "train loss:0.2184779690273999\n",
      "train loss:0.43815240710305564\n",
      "train loss:0.28932048800687793\n",
      "train loss:0.38487602946699623\n",
      "train loss:0.5476621607178079\n",
      "train loss:0.36124810010983466\n",
      "train loss:0.3297085913797806\n",
      "train loss:0.3511270501287775\n",
      "train loss:0.26634814007802143\n",
      "train loss:0.5182409624212627\n",
      "train loss:0.393560959858968\n",
      "train loss:0.2710140095547691\n",
      "train loss:0.4586282316197593\n",
      "train loss:0.3325749921639408\n",
      "train loss:0.38799196472921893\n",
      "train loss:0.46085047091499687\n",
      "train loss:0.22623321780278388\n",
      "train loss:0.3627157448162411\n",
      "train loss:0.3681797529115727\n",
      "train loss:0.30945541547828476\n",
      "train loss:0.2691687803099873\n",
      "train loss:0.38104180824450606\n",
      "train loss:0.23776100694654148\n",
      "train loss:0.35382073889561416\n",
      "train loss:0.24992893919637363\n",
      "train loss:0.37818481661530634\n",
      "train loss:0.3618231155456678\n",
      "train loss:0.2794467933476878\n",
      "train loss:0.3189754708021089\n",
      "train loss:0.32393025823211263\n",
      "train loss:0.2889774209940097\n",
      "train loss:0.34692601374946863\n",
      "train loss:0.31278566693127774\n",
      "train loss:0.35823934978131416\n",
      "train loss:0.2457023267520472\n",
      "train loss:0.23851728924918483\n",
      "train loss:0.21966045894907452\n",
      "train loss:0.27914937690218095\n",
      "train loss:0.29458501891520605\n",
      "train loss:0.3451201606002479\n",
      "train loss:0.3290539004396703\n",
      "train loss:0.36029084283306984\n",
      "train loss:0.5005359872888382\n",
      "train loss:0.4371152119894743\n",
      "train loss:0.29384626283717596\n",
      "train loss:0.5688721935411106\n",
      "train loss:0.33612960150326837\n",
      "train loss:0.20911354616105066\n",
      "train loss:0.28909418554086747\n",
      "train loss:0.21347350254989259\n",
      "train loss:0.28616171557680714\n",
      "train loss:0.20164796897214085\n",
      "train loss:0.2578550804722379\n",
      "train loss:0.40324305209953415\n",
      "train loss:0.2579843735978289\n",
      "train loss:0.20366435772042532\n",
      "train loss:0.3021746028324102\n",
      "train loss:0.34528087401873964\n",
      "train loss:0.37709172620806014\n",
      "train loss:0.4200950597682458\n",
      "train loss:0.3780969989033781\n",
      "train loss:0.4350647125626956\n",
      "train loss:0.22638665855983967\n",
      "train loss:0.4685054908477791\n",
      "train loss:0.3006648903682852\n",
      "train loss:0.30427594940398484\n",
      "train loss:0.31206647193083104\n",
      "train loss:0.34179142101053955\n",
      "train loss:0.23735063777132967\n",
      "train loss:0.44374674973504624\n",
      "train loss:0.22360384682831966\n",
      "train loss:0.38545032884533503\n",
      "train loss:0.2145152270756595\n",
      "train loss:0.23350069266500778\n",
      "train loss:0.3017258689725946\n",
      "train loss:0.47342558320303646\n",
      "train loss:0.339336416337291\n",
      "train loss:0.38966706971062787\n",
      "train loss:0.2526492018505745\n",
      "train loss:0.2880908942110348\n",
      "train loss:0.3567708162748151\n",
      "train loss:0.2778011951271762\n",
      "train loss:0.2710262097091991\n",
      "train loss:0.36668091797442537\n",
      "train loss:0.349228972584056\n",
      "train loss:0.32869867549588916\n",
      "train loss:0.27474747107366054\n",
      "train loss:0.2771906507835162\n",
      "train loss:0.34512028570983694\n",
      "train loss:0.3590019788640976\n",
      "train loss:0.5087533650336142\n",
      "train loss:0.32038112163928917\n",
      "train loss:0.3590514574413193\n",
      "train loss:0.2876358850226055\n",
      "train loss:0.3784782553936648\n",
      "train loss:0.26905817692074413\n",
      "train loss:0.25131989680536926\n",
      "train loss:0.3164172815190921\n",
      "train loss:0.44155808857078854\n",
      "train loss:0.3141574933472931\n",
      "train loss:0.39631828482332165\n",
      "train loss:0.260899860516498\n",
      "train loss:0.21551983197439054\n",
      "train loss:0.3030821661678995\n",
      "train loss:0.29344432211393384\n",
      "train loss:0.20597800452034498\n",
      "train loss:0.27643316943454965\n",
      "train loss:0.24564500561873237\n",
      "train loss:0.32299664807297795\n",
      "train loss:0.3394803884493401\n",
      "train loss:0.36123247015718307\n",
      "train loss:0.24919301499708463\n",
      "train loss:0.27632306873700097\n",
      "train loss:0.15508145448700467\n",
      "train loss:0.34093065989272503\n",
      "train loss:0.26318919051402045\n",
      "train loss:0.3984427399174599\n",
      "train loss:0.2166469738825162\n",
      "train loss:0.3615959503396603\n",
      "train loss:0.23618180575053174\n",
      "train loss:0.39345736919961277\n",
      "train loss:0.2738327445580271\n",
      "train loss:0.23533583469471625\n",
      "train loss:0.3383529326333436\n",
      "train loss:0.1958790951674735\n",
      "train loss:0.3277883694829078\n",
      "train loss:0.3352692700350981\n",
      "train loss:0.3133782907659892\n",
      "train loss:0.3015947975132308\n",
      "train loss:0.24138183572126592\n",
      "train loss:0.41816998670235256\n",
      "train loss:0.2821116238728709\n",
      "train loss:0.3785397090200709\n",
      "train loss:0.38793848900965927\n",
      "train loss:0.32654906041327\n",
      "train loss:0.2731642178669604\n",
      "train loss:0.4145102947498037\n",
      "train loss:0.20595462956370827\n",
      "train loss:0.29550447532235574\n",
      "train loss:0.4259253212294819\n",
      "train loss:0.24788789794225063\n",
      "train loss:0.40552088204023357\n",
      "train loss:0.2234441656533789\n",
      "train loss:0.39169061707063973\n",
      "train loss:0.25793499100059636\n",
      "train loss:0.31321994587801316\n",
      "train loss:0.322083031318933\n",
      "train loss:0.40148892887927395\n",
      "train loss:0.25733691170386536\n",
      "train loss:0.45110860855029933\n",
      "train loss:0.2833202986127265\n",
      "train loss:0.46162383985914185\n",
      "train loss:0.17840275039946285\n",
      "train loss:0.25161310263181574\n",
      "train loss:0.2738725766278482\n",
      "train loss:0.22313632254875407\n",
      "train loss:0.27982126234527643\n",
      "train loss:0.30508122127782084\n",
      "train loss:0.33075191143497773\n",
      "train loss:0.2504226711881632\n",
      "train loss:0.42470852825149313\n",
      "train loss:0.4021424209997413\n",
      "train loss:0.3465246901659449\n",
      "train loss:0.3923366820059513\n",
      "train loss:0.3319602531928858\n",
      "train loss:0.34682024259218347\n",
      "train loss:0.3699911010561378\n",
      "train loss:0.22753540591786625\n",
      "train loss:0.3435727595979871\n",
      "train loss:0.30515320034819376\n",
      "train loss:0.1896692084904289\n",
      "train loss:0.30300850415946223\n",
      "train loss:0.30814834772893407\n",
      "train loss:0.5268541021421365\n",
      "train loss:0.2880194587971434\n",
      "train loss:0.313781721895142\n",
      "train loss:0.3212736938331714\n",
      "train loss:0.5527589218776102\n",
      "train loss:0.2963879823264905\n",
      "train loss:0.2932988217079232\n",
      "train loss:0.33237964513001556\n",
      "train loss:0.3214041036337089\n",
      "train loss:0.28175873444474087\n",
      "train loss:0.28390989223414975\n",
      "train loss:0.2971370084988961\n",
      "train loss:0.2308029720127949\n",
      "train loss:0.18304833233704088\n",
      "train loss:0.3051502099012181\n",
      "train loss:0.25125555203303135\n",
      "train loss:0.31504226343664476\n",
      "train loss:0.24979196037299922\n",
      "train loss:0.4268076966525932\n",
      "train loss:0.36193691525817256\n",
      "train loss:0.40438543079001266\n",
      "train loss:0.35315615973521475\n",
      "train loss:0.4166928473774331\n",
      "train loss:0.2843481390151691\n",
      "train loss:0.3141664766829411\n",
      "train loss:0.26925613061260917\n",
      "train loss:0.308498289387139\n",
      "train loss:0.3631309320878709\n",
      "train loss:0.323409907590247\n",
      "train loss:0.3234030107514046\n",
      "train loss:0.47263508211289745\n",
      "train loss:0.279804453134277\n",
      "train loss:0.3621223759007986\n",
      "train loss:0.21584560646097625\n",
      "train loss:0.2223338128592023\n",
      "train loss:0.3810618920481464\n",
      "train loss:0.3393623943222457\n",
      "train loss:0.3474107603175963\n",
      "train loss:0.3335107856787618\n",
      "train loss:0.29457961655783665\n",
      "train loss:0.1852866973905259\n",
      "train loss:0.3200554802490651\n",
      "train loss:0.3125313839771606\n",
      "train loss:0.2534862333822032\n",
      "train loss:0.21605899678139462\n",
      "train loss:0.4532363111271399\n",
      "train loss:0.23867102829471815\n",
      "train loss:0.36749570320988345\n",
      "train loss:0.3175151156908079\n",
      "train loss:0.3263921043503797\n",
      "train loss:0.31857573215320795\n",
      "train loss:0.3504298550875635\n",
      "train loss:0.45763077555749754\n",
      "train loss:0.321011685515834\n",
      "train loss:0.39313418993829524\n",
      "train loss:0.3989048127241474\n",
      "train loss:0.1943758511384709\n",
      "train loss:0.3783329228519753\n",
      "train loss:0.2978461019838026\n",
      "train loss:0.37293551415079534\n",
      "train loss:0.3202400618469579\n",
      "train loss:0.24254726114294176\n",
      "train loss:0.3924614316313844\n",
      "train loss:0.5984948579043015\n",
      "train loss:0.3584323884717538\n",
      "train loss:0.269142979123873\n",
      "train loss:0.2702938599394109\n",
      "train loss:0.42061324103153347\n",
      "train loss:0.30707398945770104\n",
      "train loss:0.29044551040579897\n",
      "train loss:0.2950262201475966\n",
      "train loss:0.24235458446121363\n",
      "train loss:0.3833847824050436\n",
      "train loss:0.4292175948453131\n",
      "train loss:0.3500909765384685\n",
      "train loss:0.2595689016884922\n",
      "train loss:0.4051434010201455\n",
      "train loss:0.4335581343732395\n",
      "train loss:0.4385140009276739\n",
      "train loss:0.2661507174371386\n",
      "train loss:0.3878032195661076\n",
      "train loss:0.31534985734630455\n",
      "train loss:0.20694045765047422\n",
      "train loss:0.28261729220890075\n",
      "train loss:0.30072452261047716\n",
      "train loss:0.22981871949734153\n",
      "train loss:0.3394801143277568\n",
      "train loss:0.366983491440561\n",
      "train loss:0.4519125768915584\n",
      "train loss:0.33581387686001885\n",
      "train loss:0.3073662357414084\n",
      "train loss:0.37931963261370655\n",
      "train loss:0.3039952191163931\n",
      "train loss:0.305428685916093\n",
      "train loss:0.3231658942638975\n",
      "train loss:0.3271452020631316\n",
      "train loss:0.3027427658820891\n",
      "train loss:0.17844596281261393\n",
      "train loss:0.24842069480177162\n",
      "train loss:0.26597781602871995\n",
      "train loss:0.27769954469931885\n",
      "train loss:0.3077817674621316\n",
      "train loss:0.3680932035027887\n",
      "train loss:0.31238800584593285\n",
      "train loss:0.34741911224056865\n",
      "train loss:0.2992303239803337\n",
      "train loss:0.3871403732581078\n",
      "train loss:0.2231490421625114\n",
      "train loss:0.39681231015521784\n",
      "train loss:0.2598371621846239\n",
      "train loss:0.3027594370382633\n",
      "train loss:0.36182722497793374\n",
      "train loss:0.3418498114063885\n",
      "train loss:0.26981079113299905\n",
      "train loss:0.39663757279480516\n",
      "train loss:0.36276551184005185\n",
      "train loss:0.28105686453414136\n",
      "train loss:0.43908351518596944\n",
      "train loss:0.36705044233313955\n",
      "train loss:0.3099045791999504\n",
      "train loss:0.25212882646682383\n",
      "train loss:0.23700505201573546\n",
      "train loss:0.36806450205635777\n",
      "train loss:0.38859469899782867\n",
      "train loss:0.3040051273776758\n",
      "train loss:0.305987057872507\n",
      "train loss:0.39424730405755315\n",
      "train loss:0.35311467290340026\n",
      "train loss:0.2975964474925406\n",
      "train loss:0.37345492619229587\n",
      "train loss:0.7192141901499269\n",
      "train loss:0.2349670035019774\n",
      "train loss:0.35340999867158424\n",
      "train loss:0.263812225825214\n",
      "train loss:0.5116543252575876\n",
      "train loss:0.2862621626208776\n",
      "train loss:0.3455527792607283\n",
      "train loss:0.35567069321679873\n",
      "train loss:0.31113066047133947\n",
      "train loss:0.22488018081048966\n",
      "train loss:0.30086786411765504\n",
      "train loss:0.3683413055376056\n",
      "train loss:0.3931839965292719\n",
      "train loss:0.29749824017497833\n",
      "train loss:0.2812690843795665\n",
      "train loss:0.20615738710064535\n",
      "train loss:0.321120888350388\n",
      "train loss:0.3074744433010068\n",
      "train loss:0.3560360345125927\n",
      "train loss:0.36283399903941865\n",
      "train loss:0.13364498022869584\n",
      "train loss:0.37352925087956423\n",
      "train loss:0.2886171136919944\n",
      "train loss:0.2729754024827003\n",
      "train loss:0.38901470238283425\n",
      "train loss:0.3341654301534448\n",
      "train loss:0.297361931769631\n",
      "train loss:0.3072543516323314\n",
      "train loss:0.19638411849560725\n",
      "train loss:0.257916563106239\n",
      "train loss:0.25401601969901166\n",
      "train loss:0.35061228277179574\n",
      "train loss:0.46767393514522715\n",
      "train loss:0.3452009831912139\n",
      "train loss:0.23093589485394708\n",
      "train loss:0.22366237510612055\n",
      "train loss:0.3872290366423961\n",
      "train loss:0.4085517956051785\n",
      "train loss:0.375117218862047\n",
      "train loss:0.2907648803863999\n",
      "train loss:0.16898023504378457\n",
      "train loss:0.46698856726690624\n",
      "train loss:0.31428986735693454\n",
      "train loss:0.3108215238188812\n",
      "train loss:0.3390272646777786\n",
      "train loss:0.3126377079288291\n",
      "train loss:0.31466566626545744\n",
      "train loss:0.25566780432859143\n",
      "train loss:0.2906350031593385\n",
      "train loss:0.24504743656373734\n",
      "train loss:0.30339199336689693\n",
      "train loss:0.3218872386140674\n",
      "train loss:0.25075728358103555\n",
      "train loss:0.2941134322764348\n",
      "train loss:0.31953085409246923\n",
      "train loss:0.3846176186928888\n",
      "train loss:0.32474051205440935\n",
      "train loss:0.3108419734521596\n",
      "train loss:0.3063764835629988\n",
      "train loss:0.2401808324485143\n",
      "train loss:0.3815530054978513\n",
      "train loss:0.29512953270106435\n",
      "train loss:0.43870081222332075\n",
      "train loss:0.36068649962161786\n",
      "train loss:0.358325886039834\n",
      "train loss:0.3762667214008799\n",
      "train loss:0.25915915929859784\n",
      "train loss:0.33841211338430155\n",
      "train loss:0.3777321591732138\n",
      "train loss:0.28635974819363985\n",
      "train loss:0.3359088671238888\n",
      "train loss:0.3088517846774511\n",
      "train loss:0.3215180420970281\n",
      "train loss:0.25983130699485085\n",
      "train loss:0.30595601121463945\n",
      "train loss:0.29172571239664713\n",
      "train loss:0.2839244594553779\n",
      "train loss:0.4242129904543216\n",
      "train loss:0.33740240157593576\n",
      "train loss:0.19942750203129825\n",
      "train loss:0.30001552357844913\n",
      "train loss:0.36355444530604075\n",
      "train loss:0.3191521747015969\n",
      "train loss:0.2750519039377106\n",
      "train loss:0.39652503179331183\n",
      "train loss:0.3530181833153996\n",
      "train loss:0.3361481650701055\n",
      "train loss:0.42839381913491886\n",
      "train loss:0.300406903716889\n",
      "train loss:0.3388115772738133\n",
      "train loss:0.24790190923371733\n",
      "train loss:0.3227319227098872\n",
      "train loss:0.26202144016269663\n",
      "train loss:0.498605580060117\n",
      "train loss:0.18945635374309902\n",
      "train loss:0.28977651917192215\n",
      "train loss:0.5467851087517933\n",
      "train loss:0.3756911626057813\n",
      "train loss:0.3911427547954556\n",
      "train loss:0.3152868629918118\n",
      "train loss:0.2911753782067823\n",
      "train loss:0.3274868172753965\n",
      "train loss:0.16122556048360018\n",
      "train loss:0.29517523988317174\n",
      "train loss:0.1736886957009053\n",
      "train loss:0.3017367340396508\n",
      "train loss:0.19553371629641755\n",
      "train loss:0.2713795632267289\n",
      "train loss:0.24904639245575821\n",
      "train loss:0.37046029989332674\n",
      "train loss:0.24228399192436778\n",
      "train loss:0.38847657038744166\n",
      "train loss:0.2348008003893872\n",
      "train loss:0.3492809378712965\n",
      "train loss:0.2706016980290974\n",
      "train loss:0.23372780651093691\n",
      "train loss:0.2896646358844855\n",
      "train loss:0.5425763424897657\n",
      "train loss:0.4377366223899305\n",
      "train loss:0.2115728602455245\n",
      "train loss:0.29338350727112456\n",
      "train loss:0.35094074813179654\n",
      "train loss:0.23285858696350162\n",
      "train loss:0.27236933852880596\n",
      "train loss:0.2687660658922785\n",
      "train loss:0.30342364888045614\n",
      "train loss:0.25955637412849575\n",
      "train loss:0.3074223585506773\n",
      "train loss:0.3402842428549231\n",
      "train loss:0.31223079016999916\n",
      "train loss:0.2697541130139261\n",
      "train loss:0.3303113802445116\n",
      "train loss:0.35562170787793435\n",
      "train loss:0.29660215307451876\n",
      "train loss:0.32199775915259343\n",
      "train loss:0.32837049009621694\n",
      "train loss:0.27668307150613936\n",
      "train loss:0.33330252043485126\n",
      "train loss:0.36006628419436765\n",
      "train loss:0.28593859546975187\n",
      "train loss:0.34522910125174555\n",
      "train loss:0.38425737324049664\n",
      "train loss:0.3917586414882579\n",
      "train loss:0.2508479761214088\n",
      "train loss:0.3169324042328766\n",
      "train loss:0.23476643121705806\n",
      "train loss:0.3409800403277314\n",
      "train loss:0.3788107140445513\n",
      "train loss:0.34650502699734226\n",
      "train loss:0.33962111558049385\n",
      "train loss:0.2632233404577979\n",
      "train loss:0.4283958182395255\n",
      "train loss:0.2182133214089668\n",
      "train loss:0.26071556344742264\n",
      "train loss:0.28245421668244963\n",
      "train loss:0.29832437819376506\n",
      "train loss:0.3026897982418005\n",
      "train loss:0.38304698422585093\n",
      "train loss:0.3789207052971165\n",
      "train loss:0.18496668513129344\n",
      "train loss:0.309631688031668\n",
      "train loss:0.32512498437995624\n",
      "train loss:0.3969331970549899\n",
      "train loss:0.294815641118974\n",
      "train loss:0.38479793818675256\n",
      "train loss:0.32993674804678236\n",
      "train loss:0.34099866378482474\n",
      "train loss:0.3515769624532238\n",
      "train loss:0.3378046779464242\n",
      "train loss:0.2594581136146119\n",
      "train loss:0.3702242420836354\n",
      "train loss:0.25872795674728877\n",
      "train loss:0.43129175027532407\n",
      "train loss:0.30217188786212823\n",
      "train loss:0.31851808307189694\n",
      "train loss:0.40473193692104487\n",
      "train loss:0.2639074376517911\n",
      "train loss:0.277666124343923\n",
      "train loss:0.30582525317254433\n",
      "train loss:0.3620518977622336\n",
      "train loss:0.40280997759086673\n",
      "train loss:0.31602080438546337\n",
      "train loss:0.32846323869519545\n",
      "train loss:0.40106805126595707\n",
      "train loss:0.4271782318984348\n",
      "train loss:0.32821251824272407\n",
      "train loss:0.38879803434293814\n",
      "train loss:0.19081532984572608\n",
      "train loss:0.3423953398615083\n",
      "train loss:0.4674455410088101\n",
      "train loss:0.34121825801881217\n",
      "train loss:0.1869987842618569\n",
      "train loss:0.3713430848576772\n",
      "train loss:0.27160707869738354\n",
      "train loss:0.36276887517369816\n",
      "train loss:0.40277136320681195\n",
      "train loss:0.30446400776950266\n",
      "train loss:0.2884736625801715\n",
      "train loss:0.18460640949257928\n",
      "train loss:0.29011696374186846\n",
      "train loss:0.21739735060330911\n",
      "train loss:0.4078414419821334\n",
      "train loss:0.5314678891135608\n",
      "train loss:0.3402161957309964\n",
      "train loss:0.2641941079412744\n",
      "train loss:0.4017755538660768\n",
      "train loss:0.22360068467627134\n",
      "train loss:0.2970538245522161\n",
      "train loss:0.4901712405843392\n",
      "train loss:0.19033282239854465\n",
      "train loss:0.1564197939800837\n",
      "train loss:0.35187715521395796\n",
      "train loss:0.4893526949125181\n",
      "train loss:0.3735375105572143\n",
      "train loss:0.23036693738142922\n",
      "train loss:0.25289161668034044\n",
      "train loss:0.22440172668227157\n",
      "train loss:0.27934694204958893\n",
      "train loss:0.310607774410797\n",
      "train loss:0.32544633658638217\n",
      "train loss:0.3455470071840049\n",
      "train loss:0.505253306245417\n",
      "train loss:0.41001372238915523\n",
      "train loss:0.3704079688594982\n",
      "train loss:0.3160326112703702\n",
      "train loss:0.3986489419316359\n",
      "train loss:0.2336394783604897\n",
      "train loss:0.3044604324054551\n",
      "=== epoch:20, train acc:0.867, test acc:0.865 ===\n",
      "train loss:0.3396109510510974\n",
      "train loss:0.21961352108402538\n",
      "train loss:0.49994362217100385\n",
      "train loss:0.3326818778652126\n",
      "train loss:0.3338114759379097\n",
      "train loss:0.29323101955198355\n",
      "train loss:0.3084717469627266\n",
      "train loss:0.22649146605317938\n",
      "train loss:0.2347466323538325\n",
      "train loss:0.3385677106555892\n",
      "train loss:0.3699090546690105\n",
      "train loss:0.36153973744310014\n",
      "train loss:0.2974017717135393\n",
      "train loss:0.2666770462674256\n",
      "train loss:0.24458531586171922\n",
      "train loss:0.18612980274623073\n",
      "train loss:0.2140233567342618\n",
      "train loss:0.25691625402319695\n",
      "train loss:0.32536019103008873\n",
      "train loss:0.2592999922589441\n",
      "train loss:0.23669477734555855\n",
      "train loss:0.22659040484953688\n",
      "train loss:0.2209857000839697\n",
      "train loss:0.31335043768096094\n",
      "train loss:0.38222196436467726\n",
      "train loss:0.379043936587027\n",
      "train loss:0.20470794109271417\n",
      "train loss:0.34061397504746177\n",
      "train loss:0.2755575962829386\n",
      "train loss:0.20402571945633405\n",
      "train loss:0.24699070832013018\n",
      "train loss:0.24931672273529312\n",
      "train loss:0.341856140911005\n",
      "train loss:0.365049940971156\n",
      "train loss:0.32668449942688943\n",
      "train loss:0.29182093669984954\n",
      "train loss:0.2841935590689347\n",
      "train loss:0.3552959665199058\n",
      "train loss:0.24602675766349635\n",
      "train loss:0.26274826629956516\n",
      "train loss:0.28283804315399563\n",
      "train loss:0.27331459321685914\n",
      "train loss:0.32942550164298695\n",
      "train loss:0.2805759908782582\n",
      "train loss:0.2988796232689937\n",
      "train loss:0.2779639509347714\n",
      "train loss:0.3181305650500852\n",
      "train loss:0.40128156817818633\n",
      "train loss:0.2768478115866636\n",
      "train loss:0.29879469498534816\n",
      "train loss:0.27495829697594903\n",
      "train loss:0.2719504913871943\n",
      "train loss:0.28938862447418734\n",
      "train loss:0.2737014337046235\n",
      "train loss:0.35395556337923767\n",
      "train loss:0.41474272742408297\n",
      "train loss:0.37223737677605767\n",
      "train loss:0.40662811370385266\n",
      "train loss:0.31598499835097477\n",
      "train loss:0.42454765792806975\n",
      "train loss:0.3428475598698444\n",
      "train loss:0.4133497593567425\n",
      "train loss:0.4008349512748397\n",
      "train loss:0.20657372429156784\n",
      "train loss:0.2952696313223705\n",
      "train loss:0.3055069203695003\n",
      "train loss:0.3334827538793246\n",
      "train loss:0.3228358545119754\n",
      "train loss:0.32436612390014885\n",
      "train loss:0.24032118608807515\n",
      "train loss:0.4257864848075997\n",
      "train loss:0.27316620250268925\n",
      "train loss:0.2663324969010343\n",
      "train loss:0.2830502740456907\n",
      "train loss:0.27394835292438985\n",
      "train loss:0.19520058726636808\n",
      "train loss:0.20743302597542862\n",
      "train loss:0.29674499086582434\n",
      "train loss:0.3587323327922159\n",
      "train loss:0.34444469143494866\n",
      "train loss:0.29748685036047656\n",
      "train loss:0.2764189267901738\n",
      "train loss:0.3026465734984849\n",
      "train loss:0.28209723221027977\n",
      "train loss:0.3593549547547498\n",
      "train loss:0.3930658933044541\n",
      "train loss:0.25071147666733906\n",
      "train loss:0.36776337850503865\n",
      "train loss:0.22721636807381482\n",
      "train loss:0.2960456070872774\n",
      "train loss:0.3168517708966261\n",
      "train loss:0.2923665147483618\n",
      "train loss:0.20345664885111625\n",
      "train loss:0.3424790920977341\n",
      "train loss:0.30042766925537906\n",
      "train loss:0.39254156247857785\n",
      "train loss:0.2728341279782924\n",
      "train loss:0.19816107590226742\n",
      "train loss:0.2561743893921473\n",
      "train loss:0.39682292000286634\n",
      "train loss:0.4086963741206124\n",
      "train loss:0.21716547043978637\n",
      "train loss:0.31842020327679144\n",
      "train loss:0.2475096774059622\n",
      "train loss:0.3847949829247515\n",
      "train loss:0.18982411185042478\n",
      "train loss:0.2828057004325582\n",
      "train loss:0.32307078723899124\n",
      "train loss:0.337027425226385\n",
      "train loss:0.386676703006663\n",
      "train loss:0.30290558578780374\n",
      "train loss:0.23363790945353174\n",
      "train loss:0.2602067803013972\n",
      "train loss:0.28642401158650893\n",
      "train loss:0.2177491733255196\n",
      "train loss:0.20771987536412365\n",
      "train loss:0.3101959545918489\n",
      "train loss:0.25373502967984557\n",
      "train loss:0.2385376830912954\n",
      "train loss:0.34307377695585445\n",
      "train loss:0.293697541248907\n",
      "train loss:0.38658129401543495\n",
      "train loss:0.4571505149794312\n",
      "train loss:0.2412905774484055\n",
      "train loss:0.33230555998229017\n",
      "train loss:0.4888611754892166\n",
      "train loss:0.4033985228674735\n",
      "train loss:0.31816133684253706\n",
      "train loss:0.2741881241486001\n",
      "train loss:0.31972385902953454\n",
      "train loss:0.16132779256225177\n",
      "train loss:0.36739064426103674\n",
      "train loss:0.28877500294396563\n",
      "train loss:0.39172508333585754\n",
      "train loss:0.28445318327461805\n",
      "train loss:0.35417006688403174\n",
      "train loss:0.3843878498972975\n",
      "train loss:0.2865771831385438\n",
      "train loss:0.3380086117408077\n",
      "train loss:0.28565324656858326\n",
      "train loss:0.329027701062198\n",
      "train loss:0.31775367703636526\n",
      "train loss:0.22857773676833976\n",
      "train loss:0.43577288749022997\n",
      "train loss:0.263401988455463\n",
      "train loss:0.19918668614463184\n",
      "train loss:0.27717874752060206\n",
      "train loss:0.27965116689673253\n",
      "train loss:0.28898548526589257\n",
      "train loss:0.3444378277876872\n",
      "train loss:0.25862562425613433\n",
      "train loss:0.27581729094317525\n",
      "train loss:0.37136021033228234\n",
      "train loss:0.2795741311514784\n",
      "train loss:0.28093362585674847\n",
      "train loss:0.4490664564476038\n",
      "train loss:0.3133986900275858\n",
      "train loss:0.2271137420292473\n",
      "train loss:0.21755742354330265\n",
      "train loss:0.34738541253430333\n",
      "train loss:0.2822934902096297\n",
      "train loss:0.2144544960046342\n",
      "train loss:0.2638117737660113\n",
      "train loss:0.24953389467007747\n",
      "train loss:0.5736755955436814\n",
      "train loss:0.385327712463619\n",
      "train loss:0.3089462787715058\n",
      "train loss:0.31893486637475277\n",
      "train loss:0.3043931832492859\n",
      "train loss:0.3745108905575705\n",
      "train loss:0.3942889819608208\n",
      "train loss:0.24391372279598747\n",
      "train loss:0.39568577945537425\n",
      "train loss:0.39661980293081345\n",
      "train loss:0.4261329329651155\n",
      "train loss:0.4371587525778051\n",
      "train loss:0.2774916251797791\n",
      "train loss:0.30207585164388395\n",
      "train loss:0.27602746321376004\n",
      "train loss:0.42765868795403683\n",
      "train loss:0.24509891823430377\n",
      "train loss:0.3034240598482934\n",
      "train loss:0.2951662595672025\n",
      "train loss:0.408613326130658\n",
      "train loss:0.2857678912859463\n",
      "train loss:0.28107905499871844\n",
      "train loss:0.23069441238509492\n",
      "train loss:0.27362907713765267\n",
      "train loss:0.23796747974977367\n",
      "train loss:0.27650077666973016\n",
      "train loss:0.2218954871329324\n",
      "train loss:0.23603959262422294\n",
      "train loss:0.33168130495649784\n",
      "train loss:0.2414313500353565\n",
      "train loss:0.339160783656492\n",
      "train loss:0.3927549240737897\n",
      "train loss:0.319570938714451\n",
      "train loss:0.40182351569440933\n",
      "train loss:0.40474007928556427\n",
      "train loss:0.3960009148155406\n",
      "train loss:0.2719028236022461\n",
      "train loss:0.35024279373433026\n",
      "train loss:0.26947358611045813\n",
      "train loss:0.3222495187315937\n",
      "train loss:0.31771950529796483\n",
      "train loss:0.24200122843242777\n",
      "train loss:0.2351636738709712\n",
      "train loss:0.3411707684694174\n",
      "train loss:0.28065084650173866\n",
      "train loss:0.2813155517161195\n",
      "train loss:0.24043773273479807\n",
      "train loss:0.32422545895650445\n",
      "train loss:0.2454797256951177\n",
      "train loss:0.2959143130268201\n",
      "train loss:0.3065858929478605\n",
      "train loss:0.45642147102509595\n",
      "train loss:0.3229289149331798\n",
      "train loss:0.21050687035024068\n",
      "train loss:0.3003422817789008\n",
      "train loss:0.2591415681567474\n",
      "train loss:0.3304987262156164\n",
      "train loss:0.25468542258951027\n",
      "train loss:0.2801838186825133\n",
      "train loss:0.3152873768584418\n",
      "train loss:0.34174987800272577\n",
      "train loss:0.40405598210561605\n",
      "train loss:0.34717598184538767\n",
      "train loss:0.3333248529025403\n",
      "train loss:0.29902950845760257\n",
      "train loss:0.2176254014374711\n",
      "train loss:0.4255050045100292\n",
      "train loss:0.2999228946947113\n",
      "train loss:0.31159749475208454\n",
      "train loss:0.17044422991199812\n",
      "train loss:0.21265445750565917\n",
      "train loss:0.3618688102947904\n",
      "train loss:0.25909924935687967\n",
      "train loss:0.27445985724570787\n",
      "train loss:0.28120658662904957\n",
      "train loss:0.27594749151236364\n",
      "train loss:0.23548797852982403\n",
      "train loss:0.27125490035426786\n",
      "train loss:0.3562491021645272\n",
      "train loss:0.222088536883828\n",
      "train loss:0.2210293419812162\n",
      "train loss:0.2631972494267121\n",
      "train loss:0.28290117647380375\n",
      "train loss:0.266786640978\n",
      "train loss:0.22849431966937406\n",
      "train loss:0.48100377332386307\n",
      "train loss:0.20771348256412037\n",
      "train loss:0.3223318570604107\n",
      "train loss:0.3590906858216257\n",
      "train loss:0.4173350748774653\n",
      "train loss:0.3086095546654985\n",
      "train loss:0.263033987301425\n",
      "train loss:0.29794593633413624\n",
      "train loss:0.31530692700996005\n",
      "train loss:0.2880595502652714\n",
      "train loss:0.4028716822018907\n",
      "train loss:0.2736749377374458\n",
      "train loss:0.1577638585138594\n",
      "train loss:0.2737664997460049\n",
      "train loss:0.29218018135852863\n",
      "train loss:0.24824977499433787\n",
      "train loss:0.3832004768497555\n",
      "train loss:0.3537356306717481\n",
      "train loss:0.28772318256116686\n",
      "train loss:0.21872462130710443\n",
      "train loss:0.3281202197335016\n",
      "train loss:0.2705621201326504\n",
      "train loss:0.20399666205706635\n",
      "train loss:0.4013405360886918\n",
      "train loss:0.4837720708883657\n",
      "train loss:0.37844459977266326\n",
      "train loss:0.26597769929576154\n",
      "train loss:0.27050016205549055\n",
      "train loss:0.3112466328740572\n",
      "train loss:0.2757577000146134\n",
      "train loss:0.4108076770265903\n",
      "train loss:0.2997331354311861\n",
      "train loss:0.2311680682956741\n",
      "train loss:0.44756024027306773\n",
      "train loss:0.2532949091757737\n",
      "train loss:0.3694109044597323\n",
      "train loss:0.27699324608883047\n",
      "train loss:0.2089913020264742\n",
      "train loss:0.373817858549589\n",
      "train loss:0.24209563643231707\n",
      "train loss:0.3453887431239078\n",
      "train loss:0.30576920988015655\n",
      "train loss:0.2967838141909344\n",
      "train loss:0.3108866709676328\n",
      "train loss:0.2717115001622652\n",
      "train loss:0.29629514357631825\n",
      "train loss:0.23743733288950541\n",
      "train loss:0.2026472523000879\n",
      "train loss:0.27164639219872216\n",
      "train loss:0.4031832995004827\n",
      "train loss:0.25153721160816983\n",
      "train loss:0.42584914745771907\n",
      "train loss:0.2527193909328137\n",
      "train loss:0.36709400516404406\n",
      "train loss:0.43452788074891247\n",
      "train loss:0.32994203248404735\n",
      "train loss:0.27708776201925817\n",
      "train loss:0.40900258536087347\n",
      "train loss:0.18320955904981798\n",
      "train loss:0.3269572263905877\n",
      "train loss:0.2245837274343885\n",
      "train loss:0.21340247210566482\n",
      "train loss:0.2845755895356841\n",
      "train loss:0.45289829645221735\n",
      "train loss:0.38057121803758465\n",
      "train loss:0.2794959045125045\n",
      "train loss:0.39473764552702184\n",
      "train loss:0.2400192000703874\n",
      "train loss:0.32400453911356764\n",
      "train loss:0.41463248362933575\n",
      "train loss:0.20941254350601043\n",
      "train loss:0.308864156004484\n",
      "train loss:0.28144587898289763\n",
      "train loss:0.311060868557227\n",
      "train loss:0.2445741411697825\n",
      "train loss:0.19416155308661193\n",
      "train loss:0.2659105090382383\n",
      "train loss:0.18658852163264986\n",
      "train loss:0.3771742966941462\n",
      "train loss:0.40414490343151094\n",
      "train loss:0.26184770700219495\n",
      "train loss:0.3675343932335889\n",
      "train loss:0.2771964452112892\n",
      "train loss:0.2132849853312687\n",
      "train loss:0.29141308705352803\n",
      "train loss:0.21537441481754813\n",
      "train loss:0.34699316605210145\n",
      "train loss:0.29355530708727234\n",
      "train loss:0.2978498144324359\n",
      "train loss:0.21320136781429305\n",
      "train loss:0.3397143397187253\n",
      "train loss:0.32796131812768453\n",
      "train loss:0.32161254971579617\n",
      "train loss:0.28549666497328824\n",
      "train loss:0.3497096092154387\n",
      "train loss:0.33673935536382893\n",
      "train loss:0.22462253825923115\n",
      "train loss:0.2495256526892031\n",
      "train loss:0.2564568056211711\n",
      "train loss:0.2931449383952167\n",
      "train loss:0.30926040871317445\n",
      "train loss:0.32187747198508254\n",
      "train loss:0.39526759356643504\n",
      "train loss:0.24496459686538585\n",
      "train loss:0.20832421730366008\n",
      "train loss:0.2616810124042624\n",
      "train loss:0.4232536936028892\n",
      "train loss:0.333652628133668\n",
      "train loss:0.2594144601401938\n",
      "train loss:0.308771754738742\n",
      "train loss:0.4737476059415578\n",
      "train loss:0.27631288062022213\n",
      "train loss:0.2882213625832163\n",
      "train loss:0.36208780372871274\n",
      "train loss:0.37986873117467385\n",
      "train loss:0.28029276907773526\n",
      "train loss:0.4284699362479104\n",
      "train loss:0.33919487765242096\n",
      "train loss:0.185516814023926\n",
      "train loss:0.4020280135623102\n",
      "train loss:0.24827493237986908\n",
      "train loss:0.41327416803901396\n",
      "train loss:0.24894364263291305\n",
      "train loss:0.2583919471288479\n",
      "train loss:0.29915859999231276\n",
      "train loss:0.27745966005155986\n",
      "train loss:0.3578886001382254\n",
      "train loss:0.30427938618186323\n",
      "train loss:0.2662160303763547\n",
      "train loss:0.3316772453647442\n",
      "train loss:0.348104251888254\n",
      "train loss:0.2781112158596566\n",
      "train loss:0.20043907167331942\n",
      "train loss:0.3544833226617376\n",
      "train loss:0.24618184873882976\n",
      "train loss:0.3374288289881957\n",
      "train loss:0.3216444022459006\n",
      "train loss:0.6371797234113858\n",
      "train loss:0.29769475023408226\n",
      "train loss:0.30758368923451057\n",
      "train loss:0.27068185450927773\n",
      "train loss:0.2590033301001629\n",
      "train loss:0.3428204629697562\n",
      "train loss:0.37272817537500147\n",
      "train loss:0.2816032867560695\n",
      "train loss:0.2789114465218583\n",
      "train loss:0.3470968177683823\n",
      "train loss:0.28054545659760205\n",
      "train loss:0.17571032284623483\n",
      "train loss:0.38354481527589485\n",
      "train loss:0.2819920531531154\n",
      "train loss:0.24027367990439527\n",
      "train loss:0.20239077418219506\n",
      "train loss:0.28042766938846797\n",
      "train loss:0.2527314417292158\n",
      "train loss:0.23759911233562633\n",
      "train loss:0.23030288680962027\n",
      "train loss:0.31631801076245447\n",
      "train loss:0.3996200564809863\n",
      "train loss:0.31505190407652894\n",
      "train loss:0.239856925518128\n",
      "train loss:0.2655915395111093\n",
      "train loss:0.3136148715533122\n",
      "train loss:0.24229506490442446\n",
      "train loss:0.368406824257347\n",
      "train loss:0.2102239040227558\n",
      "train loss:0.3646049244480676\n",
      "train loss:0.379768295824422\n",
      "train loss:0.23069179872089382\n",
      "train loss:0.3796793149587656\n",
      "train loss:0.3393725271192697\n",
      "train loss:0.25617610453221074\n",
      "train loss:0.47063192500972095\n",
      "train loss:0.43186839636805874\n",
      "train loss:0.20160323201081945\n",
      "train loss:0.23787652711974183\n",
      "train loss:0.2786962581820211\n",
      "train loss:0.42423227656237894\n",
      "train loss:0.237898673139734\n",
      "train loss:0.2812944324879166\n",
      "train loss:0.2837533220598412\n",
      "train loss:0.24281770626035595\n",
      "train loss:0.3436085080106115\n",
      "train loss:0.17708531266380603\n",
      "train loss:0.20299445010608014\n",
      "train loss:0.36647600852592527\n",
      "train loss:0.26750973241013654\n",
      "train loss:0.2929676614599914\n",
      "train loss:0.35251510302875533\n",
      "train loss:0.39021235064495197\n",
      "train loss:0.204907788021288\n",
      "train loss:0.3256089617315163\n",
      "train loss:0.24300001472649735\n",
      "train loss:0.18387958111122188\n",
      "train loss:0.38291626286729685\n",
      "train loss:0.25009173094712556\n",
      "train loss:0.540723701247876\n",
      "train loss:0.27119079939821195\n",
      "train loss:0.16701017775555588\n",
      "train loss:0.3727444815337908\n",
      "train loss:0.24496516424197942\n",
      "train loss:0.2515769433925943\n",
      "train loss:0.3780639673284415\n",
      "train loss:0.2572836643011186\n",
      "train loss:0.3070032208709449\n",
      "train loss:0.35427230632201345\n",
      "train loss:0.3613599283016239\n",
      "train loss:0.31399766024637876\n",
      "train loss:0.27157962040121353\n",
      "train loss:0.2817757177102495\n",
      "train loss:0.38542622163531787\n",
      "train loss:0.3207758510334675\n",
      "train loss:0.32054220101265907\n",
      "train loss:0.26124771321635604\n",
      "train loss:0.22041300317094753\n",
      "train loss:0.3272111497048358\n",
      "train loss:0.28445966553267293\n",
      "train loss:0.25380472436359414\n",
      "train loss:0.21276444200207348\n",
      "train loss:0.32100436509528324\n",
      "train loss:0.2746128281709287\n",
      "train loss:0.2884015263822092\n",
      "train loss:0.31825401878299947\n",
      "train loss:0.3108805386701356\n",
      "train loss:0.3025301939241862\n",
      "train loss:0.19007018764251551\n",
      "train loss:0.2662956014835749\n",
      "train loss:0.30072995936022695\n",
      "train loss:0.276822235947946\n",
      "train loss:0.23536238024942108\n",
      "train loss:0.5053966356714925\n",
      "train loss:0.2740063842494908\n",
      "train loss:0.24060421092403173\n",
      "train loss:0.3208861006727346\n",
      "train loss:0.30629588369777355\n",
      "train loss:0.3462856241381096\n",
      "train loss:0.1939332059263266\n",
      "train loss:0.2781347731502709\n",
      "train loss:0.18609528059173336\n",
      "train loss:0.5360327205847646\n",
      "train loss:0.19791144759098064\n",
      "train loss:0.16048848571909777\n",
      "train loss:0.34588877658248923\n",
      "train loss:0.24813820093804892\n",
      "train loss:0.3401419134597752\n",
      "train loss:0.2832789700675012\n",
      "train loss:0.377035997783121\n",
      "train loss:0.3752953525629198\n",
      "train loss:0.14920251630755177\n",
      "train loss:0.2303203355954459\n",
      "train loss:0.22999608583275977\n",
      "train loss:0.4735762298050528\n",
      "train loss:0.23433448315077995\n",
      "train loss:0.26437544060068396\n",
      "train loss:0.21658014366521564\n",
      "train loss:0.18309718180780743\n",
      "train loss:0.2683061099394466\n",
      "train loss:0.2141343773798008\n",
      "train loss:0.4149499360377004\n",
      "train loss:0.3210919494107349\n",
      "train loss:0.3803858408938822\n",
      "train loss:0.3353394668126898\n",
      "train loss:0.31245977974870764\n",
      "train loss:0.2377919641298149\n",
      "train loss:0.26797011175669144\n",
      "train loss:0.26281339969117296\n",
      "train loss:0.2243491867030214\n",
      "train loss:0.34477895428736766\n",
      "train loss:0.15830887210213473\n",
      "train loss:0.23469803650099982\n",
      "train loss:0.3567194918202563\n",
      "train loss:0.2497575136220177\n",
      "train loss:0.27904810187876594\n",
      "train loss:0.22173624370149686\n",
      "train loss:0.239077460055451\n",
      "train loss:0.20888996357257408\n",
      "train loss:0.19563426810093124\n",
      "train loss:0.2809770717787519\n",
      "train loss:0.23007578168614354\n",
      "train loss:0.3232269708068467\n",
      "train loss:0.25988586387087514\n",
      "train loss:0.30734162975885726\n",
      "train loss:0.21021160817014775\n",
      "train loss:0.24946742574750377\n",
      "train loss:0.27254819227579963\n",
      "train loss:0.2519293678129962\n",
      "train loss:0.22969867345191333\n",
      "train loss:0.17630798906678385\n",
      "train loss:0.264637428544436\n",
      "train loss:0.22449591287372736\n",
      "train loss:0.22501058908050253\n",
      "train loss:0.35110354956567447\n",
      "train loss:0.36818380049762134\n",
      "train loss:0.2526579299505053\n",
      "train loss:0.39695143618821954\n",
      "train loss:0.3273530844433809\n",
      "train loss:0.1553877116109213\n",
      "train loss:0.33648158069851847\n",
      "train loss:0.3367649185312535\n",
      "train loss:0.24612354014440818\n",
      "train loss:0.24570112412694492\n",
      "train loss:0.33024485617258476\n",
      "train loss:0.23463381735921396\n",
      "train loss:0.28407941502805706\n",
      "train loss:0.20581701604986175\n",
      "train loss:0.22188918785953227\n",
      "train loss:0.26250864315700556\n",
      "train loss:0.28428909037817596\n",
      "train loss:0.37053645723966816\n",
      "train loss:0.28309529027194547\n",
      "train loss:0.21725975878803375\n",
      "train loss:0.23144391245307333\n",
      "train loss:0.36815775436915815\n",
      "train loss:0.31346386878241544\n",
      "train loss:0.2666113631483211\n",
      "train loss:0.25453220055207165\n",
      "train loss:0.2901952419574531\n",
      "train loss:0.40325038384747386\n",
      "train loss:0.3030679500955031\n",
      "train loss:0.24839184543540896\n",
      "train loss:0.34872043572667116\n",
      "train loss:0.30011578599146993\n",
      "train loss:0.21673490489440944\n",
      "train loss:0.35331038157999584\n",
      "train loss:0.3224155710946653\n",
      "train loss:0.33952769320346027\n",
      "train loss:0.30422201882136385\n",
      "train loss:0.251967076802782\n",
      "train loss:0.18733976948976783\n",
      "train loss:0.12121617910080683\n",
      "train loss:0.26150310031722745\n",
      "train loss:0.2096778739717365\n",
      "train loss:0.15784570312194113\n",
      "train loss:0.22048695286696623\n",
      "train loss:0.2595814024578625\n",
      "train loss:0.33167758647400364\n",
      "train loss:0.2105309143129721\n",
      "train loss:0.26794972742493117\n",
      "train loss:0.40334474900215794\n",
      "train loss:0.18099277180621826\n",
      "train loss:0.28716151281673696\n",
      "train loss:0.24251623319405433\n",
      "train loss:0.21886324105962413\n",
      "train loss:0.23894982718954158\n",
      "train loss:0.24455759034005692\n",
      "train loss:0.2997913632989271\n",
      "train loss:0.2634170136380283\n",
      "train loss:0.25929010700321553\n",
      "train loss:0.3274030272144674\n",
      "train loss:0.15923464916265598\n",
      "  : 1808.1587181091309\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.9262\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 60000 data\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100, 100], output_size=10,\n",
    "                             activation='relu', weight_init_std='relu', use_dropout=True, dropout_ration = 0.5)  \n",
    "# affine-relu-dropout -affine-relu-dropout -affine-relu-dropout -affine-relu-dropout -affine-relu-dropout \n",
    "# -affine-relu-dropout -affine-relu-dropout -affine-softmax\n",
    "\n",
    "trainer_2 = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=20, mini_batch_size=100, # 60000  // 100 = 600 * 20\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000) #  12000!!\n",
    "\n",
    "trainer_2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "466d1f00-f1a0-4041-8324-91e3c97fbbd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.867"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_2.train_acc_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "269c551a-eb8a-4512-a3d3-5d3034cb000f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15923464916265598\n",
      "0.797746090363644\n"
     ]
    }
   ],
   "source": [
    "print(trainer_2.train_loss_list[-1])\n",
    "print(trainer.train_loss_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2209e0-bc73-4102-8d57-e0748ba55883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from deep_convnet import DeepConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = DeepConvNet() \n",
    "# conv-relu-conv-relu-pool -conv-relu-conv-relu-pool -conv-relu\n",
    "# -conv-relu-pool -affine-relu-dropout -affine-dropout-softmax\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=20, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "#  \n",
    "network.save_params(\"deep_convnet_params.pkl\")\n",
    "print(\"Saved Network Parameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7bd48a1-71d1-4b52-9b31-cd2d2575f470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train_acc_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "93fb745a-0c53-4ef7-b7bb-fad0ef13ace5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600.0 12000\n"
     ]
    }
   ],
   "source": [
    "iter_per_epoch = max(60000 / 100, 1)\n",
    "max_iter = int(20 * iter_per_epoch)\n",
    "print(iter_per_epoch, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31d923cb-9b72-426c-8730-cf5e71639273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# \n",
    "x = np.arange(trainer_2.epochs)\n",
    "plt.plot(x, trainer_2.train_acc_list, marker='o', label='multilayernet')\n",
    "plt.plot(x, trainer.train_acc_list, marker='s', label='deepconvnet')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.xticks(np.arange(0, trainer_2.epochs, 5))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72ee4f74-63b4-48c2-8001-6314e487b669",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.299285690332038\n",
      "=== epoch:1, train acc:0.133, test acc:0.132 ===\n",
      "train loss:2.2933920788870714\n",
      "train loss:2.28933525644492\n",
      "train loss:2.283067904376638\n",
      "train loss:2.2812689226816545\n",
      "train loss:2.2529173027038274\n",
      "train loss:2.2366377450650448\n",
      "train loss:2.229158572445792\n",
      "train loss:2.244634955777349\n",
      "train loss:2.201370311795443\n",
      "train loss:2.163568804766224\n",
      "train loss:2.113296558797133\n",
      "train loss:2.104122826983899\n",
      "train loss:2.0788443853050413\n",
      "train loss:2.0285435026402463\n",
      "train loss:1.9368165558069261\n",
      "train loss:1.8829865078216743\n",
      "train loss:1.7841518457548258\n",
      "train loss:1.759290652691451\n",
      "train loss:1.741940823078839\n",
      "train loss:1.6344926112225406\n",
      "train loss:1.6844649571606767\n",
      "train loss:1.481083264372806\n",
      "train loss:1.43135383991817\n",
      "train loss:1.257742758831012\n",
      "train loss:1.2362828881385772\n",
      "train loss:1.1894019906936801\n",
      "train loss:1.043204194926063\n",
      "train loss:1.1242415968263413\n",
      "train loss:1.1239321024852027\n",
      "train loss:0.9599443799482696\n",
      "train loss:0.8809503914761687\n",
      "train loss:0.9163548079931247\n",
      "train loss:0.834036172503029\n",
      "train loss:0.7827437283648235\n",
      "train loss:0.6565064440402427\n",
      "train loss:0.7320792013922454\n",
      "train loss:0.7025958994956705\n",
      "train loss:0.5272225125751356\n",
      "train loss:0.7140486614938935\n",
      "train loss:0.7562188845288835\n",
      "train loss:0.7382845883638648\n",
      "train loss:0.6805530864999045\n",
      "train loss:0.5944152078071836\n",
      "train loss:0.7325791404495816\n",
      "train loss:0.6725923513409445\n",
      "train loss:0.4710903806676063\n",
      "train loss:0.5065839347026199\n",
      "train loss:0.513877680203179\n",
      "train loss:0.5100832506663162\n",
      "train loss:0.6630261454972937\n",
      "train loss:0.6087404258306903\n",
      "train loss:0.4862420735180528\n",
      "train loss:0.564470301791782\n",
      "train loss:0.43060715758450413\n",
      "train loss:0.48458137196829204\n",
      "train loss:0.4677434656968228\n",
      "train loss:0.5802006967208518\n",
      "train loss:0.3777162962898331\n",
      "train loss:0.5716006370082254\n",
      "train loss:0.4745960814910701\n",
      "train loss:0.5120677617963748\n",
      "train loss:0.37168190263983275\n",
      "train loss:0.5004329642514544\n",
      "train loss:0.39680893257875044\n",
      "train loss:0.3945572615670527\n",
      "train loss:0.570432985183736\n",
      "train loss:0.4038096115087674\n",
      "train loss:0.4469308427513557\n",
      "train loss:0.41028447520450256\n",
      "train loss:0.340872142185711\n",
      "train loss:0.45780185034761245\n",
      "train loss:0.4786431872309543\n",
      "train loss:0.6465095692954338\n",
      "train loss:0.46292776467771235\n",
      "train loss:0.5494733725723914\n",
      "train loss:0.5655622521840984\n",
      "train loss:0.42550957362365927\n",
      "train loss:0.4596980173676044\n",
      "train loss:0.5132438624479393\n",
      "train loss:0.4264294661980413\n",
      "train loss:0.41150767300647745\n",
      "train loss:0.46067085348639\n",
      "train loss:0.38009949230852386\n",
      "train loss:0.4772216051957098\n",
      "train loss:0.28202126956643214\n",
      "train loss:0.4005182441068773\n",
      "train loss:0.47145444238183737\n",
      "train loss:0.4220545775348337\n",
      "train loss:0.42854805948301583\n",
      "train loss:0.6726926594374397\n",
      "train loss:0.4749802638200223\n",
      "train loss:0.4578961613222662\n",
      "train loss:0.411535257490646\n",
      "train loss:0.498605085208348\n",
      "train loss:0.5328591160218111\n",
      "train loss:0.5413688583458767\n",
      "train loss:0.37330593374356114\n",
      "train loss:0.37316449561224085\n",
      "train loss:0.5379779279099828\n",
      "train loss:0.30177095658100883\n",
      "train loss:0.3320008823753948\n",
      "train loss:0.6641737080645559\n",
      "train loss:0.5597213475570453\n",
      "train loss:0.37907132225037493\n",
      "train loss:0.39687062039691995\n",
      "train loss:0.4158323905015442\n",
      "train loss:0.5283832181084954\n",
      "train loss:0.44975416715309124\n",
      "train loss:0.31795480167260776\n",
      "train loss:0.41586369212300495\n",
      "train loss:0.48584633700405705\n",
      "train loss:0.4393072287209228\n",
      "train loss:0.3612526764389219\n",
      "train loss:0.327366448097261\n",
      "train loss:0.39073660818411854\n",
      "train loss:0.3832191404598141\n",
      "train loss:0.49378496642500286\n",
      "train loss:0.38759074009923866\n",
      "train loss:0.40852440181364275\n",
      "train loss:0.36793113314348114\n",
      "train loss:0.4481843865043721\n",
      "train loss:0.5271332453819915\n",
      "train loss:0.42050692484118607\n",
      "train loss:0.3523364671305357\n",
      "train loss:0.3928585654427309\n",
      "train loss:0.34856942953347997\n",
      "train loss:0.3411857674243455\n",
      "train loss:0.2862889585697956\n",
      "train loss:0.3807544289351953\n",
      "train loss:0.29750308398724146\n",
      "train loss:0.28976562310209575\n",
      "train loss:0.566578191540973\n",
      "train loss:0.24479704571738106\n",
      "train loss:0.5232219352546802\n",
      "train loss:0.18227819709666016\n",
      "train loss:0.3988100436017236\n",
      "train loss:0.3293646673144608\n",
      "train loss:0.24060366981284048\n",
      "train loss:0.35738653269557885\n",
      "train loss:0.4859949386064171\n",
      "train loss:0.2651150746965246\n",
      "train loss:0.4100436338969144\n",
      "train loss:0.45076778031124426\n",
      "train loss:0.30622564143180686\n",
      "train loss:0.3884448394331635\n",
      "train loss:0.40897706706445996\n",
      "train loss:0.3323155217832619\n",
      "train loss:0.21381475442790093\n",
      "train loss:0.5378437569923153\n",
      "train loss:0.25377841386706085\n",
      "train loss:0.2648433187632809\n",
      "train loss:0.23001350137310642\n",
      "train loss:0.34036415095002404\n",
      "train loss:0.42313879141904365\n",
      "train loss:0.23771842944896185\n",
      "train loss:0.22360404868234723\n",
      "train loss:0.23840004316767907\n",
      "train loss:0.5808977575963074\n",
      "train loss:0.27106051562650796\n",
      "train loss:0.2911961096014297\n",
      "train loss:0.2446300154887537\n",
      "train loss:0.2136257594770526\n",
      "train loss:0.36909453066252085\n",
      "train loss:0.4309980769746266\n",
      "train loss:0.19864710145356823\n",
      "train loss:0.3596037909672986\n",
      "train loss:0.2978088657726219\n",
      "train loss:0.29845640535695084\n",
      "train loss:0.1842717996655017\n",
      "train loss:0.33258651753635954\n",
      "train loss:0.23560061842369634\n",
      "train loss:0.4240156629808734\n",
      "train loss:0.46073329247953737\n",
      "train loss:0.3684248182249013\n",
      "train loss:0.25439835928630017\n",
      "train loss:0.31696987535891996\n",
      "train loss:0.4859684456828947\n",
      "train loss:0.41098941235665193\n",
      "train loss:0.20971757301198188\n",
      "train loss:0.3253218795056108\n",
      "train loss:0.35160609968147155\n",
      "train loss:0.2745642167696657\n",
      "train loss:0.4332070878768011\n",
      "train loss:0.25299631128097105\n",
      "train loss:0.22519549167955163\n",
      "train loss:0.2922122329485462\n",
      "train loss:0.3569061875271304\n",
      "train loss:0.2968417186420996\n",
      "train loss:0.21661306352054605\n",
      "train loss:0.3538345110979592\n",
      "train loss:0.32908421625691875\n",
      "train loss:0.3085874112031015\n",
      "train loss:0.24309223806293137\n",
      "train loss:0.24088318697402247\n",
      "train loss:0.224782924990691\n",
      "train loss:0.22518914878042007\n",
      "train loss:0.3959171170903474\n",
      "train loss:0.30539248522877954\n",
      "train loss:0.2338184519695029\n",
      "train loss:0.4097446253895572\n",
      "train loss:0.40812134473364475\n",
      "train loss:0.21279360438400854\n",
      "train loss:0.3071808652869821\n",
      "train loss:0.2748199550825429\n",
      "train loss:0.2491109581130535\n",
      "train loss:0.30148773119683453\n",
      "train loss:0.2795824549961861\n",
      "train loss:0.2848185842860378\n",
      "train loss:0.19968868292656913\n",
      "train loss:0.41460528887792397\n",
      "train loss:0.2978954165293759\n",
      "train loss:0.47487209242412426\n",
      "train loss:0.20985410458158937\n",
      "train loss:0.2701784011272511\n",
      "train loss:0.2459790592954246\n",
      "train loss:0.19054975102533636\n",
      "train loss:0.30227468067374746\n",
      "train loss:0.29309335003094655\n",
      "train loss:0.39652119288846327\n",
      "train loss:0.3024875822914572\n",
      "train loss:0.445225844432238\n",
      "train loss:0.38983488398056226\n",
      "train loss:0.3039120921401586\n",
      "train loss:0.20727122656179414\n",
      "train loss:0.28760503455887976\n",
      "train loss:0.19735085244636102\n",
      "train loss:0.1636554260406632\n",
      "train loss:0.2406958245146093\n",
      "train loss:0.31222612401550603\n",
      "train loss:0.30221929607382714\n",
      "train loss:0.439012358824285\n",
      "train loss:0.1643049416019717\n",
      "train loss:0.24573582786042994\n",
      "train loss:0.22455557920566274\n",
      "train loss:0.307651614061507\n",
      "train loss:0.1978006838879841\n",
      "train loss:0.23477514213929965\n",
      "train loss:0.369117925202241\n",
      "train loss:0.36680877849045657\n",
      "train loss:0.22694825924359965\n",
      "train loss:0.2478594977340736\n",
      "train loss:0.2699008142769014\n",
      "train loss:0.2718833880426853\n",
      "train loss:0.34005831611911785\n",
      "train loss:0.2679199618279955\n",
      "train loss:0.21589078217377544\n",
      "train loss:0.34349357810653314\n",
      "train loss:0.2852410421114654\n",
      "train loss:0.19000939516267912\n",
      "train loss:0.26431220454305854\n",
      "train loss:0.38639258195910275\n",
      "train loss:0.18070724306246805\n",
      "train loss:0.2658288226955579\n",
      "train loss:0.36166979447953346\n",
      "train loss:0.2257416049497163\n",
      "train loss:0.21953918907833536\n",
      "train loss:0.2911542178733383\n",
      "train loss:0.17085676229306831\n",
      "train loss:0.1549137957877605\n",
      "train loss:0.29605550402018616\n",
      "train loss:0.23602515764332427\n",
      "train loss:0.32555516813920127\n",
      "train loss:0.2773821295147425\n",
      "train loss:0.2354537770109395\n",
      "train loss:0.2824787665465139\n",
      "train loss:0.15884118902876215\n",
      "train loss:0.4478093035001612\n",
      "train loss:0.16427611901166347\n",
      "train loss:0.15744402268401284\n",
      "train loss:0.30828132621568005\n",
      "train loss:0.1651233276390338\n",
      "train loss:0.36311035983988793\n",
      "train loss:0.29436166414515663\n",
      "train loss:0.1233950483719303\n",
      "train loss:0.25965703749736674\n",
      "train loss:0.23798858673156015\n",
      "train loss:0.196760913086518\n",
      "train loss:0.1898659364009912\n",
      "train loss:0.22284852229169114\n",
      "train loss:0.211537313474917\n",
      "train loss:0.20260256450343903\n",
      "train loss:0.19432886107379815\n",
      "train loss:0.2689993788595134\n",
      "train loss:0.18047420148527726\n",
      "train loss:0.2883141087809412\n",
      "train loss:0.28007504339026434\n",
      "train loss:0.20981299632210182\n",
      "train loss:0.20099469017261726\n",
      "train loss:0.13408286003269124\n",
      "train loss:0.15703413361662416\n",
      "train loss:0.19517901690453546\n",
      "train loss:0.19221248175387504\n",
      "train loss:0.22805391282457993\n",
      "train loss:0.17009168440992437\n",
      "train loss:0.16707185806437114\n",
      "train loss:0.2743642683006353\n",
      "train loss:0.22007719950511595\n",
      "train loss:0.36105276460327834\n",
      "train loss:0.14928750268200777\n",
      "train loss:0.16332100447584585\n",
      "train loss:0.18082000608551563\n",
      "train loss:0.17803357387954363\n",
      "train loss:0.33735677117048923\n",
      "train loss:0.15010501248626698\n",
      "train loss:0.2034466869548385\n",
      "train loss:0.20587979312149252\n",
      "train loss:0.1357375717763882\n",
      "train loss:0.22014952394887952\n",
      "train loss:0.28283424317285516\n",
      "train loss:0.24506236637467274\n",
      "train loss:0.18606048719553347\n",
      "train loss:0.31275920835500526\n",
      "train loss:0.19889015101761712\n",
      "train loss:0.14066507882956397\n",
      "train loss:0.2606018356177745\n",
      "train loss:0.13428139620802265\n",
      "train loss:0.15372629824210482\n",
      "train loss:0.18788971525951964\n",
      "train loss:0.19233354974662784\n",
      "train loss:0.22924005765862976\n",
      "train loss:0.22773704578579132\n",
      "train loss:0.08852628043174197\n",
      "train loss:0.12722631122398606\n",
      "train loss:0.23577061418278722\n",
      "train loss:0.17372514057688832\n",
      "train loss:0.18453257371262477\n",
      "train loss:0.21926175148258392\n",
      "train loss:0.17961326621885154\n",
      "train loss:0.3008100184629789\n",
      "train loss:0.21897695123353608\n",
      "train loss:0.23128818836558096\n",
      "train loss:0.264571768681485\n",
      "train loss:0.19537427417432254\n",
      "train loss:0.2747295555283171\n",
      "train loss:0.21054069768665662\n",
      "train loss:0.24544400606407443\n",
      "train loss:0.3204505018826669\n",
      "train loss:0.2779056908931512\n",
      "train loss:0.1708526441125946\n",
      "train loss:0.23926593869602214\n",
      "train loss:0.25679103697248534\n",
      "train loss:0.1949405275886802\n",
      "train loss:0.21282509422839727\n",
      "train loss:0.20582656829298304\n",
      "train loss:0.14464242295362642\n",
      "train loss:0.1631202638574889\n",
      "train loss:0.26595497459038914\n",
      "train loss:0.22635032453360707\n",
      "train loss:0.1913811142165267\n",
      "train loss:0.28646832853158566\n",
      "train loss:0.17038420640547058\n",
      "train loss:0.15131861501742644\n",
      "train loss:0.10369105167705951\n",
      "train loss:0.11950192572956347\n",
      "train loss:0.20886423022671882\n",
      "train loss:0.35952595245460794\n",
      "train loss:0.1806593625176812\n",
      "train loss:0.3457043659204702\n",
      "train loss:0.06530912269264111\n",
      "train loss:0.28986593236542446\n",
      "train loss:0.23325294368361377\n",
      "train loss:0.25785390542088327\n",
      "train loss:0.21276410406768506\n",
      "train loss:0.26959637513247686\n",
      "train loss:0.1930568822672777\n",
      "train loss:0.15516795585969564\n",
      "train loss:0.33279971413180376\n",
      "train loss:0.11135008039163524\n",
      "train loss:0.1432400697848361\n",
      "train loss:0.20370768936104888\n",
      "train loss:0.17644334119971547\n",
      "train loss:0.23326649859119267\n",
      "train loss:0.18359822557961075\n",
      "train loss:0.1628914249927234\n",
      "train loss:0.13956993331079756\n",
      "train loss:0.1253307922736558\n",
      "train loss:0.18257047158367912\n",
      "train loss:0.19795904898019678\n",
      "train loss:0.0893608693760239\n",
      "train loss:0.17890365595704374\n",
      "train loss:0.11388902774707219\n",
      "train loss:0.25424116613951364\n",
      "train loss:0.2870479182907052\n",
      "train loss:0.27987276533212335\n",
      "train loss:0.1756513211122598\n",
      "train loss:0.15025624580042177\n",
      "train loss:0.12375351480327672\n",
      "train loss:0.23444669002473906\n",
      "train loss:0.20525075431357923\n",
      "train loss:0.1826276826170627\n",
      "train loss:0.25356950320867555\n",
      "train loss:0.24464660502773722\n",
      "train loss:0.15431302738126124\n",
      "train loss:0.1366535627842164\n",
      "train loss:0.31611277330528714\n",
      "train loss:0.14503363303836866\n",
      "train loss:0.2374960428105498\n",
      "train loss:0.11903750449627129\n",
      "train loss:0.1833790698902352\n",
      "train loss:0.17466063463315337\n",
      "train loss:0.17701923178263393\n",
      "train loss:0.13749431243593863\n",
      "train loss:0.2611509571849424\n",
      "train loss:0.14340562013785785\n",
      "train loss:0.18181421298093883\n",
      "train loss:0.17447105650921896\n",
      "train loss:0.2876177941291255\n",
      "train loss:0.20506754565887522\n",
      "train loss:0.15280016507788727\n",
      "train loss:0.15074065427609692\n",
      "train loss:0.1725085003288764\n",
      "train loss:0.22382809059706016\n",
      "train loss:0.1591571891666296\n",
      "train loss:0.2794704237461435\n",
      "train loss:0.16520740773652512\n",
      "train loss:0.14569253286381734\n",
      "train loss:0.13490825369865656\n",
      "train loss:0.09384925541243899\n",
      "train loss:0.32870790773803177\n",
      "train loss:0.19613473256566105\n",
      "train loss:0.17465049335049168\n",
      "train loss:0.1817957136547876\n",
      "train loss:0.1798648365958657\n",
      "train loss:0.3295599846662733\n",
      "train loss:0.19532518298756985\n",
      "train loss:0.10141426081893343\n",
      "train loss:0.10045321383479164\n",
      "train loss:0.1502616143472252\n",
      "train loss:0.10820425108847745\n",
      "train loss:0.25632148855794795\n",
      "train loss:0.13303305342387245\n",
      "train loss:0.1880048644172633\n",
      "train loss:0.15415328987365215\n",
      "train loss:0.16677964681235835\n",
      "train loss:0.24964637182415253\n",
      "train loss:0.15491410921726942\n",
      "train loss:0.15470417357550473\n",
      "train loss:0.11062494227255978\n",
      "train loss:0.1249994774525614\n",
      "train loss:0.08044405964353811\n",
      "train loss:0.12078192511183246\n",
      "train loss:0.2574263583403196\n",
      "train loss:0.18370396121190716\n",
      "train loss:0.19232521375354925\n",
      "train loss:0.1442569308561609\n",
      "train loss:0.18068745433847608\n",
      "train loss:0.09123482618708899\n",
      "train loss:0.23382353544441056\n",
      "train loss:0.2778447435054067\n",
      "train loss:0.1518714009788957\n",
      "train loss:0.15980803312639655\n",
      "train loss:0.24722616458881144\n",
      "train loss:0.17539691305451627\n",
      "train loss:0.12666772189816902\n",
      "train loss:0.1904495898445539\n",
      "train loss:0.24127376295884484\n",
      "train loss:0.08363796519449995\n",
      "train loss:0.08716101532286002\n",
      "train loss:0.11373574281350234\n",
      "train loss:0.1559755991446706\n",
      "train loss:0.22461018211417585\n",
      "train loss:0.10579154596056534\n",
      "train loss:0.1787753346058677\n",
      "train loss:0.20555926351589549\n",
      "train loss:0.06857121045010077\n",
      "train loss:0.17894740551826072\n",
      "train loss:0.1964736730161799\n",
      "train loss:0.2262554840357634\n",
      "train loss:0.0913346489200856\n",
      "train loss:0.0938582423310052\n",
      "train loss:0.2464455571287005\n",
      "train loss:0.1800693135484282\n",
      "train loss:0.10366405988156074\n",
      "train loss:0.06028400755952629\n",
      "train loss:0.2493237336334089\n",
      "train loss:0.0906614412547433\n",
      "train loss:0.09343257172881574\n",
      "train loss:0.09308899731057693\n",
      "train loss:0.10942251154893289\n",
      "train loss:0.15774799903936684\n",
      "train loss:0.14819552993587748\n",
      "train loss:0.08849437840950382\n",
      "train loss:0.19625860485514146\n",
      "train loss:0.17609572286497913\n",
      "train loss:0.23068746917469962\n",
      "train loss:0.1116274099341399\n",
      "train loss:0.19882796897893293\n",
      "train loss:0.1787318381991733\n",
      "train loss:0.07271844154765546\n",
      "train loss:0.21457360197251482\n",
      "train loss:0.2180119503925853\n",
      "train loss:0.14834782112140565\n",
      "train loss:0.15544250767207846\n",
      "train loss:0.13158060879462696\n",
      "train loss:0.23849123086776722\n",
      "train loss:0.10860791092965587\n",
      "train loss:0.22505484023986017\n",
      "train loss:0.135697001716242\n",
      "train loss:0.18010191264746336\n",
      "train loss:0.23084670068185248\n",
      "train loss:0.12691689773524753\n",
      "train loss:0.060327050360065346\n",
      "train loss:0.14544005372800453\n",
      "train loss:0.11700084135957085\n",
      "train loss:0.16436592500059224\n",
      "train loss:0.21001644635759883\n",
      "train loss:0.19840907724388074\n",
      "train loss:0.23328085600527668\n",
      "train loss:0.12720177482411868\n",
      "train loss:0.16016882522662082\n",
      "train loss:0.1335651989887179\n",
      "train loss:0.12131897827335296\n",
      "train loss:0.10027550093800397\n",
      "train loss:0.1507064387848704\n",
      "train loss:0.21254812254445757\n",
      "train loss:0.12415029645603953\n",
      "train loss:0.11925219036734283\n",
      "train loss:0.2313797945321383\n",
      "train loss:0.1177738392375668\n",
      "train loss:0.07766465053334218\n",
      "train loss:0.1293855387404458\n",
      "train loss:0.1856562001308679\n",
      "train loss:0.12043788096212588\n",
      "train loss:0.07841056395172466\n",
      "train loss:0.21098514020592923\n",
      "train loss:0.06001280404642729\n",
      "train loss:0.174805472399169\n",
      "train loss:0.13592422475034147\n",
      "train loss:0.12207134727593587\n",
      "train loss:0.12235412716028443\n",
      "train loss:0.15814436676342322\n",
      "train loss:0.14952571294964329\n",
      "train loss:0.08465153304235715\n",
      "train loss:0.2520893537180588\n",
      "train loss:0.1945902780040576\n",
      "train loss:0.10068856686023185\n",
      "train loss:0.11047251386156168\n",
      "train loss:0.12236665274697484\n",
      "train loss:0.24423356208554092\n",
      "train loss:0.12272982447255602\n",
      "train loss:0.12360618315446043\n",
      "train loss:0.1008744276352289\n",
      "train loss:0.08074779278085074\n",
      "train loss:0.1849911047298008\n",
      "train loss:0.23140651592558434\n",
      "train loss:0.07516722509276548\n",
      "train loss:0.1578930525616649\n",
      "train loss:0.20024125658446434\n",
      "train loss:0.16308048637897263\n",
      "train loss:0.13789228847176957\n",
      "train loss:0.15734111422297495\n",
      "train loss:0.11982743936690463\n",
      "train loss:0.13711130461792323\n",
      "train loss:0.10982887441285274\n",
      "train loss:0.10372186751571037\n",
      "train loss:0.15122134822490468\n",
      "train loss:0.1866631439836268\n",
      "train loss:0.10639064626641138\n",
      "train loss:0.09621697685675101\n",
      "train loss:0.22085314996923383\n",
      "train loss:0.09410095466979579\n",
      "train loss:0.1415390110030928\n",
      "train loss:0.16139543601040532\n",
      "train loss:0.09936790626401447\n",
      "train loss:0.05264595624092142\n",
      "train loss:0.16512305801222527\n",
      "train loss:0.07116526699877052\n",
      "train loss:0.2302259340867109\n",
      "train loss:0.10438033446891723\n",
      "train loss:0.07693022006041852\n",
      "train loss:0.12098567873734219\n",
      "train loss:0.094446441760019\n",
      "train loss:0.14332066608385458\n",
      "train loss:0.1823806787752661\n",
      "train loss:0.14894405493920626\n",
      "train loss:0.20666128481485366\n",
      "train loss:0.08723095367035244\n",
      "train loss:0.10066629424985236\n",
      "train loss:0.05882790711053799\n",
      "train loss:0.09004443007160189\n",
      "train loss:0.13069816201147386\n",
      "train loss:0.12603879888667024\n",
      "train loss:0.07510287716465375\n",
      "train loss:0.20204063695046948\n",
      "train loss:0.12949434084610126\n",
      "train loss:0.11494168246825932\n",
      "train loss:0.055555065191488714\n",
      "train loss:0.14827437738194377\n",
      "train loss:0.12572755513580963\n",
      "train loss:0.10831111931091403\n",
      "train loss:0.15150843411952367\n",
      "train loss:0.055628252729932035\n",
      "train loss:0.05110107626587694\n",
      "train loss:0.13188416705835246\n",
      "train loss:0.09367933364194567\n",
      "train loss:0.10457199486225485\n",
      "train loss:0.2805129315388904\n",
      "train loss:0.24057375936994774\n",
      "train loss:0.07732700202625112\n",
      "train loss:0.14471254490800484\n",
      "=== epoch:2, train acc:0.947, test acc:0.947 ===\n",
      "train loss:0.1901427664100902\n",
      "train loss:0.20662303251240816\n",
      "train loss:0.14086414362170166\n",
      "train loss:0.1414484799220298\n",
      "train loss:0.15801921819418466\n",
      "train loss:0.17594721826726745\n",
      "train loss:0.25389431557742737\n",
      "train loss:0.17628015227981358\n",
      "train loss:0.17573637858685864\n",
      "train loss:0.1287203413106418\n",
      "train loss:0.20282084156598135\n",
      "train loss:0.1389561315068298\n",
      "train loss:0.10207208111299831\n",
      "train loss:0.12864956407291359\n",
      "train loss:0.14873405975323573\n",
      "train loss:0.14432670790116225\n",
      "train loss:0.13517118303677433\n",
      "train loss:0.13602437015892055\n",
      "train loss:0.08405946549828322\n",
      "train loss:0.05216726357046271\n",
      "train loss:0.24757266340810516\n",
      "train loss:0.11477433394084846\n",
      "train loss:0.053913465633369545\n",
      "train loss:0.09191895506993397\n",
      "train loss:0.26460037864473157\n",
      "train loss:0.1192029125707961\n",
      "train loss:0.09222940726711672\n",
      "train loss:0.10503067880248614\n",
      "train loss:0.08003088203974584\n",
      "train loss:0.07910098343467045\n",
      "train loss:0.09808036824679363\n",
      "train loss:0.06140123180954659\n",
      "train loss:0.0676058438577318\n",
      "train loss:0.13418710932680264\n",
      "train loss:0.18489690900249245\n",
      "train loss:0.08044547324500069\n",
      "train loss:0.09775475316624173\n",
      "train loss:0.23874492955010013\n",
      "train loss:0.09237341303885714\n",
      "train loss:0.19318245175274293\n",
      "train loss:0.08768947365280509\n",
      "train loss:0.1680022961837733\n",
      "train loss:0.13442467999374408\n",
      "train loss:0.18371807558472145\n",
      "train loss:0.03493496405766852\n",
      "train loss:0.059383721362615674\n",
      "train loss:0.0935588965178822\n",
      "train loss:0.07551467347859264\n",
      "train loss:0.12971190579448677\n",
      "train loss:0.07397902477433628\n",
      "train loss:0.12079472855784909\n",
      "train loss:0.1033449509357246\n",
      "train loss:0.08472868834215327\n",
      "train loss:0.11255862536997326\n",
      "train loss:0.07500036210028728\n",
      "train loss:0.16253881972692583\n",
      "train loss:0.10039247988114376\n",
      "train loss:0.12898776768714768\n",
      "train loss:0.11066804962875704\n",
      "train loss:0.08355746692618321\n",
      "train loss:0.08167358618433768\n",
      "train loss:0.11906163444353186\n",
      "train loss:0.10896092439727559\n",
      "train loss:0.12042011300535461\n",
      "train loss:0.11022686259385249\n",
      "train loss:0.16324656134575669\n",
      "train loss:0.07609443885881287\n",
      "train loss:0.2071537019739741\n",
      "train loss:0.09691616519366336\n",
      "train loss:0.10181971447624974\n",
      "train loss:0.14347620254695923\n",
      "train loss:0.12024196142655938\n",
      "train loss:0.08443704747960931\n",
      "train loss:0.059187966258840846\n",
      "train loss:0.06136483552522445\n",
      "train loss:0.15107604963620014\n",
      "train loss:0.14427179213418273\n",
      "train loss:0.14744122936715492\n",
      "train loss:0.1818589308939725\n",
      "train loss:0.07516169421984156\n",
      "train loss:0.046040684181121276\n",
      "train loss:0.07625173842579634\n",
      "train loss:0.11638188821012234\n",
      "train loss:0.11079665444872534\n",
      "train loss:0.0949022104768988\n",
      "train loss:0.07476446064003286\n",
      "train loss:0.13906807736086976\n",
      "train loss:0.0572263187716866\n",
      "train loss:0.0775935102998857\n",
      "train loss:0.1921168348460478\n",
      "train loss:0.05818777676124902\n",
      "train loss:0.11902786193765456\n",
      "train loss:0.11437261900572192\n",
      "train loss:0.10169390893271504\n",
      "train loss:0.12986825319630968\n",
      "train loss:0.20073557196445482\n",
      "train loss:0.1511283774234714\n",
      "train loss:0.048654760399617365\n",
      "train loss:0.14021200706632034\n",
      "train loss:0.079581705329982\n",
      "train loss:0.09188930337334522\n",
      "train loss:0.026116164854970952\n",
      "train loss:0.13372788536395144\n",
      "train loss:0.05531920092232536\n",
      "train loss:0.14602761964668215\n",
      "train loss:0.07279138310188714\n",
      "train loss:0.14762629524083626\n",
      "train loss:0.12772121734480257\n",
      "train loss:0.12367050298153494\n",
      "train loss:0.11104849376242969\n",
      "train loss:0.08591985035852588\n",
      "train loss:0.08711324802977437\n",
      "train loss:0.07655872892112633\n",
      "train loss:0.12944066497111878\n",
      "train loss:0.19628133686797508\n",
      "train loss:0.07480285065142128\n",
      "train loss:0.08701239028191145\n",
      "train loss:0.26261309582460224\n",
      "train loss:0.09543840862221797\n",
      "train loss:0.14111884571747313\n",
      "train loss:0.01812534117879455\n",
      "train loss:0.05605627532317872\n",
      "train loss:0.1201694612208502\n",
      "train loss:0.051033873241049\n",
      "train loss:0.07560526399743556\n",
      "train loss:0.13511098949044079\n",
      "train loss:0.10650974702588484\n",
      "train loss:0.18068850160924954\n",
      "train loss:0.06310421939355275\n",
      "train loss:0.06887220050288244\n",
      "train loss:0.06242153443339722\n",
      "train loss:0.0889112484231917\n",
      "train loss:0.06338443021440804\n",
      "train loss:0.07159203747837303\n",
      "train loss:0.08760294262676009\n",
      "train loss:0.061378753285897626\n",
      "train loss:0.16340116898467028\n",
      "train loss:0.05054399642298473\n",
      "train loss:0.125021316935489\n",
      "train loss:0.21422070284956388\n",
      "train loss:0.23183917350835223\n",
      "train loss:0.09233628317446\n",
      "train loss:0.0830719312265474\n",
      "train loss:0.048047813896614826\n",
      "train loss:0.1322189766166535\n",
      "train loss:0.04614414323614636\n",
      "train loss:0.1240542790016304\n",
      "train loss:0.10402143363981027\n",
      "train loss:0.06004897585926603\n",
      "train loss:0.12899539448646716\n",
      "train loss:0.10123260176521015\n",
      "train loss:0.0663334444097051\n",
      "train loss:0.10176189620170044\n",
      "train loss:0.11052833214090672\n",
      "train loss:0.06266778355799223\n",
      "train loss:0.09370498783356988\n",
      "train loss:0.05791353004617861\n",
      "train loss:0.028746071616322477\n",
      "train loss:0.10288807088717376\n",
      "train loss:0.07057599100134829\n",
      "train loss:0.08400589064573419\n",
      "train loss:0.0757975797933162\n",
      "train loss:0.07100604670223688\n",
      "train loss:0.09115393470276924\n",
      "train loss:0.09675475617397117\n",
      "train loss:0.03853596901260419\n",
      "train loss:0.13177761822630715\n",
      "train loss:0.1177135130395349\n",
      "train loss:0.04020114227760927\n",
      "train loss:0.10219694553408433\n",
      "train loss:0.1163688875715816\n",
      "train loss:0.15210485329254814\n",
      "train loss:0.0721270877610918\n",
      "train loss:0.14631125065649642\n",
      "train loss:0.12661445182163444\n",
      "train loss:0.12731496034196638\n",
      "train loss:0.0746873653802679\n",
      "train loss:0.06858840243987546\n",
      "train loss:0.15490167228458326\n",
      "train loss:0.05511679309648293\n",
      "train loss:0.10194392833008013\n",
      "train loss:0.1255504383967157\n",
      "train loss:0.09596823379626784\n",
      "train loss:0.08628200083147534\n",
      "train loss:0.05010075755539976\n",
      "train loss:0.1294520145598883\n",
      "train loss:0.13873763660353539\n",
      "train loss:0.1413021073993618\n",
      "train loss:0.14087514119684952\n",
      "train loss:0.05770340528151729\n",
      "train loss:0.08926370440586659\n",
      "train loss:0.11158180501117884\n",
      "train loss:0.040801996686860915\n",
      "train loss:0.08491453417958518\n",
      "train loss:0.07549216982045431\n",
      "train loss:0.06742117160428253\n",
      "train loss:0.2056576332984337\n",
      "train loss:0.165512122062209\n",
      "train loss:0.08908639042466988\n",
      "train loss:0.029528346168991547\n",
      "train loss:0.054921054393620175\n",
      "train loss:0.13615911727015256\n",
      "train loss:0.10321410379549677\n",
      "train loss:0.05902090605084404\n",
      "train loss:0.08661909630008881\n",
      "train loss:0.12901390509839655\n",
      "train loss:0.029600122168338044\n",
      "train loss:0.10973999310336703\n",
      "train loss:0.03795198042589846\n",
      "train loss:0.08972997628720376\n",
      "train loss:0.050595887226985464\n",
      "train loss:0.08584298174475644\n",
      "train loss:0.14524600327710468\n",
      "train loss:0.03756337721654057\n",
      "train loss:0.05254090981129461\n",
      "train loss:0.10895245699330877\n",
      "train loss:0.04874522859055745\n",
      "train loss:0.0836036450195578\n",
      "train loss:0.1094755576683833\n",
      "train loss:0.09734600800456061\n",
      "train loss:0.10545544683710491\n",
      "train loss:0.048024383691567946\n",
      "train loss:0.2806537616462472\n",
      "train loss:0.10121268684950493\n",
      "train loss:0.1032779726404767\n",
      "train loss:0.11046684174565921\n",
      "train loss:0.058174318021633156\n",
      "train loss:0.12028874310264777\n",
      "train loss:0.1015065954491098\n",
      "train loss:0.11796096359577238\n",
      "train loss:0.07455698312216341\n",
      "train loss:0.08035994727647763\n",
      "train loss:0.19502470266648145\n",
      "train loss:0.07144281563061082\n",
      "train loss:0.06934271965610003\n",
      "train loss:0.20491843260935996\n",
      "train loss:0.16620963885578757\n",
      "train loss:0.18114922469797531\n",
      "train loss:0.09270207081924052\n",
      "train loss:0.1291962097136865\n",
      "train loss:0.07906128185621662\n",
      "train loss:0.12166046417509435\n",
      "train loss:0.06851646394737224\n",
      "train loss:0.07935050934156715\n",
      "train loss:0.04567528561159784\n",
      "train loss:0.14094191197213127\n",
      "train loss:0.1668226271955663\n",
      "train loss:0.0656199231288667\n",
      "train loss:0.11582180591140158\n",
      "train loss:0.1273622155417239\n",
      "train loss:0.09823585259460478\n",
      "train loss:0.1486968357884205\n",
      "train loss:0.057055505821774884\n",
      "train loss:0.07677049458362899\n",
      "train loss:0.1561739973152979\n",
      "train loss:0.07035441606597231\n",
      "train loss:0.05034902291803875\n",
      "train loss:0.07894459120993885\n",
      "train loss:0.06747945884247074\n",
      "train loss:0.15698882428261218\n",
      "train loss:0.06455279322445145\n",
      "train loss:0.1384432515289765\n",
      "train loss:0.10443670072907443\n",
      "train loss:0.24582078516920616\n",
      "train loss:0.07270409188390116\n",
      "train loss:0.10351745837080387\n",
      "train loss:0.08773059438348614\n",
      "train loss:0.06219648673140324\n",
      "train loss:0.27709110788915486\n",
      "train loss:0.08382167353195837\n",
      "train loss:0.021836659835815227\n",
      "train loss:0.06548806289342447\n",
      "train loss:0.07787018160591219\n",
      "train loss:0.14803964187564586\n",
      "train loss:0.10094882089873272\n",
      "train loss:0.16881528227330975\n",
      "train loss:0.09681976911615957\n",
      "train loss:0.0981745219164465\n",
      "train loss:0.06627874132099308\n",
      "train loss:0.06913909868188793\n",
      "train loss:0.14762967165919788\n",
      "train loss:0.10334545820076332\n",
      "train loss:0.17644844136947063\n",
      "train loss:0.11474101975377911\n",
      "train loss:0.10447585212775562\n",
      "train loss:0.06506098817668486\n",
      "train loss:0.04144950554155006\n",
      "train loss:0.1552497697476894\n",
      "train loss:0.10128921989800407\n",
      "train loss:0.06063303309249375\n",
      "train loss:0.06034247408019271\n",
      "train loss:0.08257302151431477\n",
      "train loss:0.0646463150489023\n",
      "train loss:0.10561307611576405\n",
      "train loss:0.09727740786617468\n",
      "train loss:0.04816609052469035\n",
      "train loss:0.056655284619984786\n",
      "train loss:0.10018865009331492\n",
      "train loss:0.05053582316942295\n",
      "train loss:0.05972414549012536\n",
      "train loss:0.1740825303505838\n",
      "train loss:0.06324081282658688\n",
      "train loss:0.04810657123138923\n",
      "train loss:0.04275291165295131\n",
      "train loss:0.0746358499005568\n",
      "train loss:0.10684736904322127\n",
      "train loss:0.09425538571224408\n",
      "train loss:0.06663261551380711\n",
      "train loss:0.07056962329868219\n",
      "train loss:0.061389421209236954\n",
      "train loss:0.07364931387171005\n",
      "train loss:0.060290118704784394\n",
      "train loss:0.06115937573190364\n",
      "train loss:0.07624187425112652\n",
      "train loss:0.049628604322272965\n",
      "train loss:0.07083176791709316\n",
      "train loss:0.035867625075844706\n",
      "train loss:0.14866647560122487\n",
      "train loss:0.13221518532440465\n",
      "train loss:0.15448381405346434\n",
      "train loss:0.05017967197174981\n",
      "train loss:0.22216595877128853\n",
      "train loss:0.04248976183327879\n",
      "train loss:0.0633354589924012\n",
      "train loss:0.05973733953389284\n",
      "train loss:0.1777445943704307\n",
      "train loss:0.05156295830913769\n",
      "train loss:0.05389515882787524\n",
      "train loss:0.09217766472490403\n",
      "train loss:0.10293513256913442\n",
      "train loss:0.09800246083459452\n",
      "train loss:0.14565987164418626\n",
      "train loss:0.08410557001360106\n",
      "train loss:0.09259501470914583\n",
      "train loss:0.06280655523722918\n",
      "train loss:0.047118265772335584\n",
      "train loss:0.15841195125330768\n",
      "train loss:0.06819296059370925\n",
      "train loss:0.08962595604651387\n",
      "train loss:0.07368632336439902\n",
      "train loss:0.10874664717791456\n",
      "train loss:0.17210527587006255\n",
      "train loss:0.10964672151702053\n",
      "train loss:0.040762247585420124\n",
      "train loss:0.047589775434786145\n",
      "train loss:0.04855686702776973\n",
      "train loss:0.05513948471504165\n",
      "train loss:0.08970760652268325\n",
      "train loss:0.08115003699850522\n",
      "train loss:0.06423648243977481\n",
      "train loss:0.07312088398882956\n",
      "train loss:0.06274184859857045\n",
      "train loss:0.07996717491297929\n",
      "train loss:0.12404933693844296\n",
      "train loss:0.07989106131803102\n",
      "train loss:0.06636653645387793\n",
      "train loss:0.0970470529084839\n",
      "train loss:0.12107171039474615\n",
      "train loss:0.06483888880091988\n",
      "train loss:0.06362171579789411\n",
      "train loss:0.07600813124063537\n",
      "train loss:0.07007598766849572\n",
      "train loss:0.05761439341111667\n",
      "train loss:0.04515838000973714\n",
      "train loss:0.03743166507593564\n",
      "train loss:0.05791431384860256\n",
      "train loss:0.06123022452013675\n",
      "train loss:0.06690372792072558\n",
      "train loss:0.11019661334620469\n",
      "train loss:0.11003375594176809\n",
      "train loss:0.028476443580638527\n",
      "train loss:0.06562447731781135\n",
      "train loss:0.20676062350448518\n",
      "train loss:0.05265681731465041\n",
      "train loss:0.05276579562566331\n",
      "train loss:0.06420917695113441\n",
      "train loss:0.06700141207027781\n",
      "train loss:0.08970530450844333\n",
      "train loss:0.13830452370333912\n",
      "train loss:0.08689636031476444\n",
      "train loss:0.05133343551596461\n",
      "train loss:0.057587862193672124\n",
      "train loss:0.154967937640649\n",
      "train loss:0.02544507374355852\n",
      "train loss:0.1032956523195917\n",
      "train loss:0.09259502682408721\n",
      "train loss:0.07344638040790945\n",
      "train loss:0.05974301427222429\n",
      "train loss:0.07690003559202807\n",
      "train loss:0.03564019989777118\n",
      "train loss:0.08882744089332024\n",
      "train loss:0.06770741646699623\n",
      "train loss:0.027668506068195254\n",
      "train loss:0.0796303157546119\n",
      "train loss:0.08338887965640865\n",
      "train loss:0.06991068639241466\n",
      "train loss:0.08102385264796905\n",
      "train loss:0.08464038713190668\n",
      "train loss:0.11917722761737087\n",
      "train loss:0.08818096498970479\n",
      "train loss:0.03242142379551807\n",
      "train loss:0.04625788813643532\n",
      "train loss:0.16091274011664164\n",
      "train loss:0.15352218286881059\n",
      "train loss:0.1100596511592405\n",
      "train loss:0.06654327915654559\n",
      "train loss:0.07074195222954702\n",
      "train loss:0.08042874139525447\n",
      "train loss:0.051613261760327316\n",
      "train loss:0.038919059071751284\n",
      "train loss:0.06249301736265767\n",
      "train loss:0.07582259445698289\n",
      "train loss:0.09127798095915261\n",
      "train loss:0.10813592539686903\n",
      "train loss:0.08887115799366997\n",
      "train loss:0.07767745381626806\n",
      "train loss:0.1354216671067337\n",
      "train loss:0.06061708929723807\n",
      "train loss:0.052429505888561614\n",
      "train loss:0.04016041306062916\n",
      "train loss:0.10573111428181896\n",
      "train loss:0.035097586768122725\n",
      "train loss:0.046254092090093665\n",
      "train loss:0.09319166159465743\n",
      "train loss:0.0568371951373596\n",
      "train loss:0.04248738461028969\n",
      "train loss:0.10003951213371598\n",
      "train loss:0.03626755408682641\n",
      "train loss:0.08111230422089147\n",
      "train loss:0.09768721093459533\n",
      "train loss:0.0911292738746305\n",
      "train loss:0.02705257112908637\n",
      "train loss:0.08446252431849352\n",
      "train loss:0.06923292699445288\n",
      "train loss:0.034292447925252476\n",
      "train loss:0.03218062654033794\n",
      "train loss:0.054026527075536006\n",
      "train loss:0.06979098816704556\n",
      "train loss:0.06326503323270445\n",
      "train loss:0.18533884672664858\n",
      "train loss:0.03802020633625252\n",
      "train loss:0.12805462548013596\n",
      "train loss:0.054307949605128976\n",
      "train loss:0.03388268938824654\n",
      "train loss:0.09806645333508512\n",
      "train loss:0.05180665357300933\n",
      "train loss:0.05928449512975717\n",
      "train loss:0.06641015526193483\n",
      "train loss:0.0609520844863331\n",
      "train loss:0.10363059715583763\n",
      "train loss:0.06043013190542133\n",
      "train loss:0.08040478606817036\n",
      "train loss:0.06714235201109094\n",
      "train loss:0.06144348606016244\n",
      "train loss:0.09446342861416414\n",
      "train loss:0.07290372911655145\n",
      "train loss:0.050638689402365884\n",
      "train loss:0.057563913244041254\n",
      "train loss:0.11202909665814031\n",
      "train loss:0.12818938853792233\n",
      "train loss:0.0856502877185871\n",
      "train loss:0.06842303246467481\n",
      "train loss:0.0555040511129283\n",
      "train loss:0.1344845056771892\n",
      "train loss:0.014481066183162533\n",
      "train loss:0.11926354255266557\n",
      "train loss:0.051029289815171276\n",
      "train loss:0.18604069740833218\n",
      "train loss:0.057649718844594015\n",
      "train loss:0.05124394099722739\n",
      "train loss:0.043222556363067974\n",
      "train loss:0.15537967156642796\n",
      "train loss:0.029099562713712595\n",
      "train loss:0.0759915368482615\n",
      "train loss:0.03615615297420045\n",
      "train loss:0.07602056574570994\n",
      "train loss:0.15561818629457538\n",
      "train loss:0.034739785298665775\n",
      "train loss:0.04132293014298861\n",
      "train loss:0.04571107505630084\n",
      "train loss:0.048400387975341186\n",
      "train loss:0.10725230882894193\n",
      "train loss:0.023505436242348653\n",
      "train loss:0.12527019599335432\n",
      "train loss:0.05665836711178293\n",
      "train loss:0.03307184954625554\n",
      "train loss:0.07382042339247531\n",
      "train loss:0.034039414525192664\n",
      "train loss:0.06660649714950488\n",
      "train loss:0.09047608570912764\n",
      "train loss:0.03197242619696669\n",
      "train loss:0.06904968178458237\n",
      "train loss:0.053149739415014\n",
      "train loss:0.09694472567076964\n",
      "train loss:0.037016902407014045\n",
      "train loss:0.08400203214240996\n",
      "train loss:0.07910517954321304\n",
      "train loss:0.040743387740425366\n",
      "train loss:0.04430524468630653\n",
      "train loss:0.12058969096883089\n",
      "train loss:0.0640738662217093\n",
      "train loss:0.0504604546170759\n",
      "train loss:0.019858563247498198\n",
      "train loss:0.06388934798744532\n",
      "train loss:0.05452566304058507\n",
      "train loss:0.059229087475566475\n",
      "train loss:0.12394708786624901\n",
      "train loss:0.09710860261231949\n",
      "train loss:0.060904467952219876\n",
      "train loss:0.0405731597351192\n",
      "train loss:0.1205225784124187\n",
      "train loss:0.08143159331701087\n",
      "train loss:0.07185939368168723\n",
      "train loss:0.05653025099815659\n",
      "train loss:0.08839753356832596\n",
      "train loss:0.08911034538460721\n",
      "train loss:0.1336334959279431\n",
      "train loss:0.0927005551182701\n",
      "train loss:0.04452709602428093\n",
      "train loss:0.10509872430927455\n",
      "train loss:0.06699491337112665\n",
      "train loss:0.03638995780563577\n",
      "train loss:0.051996198353023076\n",
      "train loss:0.08381441949294598\n",
      "train loss:0.06055605369572139\n",
      "train loss:0.08803873613609085\n",
      "train loss:0.08614564181965471\n",
      "train loss:0.07037094048218621\n",
      "train loss:0.08435672762513935\n",
      "train loss:0.11619757521054193\n",
      "train loss:0.05312217573307223\n",
      "train loss:0.03974152299228858\n",
      "train loss:0.12207016502382623\n",
      "train loss:0.11191027605384746\n",
      "train loss:0.21127480601555992\n",
      "train loss:0.05366116912498324\n",
      "train loss:0.11885942632199085\n",
      "train loss:0.06833365350045338\n",
      "train loss:0.026351652594581044\n",
      "train loss:0.07465854028876116\n",
      "train loss:0.13157656775958518\n",
      "train loss:0.2060150837365487\n",
      "train loss:0.1770354131643314\n",
      "train loss:0.09203354572682214\n",
      "train loss:0.06672371338738574\n",
      "train loss:0.16946490699138356\n",
      "train loss:0.05118155806023537\n",
      "train loss:0.12041306915994257\n",
      "train loss:0.07636543894906922\n",
      "train loss:0.10202615914609056\n",
      "train loss:0.10433750316962591\n",
      "train loss:0.04561761544035559\n",
      "train loss:0.0509662544836069\n",
      "train loss:0.06072770945682496\n",
      "train loss:0.1664368572390809\n",
      "train loss:0.08906007771323778\n",
      "train loss:0.12152269498667553\n",
      "train loss:0.046621182117407865\n",
      "train loss:0.05746876268506934\n",
      "train loss:0.08435031695903016\n",
      "train loss:0.10850822816010368\n",
      "train loss:0.07052141363912272\n",
      "train loss:0.051690153294677084\n",
      "train loss:0.05660471024320654\n",
      "train loss:0.11188834246349083\n",
      "train loss:0.07514632624817008\n",
      "train loss:0.13767910094818933\n",
      "train loss:0.09316643927228385\n",
      "train loss:0.09617069688263692\n",
      "train loss:0.07375487763417013\n",
      "train loss:0.055215443943078556\n",
      "train loss:0.04457974675411708\n",
      "train loss:0.0908997180335355\n",
      "train loss:0.07279122570117984\n",
      "train loss:0.10774793206934588\n",
      "train loss:0.039428302052081275\n",
      "train loss:0.05174714274610568\n",
      "train loss:0.05379128507401662\n",
      "train loss:0.15311612883688336\n",
      "train loss:0.036486877477244925\n",
      "train loss:0.07001312574807662\n",
      "train loss:0.0662862180105437\n",
      "train loss:0.017774351794238082\n",
      "train loss:0.03816274088263989\n",
      "train loss:0.08767822849927212\n",
      "train loss:0.03915066339442877\n",
      "train loss:0.07037522463514413\n",
      "train loss:0.09165717496475143\n",
      "train loss:0.06869989309644987\n",
      "train loss:0.058505630115126904\n",
      "train loss:0.087165723478766\n",
      "train loss:0.07736731584395719\n",
      "train loss:0.09647993192293025\n",
      "train loss:0.11014801864010443\n",
      "train loss:0.048748076709725384\n",
      "train loss:0.09776463608988765\n",
      "train loss:0.11526401862705846\n",
      "train loss:0.04742691310817781\n",
      "train loss:0.08756889359323736\n",
      "train loss:0.08560322563274914\n",
      "=== epoch:3, train acc:0.974, test acc:0.973 ===\n",
      "train loss:0.05078399050903801\n",
      "train loss:0.030629492684138674\n",
      "train loss:0.06273162264033293\n",
      "train loss:0.04000723373249614\n",
      "train loss:0.12755223625619727\n",
      "train loss:0.05711030127484774\n",
      "train loss:0.08855934703666091\n",
      "train loss:0.056031633010274466\n",
      "train loss:0.03858149080925368\n",
      "train loss:0.0413352675482425\n",
      "train loss:0.07875863731723506\n",
      "train loss:0.1448847188757857\n",
      "train loss:0.02762748683194965\n",
      "train loss:0.07597757779690945\n",
      "train loss:0.13800973165418776\n",
      "train loss:0.022763769640990743\n",
      "train loss:0.08207119306905357\n",
      "train loss:0.06647675670222722\n",
      "train loss:0.12599721923014476\n",
      "train loss:0.05351506079000162\n",
      "train loss:0.025375404447244496\n",
      "train loss:0.1162305408333075\n",
      "train loss:0.04071208489894281\n",
      "train loss:0.10610847768816394\n",
      "train loss:0.06577606680736076\n",
      "train loss:0.19927064426998367\n",
      "train loss:0.04609579929212659\n",
      "train loss:0.06063831813807957\n",
      "train loss:0.10353755251581782\n",
      "train loss:0.06316209826773661\n",
      "train loss:0.10017780040491138\n",
      "train loss:0.059468762327707314\n",
      "train loss:0.11400247676598152\n",
      "train loss:0.07488773279057437\n",
      "train loss:0.1751146435542592\n",
      "train loss:0.07766037485668895\n",
      "train loss:0.11590101677850098\n",
      "train loss:0.13405531632882117\n",
      "train loss:0.06183552851276297\n",
      "train loss:0.08791946626278623\n",
      "train loss:0.04819281199101716\n",
      "train loss:0.055850646762916095\n",
      "train loss:0.03922065359046586\n",
      "train loss:0.0660616419065563\n",
      "train loss:0.09511619288204126\n",
      "train loss:0.0440318760239739\n",
      "train loss:0.025479867910823798\n",
      "train loss:0.08409603752244689\n",
      "train loss:0.0613383108007347\n",
      "train loss:0.02221814757152597\n",
      "train loss:0.13806507713489627\n",
      "train loss:0.047371571517287415\n",
      "train loss:0.1158883536283189\n",
      "train loss:0.05617017193367436\n",
      "train loss:0.08057237589695435\n",
      "train loss:0.11652763797589595\n",
      "train loss:0.023562709139889178\n",
      "train loss:0.028596191424608265\n",
      "train loss:0.09465310364489651\n",
      "train loss:0.13338314991706385\n",
      "train loss:0.05165732213705842\n",
      "train loss:0.09583880377331308\n",
      "train loss:0.07328102468569093\n",
      "train loss:0.045805425326224614\n",
      "train loss:0.03701458363438903\n",
      "train loss:0.019254007525036122\n",
      "train loss:0.04350819394056266\n",
      "train loss:0.13988308348900927\n",
      "train loss:0.050029574781756354\n",
      "train loss:0.058441312393395534\n",
      "train loss:0.1876492621703823\n",
      "train loss:0.12647501319753604\n",
      "train loss:0.03407969452907357\n",
      "train loss:0.044134330021816354\n",
      "train loss:0.06118478124317264\n",
      "train loss:0.07945147857640734\n",
      "train loss:0.035771926162036796\n",
      "train loss:0.06090276593843594\n",
      "train loss:0.09538585413673902\n",
      "train loss:0.13250042054275524\n",
      "train loss:0.0563730055114781\n",
      "train loss:0.05516700531860204\n",
      "train loss:0.08070208673448546\n",
      "train loss:0.04640995468377814\n",
      "train loss:0.13478376978905088\n",
      "train loss:0.021395116312978676\n",
      "train loss:0.048130548216599525\n",
      "train loss:0.040960450359439654\n",
      "train loss:0.034336518014247856\n",
      "train loss:0.06245585234142689\n",
      "train loss:0.07896689489189121\n",
      "train loss:0.14254781920115944\n",
      "train loss:0.03943624096473851\n",
      "train loss:0.018101135355007444\n",
      "train loss:0.11084003638786619\n",
      "train loss:0.033134338092407144\n",
      "train loss:0.05660444980630225\n",
      "train loss:0.07862391671197322\n",
      "train loss:0.058258936276125606\n",
      "train loss:0.08077659110068318\n",
      "train loss:0.047987767679848276\n",
      "train loss:0.01837194468418522\n",
      "train loss:0.0207472169833727\n",
      "train loss:0.07979834531467463\n",
      "train loss:0.0633842454799212\n",
      "train loss:0.040914375545194774\n",
      "train loss:0.04505387165026587\n",
      "train loss:0.0841747364515849\n",
      "train loss:0.06255591075269282\n",
      "train loss:0.09786982220228543\n",
      "train loss:0.05770937454448934\n",
      "train loss:0.07832534607851623\n",
      "train loss:0.014094490434241087\n",
      "train loss:0.04236398336846169\n",
      "train loss:0.31074517940441365\n",
      "train loss:0.08304994066447575\n",
      "train loss:0.038409423496619695\n",
      "train loss:0.08131001714476829\n",
      "train loss:0.0648267296784685\n",
      "train loss:0.06516782590300446\n",
      "train loss:0.019473502488257573\n",
      "train loss:0.03955447205282632\n",
      "train loss:0.019216404365875333\n",
      "train loss:0.06744408798006363\n",
      "train loss:0.08679228481151084\n",
      "train loss:0.05161773278888633\n",
      "train loss:0.12005065651075329\n",
      "train loss:0.04394715261195846\n",
      "train loss:0.03241111075173578\n",
      "train loss:0.05040617062037284\n",
      "train loss:0.02799386870139378\n",
      "train loss:0.021106066888757987\n",
      "train loss:0.047027102077023146\n",
      "train loss:0.07302332041665713\n",
      "train loss:0.041182484700649516\n",
      "train loss:0.06461923779838835\n",
      "train loss:0.09943150453694823\n",
      "train loss:0.06919757936548508\n",
      "train loss:0.08730139794069515\n",
      "train loss:0.04020676071779639\n",
      "train loss:0.05975417050037249\n",
      "train loss:0.09038083815105959\n",
      "train loss:0.03035727338774901\n",
      "train loss:0.09569809394727175\n",
      "train loss:0.06922420440095602\n",
      "train loss:0.03126135312318096\n",
      "train loss:0.048843746036466865\n",
      "train loss:0.06922560521042062\n",
      "train loss:0.08021189259372359\n",
      "train loss:0.05084665364305157\n",
      "train loss:0.10323005079297103\n",
      "train loss:0.11889081306008549\n",
      "train loss:0.11527991416320628\n",
      "train loss:0.057870129751471044\n",
      "train loss:0.16992978866747308\n",
      "train loss:0.0653053104163365\n",
      "train loss:0.05565915754150379\n",
      "train loss:0.12210461942890857\n",
      "train loss:0.17159490045854006\n",
      "train loss:0.11391560883488633\n",
      "train loss:0.05075075085646968\n",
      "train loss:0.10011684242716992\n",
      "train loss:0.06701204596631101\n",
      "train loss:0.06627940937665805\n",
      "train loss:0.06706160945837457\n",
      "train loss:0.05079108878878863\n",
      "train loss:0.0191540132224306\n",
      "train loss:0.08324950294610982\n",
      "train loss:0.11727905620943281\n",
      "train loss:0.038863171427439405\n",
      "train loss:0.10810412767739858\n",
      "train loss:0.08101831305017233\n",
      "train loss:0.07472873228137318\n",
      "train loss:0.0686526647409871\n",
      "train loss:0.10567086694290136\n",
      "train loss:0.060346426920538446\n",
      "train loss:0.07164049522970839\n",
      "train loss:0.0814609216966428\n",
      "train loss:0.178273756009536\n",
      "train loss:0.05295930650612126\n",
      "train loss:0.1526514264729623\n",
      "train loss:0.019163617927774882\n",
      "train loss:0.028883514034036096\n",
      "train loss:0.12570857437266553\n",
      "train loss:0.0836818708306692\n",
      "train loss:0.05270301653888906\n",
      "train loss:0.045825794110657896\n",
      "train loss:0.06569249340941455\n",
      "train loss:0.050268901938417924\n",
      "train loss:0.035330285159462835\n",
      "train loss:0.0277047974355029\n",
      "train loss:0.043584496763749155\n",
      "train loss:0.028440731480800827\n",
      "train loss:0.040781812351012456\n",
      "train loss:0.02477838550507764\n",
      "train loss:0.03993864923099972\n",
      "train loss:0.0731240414706592\n",
      "train loss:0.1002366130318901\n",
      "train loss:0.01628610353867147\n",
      "train loss:0.0671909439894845\n",
      "train loss:0.03314692279434135\n",
      "train loss:0.07933200283372083\n",
      "train loss:0.040437139680809746\n",
      "train loss:0.12839988600237975\n",
      "train loss:0.029127442737892983\n",
      "train loss:0.09394465204723859\n",
      "train loss:0.052653548984887735\n",
      "train loss:0.1345293424774826\n",
      "train loss:0.07523966069201041\n",
      "train loss:0.05369435404508316\n",
      "train loss:0.07086286261327873\n",
      "train loss:0.012720554450458131\n",
      "train loss:0.01938785053156788\n",
      "train loss:0.06557042378356451\n",
      "train loss:0.047025367760294666\n",
      "train loss:0.07018331495815561\n",
      "train loss:0.06992177037214513\n",
      "train loss:0.027858065139894216\n",
      "train loss:0.09593361540660127\n",
      "train loss:0.05223732886255921\n",
      "train loss:0.07943591429907836\n",
      "train loss:0.02476249326817324\n",
      "train loss:0.0450099315711805\n",
      "train loss:0.08339347563485962\n",
      "train loss:0.09654376280053867\n",
      "train loss:0.026966319212061794\n",
      "train loss:0.029175876990319618\n",
      "train loss:0.04654027769338954\n",
      "train loss:0.14572431462767077\n",
      "train loss:0.03155380135211584\n",
      "train loss:0.09366853486059\n",
      "train loss:0.08442649558057388\n",
      "train loss:0.04866071043292766\n",
      "train loss:0.06672389097592059\n",
      "train loss:0.056585580449300156\n",
      "train loss:0.059762589091743194\n",
      "train loss:0.09368706025814405\n",
      "train loss:0.024727638370542194\n",
      "train loss:0.07013303714645713\n",
      "train loss:0.1001235300447983\n",
      "train loss:0.07981396670525577\n",
      "train loss:0.08981465208197673\n",
      "train loss:0.037823078665487946\n",
      "train loss:0.03300866066227972\n",
      "train loss:0.022241595885771803\n",
      "train loss:0.0419977316012365\n",
      "train loss:0.058398210467227755\n",
      "train loss:0.0382625045004557\n",
      "train loss:0.06638961582297265\n",
      "train loss:0.11567678957641064\n",
      "train loss:0.0291786595165531\n",
      "train loss:0.028544493763111724\n",
      "train loss:0.1744740638243912\n",
      "train loss:0.060590138958188204\n",
      "train loss:0.07054099961094316\n",
      "train loss:0.07028821360116132\n",
      "train loss:0.0492226307963221\n",
      "train loss:0.043846399386556316\n",
      "train loss:0.05119460684408238\n",
      "train loss:0.05679816129292926\n",
      "train loss:0.07906977945532495\n",
      "train loss:0.03884361041318235\n",
      "train loss:0.03472744768870137\n",
      "train loss:0.025712930094326486\n",
      "train loss:0.04698927713284446\n",
      "train loss:0.04750359219073864\n",
      "train loss:0.05300777147864015\n",
      "train loss:0.04441934842423047\n",
      "train loss:0.08698126493377016\n",
      "train loss:0.032009875187299\n",
      "train loss:0.018060780167402906\n",
      "train loss:0.04664009937279328\n",
      "train loss:0.042577978195752035\n",
      "train loss:0.06091969791645055\n",
      "train loss:0.015068523567949403\n",
      "train loss:0.0642110653523305\n",
      "train loss:0.015354460822338233\n",
      "train loss:0.07468281625960418\n",
      "train loss:0.04138967959516464\n",
      "train loss:0.019891195630481293\n",
      "train loss:0.029218982229341172\n",
      "train loss:0.06960083343464812\n",
      "train loss:0.08738819249874608\n",
      "train loss:0.013442018552310473\n",
      "train loss:0.09543275752301533\n",
      "train loss:0.028740202051289812\n",
      "train loss:0.025385476489454894\n",
      "train loss:0.06150971231173545\n",
      "train loss:0.0418726832908118\n",
      "train loss:0.0673605654160471\n",
      "train loss:0.04252807913745823\n",
      "train loss:0.11951541688706017\n",
      "train loss:0.021669010418008377\n",
      "train loss:0.06605197784052814\n",
      "train loss:0.04487765350283742\n",
      "train loss:0.10561621839463484\n",
      "train loss:0.043639661797672584\n",
      "train loss:0.03411578225736712\n",
      "train loss:0.008568199396265568\n",
      "train loss:0.02801730476003414\n",
      "train loss:0.027983640830205426\n",
      "train loss:0.06122465492712016\n",
      "train loss:0.025054221296516414\n",
      "train loss:0.016450777558677587\n",
      "train loss:0.11553793782292782\n",
      "train loss:0.039556539920740194\n",
      "train loss:0.0628450282740311\n",
      "train loss:0.03285088504103809\n",
      "train loss:0.06865745406331547\n",
      "train loss:0.06079135770288699\n",
      "train loss:0.06414562489493254\n",
      "train loss:0.02361329261034872\n",
      "train loss:0.05241357595346569\n",
      "train loss:0.05006649208619658\n",
      "train loss:0.029024633930240976\n",
      "train loss:0.06078245146577401\n",
      "train loss:0.05673314989201099\n",
      "train loss:0.04577233659740362\n",
      "train loss:0.019452162900556066\n",
      "train loss:0.03550772705321806\n",
      "train loss:0.09468440010964221\n",
      "train loss:0.023192708015521965\n",
      "train loss:0.09197992738369909\n",
      "train loss:0.03797555985100694\n",
      "train loss:0.0559277364396793\n",
      "train loss:0.04352667311481539\n",
      "train loss:0.08179563348829705\n",
      "train loss:0.048369914892781744\n",
      "train loss:0.05949020719977075\n",
      "train loss:0.15551466376256418\n",
      "train loss:0.05886358992480775\n",
      "train loss:0.1460390992584161\n",
      "train loss:0.04236190149166866\n",
      "train loss:0.07585677338789448\n",
      "train loss:0.07796880634593967\n",
      "train loss:0.09434046690589583\n",
      "train loss:0.04721442840224328\n",
      "train loss:0.10065282215850584\n",
      "train loss:0.09551316029379837\n",
      "train loss:0.044106013597584114\n",
      "train loss:0.06037591407692698\n",
      "train loss:0.04982805108899718\n",
      "train loss:0.08863979559858008\n",
      "train loss:0.03269984615078442\n",
      "train loss:0.052132964532074874\n",
      "train loss:0.011640190216721625\n",
      "train loss:0.030233301206793274\n",
      "train loss:0.025877031623654827\n",
      "train loss:0.024019023170661295\n",
      "train loss:0.03146960017098309\n",
      "train loss:0.08335995952088596\n",
      "train loss:0.07302569773070536\n",
      "train loss:0.05001486698824576\n",
      "train loss:0.02177423045138375\n",
      "train loss:0.022635764270248097\n",
      "train loss:0.15121839250389305\n",
      "train loss:0.009747597871446759\n",
      "train loss:0.024842730643070566\n",
      "train loss:0.020189416526725935\n",
      "train loss:0.10087278647870022\n",
      "train loss:0.06801603303709497\n",
      "train loss:0.0741840826919969\n",
      "train loss:0.045590349698532436\n",
      "train loss:0.03082322379060582\n",
      "train loss:0.08549642400345675\n",
      "train loss:0.07900353970128723\n",
      "train loss:0.041388630464073276\n",
      "train loss:0.02028214877931269\n",
      "train loss:0.03188337899512659\n",
      "train loss:0.15282434681122403\n",
      "train loss:0.11123225696764485\n",
      "train loss:0.060700347537290054\n",
      "train loss:0.04550449557494941\n",
      "train loss:0.06542315547441471\n",
      "train loss:0.02914531604364509\n",
      "train loss:0.09696167931467295\n",
      "train loss:0.07263378930534653\n",
      "train loss:0.061141574239798005\n",
      "train loss:0.04814767044123155\n",
      "train loss:0.026060723059033886\n",
      "train loss:0.03469709039241021\n",
      "train loss:0.06519742380985878\n",
      "train loss:0.05542344288574078\n",
      "train loss:0.051092165178773\n",
      "train loss:0.047617549053425766\n",
      "train loss:0.0965133010047175\n",
      "train loss:0.06467043790523642\n",
      "train loss:0.01737977180141269\n",
      "train loss:0.01092957394136505\n",
      "train loss:0.06271218031209415\n",
      "train loss:0.10505181439771176\n",
      "train loss:0.03102252142113338\n",
      "train loss:0.11836293323222975\n",
      "train loss:0.11520574136737192\n",
      "train loss:0.0218430303908539\n",
      "train loss:0.03954582272800309\n",
      "train loss:0.08406171486949038\n",
      "train loss:0.06405289064674412\n",
      "train loss:0.05247659448767589\n",
      "train loss:0.09281522432989417\n",
      "train loss:0.06700166049364288\n",
      "train loss:0.07874442734682127\n",
      "train loss:0.09005881305509078\n",
      "train loss:0.016441726496018164\n",
      "train loss:0.014990744665930766\n",
      "train loss:0.03464736521776696\n",
      "train loss:0.09795510379716538\n",
      "train loss:0.008933209893992487\n",
      "train loss:0.03540856602036357\n",
      "train loss:0.05965634789062225\n",
      "train loss:0.08163732461684049\n",
      "train loss:0.026237773475688737\n",
      "train loss:0.0122959235172177\n",
      "train loss:0.06123748646118198\n",
      "train loss:0.09157972992676472\n",
      "train loss:0.0815855881083689\n",
      "train loss:0.06938802980606826\n",
      "train loss:0.021717415768563106\n",
      "train loss:0.08896149300204881\n",
      "train loss:0.08019942263959413\n",
      "train loss:0.04068115027098153\n",
      "train loss:0.04827891658828782\n",
      "train loss:0.04211396475853567\n",
      "train loss:0.027381715422965476\n",
      "train loss:0.030626761342166228\n",
      "train loss:0.0667430464446082\n",
      "train loss:0.0433115183231281\n",
      "train loss:0.05911531188609395\n",
      "train loss:0.037350224565456024\n",
      "train loss:0.062317717343006694\n",
      "train loss:0.0339344320856599\n",
      "train loss:0.017094771790037523\n",
      "train loss:0.05299476868448438\n",
      "train loss:0.06894601351508\n",
      "train loss:0.08774130154833464\n",
      "train loss:0.058696523649442396\n",
      "train loss:0.02163608774030132\n",
      "train loss:0.04971770504822941\n",
      "train loss:0.10273908989043712\n",
      "train loss:0.10504272513352195\n",
      "train loss:0.02653172606565907\n",
      "train loss:0.14042679586297402\n",
      "train loss:0.07924535466331091\n",
      "train loss:0.020784794882409762\n",
      "train loss:0.06323944785579447\n",
      "train loss:0.019546238682960116\n",
      "train loss:0.02997490274075638\n",
      "train loss:0.1829497384601884\n",
      "train loss:0.1073428522340156\n",
      "train loss:0.027457990193215195\n",
      "train loss:0.0644721470414867\n",
      "train loss:0.08346842476817656\n",
      "train loss:0.029150505591319216\n",
      "train loss:0.06797246339397581\n",
      "train loss:0.025439261313554992\n",
      "train loss:0.02284653830250884\n",
      "train loss:0.034919593789145276\n",
      "train loss:0.05060101171840206\n",
      "train loss:0.09400171364500964\n",
      "train loss:0.050324351244265556\n",
      "train loss:0.054676807923305176\n",
      "train loss:0.00891436634540608\n",
      "train loss:0.09461311391499928\n",
      "train loss:0.09105846554472864\n",
      "train loss:0.12762167189849521\n",
      "train loss:0.04906388330624875\n",
      "train loss:0.03167043338067674\n",
      "train loss:0.04798500326840256\n",
      "train loss:0.06829277454186919\n",
      "train loss:0.03457302384692723\n",
      "train loss:0.024900342849090443\n",
      "train loss:0.07298840801499305\n",
      "train loss:0.028422919095815232\n",
      "train loss:0.04139928677660138\n",
      "train loss:0.06232239342376171\n",
      "train loss:0.026904088777501074\n",
      "train loss:0.016750449256121274\n",
      "train loss:0.05101071547586737\n",
      "train loss:0.04791069150607174\n",
      "train loss:0.030640367911729405\n",
      "train loss:0.1219512356795227\n",
      "train loss:0.14727810035504832\n",
      "train loss:0.06023192515995804\n",
      "train loss:0.03776734439691037\n",
      "train loss:0.030251210789137187\n",
      "train loss:0.06030521335031262\n",
      "train loss:0.08640165885553476\n",
      "train loss:0.06712079800711987\n",
      "train loss:0.03311439719348619\n",
      "train loss:0.026927111115443277\n",
      "train loss:0.0074710883671855535\n",
      "train loss:0.06658850051790542\n",
      "train loss:0.018868880299078804\n",
      "train loss:0.04096792324011009\n",
      "train loss:0.04940394083949472\n",
      "train loss:0.03929662357039552\n",
      "train loss:0.03702997940441946\n",
      "train loss:0.02240679756834164\n",
      "train loss:0.0833145949115454\n",
      "train loss:0.022821171902262263\n",
      "train loss:0.057023436446897724\n",
      "train loss:0.14222232823878111\n",
      "train loss:0.07001343200968836\n",
      "train loss:0.07116427944487533\n",
      "train loss:0.03429484224089925\n",
      "train loss:0.08941712182532012\n",
      "train loss:0.03228131332761754\n",
      "train loss:0.013176450431299159\n",
      "train loss:0.03146035809835992\n",
      "train loss:0.1154250219829019\n",
      "train loss:0.06797801801432801\n",
      "train loss:0.014288636234896242\n",
      "train loss:0.025554180668580413\n",
      "train loss:0.030807762694772464\n",
      "train loss:0.0391076969281071\n",
      "train loss:0.01649758541373948\n",
      "train loss:0.00693477724008745\n",
      "train loss:0.04227122806572516\n",
      "train loss:0.05412510754824301\n",
      "train loss:0.03670659277741459\n",
      "train loss:0.09157825091687136\n",
      "train loss:0.01955551834284054\n",
      "train loss:0.020526004145209643\n",
      "train loss:0.02630520806382898\n",
      "train loss:0.04572253517821005\n",
      "train loss:0.01867996076852831\n",
      "train loss:0.025871775054322664\n",
      "train loss:0.030774835384933783\n",
      "train loss:0.07234262417168832\n",
      "train loss:0.09487493769828237\n",
      "train loss:0.05892488181782567\n",
      "train loss:0.05131490995797752\n",
      "train loss:0.010961451422309174\n",
      "train loss:0.015033053034981751\n",
      "train loss:0.04032985216988709\n",
      "train loss:0.03050257638634215\n",
      "train loss:0.020541560879285842\n",
      "train loss:0.015792655686670112\n",
      "train loss:0.008187841421297156\n",
      "train loss:0.1244081462619181\n",
      "train loss:0.06491933112992289\n",
      "train loss:0.12072995414276484\n",
      "train loss:0.03363359211514112\n",
      "train loss:0.02339488107802859\n",
      "train loss:0.06588458743172912\n",
      "train loss:0.01776955017759489\n",
      "train loss:0.06026164833938897\n",
      "train loss:0.040193371880992125\n",
      "train loss:0.07558020362409427\n",
      "train loss:0.12191564495090962\n",
      "train loss:0.028514589575638963\n",
      "train loss:0.08778741135289504\n",
      "train loss:0.06148706258241808\n",
      "train loss:0.08595873427881584\n",
      "train loss:0.04063497431216457\n",
      "train loss:0.09052351939092143\n",
      "train loss:0.08416001635855594\n",
      "train loss:0.01906207243614031\n",
      "train loss:0.0650381986972518\n",
      "train loss:0.0939931252811274\n",
      "train loss:0.05681593372690444\n",
      "train loss:0.039158210762018066\n",
      "train loss:0.022813545384065298\n",
      "train loss:0.05761035863097835\n",
      "train loss:0.05194939396655433\n",
      "train loss:0.01946807926431495\n",
      "train loss:0.01600398811915605\n",
      "train loss:0.09150189370963822\n",
      "train loss:0.05026303242549372\n",
      "train loss:0.03477919910025171\n",
      "train loss:0.036016901952054925\n",
      "train loss:0.05407371927966891\n",
      "train loss:0.12092492063999088\n",
      "train loss:0.01611809809707161\n",
      "train loss:0.03163190857772771\n",
      "train loss:0.13607888524312994\n",
      "train loss:0.08716035181949099\n",
      "train loss:0.025401642960149306\n",
      "train loss:0.04608001314276687\n",
      "train loss:0.0215501937539485\n",
      "train loss:0.0885223446483207\n",
      "train loss:0.05864020476285301\n",
      "train loss:0.03019355397293955\n",
      "train loss:0.012269882522545038\n",
      "train loss:0.04762064581891979\n",
      "train loss:0.03735758860086821\n",
      "train loss:0.09340900472100748\n",
      "train loss:0.04500787825499269\n",
      "train loss:0.03337588236179974\n",
      "train loss:0.07027352174848589\n",
      "train loss:0.03167998550499607\n",
      "train loss:0.031171949251561817\n",
      "train loss:0.033382545317025225\n",
      "train loss:0.042828660420437314\n",
      "train loss:0.03791681584932207\n",
      "train loss:0.042338516583556945\n",
      "train loss:0.03838664240570615\n",
      "train loss:0.046417737297178824\n",
      "train loss:0.02023327050161696\n",
      "train loss:0.03276125231517691\n",
      "=== epoch:4, train acc:0.986, test acc:0.98 ===\n",
      "train loss:0.09790377319085772\n",
      "train loss:0.02487187274025052\n",
      "train loss:0.10031034526291657\n",
      "train loss:0.06667994380624682\n",
      "train loss:0.06546637667783056\n",
      "train loss:0.031205678900253032\n",
      "train loss:0.11754767982035708\n",
      "train loss:0.022208446247975532\n",
      "train loss:0.013776473233712198\n",
      "train loss:0.016242029501457665\n",
      "train loss:0.041925237512103024\n",
      "train loss:0.02375367874400322\n",
      "train loss:0.05806732172764661\n",
      "train loss:0.06872939014265624\n",
      "train loss:0.02950487052783248\n",
      "train loss:0.06558050706398458\n",
      "train loss:0.013890929018504122\n",
      "train loss:0.06284810439965269\n",
      "train loss:0.0365743141889036\n",
      "train loss:0.03035913341195557\n",
      "train loss:0.05349930473379198\n",
      "train loss:0.05867675581770364\n",
      "train loss:0.029086326179365473\n",
      "train loss:0.0680964020861835\n",
      "train loss:0.02684074019119715\n",
      "train loss:0.02603835398584582\n",
      "train loss:0.08924426925943912\n",
      "train loss:0.13020141880249658\n",
      "train loss:0.09114515707699575\n",
      "train loss:0.03135944482756302\n",
      "train loss:0.013129384601562589\n",
      "train loss:0.04350762052002492\n",
      "train loss:0.020057795696657964\n",
      "train loss:0.03649490945594053\n",
      "train loss:0.06066042021774971\n",
      "train loss:0.026536194611640612\n",
      "train loss:0.029100307041240638\n",
      "train loss:0.021013935049574953\n",
      "train loss:0.0258834824543172\n",
      "train loss:0.09245930954024473\n",
      "train loss:0.042910515995678813\n",
      "train loss:0.043011339777468585\n",
      "train loss:0.02718167159056138\n",
      "train loss:0.035331800957772636\n",
      "train loss:0.009541908865082173\n",
      "train loss:0.11603245891036604\n",
      "train loss:0.034002548867546535\n",
      "train loss:0.0225811242289788\n",
      "train loss:0.05797380753441522\n",
      "train loss:0.043378963759145774\n",
      "train loss:0.05085933515105756\n",
      "train loss:0.05198829708202531\n",
      "train loss:0.05431417073796324\n",
      "train loss:0.05277352830304223\n",
      "train loss:0.03302821296126588\n",
      "train loss:0.016521169703496846\n",
      "train loss:0.017933717224433095\n",
      "train loss:0.04619331735495763\n",
      "train loss:0.015994085521653816\n",
      "train loss:0.01846514256211282\n",
      "train loss:0.038685799221402875\n",
      "train loss:0.050018704178385685\n",
      "train loss:0.0558732618131645\n",
      "train loss:0.040938380187204866\n",
      "train loss:0.0397530556008176\n",
      "train loss:0.04150367680867578\n",
      "train loss:0.029909981702310882\n",
      "train loss:0.014685583734281957\n",
      "train loss:0.013704975681737414\n",
      "train loss:0.019152836820868716\n",
      "train loss:0.053641300018793324\n",
      "train loss:0.04227897143246527\n",
      "train loss:0.018945815752813108\n",
      "train loss:0.10238493687468006\n",
      "train loss:0.060597087976519214\n",
      "train loss:0.024063252000346525\n",
      "train loss:0.03445677693869549\n",
      "train loss:0.08687382240531123\n",
      "train loss:0.025959367370684\n",
      "train loss:0.07087345353111205\n",
      "train loss:0.09563065334122633\n",
      "train loss:0.0594987829361531\n",
      "train loss:0.036142256677946356\n",
      "train loss:0.047798695908201516\n",
      "train loss:0.07649418059292715\n",
      "train loss:0.06561828163553377\n",
      "train loss:0.019579972889264413\n",
      "train loss:0.028570248437134436\n",
      "train loss:0.037414020691894596\n",
      "train loss:0.04926211861953351\n",
      "train loss:0.03866783319509684\n",
      "train loss:0.11062087259858666\n",
      "train loss:0.011026514154545999\n",
      "train loss:0.03072638700818412\n",
      "train loss:0.012557112837005644\n",
      "train loss:0.06909027140304656\n",
      "train loss:0.014946515512848783\n",
      "train loss:0.05092228046178204\n",
      "train loss:0.014606150951864308\n",
      "train loss:0.1282650082952942\n",
      "train loss:0.013636550844798393\n",
      "train loss:0.022216313176430903\n",
      "train loss:0.025963819464178573\n",
      "train loss:0.03607704671742863\n",
      "train loss:0.011574016888360496\n",
      "train loss:0.01700454054649043\n",
      "train loss:0.05906798437599848\n",
      "train loss:0.01716275019972479\n",
      "train loss:0.017260630210289646\n",
      "train loss:0.05231512213731713\n",
      "train loss:0.07985513478421764\n",
      "train loss:0.050346826989049874\n",
      "train loss:0.02885691636340257\n",
      "train loss:0.04809018657103003\n",
      "train loss:0.00881847533040584\n",
      "train loss:0.03536706161357997\n",
      "train loss:0.03608187396566226\n",
      "train loss:0.03438743015924986\n",
      "train loss:0.05264819515461702\n",
      "train loss:0.012866584193199531\n",
      "train loss:0.03482254328702194\n",
      "train loss:0.1829456322134426\n",
      "train loss:0.04467332260772878\n",
      "train loss:0.015909174770463032\n",
      "train loss:0.06702541895433617\n",
      "train loss:0.17190643913358833\n",
      "train loss:0.033615874475432024\n",
      "train loss:0.07046200426463999\n",
      "train loss:0.00958218419177502\n",
      "train loss:0.046190800647280435\n",
      "train loss:0.016181683124493723\n",
      "train loss:0.04078089230210893\n",
      "train loss:0.08073521431216428\n",
      "train loss:0.012567215460521362\n",
      "train loss:0.011831506006271171\n",
      "train loss:0.014005752811589015\n",
      "train loss:0.0994545788695365\n",
      "train loss:0.02417924434800276\n",
      "train loss:0.02944550221775477\n",
      "train loss:0.01592877068990242\n",
      "train loss:0.03882341825280607\n",
      "train loss:0.07950236286031295\n",
      "train loss:0.037395668781330944\n",
      "train loss:0.03165223214730884\n",
      "train loss:0.01773952760690231\n",
      "train loss:0.030862071712255013\n",
      "train loss:0.04591614536528228\n",
      "train loss:0.018267220780590662\n",
      "train loss:0.16927629198411406\n",
      "train loss:0.01877420950980855\n",
      "train loss:0.06208456105010471\n",
      "train loss:0.04554528272580766\n",
      "train loss:0.0846239014932507\n",
      "train loss:0.058913277779426274\n",
      "train loss:0.016248575623120015\n",
      "train loss:0.02592422239365763\n",
      "train loss:0.08653391082053644\n",
      "train loss:0.0993272397129484\n",
      "train loss:0.03296081385965911\n",
      "train loss:0.050559129071437024\n",
      "train loss:0.016379030368392482\n",
      "train loss:0.033484860618316285\n",
      "train loss:0.013503800230884478\n",
      "train loss:0.02561949904213173\n",
      "train loss:0.02292753823850311\n",
      "train loss:0.021546375764605896\n",
      "train loss:0.09108332873643026\n",
      "train loss:0.08234019090263436\n",
      "train loss:0.01694114955193446\n",
      "train loss:0.048627098685754565\n",
      "train loss:0.02235184001703123\n",
      "train loss:0.03061039315928628\n",
      "train loss:0.019174908613367344\n",
      "train loss:0.05330141397509907\n",
      "train loss:0.009071110340220583\n",
      "train loss:0.02379405019001027\n",
      "train loss:0.13713249318059517\n",
      "train loss:0.024311588802261173\n",
      "train loss:0.033767398401618284\n",
      "train loss:0.011721265012946293\n",
      "train loss:0.018992989389218302\n",
      "train loss:0.042839718098409814\n",
      "train loss:0.04895981469904263\n",
      "train loss:0.086664658726923\n",
      "train loss:0.02834023688337163\n",
      "train loss:0.019801893600041084\n",
      "train loss:0.14772825287603208\n",
      "train loss:0.04459360202067639\n",
      "train loss:0.029084711769627028\n",
      "train loss:0.04738751399597223\n",
      "train loss:0.04119732666859062\n",
      "train loss:0.03253525768065808\n",
      "train loss:0.008837281669662385\n",
      "train loss:0.01890352336295382\n",
      "train loss:0.026796837725313084\n",
      "train loss:0.01201088240631665\n",
      "train loss:0.1276774303663117\n",
      "train loss:0.035464784078001195\n",
      "train loss:0.03168303450248957\n",
      "train loss:0.07905181348257959\n",
      "train loss:0.05340325707398137\n",
      "train loss:0.005212921119224846\n",
      "train loss:0.05516941582960268\n",
      "train loss:0.026918630916619876\n",
      "train loss:0.05834310210946879\n",
      "train loss:0.02533240872317019\n",
      "train loss:0.029587637198622733\n",
      "train loss:0.04848075111696246\n",
      "train loss:0.0457865240465849\n",
      "train loss:0.045136915918782626\n",
      "train loss:0.018253213318837024\n",
      "train loss:0.02481326665068269\n",
      "train loss:0.03220726952703989\n",
      "train loss:0.049742603369936196\n",
      "train loss:0.02702783984848597\n",
      "train loss:0.033245929060329825\n",
      "train loss:0.019462419126159463\n",
      "train loss:0.04275674469609131\n",
      "train loss:0.021327879934986475\n",
      "train loss:0.031321467232825535\n",
      "train loss:0.04324753153236163\n",
      "train loss:0.06179322385763711\n",
      "train loss:0.01167021078090844\n",
      "train loss:0.012614620864260583\n",
      "train loss:0.07701758278879653\n",
      "train loss:0.04205754086897604\n",
      "train loss:0.046834538451734416\n",
      "train loss:0.01104613234788068\n",
      "train loss:0.06810398111277134\n",
      "train loss:0.03484730198745746\n",
      "train loss:0.008427731088380514\n",
      "train loss:0.01728113835473353\n",
      "train loss:0.050270174918179276\n",
      "train loss:0.041460528340093644\n",
      "train loss:0.04431992624584323\n",
      "train loss:0.015631942048092543\n",
      "train loss:0.017209225665280683\n",
      "train loss:0.03495096660229505\n",
      "train loss:0.040410735759810794\n",
      "train loss:0.02695942255870282\n",
      "train loss:0.025842179000143628\n",
      "train loss:0.0618566285515486\n",
      "train loss:0.03782578900279379\n",
      "train loss:0.020703757230330896\n",
      "train loss:0.01792769949703029\n",
      "train loss:0.03824125663217922\n",
      "train loss:0.062347710062531585\n",
      "train loss:0.011697301796014565\n",
      "train loss:0.08405230206770119\n",
      "train loss:0.02883187036793969\n",
      "train loss:0.06407434685459014\n",
      "train loss:0.13526780066753635\n",
      "train loss:0.03652858094530372\n",
      "train loss:0.04087930577436846\n",
      "train loss:0.08678446978307561\n",
      "train loss:0.0338312009477354\n",
      "train loss:0.01592623532802794\n",
      "train loss:0.12938146807004486\n",
      "train loss:0.033371459299980824\n",
      "train loss:0.04629011245341091\n",
      "train loss:0.022389584828898624\n",
      "train loss:0.024867446631970783\n",
      "train loss:0.011791017020083972\n",
      "train loss:0.0655485235426101\n",
      "train loss:0.07161302101945485\n",
      "train loss:0.045447404638850095\n",
      "train loss:0.0179863188945101\n",
      "train loss:0.025027323757474625\n",
      "train loss:0.04938454837571442\n",
      "train loss:0.031003075006330958\n",
      "train loss:0.024807618145139113\n",
      "train loss:0.13656048884651406\n",
      "train loss:0.046803239981220116\n",
      "train loss:0.04601419609439153\n",
      "train loss:0.035919538328634405\n",
      "train loss:0.02089158411859249\n",
      "train loss:0.07600117662268029\n",
      "train loss:0.008191766499740459\n",
      "train loss:0.025153135815013692\n",
      "train loss:0.03702489023184457\n",
      "train loss:0.04239165875630719\n",
      "train loss:0.05839532894189507\n",
      "train loss:0.046852071077328465\n",
      "train loss:0.03690116977247936\n",
      "train loss:0.022571096179390994\n",
      "train loss:0.012994356834894048\n",
      "train loss:0.09576259834267895\n",
      "train loss:0.04701725538173837\n",
      "train loss:0.033127935152477465\n",
      "train loss:0.0797624466399622\n",
      "train loss:0.03620959548147181\n",
      "train loss:0.07562246513598493\n",
      "train loss:0.03835750810746825\n",
      "train loss:0.03872558293343122\n",
      "train loss:0.005966585998535889\n",
      "train loss:0.027085677693341553\n",
      "train loss:0.010896437449052799\n",
      "train loss:0.05288703897804936\n",
      "train loss:0.024147738264602175\n",
      "train loss:0.04301304822981615\n",
      "train loss:0.032375696671938174\n",
      "train loss:0.010759448125951863\n",
      "train loss:0.035257938962602944\n",
      "train loss:0.03753449825725242\n",
      "train loss:0.04590331440214435\n",
      "train loss:0.03912297631765494\n",
      "train loss:0.00591581768048094\n",
      "train loss:0.11476426710493866\n",
      "train loss:0.035357179844130185\n",
      "train loss:0.059470255051899484\n",
      "train loss:0.0760672147185628\n",
      "train loss:0.024046091131521615\n",
      "train loss:0.09292330473544366\n",
      "train loss:0.01464851688556865\n",
      "train loss:0.040505813085868786\n",
      "train loss:0.03463492542415315\n",
      "train loss:0.022682744637187176\n",
      "train loss:0.05209902290399635\n",
      "train loss:0.04088903025549774\n",
      "train loss:0.03070094617336935\n",
      "train loss:0.027884015650234125\n",
      "train loss:0.04545021003207492\n",
      "train loss:0.04652115962227226\n",
      "train loss:0.02713292444944376\n",
      "train loss:0.04091923087698881\n",
      "train loss:0.034766913375646906\n",
      "train loss:0.15354686504151768\n",
      "train loss:0.06461441102087562\n",
      "train loss:0.03476359535546544\n",
      "train loss:0.03307221056737517\n",
      "train loss:0.035741429792403384\n",
      "train loss:0.014259567722184209\n",
      "train loss:0.09716327431380714\n",
      "train loss:0.018343361919741744\n",
      "train loss:0.03616640920647552\n",
      "train loss:0.011845079556338407\n",
      "train loss:0.02187972697811451\n",
      "train loss:0.04035737354237212\n",
      "train loss:0.09467793343697074\n",
      "train loss:0.03880504535117687\n",
      "train loss:0.021781230049085703\n",
      "train loss:0.021075073178197696\n",
      "train loss:0.022002495561709905\n",
      "train loss:0.01317926356061088\n",
      "train loss:0.022733439575415643\n",
      "train loss:0.04439363451829826\n",
      "train loss:0.07087731589145176\n",
      "train loss:0.02598289222149039\n",
      "train loss:0.14290099045619972\n",
      "train loss:0.005589192119688642\n",
      "train loss:0.05099162673760897\n",
      "train loss:0.02314512078466997\n",
      "train loss:0.06038857023410964\n",
      "train loss:0.08087647383329927\n",
      "train loss:0.04796463897651992\n",
      "train loss:0.06562733125652839\n",
      "train loss:0.014941416979874245\n",
      "train loss:0.009519835142884028\n",
      "train loss:0.08892925490693697\n",
      "train loss:0.009252653591731993\n",
      "train loss:0.04318139492501514\n",
      "train loss:0.04514495608974316\n",
      "train loss:0.03144050575806838\n",
      "train loss:0.04939988422710577\n",
      "train loss:0.06440004388780911\n",
      "train loss:0.025872874066947427\n",
      "train loss:0.06320647174000323\n",
      "train loss:0.030720410860457358\n",
      "train loss:0.0534418877321203\n",
      "train loss:0.032326429934940795\n",
      "train loss:0.02893049837581495\n",
      "train loss:0.06504611059029602\n",
      "train loss:0.05993779916527834\n",
      "train loss:0.0216564630857409\n",
      "train loss:0.07021348902569069\n",
      "train loss:0.026932388655852294\n",
      "train loss:0.026020843870125442\n",
      "train loss:0.05995348197772349\n",
      "train loss:0.09675923162432225\n",
      "train loss:0.008581994756829532\n",
      "train loss:0.1061778572857238\n",
      "train loss:0.028503729719590476\n",
      "train loss:0.019849328793345493\n",
      "train loss:0.022192818915570935\n",
      "train loss:0.01740637538854461\n",
      "train loss:0.06310317388689493\n",
      "train loss:0.040155785709633435\n",
      "train loss:0.04858854480839474\n",
      "train loss:0.012188248455992754\n",
      "train loss:0.008547527682632535\n",
      "train loss:0.01192128056205257\n",
      "train loss:0.020226018415152643\n",
      "train loss:0.011943064730402686\n",
      "train loss:0.09084451989971443\n",
      "train loss:0.03274599593614111\n",
      "train loss:0.03554173079798042\n",
      "train loss:0.03346946232308485\n",
      "train loss:0.035472114824684514\n",
      "train loss:0.04670780676731764\n",
      "train loss:0.03657134707084431\n",
      "train loss:0.06193237780008496\n",
      "train loss:0.06999218402015112\n",
      "train loss:0.1314550194787774\n",
      "train loss:0.02137319469033707\n",
      "train loss:0.02157062938210776\n",
      "train loss:0.04972533167618992\n",
      "train loss:0.06685322464633336\n",
      "train loss:0.013862791096919623\n",
      "train loss:0.05847981677990214\n",
      "train loss:0.06269530117947496\n",
      "train loss:0.03593761326946763\n",
      "train loss:0.057675555864659354\n",
      "train loss:0.04121647793613414\n",
      "train loss:0.03832150792757017\n",
      "train loss:0.05426120432464804\n",
      "train loss:0.12224681563064754\n",
      "train loss:0.10837212782964922\n",
      "train loss:0.07529843551751239\n",
      "train loss:0.012098972785592996\n",
      "train loss:0.03373282370068097\n",
      "train loss:0.015632609624267817\n",
      "train loss:0.06815283663165832\n",
      "train loss:0.011628682868517965\n",
      "train loss:0.035832362292273025\n",
      "train loss:0.06999371038646285\n",
      "train loss:0.07096347850214997\n",
      "train loss:0.023657117050903217\n",
      "train loss:0.030338798670506043\n",
      "train loss:0.08428011330480338\n",
      "train loss:0.011885673013780938\n",
      "train loss:0.03732207247736594\n",
      "train loss:0.02884923382569847\n",
      "train loss:0.05613541915008915\n",
      "train loss:0.019876869754906778\n",
      "train loss:0.024924873283199034\n",
      "train loss:0.06178510310027899\n",
      "train loss:0.07108593894614948\n",
      "train loss:0.06615288239649592\n",
      "train loss:0.021460883000865842\n",
      "train loss:0.04901323024020885\n",
      "train loss:0.0210504015737522\n",
      "train loss:0.01749496236446722\n",
      "train loss:0.012096593113809555\n",
      "train loss:0.021323291051950476\n",
      "train loss:0.018149876117371785\n",
      "train loss:0.025718227508668912\n",
      "train loss:0.018887849333678795\n",
      "train loss:0.02541176715491388\n",
      "train loss:0.05437992783736993\n",
      "train loss:0.023889089490703896\n",
      "train loss:0.04936995137947881\n",
      "train loss:0.027385690813806422\n",
      "train loss:0.038736081303076646\n",
      "train loss:0.043887520043068874\n",
      "train loss:0.03978733038370381\n",
      "train loss:0.03086144575785776\n",
      "train loss:0.00828263764567736\n",
      "train loss:0.021429438775452066\n",
      "train loss:0.009423640692991205\n",
      "train loss:0.044532104268391375\n",
      "train loss:0.013570668655087697\n",
      "train loss:0.022487627727997027\n",
      "train loss:0.07935173854949935\n",
      "train loss:0.07710944098847519\n",
      "train loss:0.041777467477089214\n",
      "train loss:0.03897206119358762\n",
      "train loss:0.06461523145365651\n",
      "train loss:0.025396246637160134\n",
      "train loss:0.01694218764524066\n",
      "train loss:0.023584331862101474\n",
      "train loss:0.07425158586043742\n",
      "train loss:0.022648084532366105\n",
      "train loss:0.007557395082066659\n",
      "train loss:0.030170675010386797\n",
      "train loss:0.03564113170436425\n",
      "train loss:0.08036950740067535\n",
      "train loss:0.05161156632202301\n",
      "train loss:0.025989522529474948\n",
      "train loss:0.030707198328837032\n",
      "train loss:0.05288783974596766\n",
      "train loss:0.03611430028679027\n",
      "train loss:0.01589247691604586\n",
      "train loss:0.0295149138269085\n",
      "train loss:0.025409640450198813\n",
      "train loss:0.04543826896138447\n",
      "train loss:0.01220449137529324\n",
      "train loss:0.011773366659424484\n",
      "train loss:0.014516258763458509\n",
      "train loss:0.05094102036081579\n",
      "train loss:0.007686769782045313\n",
      "train loss:0.012959154894765519\n",
      "train loss:0.023786886647916398\n",
      "train loss:0.08732948642757711\n",
      "train loss:0.03521558875300406\n",
      "train loss:0.010502806189242275\n",
      "train loss:0.022469277375636164\n",
      "train loss:0.022075404415793656\n",
      "train loss:0.03884364342027094\n",
      "train loss:0.06530587971530037\n",
      "train loss:0.05558287160009805\n",
      "train loss:0.06827532265265711\n",
      "train loss:0.014558724247534578\n",
      "train loss:0.040406666554984344\n",
      "train loss:0.014631762619719258\n",
      "train loss:0.01501784164283056\n",
      "train loss:0.04528171761972226\n",
      "train loss:0.011909810845413347\n",
      "train loss:0.019294522558409827\n",
      "train loss:0.020522712121684862\n",
      "train loss:0.027112372954854903\n",
      "train loss:0.01246640001390163\n",
      "train loss:0.021182339418780095\n",
      "train loss:0.032253731294057036\n",
      "train loss:0.03861142243398176\n",
      "train loss:0.022471933891512284\n",
      "train loss:0.05144782936975346\n",
      "train loss:0.052299093284265394\n",
      "train loss:0.010900803958868941\n",
      "train loss:0.015584152865434766\n",
      "train loss:0.0367634221164647\n",
      "train loss:0.03899441893776779\n",
      "train loss:0.043065228862641476\n",
      "train loss:0.013809256168387405\n",
      "train loss:0.014738229245617612\n",
      "train loss:0.02358050701397515\n",
      "train loss:0.025033301793551347\n",
      "train loss:0.040157119625826694\n",
      "train loss:0.09344119070867205\n",
      "train loss:0.03101686617770871\n",
      "train loss:0.02556481025561834\n",
      "train loss:0.052767560911461286\n",
      "train loss:0.04786759231187079\n",
      "train loss:0.029662798932507176\n",
      "train loss:0.03247364849732966\n",
      "train loss:0.030869630065879113\n",
      "train loss:0.023546911282002026\n",
      "train loss:0.02997566250081615\n",
      "train loss:0.019945985084775884\n",
      "train loss:0.029315570666925604\n",
      "train loss:0.020054701423992044\n",
      "train loss:0.03535766283059544\n",
      "train loss:0.016494287050294747\n",
      "train loss:0.025051443856849744\n",
      "train loss:0.011783171597825402\n",
      "train loss:0.04163615099928717\n",
      "train loss:0.03717729613499845\n",
      "train loss:0.05121540468041295\n",
      "train loss:0.07293955392780709\n",
      "train loss:0.03376252518352842\n",
      "train loss:0.05178416299889523\n",
      "train loss:0.019559560636447627\n",
      "train loss:0.0129451867747965\n",
      "train loss:0.03789033303492203\n",
      "train loss:0.07882265295278329\n",
      "train loss:0.015177218991133456\n",
      "train loss:0.04484138182984609\n",
      "train loss:0.04444976156830927\n",
      "train loss:0.06993623037755722\n",
      "train loss:0.031066043564462745\n",
      "train loss:0.014046330296100617\n",
      "train loss:0.03933053578116115\n",
      "train loss:0.013602038964634945\n",
      "train loss:0.06702139774837884\n",
      "train loss:0.05040818127410333\n",
      "train loss:0.010112526647569557\n",
      "train loss:0.022022472758669573\n",
      "train loss:0.022936942672917005\n",
      "train loss:0.04754903272645836\n",
      "train loss:0.06051405597741719\n",
      "train loss:0.017999309057040027\n",
      "train loss:0.04020187405996811\n",
      "train loss:0.012332295719415904\n",
      "train loss:0.0525140291443054\n",
      "train loss:0.03601241595006397\n",
      "train loss:0.095724691647898\n",
      "train loss:0.006224502807333055\n",
      "train loss:0.011482506258122393\n",
      "train loss:0.022274070131702493\n",
      "train loss:0.015232177206952994\n",
      "train loss:0.05683181128108185\n",
      "train loss:0.00424905454603206\n",
      "train loss:0.01938115187502346\n",
      "train loss:0.009329319077488558\n",
      "train loss:0.016828002721498897\n",
      "train loss:0.061565354563953685\n",
      "train loss:0.05083470562176128\n",
      "train loss:0.0201032010331398\n",
      "train loss:0.055717518279979415\n",
      "train loss:0.009451605758655467\n",
      "train loss:0.027546776319970965\n",
      "train loss:0.03990091461377179\n",
      "train loss:0.005422362248692044\n",
      "train loss:0.012138619724064771\n",
      "train loss:0.025662870631623324\n",
      "train loss:0.03263105705202687\n",
      "train loss:0.010601236798988112\n",
      "train loss:0.007919893744191909\n",
      "train loss:0.093492031485677\n",
      "train loss:0.01784295455977215\n",
      "train loss:0.09982421158024232\n",
      "=== epoch:5, train acc:0.986, test acc:0.98 ===\n",
      "train loss:0.03293566402333428\n",
      "train loss:0.01859333022095085\n",
      "train loss:0.011410838463285843\n",
      "train loss:0.029527513048843684\n",
      "train loss:0.04003635534650687\n",
      "train loss:0.01877615527749379\n",
      "train loss:0.052535079918200014\n",
      "train loss:0.07315244774175704\n",
      "train loss:0.030824256187168286\n",
      "train loss:0.012734279061325949\n",
      "train loss:0.011013475766716643\n",
      "train loss:0.02266321574573592\n",
      "train loss:0.019811612626854158\n",
      "train loss:0.090067379692794\n",
      "train loss:0.01568701880625387\n",
      "train loss:0.01456032981197942\n",
      "train loss:0.07647864936484239\n",
      "train loss:0.018645126503130008\n",
      "train loss:0.07141396934402955\n",
      "train loss:0.02048215370988787\n",
      "train loss:0.0327738262863855\n",
      "train loss:0.11832097363189463\n",
      "train loss:0.04652510064868583\n",
      "train loss:0.011716422561632675\n",
      "train loss:0.02514987576940329\n",
      "train loss:0.017387694804328634\n",
      "train loss:0.016735806936828114\n",
      "train loss:0.07181419462515334\n",
      "train loss:0.005789978151122549\n",
      "train loss:0.016869367115096644\n",
      "train loss:0.0728030235005695\n",
      "train loss:0.031490631508620893\n",
      "train loss:0.05253619706898453\n",
      "train loss:0.02615955786543588\n",
      "train loss:0.09370075602446523\n",
      "train loss:0.033534448876300534\n",
      "train loss:0.011862349662363382\n",
      "train loss:0.023134869204526665\n",
      "train loss:0.03689054428034705\n",
      "train loss:0.05533518949223534\n",
      "train loss:0.0314521775217393\n",
      "train loss:0.036742898191028134\n",
      "train loss:0.09831972578834294\n",
      "train loss:0.01674601181393461\n",
      "train loss:0.028336974586417703\n",
      "train loss:0.02744324656963229\n",
      "train loss:0.009493679294164298\n",
      "train loss:0.032407786612661804\n",
      "train loss:0.06052647730932329\n",
      "train loss:0.036057523527917495\n",
      "train loss:0.0915067950835902\n",
      "train loss:0.04047591489985566\n",
      "train loss:0.029249216716086\n",
      "train loss:0.024616724762928123\n",
      "train loss:0.011033392383428193\n",
      "train loss:0.03261012051021058\n",
      "train loss:0.016559635697863503\n",
      "train loss:0.01632665473735533\n",
      "train loss:0.030904724564926704\n",
      "train loss:0.08529007499132618\n",
      "train loss:0.02792711773610498\n",
      "train loss:0.024533587411046453\n",
      "train loss:0.04141672844105344\n",
      "train loss:0.029455610149733705\n",
      "train loss:0.027324241891433586\n",
      "train loss:0.008033747520840104\n",
      "train loss:0.020827077117811395\n",
      "train loss:0.05669521030491956\n",
      "train loss:0.03511548103104442\n",
      "train loss:0.007122933669919595\n",
      "train loss:0.023428291760200115\n",
      "train loss:0.014832562751608758\n",
      "train loss:0.03912072278534344\n",
      "train loss:0.017353671366481693\n",
      "train loss:0.020919100438564105\n",
      "train loss:0.05691619940460713\n",
      "train loss:0.060208859069748995\n",
      "train loss:0.01857779117499964\n",
      "train loss:0.018358705725742778\n",
      "train loss:0.01648160442178508\n",
      "train loss:0.023667193818888852\n",
      "train loss:0.04079211526179148\n",
      "train loss:0.02380574350863857\n",
      "train loss:0.03344048491115093\n",
      "train loss:0.03109203687688229\n",
      "train loss:0.025943720239601765\n",
      "train loss:0.02917605502180942\n",
      "train loss:0.006937850684702664\n",
      "train loss:0.011920493764040554\n",
      "train loss:0.06653107846307343\n",
      "train loss:0.011785694400592777\n",
      "train loss:0.053156886914627395\n",
      "train loss:0.021091207466039905\n",
      "train loss:0.053520109696756336\n",
      "train loss:0.05102752167401951\n",
      "train loss:0.29303209495126953\n",
      "train loss:0.04348332676373315\n",
      "train loss:0.022761433257437465\n",
      "train loss:0.03689993167711285\n",
      "train loss:0.0136500847582154\n",
      "train loss:0.021743113879844244\n",
      "train loss:0.043123092137957265\n",
      "train loss:0.02052734976442474\n",
      "train loss:0.04555108837414121\n",
      "train loss:0.022998466506790338\n",
      "train loss:0.0550218108803507\n",
      "train loss:0.008181906923662783\n",
      "train loss:0.008949032826522943\n",
      "train loss:0.04959184996289646\n",
      "train loss:0.06298952727511\n",
      "train loss:0.033908943153805576\n",
      "train loss:0.011644818120226157\n",
      "train loss:0.00954773561188553\n",
      "train loss:0.11730057799075588\n",
      "train loss:0.04661660560312574\n",
      "train loss:0.062252827636947804\n",
      "train loss:0.04261049664830062\n",
      "train loss:0.027670854807923376\n",
      "train loss:0.02364494171840727\n",
      "train loss:0.01637464297874885\n",
      "train loss:0.09627838984165804\n",
      "train loss:0.018000350689033898\n",
      "train loss:0.004209514807214505\n",
      "train loss:0.009565742206170036\n",
      "train loss:0.03726007140820413\n",
      "train loss:0.010797246141511521\n",
      "train loss:0.03333759790356867\n",
      "train loss:0.015708299571439696\n",
      "train loss:0.014930408318326838\n",
      "train loss:0.03056880549077975\n",
      "train loss:0.019102802606833117\n",
      "train loss:0.08598905621054558\n",
      "train loss:0.025504239994802124\n",
      "train loss:0.01983905141176085\n",
      "train loss:0.020545979309051082\n",
      "train loss:0.012320043496260664\n",
      "train loss:0.026269097997293097\n",
      "train loss:0.011160686809580454\n",
      "train loss:0.07465004548482859\n",
      "train loss:0.014557257884226966\n",
      "train loss:0.014589241509286296\n",
      "train loss:0.013361194851907674\n",
      "train loss:0.009971313455178467\n",
      "train loss:0.045779111129480035\n",
      "train loss:0.015189924694789277\n",
      "train loss:0.03587972378532207\n",
      "train loss:0.02577475413836907\n",
      "train loss:0.00557927054324951\n",
      "train loss:0.07116674056855599\n",
      "train loss:0.02405729884259923\n",
      "train loss:0.0200956558887155\n",
      "train loss:0.001994476764921383\n",
      "train loss:0.011010602994458113\n",
      "train loss:0.06118969340914002\n",
      "train loss:0.005228664658069393\n",
      "train loss:0.022701562103010123\n",
      "train loss:0.03183825931753506\n",
      "train loss:0.006485817905529759\n",
      "train loss:0.012073222551317623\n",
      "train loss:0.024501058930969703\n",
      "train loss:0.03759712560273626\n",
      "train loss:0.02068976786819319\n",
      "train loss:0.022784981874310953\n",
      "train loss:0.013326877684767124\n",
      "train loss:0.03289494371040214\n",
      "train loss:0.015830752818890396\n",
      "train loss:0.11691312074968754\n",
      "train loss:0.04216080310055781\n",
      "train loss:0.05474274224927903\n",
      "train loss:0.06260921212510702\n",
      "train loss:0.012329010203318454\n",
      "train loss:0.025082986599484772\n",
      "train loss:0.019890171379286576\n",
      "train loss:0.05363187765191386\n",
      "train loss:0.04850435092258226\n",
      "train loss:0.03339828864095936\n",
      "train loss:0.009357744618468397\n",
      "train loss:0.009948155582968396\n",
      "train loss:0.0600901605832071\n",
      "train loss:0.02509815172213406\n",
      "train loss:0.028767523242795665\n",
      "train loss:0.025667472672633248\n",
      "train loss:0.03168562904838059\n",
      "train loss:0.06455403311806987\n",
      "train loss:0.042041149070496726\n",
      "train loss:0.024138200279707287\n",
      "train loss:0.020624852496173726\n",
      "train loss:0.02217492967556836\n",
      "train loss:0.045513288597101374\n",
      "train loss:0.04268334508082176\n",
      "train loss:0.019558076596456645\n",
      "train loss:0.012958547423390468\n",
      "train loss:0.04308979608115166\n",
      "train loss:0.017102867106178605\n",
      "train loss:0.01907196280855931\n",
      "train loss:0.03447031394604534\n",
      "train loss:0.020936943012285394\n",
      "train loss:0.058250007200190315\n",
      "train loss:0.03175319262272675\n",
      "train loss:0.11315841629768096\n",
      "train loss:0.054942417082423416\n",
      "train loss:0.016254798213976178\n",
      "train loss:0.01939934394562512\n",
      "train loss:0.025641027769358334\n",
      "train loss:0.08311975830192138\n",
      "train loss:0.0332216639843096\n",
      "train loss:0.021790806177971418\n",
      "train loss:0.0108364809330308\n",
      "train loss:0.018926376667023296\n",
      "train loss:0.02534420437740487\n",
      "train loss:0.00785367496164906\n",
      "train loss:0.012398660709133001\n",
      "train loss:0.025273724550603477\n",
      "train loss:0.04469189521794423\n",
      "train loss:0.04289952898278306\n",
      "train loss:0.056810222988350326\n",
      "train loss:0.03342828757560998\n",
      "train loss:0.04917481911380258\n",
      "train loss:0.055829230415689185\n",
      "train loss:0.026786967231761896\n",
      "train loss:0.044564678536829126\n",
      "train loss:0.021392275673016867\n",
      "train loss:0.015294880522966276\n",
      "train loss:0.032185977004370335\n",
      "train loss:0.11671524794092467\n",
      "train loss:0.010020733367670115\n",
      "train loss:0.013564184599324751\n",
      "train loss:0.026173701250198834\n",
      "train loss:0.015118787715444086\n",
      "train loss:0.06792657541820257\n",
      "train loss:0.03215731837753985\n",
      "train loss:0.023433984358073784\n",
      "train loss:0.047809602659096485\n",
      "train loss:0.0376001459432854\n",
      "train loss:0.08232163691602469\n",
      "train loss:0.0091600681792536\n",
      "train loss:0.045658664818354816\n",
      "train loss:0.025427591843720166\n",
      "train loss:0.059035988875987676\n",
      "train loss:0.08163768449469275\n",
      "train loss:0.01228156949418584\n",
      "train loss:0.02189508149207245\n",
      "train loss:0.021407109359281966\n",
      "train loss:0.029075302115815736\n",
      "train loss:0.023007242305208134\n",
      "train loss:0.01851435475597991\n",
      "train loss:0.017796027461509836\n",
      "train loss:0.02841021334146771\n",
      "train loss:0.018874924704887542\n",
      "train loss:0.015691042664850382\n",
      "train loss:0.013380267137181354\n",
      "train loss:0.009323241611243297\n",
      "train loss:0.031416728062806515\n",
      "train loss:0.011233687666111974\n",
      "train loss:0.007026828480738633\n",
      "train loss:0.017588798849643806\n",
      "train loss:0.012782080113556667\n",
      "train loss:0.028101673503567612\n",
      "train loss:0.02229843149638027\n",
      "train loss:0.018659161137528482\n",
      "train loss:0.016487935448542216\n",
      "train loss:0.017828541267844503\n",
      "train loss:0.03457673305238196\n",
      "train loss:0.014934120082619795\n",
      "train loss:0.048990723337697294\n",
      "train loss:0.005458524956057468\n",
      "train loss:0.03310695148956737\n",
      "train loss:0.07717979457104641\n",
      "train loss:0.009253999003339824\n",
      "train loss:0.022241155846666975\n",
      "train loss:0.04348348039716667\n",
      "train loss:0.03730510092473856\n",
      "train loss:0.03399884913882555\n",
      "train loss:0.03237732670709389\n",
      "train loss:0.034837183968545425\n",
      "train loss:0.010084472590384691\n",
      "train loss:0.013795585503823971\n",
      "train loss:0.0696731938718768\n",
      "train loss:0.01917844718440886\n",
      "train loss:0.029474050751273073\n",
      "train loss:0.01294892070088137\n",
      "train loss:0.07306811097311075\n",
      "train loss:0.025975545197658816\n",
      "train loss:0.026870959914258873\n",
      "train loss:0.05534266755160134\n",
      "train loss:0.021030274830377085\n",
      "train loss:0.026552225830298433\n",
      "train loss:0.025792528074238347\n",
      "train loss:0.011130727469661903\n",
      "train loss:0.018586579484175057\n",
      "train loss:0.07291947087262775\n",
      "train loss:0.04030646760239956\n",
      "train loss:0.02537294437346208\n",
      "train loss:0.009437991837089599\n",
      "train loss:0.02082132129309341\n",
      "train loss:0.009797653545312958\n",
      "train loss:0.012387240046572576\n",
      "train loss:0.012025718161242056\n",
      "train loss:0.010189361416295879\n",
      "train loss:0.0058930172553577605\n",
      "train loss:0.03850143487168465\n",
      "train loss:0.015930871463110233\n",
      "train loss:0.03162198933527915\n",
      "train loss:0.01445599880041372\n",
      "train loss:0.07306952821222946\n",
      "train loss:0.06557072131571791\n",
      "train loss:0.01104055208075736\n",
      "train loss:0.010474532391029988\n",
      "train loss:0.030471273321459883\n",
      "train loss:0.0070562211500233216\n",
      "train loss:0.0099948628470839\n",
      "train loss:0.007791249207214481\n",
      "train loss:0.019083683463784498\n",
      "train loss:0.00785935397961336\n",
      "train loss:0.012849446568312366\n",
      "train loss:0.11873775482312465\n",
      "train loss:0.0822796297160757\n",
      "train loss:0.010568931963116434\n",
      "train loss:0.011093548430089363\n",
      "train loss:0.05396147819150537\n",
      "train loss:0.036100343344223754\n",
      "train loss:0.021697723329424986\n",
      "train loss:0.017222802427442757\n",
      "train loss:0.0495285384120523\n",
      "train loss:0.08136737620266637\n",
      "train loss:0.05246890103728547\n",
      "train loss:0.09544207947552896\n",
      "train loss:0.1917899739647033\n",
      "train loss:0.022843428729806017\n",
      "train loss:0.034436906274265516\n",
      "train loss:0.06104258687306162\n",
      "train loss:0.04971469801422988\n",
      "train loss:0.05702460517429293\n",
      "train loss:0.031313700599534755\n",
      "train loss:0.035833239889194485\n",
      "train loss:0.06855461693263533\n",
      "train loss:0.06589112539045637\n",
      "train loss:0.027841390152217645\n",
      "train loss:0.05808830250447987\n",
      "train loss:0.02822908417814289\n",
      "train loss:0.020561914871701906\n",
      "train loss:0.027374462067795983\n",
      "train loss:0.025588082116621776\n",
      "train loss:0.026697890941919877\n",
      "train loss:0.05116324917305817\n",
      "train loss:0.009497447287818275\n",
      "train loss:0.03191943059868414\n",
      "train loss:0.06492703231615592\n",
      "train loss:0.1160705665628698\n",
      "train loss:0.01702830382323093\n",
      "train loss:0.03536620883123784\n",
      "train loss:0.03766957234286868\n",
      "train loss:0.009590896517963637\n",
      "train loss:0.011704500940280931\n",
      "train loss:0.02729657342955066\n",
      "train loss:0.04737053850096126\n",
      "train loss:0.015465887937352487\n",
      "train loss:0.055970067861612495\n",
      "train loss:0.03359144888471659\n",
      "train loss:0.015127014551799505\n",
      "train loss:0.04635384208621498\n",
      "train loss:0.02992841401683724\n",
      "train loss:0.01960755434260472\n",
      "train loss:0.011345255778983463\n",
      "train loss:0.017468539660294487\n",
      "train loss:0.019166866628599427\n",
      "train loss:0.06507790379506034\n",
      "train loss:0.0408605536382069\n",
      "train loss:0.02507022023492181\n",
      "train loss:0.06929222136971867\n",
      "train loss:0.07367120611846108\n",
      "train loss:0.12330296735301652\n",
      "train loss:0.023312929247082517\n",
      "train loss:0.051875194818198925\n",
      "train loss:0.02562422137230604\n",
      "train loss:0.035653435115188536\n",
      "train loss:0.04037946693696916\n",
      "train loss:0.029011907275081284\n",
      "train loss:0.009243042751065404\n",
      "train loss:0.009909557815039167\n",
      "train loss:0.03709393296424806\n",
      "train loss:0.06106635372113603\n",
      "train loss:0.057559292882753736\n",
      "train loss:0.040511683244111606\n",
      "train loss:0.007738546182250082\n",
      "train loss:0.011218760576683438\n",
      "train loss:0.011099225260107799\n",
      "train loss:0.06679354010806664\n",
      "train loss:0.03818896999189531\n",
      "train loss:0.10955275630685414\n",
      "train loss:0.008868150553987937\n",
      "train loss:0.02343265071041226\n",
      "train loss:0.055717809235791185\n",
      "train loss:0.03505033107855403\n",
      "train loss:0.1301820804056909\n",
      "train loss:0.03867953739494958\n",
      "train loss:0.03418053336166085\n",
      "train loss:0.013784531374256418\n",
      "train loss:0.05314292883499722\n",
      "train loss:0.07578503347991246\n",
      "train loss:0.045327507629967005\n",
      "train loss:0.04374156151790408\n",
      "train loss:0.03148831621134992\n",
      "train loss:0.013069139048804093\n",
      "train loss:0.04793398580034391\n",
      "train loss:0.015191285315208308\n",
      "train loss:0.024995196786701954\n",
      "train loss:0.01956006872878868\n",
      "train loss:0.04655692283644946\n",
      "train loss:0.046021504172252724\n",
      "train loss:0.06584523670224858\n",
      "train loss:0.0341661102718676\n",
      "train loss:0.01979970434814768\n",
      "train loss:0.011354623706347756\n",
      "train loss:0.036796481834094\n",
      "train loss:0.007114779427134367\n",
      "train loss:0.006282640033104981\n",
      "train loss:0.02228179985906471\n",
      "train loss:0.015192724259335535\n",
      "train loss:0.012051740611042227\n",
      "train loss:0.018021481737924342\n",
      "train loss:0.025870498639047158\n",
      "train loss:0.14623346746236549\n",
      "train loss:0.0071154597493113415\n",
      "train loss:0.049374387150434906\n",
      "train loss:0.02019873082654083\n",
      "train loss:0.007120446369740433\n",
      "train loss:0.01771234737201949\n",
      "train loss:0.01321148719568732\n",
      "train loss:0.018097520973961143\n",
      "train loss:0.037809035901088056\n",
      "train loss:0.02024937694985081\n",
      "train loss:0.01396357345695101\n",
      "train loss:0.008076903036019756\n",
      "train loss:0.011050501326275455\n",
      "train loss:0.026728510321783493\n",
      "train loss:0.030988613765377607\n",
      "train loss:0.002856026334488204\n",
      "train loss:0.035269728268787444\n",
      "train loss:0.03568949617533005\n",
      "train loss:0.02832105767112515\n",
      "train loss:0.09728542554958931\n",
      "train loss:0.016790217936330464\n",
      "train loss:0.03852218044174966\n",
      "train loss:0.05027967975700364\n",
      "train loss:0.016556861904431387\n",
      "train loss:0.01192011289589081\n",
      "train loss:0.01662932230153148\n",
      "train loss:0.01413202278587692\n",
      "train loss:0.01691850750844639\n",
      "train loss:0.04555559519246815\n",
      "train loss:0.013717466015930541\n",
      "train loss:0.024012774816451733\n",
      "train loss:0.020743422087524985\n",
      "train loss:0.02421431819630706\n",
      "train loss:0.013433270280033638\n",
      "train loss:0.041745849494947994\n",
      "train loss:0.0061000165805912545\n",
      "train loss:0.14004729653188636\n",
      "train loss:0.02276404544718698\n",
      "train loss:0.02484172783221422\n",
      "train loss:0.011823084407719764\n",
      "train loss:0.020824910304646938\n",
      "train loss:0.027854078141170066\n",
      "train loss:0.01838486835829124\n",
      "train loss:0.024704996771104583\n",
      "train loss:0.01593038041981992\n",
      "train loss:0.025715423949934802\n",
      "train loss:0.050066979147554605\n",
      "train loss:0.008256624314574142\n",
      "train loss:0.060623651200071106\n",
      "train loss:0.03593816422023886\n",
      "train loss:0.017497561599403452\n",
      "train loss:0.026220543368729133\n",
      "train loss:0.0709678525467661\n",
      "train loss:0.02848830547057001\n",
      "train loss:0.052225087625833166\n",
      "train loss:0.02174137630465974\n",
      "train loss:0.055966503001587294\n",
      "train loss:0.06749491416327708\n",
      "train loss:0.022891079812211196\n",
      "train loss:0.060863898144308454\n",
      "train loss:0.016840567691248646\n",
      "train loss:0.007526418666395377\n",
      "train loss:0.040232699602697305\n",
      "train loss:0.03881855810440317\n",
      "train loss:0.010542177628499788\n",
      "train loss:0.013184599711555514\n",
      "train loss:0.008329454236152578\n",
      "train loss:0.021724333054541002\n",
      "train loss:0.03541215824436273\n",
      "train loss:0.023430981783448983\n",
      "train loss:0.1305001399101187\n",
      "train loss:0.13592397221829652\n",
      "train loss:0.03628063303062465\n",
      "train loss:0.026365633020445617\n",
      "train loss:0.040165231612040796\n",
      "train loss:0.032785364296563646\n",
      "train loss:0.018201778206223443\n",
      "train loss:0.012589219093365864\n",
      "train loss:0.040433273768149064\n",
      "train loss:0.1807782969035424\n",
      "train loss:0.031807762069694684\n",
      "train loss:0.014624176226061207\n",
      "train loss:0.020778670623043182\n",
      "train loss:0.032674577468543416\n",
      "train loss:0.007421749479591319\n",
      "train loss:0.033411773208513434\n",
      "train loss:0.034586424693506014\n",
      "train loss:0.007010898417524063\n",
      "train loss:0.02502751573831337\n",
      "train loss:0.04211555011870322\n",
      "train loss:0.01773535598217025\n",
      "train loss:0.02116503903207386\n",
      "train loss:0.05070631923519693\n",
      "train loss:0.09809884952805226\n",
      "train loss:0.0055160126066251414\n",
      "train loss:0.016221127060315893\n",
      "train loss:0.01599766042174108\n",
      "train loss:0.015081523581423761\n",
      "train loss:0.032982970936749116\n",
      "train loss:0.0707819967283895\n",
      "train loss:0.017388941406352273\n",
      "train loss:0.01890864765398978\n",
      "train loss:0.011519610898847095\n",
      "train loss:0.021725760365041164\n",
      "train loss:0.03862446969858779\n",
      "train loss:0.04599582340374011\n",
      "train loss:0.02690388741893015\n",
      "train loss:0.01708115364608672\n",
      "train loss:0.03602494293463967\n",
      "train loss:0.03467183032686855\n",
      "train loss:0.025345548248887528\n",
      "train loss:0.010952279914316082\n",
      "train loss:0.015272267048335196\n",
      "train loss:0.013865225562914923\n",
      "train loss:0.02361952575378983\n",
      "train loss:0.031526548033582284\n",
      "train loss:0.0322467244547919\n",
      "train loss:0.019635217978803715\n",
      "train loss:0.010233594138448347\n",
      "train loss:0.038350862434594075\n",
      "train loss:0.03201800964944094\n",
      "train loss:0.028583153641745532\n",
      "train loss:0.00636125484774949\n",
      "train loss:0.003577011917250988\n",
      "train loss:0.013111832043628415\n",
      "train loss:0.03643242481690445\n",
      "train loss:0.01669565899752054\n",
      "train loss:0.0885645486130911\n",
      "train loss:0.06017305118733882\n",
      "train loss:0.015532825240373855\n",
      "train loss:0.02481824345257404\n",
      "train loss:0.020196927277265096\n",
      "train loss:0.05878715875609296\n",
      "train loss:0.03628554474066124\n",
      "train loss:0.04859544123749237\n",
      "train loss:0.011707000198712039\n",
      "train loss:0.058637410786423115\n",
      "train loss:0.03227922565956527\n",
      "train loss:0.0032204231716973885\n",
      "train loss:0.019885908314265396\n",
      "train loss:0.007539871416806802\n",
      "train loss:0.019027481346254108\n",
      "train loss:0.0864602971362693\n",
      "train loss:0.005837023965902152\n",
      "train loss:0.014366159366894317\n",
      "train loss:0.016312513504986116\n",
      "train loss:0.035250672843227936\n",
      "train loss:0.010075922683846472\n",
      "train loss:0.004065435001917342\n",
      "train loss:0.030279993047210664\n",
      "train loss:0.01890677675154822\n",
      "train loss:0.03980479530347125\n",
      "train loss:0.027720856404552765\n",
      "train loss:0.027144906930203362\n",
      "train loss:0.05604273216214982\n",
      "train loss:0.010259755101768997\n",
      "train loss:0.02962336328128927\n",
      "train loss:0.045617816967726404\n",
      "train loss:0.003784787803034339\n",
      "train loss:0.015883488427224823\n",
      "train loss:0.016326003830774826\n",
      "train loss:0.01810843711995953\n",
      "train loss:0.06050526547466099\n",
      "train loss:0.020465817758083506\n",
      "train loss:0.021619896109135915\n",
      "train loss:0.07426044879692364\n",
      "train loss:0.11273921855475408\n",
      "train loss:0.036391533458583876\n",
      "train loss:0.022505484868501232\n",
      "train loss:0.012467585754464626\n",
      "train loss:0.026074286216130407\n",
      "train loss:0.008164740570156878\n",
      "train loss:0.022503496562876077\n",
      "train loss:0.042540948807104566\n",
      "train loss:0.004661972941679609\n",
      "train loss:0.03589309104883117\n",
      "train loss:0.021778813919711874\n",
      "train loss:0.009906092900248395\n",
      "=== epoch:6, train acc:0.987, test acc:0.988 ===\n",
      "train loss:0.05391684560908824\n",
      "train loss:0.009512421182096614\n",
      "train loss:0.02059528683375283\n",
      "train loss:0.019334669144557903\n",
      "train loss:0.0621793590949193\n",
      "train loss:0.034740464298198985\n",
      "train loss:0.031123857435472257\n",
      "train loss:0.018475238663663247\n",
      "train loss:0.03622481315110105\n",
      "train loss:0.007717124763419855\n",
      "train loss:0.019702553996924554\n",
      "train loss:0.017065246095288252\n",
      "train loss:0.04689783986982597\n",
      "train loss:0.08449255743155361\n",
      "train loss:0.03209396074521598\n",
      "train loss:0.011861351410086858\n",
      "train loss:0.007970465481227582\n",
      "train loss:0.025175916407585656\n",
      "train loss:0.030259145885249583\n",
      "train loss:0.01935703454717142\n",
      "train loss:0.09978691641590819\n",
      "train loss:0.06333926699515319\n",
      "train loss:0.011337429919438681\n",
      "train loss:0.0066746961815523495\n",
      "train loss:0.012053953339055354\n",
      "train loss:0.09246758990572093\n",
      "train loss:0.041286071963562855\n",
      "train loss:0.012799009283137597\n",
      "train loss:0.0212670541766337\n",
      "train loss:0.026433535234635337\n",
      "train loss:0.01935442457901738\n",
      "train loss:0.053274904988122825\n",
      "train loss:0.01993931556607376\n",
      "train loss:0.04221666135771316\n",
      "train loss:0.00890156377554341\n",
      "train loss:0.044457054321707504\n",
      "train loss:0.06869397470521965\n",
      "train loss:0.024888257726707025\n",
      "train loss:0.0175971173337567\n",
      "train loss:0.02024670976381257\n",
      "train loss:0.006129192053755828\n",
      "train loss:0.005357935313832273\n",
      "train loss:0.023138167825851586\n",
      "train loss:0.004370949116514244\n",
      "train loss:0.02536089037392505\n",
      "train loss:0.02401685885955415\n",
      "train loss:0.027985009816274248\n",
      "train loss:0.01184120972512815\n",
      "train loss:0.011631046152760411\n",
      "train loss:0.027995510032018722\n",
      "train loss:0.015721268036270913\n",
      "train loss:0.0070064629221873505\n",
      "train loss:0.012661502345097045\n",
      "train loss:0.021833790709417968\n",
      "train loss:0.03994143766034547\n",
      "train loss:0.03807935884169258\n",
      "train loss:0.011333616097124941\n",
      "train loss:0.06796772369417811\n",
      "train loss:0.016334848426913177\n",
      "train loss:0.025890724287845456\n",
      "train loss:0.011619181044243454\n",
      "train loss:0.016205938319866654\n",
      "train loss:0.01757320104169787\n",
      "train loss:0.012325052893297408\n",
      "train loss:0.05443919088195887\n",
      "train loss:0.010294070566572007\n",
      "train loss:0.0487430553103202\n",
      "train loss:0.01486963123384957\n",
      "train loss:0.013133213040899414\n",
      "train loss:0.010871733181113447\n",
      "train loss:0.012285418963282944\n",
      "train loss:0.027399431384487954\n",
      "train loss:0.015749752366405922\n",
      "train loss:0.01626277433146951\n",
      "train loss:0.030478167489098363\n",
      "train loss:0.0257158319594639\n",
      "train loss:0.00585393309586864\n",
      "train loss:0.010516419274352817\n",
      "train loss:0.011100774185288374\n",
      "train loss:0.007886543477673973\n",
      "train loss:0.013695077246588203\n",
      "train loss:0.025917790664289394\n",
      "train loss:0.03440476155973029\n",
      "train loss:0.01696322960517492\n",
      "train loss:0.010172778133531511\n",
      "train loss:0.00784267626122768\n",
      "train loss:0.004090200608551457\n",
      "train loss:0.0029628762475006386\n",
      "train loss:0.007059636638872641\n",
      "train loss:0.0248902880872786\n",
      "train loss:0.03544503303995607\n",
      "train loss:0.05060387933922596\n",
      "train loss:0.006332399301068682\n",
      "train loss:0.008868594753711281\n",
      "train loss:0.035310611059222535\n",
      "train loss:0.04382966156134453\n",
      "train loss:0.004837487535272925\n",
      "train loss:0.0353478097620148\n",
      "train loss:0.0203474141894442\n",
      "train loss:0.007182944428684771\n",
      "train loss:0.01669497851071544\n",
      "train loss:0.027704202708026503\n",
      "train loss:0.01108714536542576\n",
      "train loss:0.056490688060170616\n",
      "train loss:0.008429387695646632\n",
      "train loss:0.0035896364133151458\n",
      "train loss:0.008048647662645753\n",
      "train loss:0.035916202472435634\n",
      "train loss:0.04675686838689946\n",
      "train loss:0.04471979258116449\n",
      "train loss:0.016606268987329306\n",
      "train loss:0.05368007875277859\n",
      "train loss:0.028877300983472208\n",
      "train loss:0.005873439274199413\n",
      "train loss:0.022970165759055604\n",
      "train loss:0.017843362803320658\n",
      "train loss:0.04546956046759797\n",
      "train loss:0.01283697890728436\n",
      "train loss:0.03505212911332845\n",
      "train loss:0.02209144288171274\n",
      "train loss:0.006140449536251928\n",
      "train loss:0.014617804779079051\n",
      "train loss:0.0383878717172813\n",
      "train loss:0.004976941603834257\n",
      "train loss:0.014253502209380193\n",
      "train loss:0.02379260125101693\n",
      "train loss:0.009053812212583983\n",
      "train loss:0.027122673690410402\n",
      "train loss:0.034270223623742144\n",
      "train loss:0.004751941900343878\n",
      "train loss:0.009054100862296908\n",
      "train loss:0.013574922196428895\n",
      "train loss:0.04465840042244284\n",
      "train loss:0.06931991146260351\n",
      "train loss:0.006388484903454884\n",
      "train loss:0.016335647254177202\n",
      "train loss:0.019233622560738534\n",
      "train loss:0.011297024603768795\n",
      "train loss:0.019100630560079192\n",
      "train loss:0.021090959357132205\n",
      "train loss:0.008614246504650971\n",
      "train loss:0.002941723426576902\n",
      "train loss:0.005804416688410066\n",
      "train loss:0.01908684992214822\n",
      "train loss:0.0969671776117098\n",
      "train loss:0.008503027978053843\n",
      "train loss:0.01234122861958033\n",
      "train loss:0.03125414652214037\n",
      "train loss:0.019343759806746342\n",
      "train loss:0.004005967881271679\n",
      "train loss:0.04258473680414049\n",
      "train loss:0.018682992092777618\n",
      "train loss:0.011551264428112037\n",
      "train loss:0.012626404432830281\n",
      "train loss:0.014032079357151239\n",
      "train loss:0.020969638546955216\n",
      "train loss:0.03920317696811793\n",
      "train loss:0.007009586945004829\n",
      "train loss:0.06761126570602975\n",
      "train loss:0.02280727379223348\n",
      "train loss:0.01997083989502988\n",
      "train loss:0.07059666373870048\n",
      "train loss:0.011473737607493532\n",
      "train loss:0.019782531918733456\n",
      "train loss:0.0049146115415723965\n",
      "train loss:0.02822071646783807\n",
      "train loss:0.011282229087507891\n",
      "train loss:0.015169143869685786\n",
      "train loss:0.018302428810593675\n",
      "train loss:0.00202567594937802\n",
      "train loss:0.026292376844889342\n",
      "train loss:0.005793180452007615\n",
      "train loss:0.012451714604790977\n",
      "train loss:0.021975108585465877\n",
      "train loss:0.016360329112368538\n",
      "train loss:0.03164837944322836\n",
      "train loss:0.054422311536848476\n",
      "train loss:0.011548943018178038\n",
      "train loss:0.01596057930191183\n",
      "train loss:0.06597442147001936\n",
      "train loss:0.05834361973613951\n",
      "train loss:0.06970374157006907\n",
      "train loss:0.084796163254656\n",
      "train loss:0.020363351988884112\n",
      "train loss:0.019156796784572758\n",
      "train loss:0.008213976333140582\n",
      "train loss:0.0136146078224918\n",
      "train loss:0.025946596703974225\n",
      "train loss:0.05261338297578119\n",
      "train loss:0.07862718683174476\n",
      "train loss:0.05242621583489887\n",
      "train loss:0.015176419626452595\n",
      "train loss:0.008960854663801003\n",
      "train loss:0.05126697869883616\n",
      "train loss:0.017477092906019738\n",
      "train loss:0.011845479279682889\n",
      "train loss:0.04989345390764402\n",
      "train loss:0.0722343300644667\n",
      "train loss:0.04521453685230657\n",
      "train loss:0.017453010720969872\n",
      "train loss:0.0465921485193506\n",
      "train loss:0.017528656406631597\n",
      "train loss:0.02288551656525977\n",
      "train loss:0.019163714124526256\n",
      "train loss:0.04186629858156962\n",
      "train loss:0.009243297234049425\n",
      "train loss:0.03097465783045448\n",
      "train loss:0.04092023391123302\n",
      "train loss:0.05315747764134923\n",
      "train loss:0.0313224526517999\n",
      "train loss:0.017796437364253032\n",
      "train loss:0.01990817076162716\n",
      "train loss:0.013042295018640356\n",
      "train loss:0.049457930517114425\n",
      "train loss:0.010984777821971426\n",
      "train loss:0.054811210410491536\n",
      "train loss:0.020211580656229557\n",
      "train loss:0.0065456640133336776\n",
      "train loss:0.02917932845578267\n",
      "train loss:0.0291895701914562\n",
      "train loss:0.01594458917404722\n",
      "train loss:0.015734166478604077\n",
      "train loss:0.009220365085688275\n",
      "train loss:0.12217801341481335\n",
      "train loss:0.027213129956113545\n",
      "train loss:0.025864756631813857\n",
      "train loss:0.01656573622351283\n",
      "train loss:0.012518078616140762\n",
      "train loss:0.027536479075558184\n",
      "train loss:0.01784672862838213\n",
      "train loss:0.005747771133214189\n",
      "train loss:0.010580824637154772\n",
      "train loss:0.01697126550418695\n",
      "train loss:0.019697716627939125\n",
      "train loss:0.015170410765168769\n",
      "train loss:0.01574157581698514\n",
      "train loss:0.028164498583142118\n",
      "train loss:0.023607105342901398\n",
      "train loss:0.00905515152458441\n",
      "train loss:0.01578687606552659\n",
      "train loss:0.03138916479025297\n",
      "train loss:0.011396134649737107\n",
      "train loss:0.020447149617431114\n",
      "train loss:0.040480110794067735\n",
      "train loss:0.0036735129042137953\n",
      "train loss:0.025031373504922065\n",
      "train loss:0.03247939033670415\n",
      "train loss:0.015388481299559187\n",
      "train loss:0.023579024950124337\n",
      "train loss:0.004516382499326551\n",
      "train loss:0.013401850505435275\n",
      "train loss:0.006764771360138814\n",
      "train loss:0.018118423205615432\n",
      "train loss:0.08114191635392012\n",
      "train loss:0.03511089387924998\n",
      "train loss:0.031966560289435016\n",
      "train loss:0.043463466681593725\n",
      "train loss:0.0058394074302575375\n",
      "train loss:0.07855747414326011\n",
      "train loss:0.005080202479853976\n",
      "train loss:0.010560028369856214\n",
      "train loss:0.01229980635394738\n",
      "train loss:0.01290234483449158\n",
      "train loss:0.023941062127643336\n",
      "train loss:0.013541726685514873\n",
      "train loss:0.034111482302972916\n",
      "train loss:0.0027083825042636743\n",
      "train loss:0.022777746856593107\n",
      "train loss:0.010077400612657666\n",
      "train loss:0.009310844050390234\n",
      "train loss:0.0522291053693503\n",
      "train loss:0.007190778891691694\n",
      "train loss:0.033425253062927734\n",
      "train loss:0.006110529309871017\n",
      "train loss:0.018042146971513153\n",
      "train loss:0.00922883634194798\n",
      "train loss:0.03080690945514153\n",
      "train loss:0.0018186963316143728\n",
      "train loss:0.013849084273966826\n",
      "train loss:0.007562500668248335\n",
      "train loss:0.029782842388071936\n",
      "train loss:0.02425432140353572\n",
      "train loss:0.022306159826191104\n",
      "train loss:0.024941659966260684\n",
      "train loss:0.0023641626792045957\n",
      "train loss:0.009035544546052716\n",
      "train loss:0.015954826936329443\n",
      "train loss:0.007942186009304153\n",
      "train loss:0.027931697560477656\n",
      "train loss:0.007824461577625851\n",
      "train loss:0.03989417746975401\n",
      "train loss:0.016221541800368017\n",
      "train loss:0.004538096790015036\n",
      "train loss:0.0016560081814819697\n",
      "train loss:0.04263902408985705\n",
      "train loss:0.005755002256293138\n",
      "train loss:0.020957853265137036\n",
      "train loss:0.005168955174675363\n",
      "train loss:0.043938855083692226\n",
      "train loss:0.015958401261899433\n",
      "train loss:0.0011338788803312014\n",
      "train loss:0.010579025849824673\n",
      "train loss:0.0245854039260524\n",
      "train loss:0.0320110434764907\n",
      "train loss:0.009395463734402181\n",
      "train loss:0.016306425760172962\n",
      "train loss:0.026251113726983303\n",
      "train loss:0.0018549772693391032\n",
      "train loss:0.07316626330324326\n",
      "train loss:0.010939184779111733\n",
      "train loss:0.011598264924190837\n",
      "train loss:0.017519368146073622\n",
      "train loss:0.012577514335269315\n",
      "train loss:0.043127868242563636\n",
      "train loss:0.046304601949665375\n",
      "train loss:0.028457773871910397\n",
      "train loss:0.04011958913364906\n",
      "train loss:0.04334490192363259\n",
      "train loss:0.020618656016480306\n",
      "train loss:0.0042909761654755935\n",
      "train loss:0.009131129206663749\n",
      "train loss:0.03349291249859431\n",
      "train loss:0.018410816986194966\n",
      "train loss:0.012921257605554692\n",
      "train loss:0.013923195044422165\n",
      "train loss:0.03730688812560641\n",
      "train loss:0.028434133814537875\n",
      "train loss:0.0191718188002683\n",
      "train loss:0.026238337539734494\n",
      "train loss:0.004389102065245097\n",
      "train loss:0.012336618803019254\n",
      "train loss:0.1830835493171804\n",
      "train loss:0.044669086314375855\n",
      "train loss:0.024225534601649546\n",
      "train loss:0.053656659667240716\n",
      "train loss:0.020855366874445544\n",
      "train loss:0.015000967418826219\n",
      "train loss:0.002104545321596529\n",
      "train loss:0.017573592052782687\n",
      "train loss:0.013758235022181773\n",
      "train loss:0.026022380686766523\n",
      "train loss:0.006921884992781605\n",
      "train loss:0.015819750995030288\n",
      "train loss:0.015416531364250806\n",
      "train loss:0.1670253880151246\n",
      "train loss:0.046878260652566374\n",
      "train loss:0.08352165389885988\n",
      "train loss:0.06701178776611326\n",
      "train loss:0.007858814005300982\n",
      "train loss:0.05405144918521172\n",
      "train loss:0.020062155413474333\n",
      "train loss:0.00832966162796483\n",
      "train loss:0.033657658775492615\n",
      "train loss:0.01413518601309503\n",
      "train loss:0.00994471671045592\n",
      "train loss:0.012677358643811252\n",
      "train loss:0.035696316779101414\n",
      "train loss:0.025925164070276997\n",
      "train loss:0.016848263193504277\n",
      "train loss:0.013399995366510603\n",
      "train loss:0.013797211245716489\n",
      "train loss:0.009259234137236375\n",
      "train loss:0.009875553518201545\n",
      "train loss:0.007707685901650333\n",
      "train loss:0.004018829012557783\n",
      "train loss:0.04219400415137603\n",
      "train loss:0.018216795358406958\n",
      "train loss:0.0370435902723464\n",
      "train loss:0.015877673493282073\n",
      "train loss:0.027264893117341208\n",
      "train loss:0.02836457050257633\n",
      "train loss:0.01750186917109735\n",
      "train loss:0.0028394137276310165\n",
      "train loss:0.008881276617898082\n",
      "train loss:0.022042522359148012\n",
      "train loss:0.024036804168689162\n",
      "train loss:0.009384778836845187\n",
      "train loss:0.010818616499211498\n",
      "train loss:0.0073766938463983075\n",
      "train loss:0.029980484770427848\n",
      "train loss:0.005771247145091769\n",
      "train loss:0.018088857962011835\n",
      "train loss:0.017574739647652337\n",
      "train loss:0.013676567530004933\n",
      "train loss:0.03745818547606616\n",
      "train loss:0.04477828167298352\n",
      "train loss:0.013068774359869646\n",
      "train loss:0.0539235908732727\n",
      "train loss:0.005037897298086796\n",
      "train loss:0.02250676779945661\n",
      "train loss:0.057919322391822854\n",
      "train loss:0.009690372623323834\n",
      "train loss:0.02803073512284394\n",
      "train loss:0.004887594022507928\n",
      "train loss:0.018442859214559924\n",
      "train loss:0.009238221118863936\n",
      "train loss:0.02365467914423124\n",
      "train loss:0.006035374060576963\n",
      "train loss:0.05279774521907063\n",
      "train loss:0.013108738936724356\n",
      "train loss:0.009502706431019603\n",
      "train loss:0.013046947415603255\n",
      "train loss:0.014723968318337301\n",
      "train loss:0.037248640546181676\n",
      "train loss:0.014257427132001888\n",
      "train loss:0.006865029781047583\n",
      "train loss:0.022146418404255602\n",
      "train loss:0.1004455199388247\n",
      "train loss:0.010943467231563612\n",
      "train loss:0.007961229744315292\n",
      "train loss:0.007009655608238189\n",
      "train loss:0.022427867847555048\n",
      "train loss:0.017903104205755267\n",
      "train loss:0.04087579617028195\n",
      "train loss:0.02447417119510943\n",
      "train loss:0.027557523622404392\n",
      "train loss:0.016512141842120374\n",
      "train loss:0.012994528178878426\n",
      "train loss:0.034016138789726195\n",
      "train loss:0.004997419245903708\n",
      "train loss:0.00559462710986627\n",
      "train loss:0.004717275435210726\n",
      "train loss:0.0059383491530174805\n",
      "train loss:0.02690193490953945\n",
      "train loss:0.004658465802340229\n",
      "train loss:0.020006498404104435\n",
      "train loss:0.13667932602135224\n",
      "train loss:0.026826680457836063\n",
      "train loss:0.005279755351791483\n",
      "train loss:0.021373286449371448\n",
      "train loss:0.02426054821476166\n",
      "train loss:0.01330468557883838\n",
      "train loss:0.019334206839794543\n",
      "train loss:0.009147592376762674\n",
      "train loss:0.01344704711820699\n",
      "train loss:0.024810827229195226\n",
      "train loss:0.010887299635570287\n",
      "train loss:0.0030283177915392734\n",
      "train loss:0.0124335729655394\n",
      "train loss:0.01769432487366738\n",
      "train loss:0.045195820577453645\n",
      "train loss:0.017737155030592765\n",
      "train loss:0.03115868184478602\n",
      "train loss:0.007210586968968086\n",
      "train loss:0.038206824281122834\n",
      "train loss:0.007133966179500344\n",
      "train loss:0.029090212929734963\n",
      "train loss:0.010430077207093246\n",
      "train loss:0.012680464108264691\n",
      "train loss:0.04084757887330937\n",
      "train loss:0.007564154901609076\n",
      "train loss:0.013470229744055491\n",
      "train loss:0.007851669494714855\n",
      "train loss:0.010428392382577615\n",
      "train loss:0.04724529650205796\n",
      "train loss:0.012254018399313182\n",
      "train loss:0.04158199705826424\n",
      "train loss:0.009269384340541953\n",
      "train loss:0.01722187483091254\n",
      "train loss:0.021297233439090048\n",
      "train loss:0.00222389434118389\n",
      "train loss:0.02276915185038576\n",
      "train loss:0.01203339079980116\n",
      "train loss:0.02385732029867921\n",
      "train loss:0.02686355581037687\n",
      "train loss:0.009226493692798883\n",
      "train loss:0.013460630071889062\n",
      "train loss:0.019790143509990386\n",
      "train loss:0.026262412356891925\n",
      "train loss:0.023392270033241583\n",
      "train loss:0.010352816219028583\n",
      "train loss:0.03883169881541449\n",
      "train loss:0.014500807011690537\n",
      "train loss:0.03217977935426017\n",
      "train loss:0.007140256177569689\n",
      "train loss:0.004994229304372913\n",
      "train loss:0.1667618902868353\n",
      "train loss:0.025355082394465468\n",
      "train loss:0.0615956827635504\n",
      "train loss:0.03848203712949075\n",
      "train loss:0.10938504467897942\n",
      "train loss:0.020843435936139763\n",
      "train loss:0.016549996730834526\n",
      "train loss:0.012864580632054267\n",
      "train loss:0.023442519167736476\n",
      "train loss:0.04689813563092216\n",
      "train loss:0.020223319478519537\n",
      "train loss:0.0018351354963980152\n",
      "train loss:0.00709032413034909\n",
      "train loss:0.008592078063241216\n",
      "train loss:0.005406928622156132\n",
      "train loss:0.004128831761168906\n",
      "train loss:0.026423622511039503\n",
      "train loss:0.01144238617635557\n",
      "train loss:0.006849096863039806\n",
      "train loss:0.024116013448709773\n",
      "train loss:0.02213148259899652\n",
      "train loss:0.01818671145583294\n",
      "train loss:0.030524622350041327\n",
      "train loss:0.0048247464293009554\n",
      "train loss:0.030844858139154357\n",
      "train loss:0.08570609030604319\n",
      "train loss:0.007605143892419316\n",
      "train loss:0.030902889215650257\n",
      "train loss:0.015936003398461497\n",
      "train loss:0.054223917485809923\n",
      "train loss:0.024099851424972032\n",
      "train loss:0.018971149566917266\n",
      "train loss:0.08009514900377775\n",
      "train loss:0.01378453666655347\n",
      "train loss:0.010633623848687011\n",
      "train loss:0.03702418321572953\n",
      "train loss:0.013182550346012473\n",
      "train loss:0.02610101493919066\n",
      "train loss:0.005531582407816699\n",
      "train loss:0.013348235726919115\n",
      "train loss:0.009780733941619579\n",
      "train loss:0.0051739876921752135\n",
      "train loss:0.02909612984871957\n",
      "train loss:0.02881159834516833\n",
      "train loss:0.0026588256020736523\n",
      "train loss:0.017417863309778955\n",
      "train loss:0.009306323977873367\n",
      "train loss:0.023845509113153668\n",
      "train loss:0.015781203912022475\n",
      "train loss:0.04654126833478671\n",
      "train loss:0.012915657621801935\n",
      "train loss:0.02463371992485559\n",
      "train loss:0.008223889500962013\n",
      "train loss:0.004655234060393022\n",
      "train loss:0.009475984604118698\n",
      "train loss:0.007598203937389248\n",
      "train loss:0.01736888549187568\n",
      "train loss:0.01387515563731591\n",
      "train loss:0.03439020734160944\n",
      "train loss:0.00681353411791433\n",
      "train loss:0.002219602826567815\n",
      "train loss:0.01566014931739657\n",
      "train loss:0.007433729946915828\n",
      "train loss:0.019112050558897087\n",
      "train loss:0.00959197041041018\n",
      "train loss:0.031009961706521774\n",
      "train loss:0.013468821520179508\n",
      "train loss:0.006909120682999994\n",
      "train loss:0.031209713738368516\n",
      "train loss:0.004879072237278278\n",
      "train loss:0.00957008995588446\n",
      "train loss:0.019983421003703084\n",
      "train loss:0.09309806620553869\n",
      "train loss:0.054782827037600415\n",
      "train loss:0.018743654823578208\n",
      "train loss:0.020704200824523428\n",
      "train loss:0.0354120473359977\n",
      "train loss:0.015192795651669987\n",
      "train loss:0.009568155140599166\n",
      "train loss:0.003605194648163757\n",
      "train loss:0.008760218309806101\n",
      "train loss:0.01316748826191202\n",
      "train loss:0.023648566564090832\n",
      "train loss:0.007080563573585417\n",
      "train loss:0.06978570964215111\n",
      "train loss:0.024122123734368995\n",
      "train loss:0.0289643067130275\n",
      "train loss:0.01186700072319264\n",
      "train loss:0.028709375843375615\n",
      "train loss:0.009976890702354494\n",
      "train loss:0.03691359363454685\n",
      "train loss:0.056564751069153874\n",
      "train loss:0.016097828304882107\n",
      "train loss:0.011415926656205904\n",
      "train loss:0.02718281779280114\n",
      "train loss:0.046826919886167724\n",
      "train loss:0.023373069508408107\n",
      "train loss:0.011107875356558026\n",
      "train loss:0.014017616685401463\n",
      "train loss:0.08701362828456753\n",
      "train loss:0.031866373380180106\n",
      "train loss:0.012410653023064872\n",
      "train loss:0.08283059097535679\n",
      "train loss:0.005915484464553725\n",
      "train loss:0.0020634311000519677\n",
      "train loss:0.010682202869325252\n",
      "train loss:0.007303091119967864\n",
      "train loss:0.010364419621241761\n",
      "train loss:0.031700220894483475\n",
      "train loss:0.06010107176918525\n",
      "train loss:0.033090362091010676\n",
      "train loss:0.006604183146230911\n",
      "train loss:0.061894460897770696\n",
      "train loss:0.10697661270100824\n",
      "train loss:0.006312452886664808\n",
      "train loss:0.01169761508859453\n",
      "train loss:0.025840077151495187\n",
      "train loss:0.011587196786282436\n",
      "train loss:0.009813536172090197\n",
      "train loss:0.019520999509050424\n",
      "train loss:0.04972458061976742\n",
      "train loss:0.019929187630267863\n",
      "train loss:0.030219309116970218\n",
      "train loss:0.007637953698948682\n",
      "=== epoch:7, train acc:0.985, test acc:0.986 ===\n",
      "train loss:0.00969376577651369\n",
      "train loss:0.06198586022657108\n",
      "train loss:0.015410767100992508\n",
      "train loss:0.006502149733188593\n",
      "train loss:0.016315718131642808\n",
      "train loss:0.012117237674774047\n",
      "train loss:0.01654106008322941\n",
      "train loss:0.01111170034273444\n",
      "train loss:0.00851108356369377\n",
      "train loss:0.014613705093173668\n",
      "train loss:0.034192407527683405\n",
      "train loss:0.01918475488289739\n",
      "train loss:0.004704687951063493\n",
      "train loss:0.014898493070146792\n",
      "train loss:0.005388095842561069\n",
      "train loss:0.04533701596638969\n",
      "train loss:0.06818782127245517\n",
      "train loss:0.011308501379282805\n",
      "train loss:0.03490183187566117\n",
      "train loss:0.014418917454139535\n",
      "train loss:0.025713245105511765\n",
      "train loss:0.018799077247525175\n",
      "train loss:0.03326037135570435\n",
      "train loss:0.033881548611925334\n",
      "train loss:0.010080302370554038\n",
      "train loss:0.022425325941333216\n",
      "train loss:0.03177856339518498\n",
      "train loss:0.03689328628198262\n",
      "train loss:0.009726901785962125\n",
      "train loss:0.007546405993451863\n",
      "train loss:0.016919200185669996\n",
      "train loss:0.051156032692802354\n",
      "train loss:0.0038518466976728586\n",
      "train loss:0.011971250921104588\n",
      "train loss:0.018648733046219015\n",
      "train loss:0.014719571190399365\n",
      "train loss:0.009987462101888467\n",
      "train loss:0.016325268260797905\n",
      "train loss:0.04953205128519709\n",
      "train loss:0.00899776154239809\n",
      "train loss:0.008727809182726204\n",
      "train loss:0.008714227476875584\n",
      "train loss:0.01204327477308159\n",
      "train loss:0.006898552922520252\n",
      "train loss:0.04664700158570801\n",
      "train loss:0.02585846376266796\n",
      "train loss:0.0036151057875769936\n",
      "train loss:0.04272615139705078\n",
      "train loss:0.010321857440462793\n",
      "train loss:0.04402606965638541\n",
      "train loss:0.010300017384896087\n",
      "train loss:0.03562924247748517\n",
      "train loss:0.006253287223189734\n",
      "train loss:0.022744582325296595\n",
      "train loss:0.08179281479198487\n",
      "train loss:0.01687759585964245\n",
      "train loss:0.027793556245841632\n",
      "train loss:0.009961576802819843\n",
      "train loss:0.01507954389690493\n",
      "train loss:0.028276242618159766\n",
      "train loss:0.019628846948027523\n",
      "train loss:0.022331103763537544\n",
      "train loss:0.04747913628073344\n",
      "train loss:0.0337913769580478\n",
      "train loss:0.007905608968159745\n",
      "train loss:0.04644654850060708\n",
      "train loss:0.02125860188926683\n",
      "train loss:0.004480863460099348\n",
      "train loss:0.01580955724894574\n",
      "train loss:0.0143998584511404\n",
      "train loss:0.012078551466451554\n",
      "train loss:0.01800523907562354\n",
      "train loss:0.01052755252086357\n",
      "train loss:0.015254397554373118\n",
      "train loss:0.030403552248404096\n",
      "train loss:0.014562891398014557\n",
      "train loss:0.0629409366219484\n",
      "train loss:0.024457856393892138\n",
      "train loss:0.0035343150090699055\n",
      "train loss:0.0016482795412783918\n",
      "train loss:0.005834432159042958\n",
      "train loss:0.009606327686866829\n",
      "train loss:0.02622780569620323\n",
      "train loss:0.14435659393784248\n",
      "train loss:0.01225294530331033\n",
      "train loss:0.012289034507486111\n",
      "train loss:0.03181955758084114\n",
      "train loss:0.018655136190044438\n",
      "train loss:0.031056200804419124\n",
      "train loss:0.039764796762832906\n",
      "train loss:0.010856098687101266\n",
      "train loss:0.037131261424629566\n",
      "train loss:0.022639495900688367\n",
      "train loss:0.02332048718863485\n",
      "train loss:0.012579831512251179\n",
      "train loss:0.0018629575755150876\n",
      "train loss:0.009225148395861358\n",
      "train loss:0.011384101061754534\n",
      "train loss:0.09143914352762618\n",
      "train loss:0.009052397049176876\n",
      "train loss:0.04231093379469217\n",
      "train loss:0.025621283328893595\n",
      "train loss:0.005114542071751005\n",
      "train loss:0.027907149391005404\n",
      "train loss:0.037221650687667245\n",
      "train loss:0.01707952230979367\n",
      "train loss:0.015589242572735407\n",
      "train loss:0.004909756234492621\n",
      "train loss:0.015647308843639712\n",
      "train loss:0.006405100620363251\n",
      "train loss:0.06347604460226487\n",
      "train loss:0.02560401257223715\n",
      "train loss:0.03973915348287489\n",
      "train loss:0.020552721417630537\n",
      "train loss:0.012632031765199815\n",
      "train loss:0.020733727508533015\n",
      "train loss:0.0098534892718568\n",
      "train loss:0.022560883811732352\n",
      "train loss:0.005298969585680577\n",
      "train loss:0.01275817652090549\n",
      "train loss:0.042436588073853934\n",
      "train loss:0.011454237600135801\n",
      "train loss:0.011424453742201619\n",
      "train loss:0.019540003376793912\n",
      "train loss:0.01576419940751544\n",
      "train loss:0.03258797601555434\n",
      "train loss:0.009113711701689132\n",
      "train loss:0.008646634784774282\n",
      "train loss:0.009862403797437542\n",
      "train loss:0.008141270802963486\n",
      "train loss:0.029921727952650638\n",
      "train loss:0.010488197234489785\n",
      "train loss:0.021499539794082714\n",
      "train loss:0.013898647958364829\n",
      "train loss:0.008702753081238024\n",
      "train loss:0.003051727643913765\n",
      "train loss:0.006451285607932173\n",
      "train loss:0.0031446427583695594\n",
      "train loss:0.020102807445838387\n",
      "train loss:0.004645218999113357\n",
      "train loss:0.023669693723686658\n",
      "train loss:0.011106093998893549\n",
      "train loss:0.016334531067538266\n",
      "train loss:0.008153620127061398\n",
      "train loss:0.03318837285081727\n",
      "train loss:0.012975684861157648\n",
      "train loss:0.08111448213303854\n",
      "train loss:0.026051793624604986\n",
      "train loss:0.013274726812757452\n",
      "train loss:0.009250533572127543\n",
      "train loss:0.01605646806815233\n",
      "train loss:0.01094885737500281\n",
      "train loss:0.003518225285031074\n",
      "train loss:0.017462204793319547\n",
      "train loss:0.00692068168316545\n",
      "train loss:0.009107918470798834\n",
      "train loss:0.00962529782654497\n",
      "train loss:0.00603737885856128\n",
      "train loss:0.01192249328644892\n",
      "train loss:0.00493890619020562\n",
      "train loss:0.045405940769370365\n",
      "train loss:0.05422997574044341\n",
      "train loss:0.016284790818788374\n",
      "train loss:0.006908787739427322\n",
      "train loss:0.042234111871148505\n",
      "train loss:0.004836296781826552\n",
      "train loss:0.010272257792243483\n",
      "train loss:0.003003861929005538\n",
      "train loss:0.0039195219632599835\n",
      "train loss:0.011181136964032312\n",
      "train loss:0.09088279042846657\n",
      "train loss:0.004833437128367901\n",
      "train loss:0.033970466125818964\n",
      "train loss:0.039632207177868944\n",
      "train loss:0.007219187495260587\n",
      "train loss:0.030029788765194586\n",
      "train loss:0.06807380417098444\n",
      "train loss:0.01614832944717734\n",
      "train loss:0.025094464976010172\n",
      "train loss:0.014277142713555988\n",
      "train loss:0.021082195363203923\n",
      "train loss:0.015807077999867292\n",
      "train loss:0.04816697629392893\n",
      "train loss:0.005681998096356412\n",
      "train loss:0.06048824653584441\n",
      "train loss:0.026003200653564207\n",
      "train loss:0.011159270838277757\n",
      "train loss:0.013844264306699178\n",
      "train loss:0.0018710024702287356\n",
      "train loss:0.03731109428924644\n",
      "train loss:0.01099932724337327\n",
      "train loss:0.08276625390158936\n",
      "train loss:0.012851228783502349\n",
      "train loss:0.028106792173657497\n",
      "train loss:0.018306841573959044\n",
      "train loss:0.012304169126279452\n",
      "train loss:0.019586596610618557\n",
      "train loss:0.004300273427139271\n",
      "train loss:0.012651681444936856\n",
      "train loss:0.03012020921047217\n",
      "train loss:0.0048385050458815285\n",
      "train loss:0.016597938690588912\n",
      "train loss:0.004056777902190944\n",
      "train loss:0.006623017500306649\n",
      "train loss:0.00903585117147774\n",
      "train loss:0.005689062195598835\n",
      "train loss:0.029366304289261833\n",
      "train loss:0.015330109487911586\n",
      "train loss:0.006174882694929334\n",
      "train loss:0.028345537595727714\n",
      "train loss:0.005463140346271282\n",
      "train loss:0.0017439567842408412\n",
      "train loss:0.02533991377151756\n",
      "train loss:0.009799162879404867\n",
      "train loss:0.006632001908712436\n",
      "train loss:0.013992964430827244\n",
      "train loss:0.016727261293019494\n",
      "train loss:0.026079029019051327\n",
      "train loss:0.013651277496834766\n",
      "train loss:0.056090256531084084\n",
      "train loss:0.013452586521911704\n",
      "train loss:0.024436281845081505\n",
      "train loss:0.05808096884735548\n",
      "train loss:0.035637882997060014\n",
      "train loss:0.032924429827946085\n",
      "train loss:0.015550532701982145\n",
      "train loss:0.01093591334691746\n",
      "train loss:0.007252898659061494\n",
      "train loss:0.021527583299891028\n",
      "train loss:0.005977224954660311\n",
      "train loss:0.013606202403209805\n",
      "train loss:0.10650423588581967\n",
      "train loss:0.003557799727358922\n",
      "train loss:0.0037191431913821582\n",
      "train loss:0.06341626325128805\n",
      "train loss:0.06237094024624378\n",
      "train loss:0.01864414428812752\n",
      "train loss:0.021628574512312176\n",
      "train loss:0.0191605567815915\n",
      "train loss:0.03797408272647814\n",
      "train loss:0.008012652970458766\n",
      "train loss:0.007507725318909816\n",
      "train loss:0.033929323373951985\n",
      "train loss:0.012299384121390372\n",
      "train loss:0.05503672928652683\n",
      "train loss:0.008943186433672659\n",
      "train loss:0.0189392624181055\n",
      "train loss:0.05026568353530523\n",
      "train loss:0.008510258381048234\n",
      "train loss:0.017022103873169266\n",
      "train loss:0.008755973765089032\n",
      "train loss:0.015136910660115698\n",
      "train loss:0.022350347080142847\n",
      "train loss:0.039665712879644426\n",
      "train loss:0.013646137571187291\n",
      "train loss:0.013392193266522889\n",
      "train loss:0.011603751104625647\n",
      "train loss:0.010565709112839486\n",
      "train loss:0.0039208311596473115\n",
      "train loss:0.0015635509955003067\n",
      "train loss:0.024075649665712642\n",
      "train loss:0.005945307636879077\n",
      "train loss:0.006139372076089948\n",
      "train loss:0.02066499364227857\n",
      "train loss:0.012660078656047244\n",
      "train loss:0.01674231689167334\n",
      "train loss:0.0077017027160915\n",
      "train loss:0.0038324997291133878\n",
      "train loss:0.01662252219700355\n",
      "train loss:0.015064713879866305\n",
      "train loss:0.01685755289503626\n",
      "train loss:0.006002521145388145\n",
      "train loss:0.03772395788803244\n",
      "train loss:0.009089982472521766\n",
      "train loss:0.010506690880660587\n",
      "train loss:0.018992188275416103\n",
      "train loss:0.013856214171602514\n",
      "train loss:0.0031826570338356454\n",
      "train loss:0.017260405163257875\n",
      "train loss:0.018484153935178698\n",
      "train loss:0.034937073801808195\n",
      "train loss:0.015815379363886238\n",
      "train loss:0.0160706960259109\n",
      "train loss:0.00845181065098189\n",
      "train loss:0.012560393147156608\n",
      "train loss:0.006892438417111869\n",
      "train loss:0.034150116175126755\n",
      "train loss:0.008178239988706423\n",
      "train loss:0.01592108321191147\n",
      "train loss:0.02715138560762837\n",
      "train loss:0.015206768365447678\n",
      "train loss:0.0054787409492255855\n",
      "train loss:0.0066846163210655994\n",
      "train loss:0.021348642576187817\n",
      "train loss:0.02887945221340359\n",
      "train loss:0.0057474968467758234\n",
      "train loss:0.019402829515397565\n",
      "train loss:0.0033352546815257284\n",
      "train loss:0.0030105460912242038\n",
      "train loss:0.02443897993937826\n",
      "train loss:0.049560288336307545\n",
      "train loss:0.017279977389625462\n",
      "train loss:0.006885305159916486\n",
      "train loss:0.006373324010068994\n",
      "train loss:0.01804992728103087\n",
      "train loss:0.005564420720994024\n",
      "train loss:0.013822740829451684\n",
      "train loss:0.010391316240809688\n",
      "train loss:0.003563525082362969\n",
      "train loss:0.016729467495115655\n",
      "train loss:0.003689691979660228\n",
      "train loss:0.0039815121621987765\n",
      "train loss:0.0020818467721305496\n",
      "train loss:0.012744366862102348\n",
      "train loss:0.005091116872301526\n",
      "train loss:0.005681844496829708\n",
      "train loss:0.0057432170330962784\n",
      "train loss:0.008517905330489997\n",
      "train loss:0.012337780320971799\n",
      "train loss:0.017448690145292316\n",
      "train loss:0.06167722617917947\n",
      "train loss:0.03616798082042885\n",
      "train loss:0.007077201169603534\n",
      "train loss:0.00413523030539528\n",
      "train loss:0.0019149997660012586\n",
      "train loss:0.02215405574210426\n",
      "train loss:0.012020844755809936\n",
      "train loss:0.013981591024919488\n",
      "train loss:0.006380885487212243\n",
      "train loss:0.00515029254435371\n",
      "train loss:0.008497019794956524\n",
      "train loss:0.004842941666991019\n",
      "train loss:0.005306295620570797\n",
      "train loss:0.011776309901315203\n",
      "train loss:0.059987403019661885\n",
      "train loss:0.02709280084821152\n",
      "train loss:0.01505797769027164\n",
      "train loss:0.0039464869234729595\n",
      "train loss:0.004618707024658138\n",
      "train loss:0.037451200610166734\n",
      "train loss:0.003635345284165644\n",
      "train loss:0.0022986934724291224\n",
      "train loss:0.013302571732913049\n",
      "train loss:0.06433653781661743\n",
      "train loss:0.016788818986547438\n",
      "train loss:0.0023464477914391197\n",
      "train loss:0.0025252972839001774\n",
      "train loss:0.004991944102827708\n",
      "train loss:0.007002194985003765\n",
      "train loss:0.004946885554457484\n",
      "train loss:0.0060502256415929405\n",
      "train loss:0.017629370366834346\n",
      "train loss:0.013919300827612642\n",
      "train loss:0.006817140889966488\n",
      "train loss:0.032373022955297426\n",
      "train loss:0.002654441307207802\n",
      "train loss:0.0026963491621927104\n",
      "train loss:0.015448154252386326\n",
      "train loss:0.008885191704804798\n",
      "train loss:0.02018069394516033\n",
      "train loss:0.024646895759913457\n",
      "train loss:0.002355882192981133\n",
      "train loss:0.010501768590932724\n",
      "train loss:0.007558709772541919\n",
      "train loss:0.017896315134759432\n",
      "train loss:0.008891564645298111\n",
      "train loss:0.052359357573276866\n",
      "train loss:0.017124298521609436\n",
      "train loss:0.02927660887331539\n",
      "train loss:0.04178757874685665\n",
      "train loss:0.007627401862451433\n",
      "train loss:0.014021263866511719\n",
      "train loss:0.03023440027942621\n",
      "train loss:0.02102128467894053\n",
      "train loss:0.002538903017544591\n",
      "train loss:0.00965347468906747\n",
      "train loss:0.019523972805720897\n",
      "train loss:0.0014260697994489286\n",
      "train loss:0.013624909800025337\n",
      "train loss:0.006319518507948639\n",
      "train loss:0.009005149980738314\n",
      "train loss:0.007864927947053383\n",
      "train loss:0.02869370495800943\n",
      "train loss:0.016978946275787377\n",
      "train loss:0.012124883444528783\n",
      "train loss:0.018060588676461913\n",
      "train loss:0.012175386396296941\n",
      "train loss:0.06604875500822514\n",
      "train loss:0.022120991716206108\n",
      "train loss:0.018770497724278336\n",
      "train loss:0.01084799793583282\n",
      "train loss:0.035754086944678555\n",
      "train loss:0.02035901834947789\n",
      "train loss:0.010788311132374336\n",
      "train loss:0.004986904127462865\n",
      "train loss:0.02627732332283319\n",
      "train loss:0.03603403703264549\n",
      "train loss:0.01573272930698441\n",
      "train loss:0.012806163969737903\n",
      "train loss:0.0049374266925727925\n",
      "train loss:0.028002955955383105\n",
      "train loss:0.005499188934592983\n",
      "train loss:0.017755389183709433\n",
      "train loss:0.012258864480693086\n",
      "train loss:0.005906038017499044\n",
      "train loss:0.005881381521021091\n",
      "train loss:0.005176951464501992\n",
      "train loss:0.024459110900136073\n",
      "train loss:0.003039258158395925\n",
      "train loss:0.009672897998734685\n",
      "train loss:0.017817245858647218\n",
      "train loss:0.004619478903526433\n",
      "train loss:0.01369112507234689\n",
      "train loss:0.014109838213489727\n",
      "train loss:0.018591372908099885\n",
      "train loss:0.029489806074519623\n",
      "train loss:0.03819813796468958\n",
      "train loss:0.019617713638104765\n",
      "train loss:0.013619365904730583\n",
      "train loss:0.006252389390571101\n",
      "train loss:0.01705360646948488\n",
      "train loss:0.003317751335891652\n",
      "train loss:0.011233599523938802\n",
      "train loss:0.04597169462404091\n",
      "train loss:0.004303126961746548\n",
      "train loss:0.07400265799849666\n",
      "train loss:0.017460613097131622\n",
      "train loss:0.01400405292475702\n",
      "train loss:0.006262290762177492\n",
      "train loss:0.007715150513993003\n",
      "train loss:0.0013091247233197436\n",
      "train loss:0.004988095344951159\n",
      "train loss:0.06365887992812908\n",
      "train loss:0.05236813132551652\n",
      "train loss:0.010275312121483802\n",
      "train loss:0.013412083555496142\n",
      "train loss:0.019717929951726366\n",
      "train loss:0.015670900505345052\n",
      "train loss:0.005218379494278217\n",
      "train loss:0.008891686947016485\n",
      "train loss:0.02135582228351562\n",
      "train loss:0.009931861693314257\n",
      "train loss:0.006949143159414709\n",
      "train loss:0.04369553667556735\n",
      "train loss:0.014722956397682466\n",
      "train loss:0.008932024361708453\n",
      "train loss:0.024446092740560907\n",
      "train loss:0.0017652999142193082\n",
      "train loss:0.05545973611565\n",
      "train loss:0.003703002568159001\n",
      "train loss:0.013081280463985519\n",
      "train loss:0.036916679671068285\n",
      "train loss:0.10007159435575876\n",
      "train loss:0.009351816186425655\n",
      "train loss:0.015049382201737172\n",
      "train loss:0.0023060615658048304\n",
      "train loss:0.01318067244517988\n",
      "train loss:0.04458220273292093\n",
      "train loss:0.011546577909295157\n",
      "train loss:0.008122591316927545\n",
      "train loss:0.009844038343377983\n",
      "train loss:0.02441309881998705\n",
      "train loss:0.03930931642276431\n",
      "train loss:0.011269970633795548\n",
      "train loss:0.04741891722390619\n",
      "train loss:0.007130344641813708\n",
      "train loss:0.04103762876031251\n",
      "train loss:0.0316192330425574\n",
      "train loss:0.00870476929275458\n",
      "train loss:0.007053588022250293\n",
      "train loss:0.01211279040941728\n",
      "train loss:0.007370134054891972\n",
      "train loss:0.003385877144645855\n",
      "train loss:0.05132097631567888\n",
      "train loss:0.006002425015891718\n",
      "train loss:0.004216279525196522\n",
      "train loss:0.014379702533687396\n",
      "train loss:0.024282031462160743\n",
      "train loss:0.0192240253120614\n",
      "train loss:0.024014604632002733\n",
      "train loss:0.04339796089543813\n",
      "train loss:0.0417227749977057\n",
      "train loss:0.02277005056301803\n",
      "train loss:0.03436559070672061\n",
      "train loss:0.008874791961149141\n",
      "train loss:0.010435634931411005\n",
      "train loss:0.006371936537899348\n",
      "train loss:0.016327803458409756\n",
      "train loss:0.012705998189234225\n",
      "train loss:0.07479547010073173\n",
      "train loss:0.011490055470483217\n",
      "train loss:0.025861479124388423\n",
      "train loss:0.026771095365018933\n",
      "train loss:0.015482789876396326\n",
      "train loss:0.01497969812356908\n",
      "train loss:0.016384764154048204\n",
      "train loss:0.011352202034995327\n",
      "train loss:0.01874930253097392\n",
      "train loss:0.012531227376397116\n",
      "train loss:0.007679134942435059\n",
      "train loss:0.025222474133469147\n",
      "train loss:0.007811228813712651\n",
      "train loss:0.0014319868118343555\n",
      "train loss:0.0028755955103509836\n",
      "train loss:0.007727000060427889\n",
      "train loss:0.006874837389958894\n",
      "train loss:0.030921764680659325\n",
      "train loss:0.009956684801475596\n",
      "train loss:0.0033514610167501035\n",
      "train loss:0.03881143819106304\n",
      "train loss:0.03194888035587871\n",
      "train loss:0.0174986251997537\n",
      "train loss:0.008331198789596879\n",
      "train loss:0.005786017329014455\n",
      "train loss:0.006338171522674809\n",
      "train loss:0.008747742925972322\n",
      "train loss:0.005678065301000868\n",
      "train loss:0.0095578397673025\n",
      "train loss:0.0029694554590967014\n",
      "train loss:0.005161849489017433\n",
      "train loss:0.022566140241457592\n",
      "train loss:0.03065333370771921\n",
      "train loss:0.00911513771717559\n",
      "train loss:0.016635258648841827\n",
      "train loss:0.007200042673535345\n",
      "train loss:0.0033005256522615546\n",
      "train loss:0.04810673532639062\n",
      "train loss:0.0017672744549470437\n",
      "train loss:0.008546996225439894\n",
      "train loss:0.04546186619616074\n",
      "train loss:0.019990431621460667\n",
      "train loss:0.006200055919660965\n",
      "train loss:0.029719009302677266\n",
      "train loss:0.03817253905279761\n",
      "train loss:0.023997203055585867\n",
      "train loss:0.01839211993466558\n",
      "train loss:0.0025521979882498925\n",
      "train loss:0.03381946221684912\n",
      "train loss:0.04247572486697915\n",
      "train loss:0.021222674970852336\n",
      "train loss:0.003738557491271795\n",
      "train loss:0.007264133335520805\n",
      "train loss:0.03060684000743701\n",
      "train loss:0.007518835745884764\n",
      "train loss:0.013904343679830957\n",
      "train loss:0.0067823811361411536\n",
      "train loss:0.017230428698680855\n",
      "train loss:0.020871881713840496\n",
      "train loss:0.013955211199231918\n",
      "train loss:0.016008601484247136\n",
      "train loss:0.03813229043493214\n",
      "train loss:0.005657188239441242\n",
      "train loss:0.03794282380516406\n",
      "train loss:0.07694125208068092\n",
      "train loss:0.009054386567263857\n",
      "train loss:0.03549263181664026\n",
      "train loss:0.007911310444438081\n",
      "train loss:0.004715869201913688\n",
      "train loss:0.01154577234838084\n",
      "train loss:0.010984861516968772\n",
      "train loss:0.005206147983811784\n",
      "train loss:0.02583530948854273\n",
      "train loss:0.006685084207570484\n",
      "train loss:0.03514131745547347\n",
      "train loss:0.015240872790982645\n",
      "train loss:0.009342504671622233\n",
      "train loss:0.007997009024254674\n",
      "train loss:0.00895610473651693\n",
      "train loss:0.030192613399401776\n",
      "train loss:0.002696354249088075\n",
      "train loss:0.029675359136849357\n",
      "train loss:0.006835143105998819\n",
      "train loss:0.004659627951332787\n",
      "train loss:0.07201765574279805\n",
      "train loss:0.004450970643285605\n",
      "train loss:0.003959283051432609\n",
      "train loss:0.01191759300235793\n",
      "train loss:0.016012447317370768\n",
      "train loss:0.014697430048301872\n",
      "train loss:0.0040717525966407235\n",
      "train loss:0.030362356230590245\n",
      "train loss:0.03999462329128973\n",
      "train loss:0.0054136173579319834\n",
      "train loss:0.0037717466528880334\n",
      "train loss:0.009846188169048437\n",
      "train loss:0.0046095425249005305\n",
      "train loss:0.006583417735583823\n",
      "train loss:0.007992816119129205\n",
      "train loss:0.0119078127199618\n",
      "train loss:0.006606653154417464\n",
      "train loss:0.021015153086888443\n",
      "train loss:0.013781445554928828\n",
      "train loss:0.04456083515377985\n",
      "train loss:0.0059066015420543225\n",
      "train loss:0.01490136738658503\n",
      "train loss:0.00809299410150161\n",
      "train loss:0.011502511788086282\n",
      "train loss:0.009972068021567382\n",
      "train loss:0.015674854346224926\n",
      "train loss:0.02248299458082924\n",
      "=== epoch:8, train acc:0.992, test acc:0.987 ===\n",
      "train loss:0.002759495015232721\n",
      "train loss:0.023684979593773945\n",
      "train loss:0.0035391441817296714\n",
      "train loss:0.006270086963730023\n",
      "train loss:0.01778730642538088\n",
      "train loss:0.04869012178437649\n",
      "train loss:0.053944177220223766\n",
      "train loss:0.003411578365525848\n",
      "train loss:0.005312596749711073\n",
      "train loss:0.011583453737299784\n",
      "train loss:0.025430133188585922\n",
      "train loss:0.054409338908246674\n",
      "train loss:0.00776740132459313\n",
      "train loss:0.008102196102571606\n",
      "train loss:0.02008741294729597\n",
      "train loss:0.044300641853861296\n",
      "train loss:0.012926379399637864\n",
      "train loss:0.008532092838892303\n",
      "train loss:0.0016347845256114279\n",
      "train loss:0.014261265119794276\n",
      "train loss:0.06493599851912171\n",
      "train loss:0.01771227842605572\n",
      "train loss:0.008365180413273714\n",
      "train loss:0.008589401638047308\n",
      "train loss:0.01324302115928207\n",
      "train loss:0.007142515076249618\n",
      "train loss:0.013038042250579305\n",
      "train loss:0.027541269100628058\n",
      "train loss:0.03596416084314399\n",
      "train loss:0.028306947154426224\n",
      "train loss:0.09420030270982414\n",
      "train loss:0.011946282757686293\n",
      "train loss:0.010014274400062372\n",
      "train loss:0.010042259767491108\n",
      "train loss:0.004375834226334741\n",
      "train loss:0.014589333718004463\n",
      "train loss:0.03133441059623277\n",
      "train loss:0.011895369449477616\n",
      "train loss:0.04181235351103965\n",
      "train loss:0.006553614843718194\n",
      "train loss:0.014800677729220042\n",
      "train loss:0.005985632134814972\n",
      "train loss:0.014529818781546496\n",
      "train loss:0.009316001292438626\n",
      "train loss:0.01822395952748148\n",
      "train loss:0.028836724833441317\n",
      "train loss:0.014267087914197838\n",
      "train loss:0.007604172205368318\n",
      "train loss:0.002276467026656099\n",
      "train loss:0.005370088153744363\n",
      "train loss:0.03617866340455702\n",
      "train loss:0.008759735743584008\n",
      "train loss:0.00376746787308601\n",
      "train loss:0.006698843282448128\n",
      "train loss:0.0055560833242003895\n",
      "train loss:0.05023359221812301\n",
      "train loss:0.009310104393862153\n",
      "train loss:0.04853392877734766\n",
      "train loss:0.0038880655204424886\n",
      "train loss:0.020009772789724874\n",
      "train loss:0.007453733997442614\n",
      "train loss:0.0032914605651505457\n",
      "train loss:0.00395768489727673\n",
      "train loss:0.0065551477067457295\n",
      "train loss:0.009115625491075318\n",
      "train loss:0.004817234961354488\n",
      "train loss:0.008204069800109168\n",
      "train loss:0.005788928391999699\n",
      "train loss:0.03526335144660884\n",
      "train loss:0.004373248854509997\n",
      "train loss:0.01739533990686467\n",
      "train loss:0.015956428249298146\n",
      "train loss:0.01245719675341158\n",
      "train loss:0.0028035665166455054\n",
      "train loss:0.031597359080891896\n",
      "train loss:0.04911235589876504\n",
      "train loss:0.02568718456364626\n",
      "train loss:0.005388205975456977\n",
      "train loss:0.0726510284356939\n",
      "train loss:0.01939099278343675\n",
      "train loss:0.01659499890079768\n",
      "train loss:0.006276834161377697\n",
      "train loss:0.0027230288290525195\n",
      "train loss:0.0057576051780759375\n",
      "train loss:0.0013032888146288493\n",
      "train loss:0.007858958900375568\n",
      "train loss:0.0026763623482528092\n",
      "train loss:0.003903200108558361\n",
      "train loss:0.013722169994774214\n",
      "train loss:0.0639110585708236\n",
      "train loss:0.013676389249665135\n",
      "train loss:0.0440438397548688\n",
      "train loss:0.023078951145463007\n",
      "train loss:0.010081979084655903\n",
      "train loss:0.010822179369885188\n",
      "train loss:0.007497839201404024\n",
      "train loss:0.0065919433669926805\n",
      "train loss:0.07682178610420513\n",
      "train loss:0.004596610458160688\n",
      "train loss:0.012408582491347084\n",
      "train loss:0.005048998860468401\n",
      "train loss:0.01699701431161636\n",
      "train loss:0.009883065276616705\n",
      "train loss:0.07330332322130802\n",
      "train loss:0.013931169995979466\n",
      "train loss:0.025272271936399412\n",
      "train loss:0.006238140505734842\n",
      "train loss:0.012829774212205915\n",
      "train loss:0.05550352309927586\n",
      "train loss:0.013367803583234139\n",
      "train loss:0.014676190417432728\n",
      "train loss:0.01936019814306834\n",
      "train loss:0.009659564740892381\n",
      "train loss:0.005152334637499783\n",
      "train loss:0.017801656253268117\n",
      "train loss:0.018081398268484632\n",
      "train loss:0.0041796894090310515\n",
      "train loss:0.007863810156365765\n",
      "train loss:0.06968563756337096\n",
      "train loss:0.005589076922607489\n",
      "train loss:0.03346149511331875\n",
      "train loss:0.0032854387649455486\n",
      "train loss:0.010561550485187534\n",
      "train loss:0.011024320259427038\n",
      "train loss:0.01521824956285818\n",
      "train loss:0.006116636230452815\n",
      "train loss:0.03463200012965464\n",
      "train loss:0.009710876827809863\n",
      "train loss:0.0186270358431981\n",
      "train loss:0.010564084302052738\n",
      "train loss:0.014391418009015278\n",
      "train loss:0.027310877446601812\n",
      "train loss:0.024842845522287074\n",
      "train loss:0.038627338556246345\n",
      "train loss:0.009587777256748548\n",
      "train loss:0.028204943062508914\n",
      "train loss:0.004712911367411244\n",
      "train loss:0.04514171092079983\n",
      "train loss:0.02004707534021023\n",
      "train loss:0.1168651170197298\n",
      "train loss:0.011722561120127949\n",
      "train loss:0.02135435185642062\n",
      "train loss:0.03760935996541339\n",
      "train loss:0.0019617246722557886\n",
      "train loss:0.0072603038246233575\n",
      "train loss:0.015302276264964065\n",
      "train loss:0.031816814613658345\n",
      "train loss:0.03301839737198401\n",
      "train loss:0.004112209179105419\n",
      "train loss:0.026251088425659777\n",
      "train loss:0.009075330696763124\n",
      "train loss:0.02140874450362553\n",
      "train loss:0.005911454644565054\n",
      "train loss:0.0042310811225048026\n",
      "train loss:0.013197396795503483\n",
      "train loss:0.01387215326629627\n",
      "train loss:0.012986893372871509\n",
      "train loss:0.012741482946560688\n",
      "train loss:0.013190113329922022\n",
      "train loss:0.03796534149505189\n",
      "train loss:0.004305046280628404\n",
      "train loss:0.010755015638250463\n",
      "train loss:0.013064954301009114\n",
      "train loss:0.0038695534559314\n",
      "train loss:0.016484871791974807\n",
      "train loss:0.018169900892116123\n",
      "train loss:0.054710922794127315\n",
      "train loss:0.01490139043506251\n",
      "train loss:0.04371558089639099\n",
      "train loss:0.02651566143291208\n",
      "train loss:0.0075838443527816\n",
      "train loss:0.0016450776152004555\n",
      "train loss:0.03476628574364544\n",
      "train loss:0.011046603657708248\n",
      "train loss:0.026001874329579566\n",
      "train loss:0.004038824969995923\n",
      "train loss:0.04093963829108401\n",
      "train loss:0.0221721641955598\n",
      "train loss:0.045676879997960204\n",
      "train loss:0.0048314331552424495\n",
      "train loss:0.01456356209531246\n",
      "train loss:0.003759557580559958\n",
      "train loss:0.007899849150265951\n",
      "train loss:0.003949944081316422\n",
      "train loss:0.010260025978174343\n",
      "train loss:0.020515231093159697\n",
      "train loss:0.015537885782625199\n",
      "train loss:0.09430611821580849\n",
      "train loss:0.014589636684436724\n",
      "train loss:0.009868107611229415\n",
      "train loss:0.01361614904199283\n",
      "train loss:0.007469723004013251\n",
      "train loss:0.04298411825180911\n",
      "train loss:0.0028260412340128734\n",
      "train loss:0.01650875613130132\n",
      "train loss:0.0011232554732312674\n",
      "train loss:0.01036735736457832\n",
      "train loss:0.007589924920817103\n",
      "train loss:0.0328750011438875\n",
      "train loss:0.005146696513837966\n",
      "train loss:0.006218955953177062\n",
      "train loss:0.07212573414777176\n",
      "train loss:0.008347152733812533\n",
      "train loss:0.03846040795566639\n",
      "train loss:0.009239173315083617\n",
      "train loss:0.044916366036140375\n",
      "train loss:0.01088903220536727\n",
      "train loss:0.03147581908355148\n",
      "train loss:0.017162244021772344\n",
      "train loss:0.021658509013883186\n",
      "train loss:0.0019456104999495368\n",
      "train loss:0.009753212146154272\n",
      "train loss:0.019133905891142534\n",
      "train loss:0.002361592986978475\n",
      "train loss:0.01255797953684462\n",
      "train loss:0.008060922352442872\n",
      "train loss:0.042577465970337985\n",
      "train loss:0.034025417002656815\n",
      "train loss:0.006616475930050752\n",
      "train loss:0.0011808740019791657\n",
      "train loss:0.031347251575249244\n",
      "train loss:0.014885270384109726\n",
      "train loss:0.003039043928642467\n",
      "train loss:0.008955802712348098\n",
      "train loss:0.0469074356829835\n",
      "train loss:0.016551677394469284\n",
      "train loss:0.03693160281944877\n",
      "train loss:0.012203113232631369\n",
      "train loss:0.0032738823091909073\n",
      "train loss:0.016365638132588192\n",
      "train loss:0.026523727674663503\n",
      "train loss:0.0534142038144598\n",
      "train loss:0.01689594085650569\n",
      "train loss:0.01076270140770783\n",
      "train loss:0.005754597937482853\n",
      "train loss:0.009018661652647263\n",
      "train loss:0.011308469893462746\n",
      "train loss:0.008316638288617966\n",
      "train loss:0.01914274947382689\n",
      "train loss:0.010269897640100245\n",
      "train loss:0.007229330311050855\n",
      "train loss:0.05646793079374505\n",
      "train loss:0.03837801091498712\n",
      "train loss:0.02146183860202422\n",
      "train loss:0.055820960093447634\n",
      "train loss:0.008312632695345849\n",
      "train loss:0.005870067107075726\n",
      "train loss:0.004974430869249874\n",
      "train loss:0.004234341149894872\n",
      "train loss:0.05110395636945179\n",
      "train loss:0.01651508472269242\n",
      "train loss:0.041305198005455325\n",
      "train loss:0.009353615179074034\n",
      "train loss:0.004774445033938034\n",
      "train loss:0.028937105186283354\n",
      "train loss:0.014101151791760563\n",
      "train loss:0.007449080537500488\n",
      "train loss:0.025707705526481\n",
      "train loss:0.0035840447636880358\n",
      "train loss:0.012546090210269463\n",
      "train loss:0.01363140782480211\n",
      "train loss:0.02312107937258292\n",
      "train loss:0.009738426286355307\n",
      "train loss:0.010518980101651993\n",
      "train loss:0.039099948618052174\n",
      "train loss:0.001997749752829124\n",
      "train loss:0.06066390332370619\n",
      "train loss:0.016253665714419364\n",
      "train loss:0.0028091262730584564\n",
      "train loss:0.0058207995089373885\n",
      "train loss:0.006476360331976485\n",
      "train loss:0.059478632394803305\n",
      "train loss:0.014105307016608852\n",
      "train loss:0.006199243256578156\n",
      "train loss:0.003316490827762729\n",
      "train loss:0.014517405235186602\n",
      "train loss:0.008615519197776844\n",
      "train loss:0.002466875243984038\n",
      "train loss:0.00736691740193438\n",
      "train loss:0.038323993285356114\n",
      "train loss:0.005101279527895456\n",
      "train loss:0.013023213781388399\n",
      "train loss:0.02084228261249017\n",
      "train loss:0.006862276238004279\n",
      "train loss:0.004453041949324569\n",
      "train loss:0.010011690285771084\n",
      "train loss:0.006246773731040132\n",
      "train loss:0.036964374263464814\n",
      "train loss:0.010305944458348674\n",
      "train loss:0.005709993455529219\n",
      "train loss:0.02897707697652894\n",
      "train loss:0.012265100096981096\n",
      "train loss:0.03792795390983414\n",
      "train loss:0.020207243626244483\n",
      "train loss:0.002962922308139851\n",
      "train loss:0.005730180847825783\n",
      "train loss:0.013658071895061727\n",
      "train loss:0.0048045023176132426\n",
      "train loss:0.026384740555569674\n",
      "train loss:0.02202519327654572\n",
      "train loss:0.04165693730787183\n",
      "train loss:0.003747270443637062\n",
      "train loss:0.017388478299434067\n",
      "train loss:0.007782961022841816\n",
      "train loss:0.013692687424037994\n",
      "train loss:0.04817147800928089\n",
      "train loss:0.02851865657332636\n",
      "train loss:0.012896357952093776\n",
      "train loss:0.005493624198249339\n",
      "train loss:0.0036517435946049025\n",
      "train loss:0.014459762011678154\n",
      "train loss:0.006778696193365109\n",
      "train loss:0.012383094415256044\n",
      "train loss:0.001859339137839116\n",
      "train loss:0.010234133940054395\n",
      "train loss:0.010635881411916424\n",
      "train loss:0.032184295239112155\n",
      "train loss:0.005860877357859153\n",
      "train loss:0.021806312824341072\n",
      "train loss:0.006644875204768729\n",
      "train loss:0.02595115981657005\n",
      "train loss:0.008146625508800982\n",
      "train loss:0.007962421445776288\n",
      "train loss:0.04548917730863347\n",
      "train loss:0.012622772597705494\n",
      "train loss:0.021198687008419065\n",
      "train loss:0.02010518140436042\n",
      "train loss:0.025798806659692808\n",
      "train loss:0.005489947628830344\n",
      "train loss:0.015496360842150196\n",
      "train loss:0.022381807501095464\n",
      "train loss:0.06880843723143155\n",
      "train loss:0.013768079741232054\n",
      "train loss:0.01980389609174267\n",
      "train loss:0.024802081345685754\n",
      "train loss:0.007204000825368033\n",
      "train loss:0.001568868270930571\n",
      "train loss:0.004514492290252631\n",
      "train loss:0.009646246088990373\n",
      "train loss:0.027873656752660868\n",
      "train loss:0.0053916140493594465\n",
      "train loss:0.004343671813028853\n",
      "train loss:0.027272371884342415\n",
      "train loss:0.006107224385353504\n",
      "train loss:0.020071761570689187\n",
      "train loss:0.01692001640003021\n",
      "train loss:0.008202636882115128\n",
      "train loss:0.0063374065420622235\n",
      "train loss:0.002253732028260972\n",
      "train loss:0.006329747947828024\n",
      "train loss:0.015874245480310144\n",
      "train loss:0.02069265043956179\n",
      "train loss:0.0030814364699802664\n",
      "train loss:0.018292253995946774\n",
      "train loss:0.0016865001309396002\n",
      "train loss:0.03610590034631475\n",
      "train loss:0.010259258152857354\n",
      "train loss:0.06818579595935505\n",
      "train loss:0.0074732695503312244\n",
      "train loss:0.010245872286897438\n",
      "train loss:0.009257400404468836\n",
      "train loss:0.007307149949809319\n",
      "train loss:0.016799132425440188\n",
      "train loss:0.014552930379801496\n",
      "train loss:0.007613817235045644\n",
      "train loss:0.09182902472722083\n",
      "train loss:0.006037727199797026\n",
      "train loss:0.0032844575906851507\n",
      "train loss:0.0077953132930557315\n",
      "train loss:0.0077248352100248\n",
      "train loss:0.00610322283650545\n",
      "train loss:0.02237944408189863\n",
      "train loss:0.011014403263528025\n",
      "train loss:0.06421266469385162\n",
      "train loss:0.012257612044484738\n",
      "train loss:0.012793003569997921\n",
      "train loss:0.028678866726016286\n",
      "train loss:0.011847792154146384\n",
      "train loss:0.003670860127323322\n",
      "train loss:0.003425965859607846\n",
      "train loss:0.016628516320034484\n",
      "train loss:0.013786920265806031\n",
      "train loss:0.005574657589144427\n",
      "train loss:0.01828425922284509\n",
      "train loss:0.013611526853891377\n",
      "train loss:0.009788072586897307\n",
      "train loss:0.06652846954521546\n",
      "train loss:0.005955998837553732\n",
      "train loss:0.00664974123819515\n",
      "train loss:0.011111867751324802\n",
      "train loss:0.00607369259370324\n",
      "train loss:0.05325295339061224\n",
      "train loss:0.0032380072425330286\n",
      "train loss:0.007104584800601219\n",
      "train loss:0.009981693723621905\n",
      "train loss:0.02175361706963734\n",
      "train loss:0.00614313513186316\n",
      "train loss:0.0039513289929508045\n",
      "train loss:0.03476436088524019\n",
      "train loss:0.0016163884929254813\n",
      "train loss:0.0038525227664901853\n",
      "train loss:0.008118247593065947\n",
      "train loss:0.004249162771595536\n",
      "train loss:0.0014122660386711747\n",
      "train loss:0.004683876739513621\n",
      "train loss:0.07895331767481432\n",
      "train loss:0.015877468278779713\n",
      "train loss:0.016687873094645294\n",
      "train loss:0.0037812498033029836\n",
      "train loss:0.0014753981547543965\n",
      "train loss:0.005591534258892027\n",
      "train loss:0.004107907429855083\n",
      "train loss:0.005773350545215863\n",
      "train loss:0.058948349707924105\n",
      "train loss:0.013413334835412717\n",
      "train loss:0.015019526329947915\n",
      "train loss:0.0068901281116756155\n",
      "train loss:0.011464317726496018\n",
      "train loss:0.033612363563884674\n",
      "train loss:0.01167740190884921\n",
      "train loss:0.008406438062748114\n",
      "train loss:0.0009868272584199827\n",
      "train loss:0.004750957170310664\n",
      "train loss:0.008109164283134235\n",
      "train loss:0.007524239577534421\n",
      "train loss:0.003344718112856612\n",
      "train loss:0.01585073172777179\n",
      "train loss:0.014509083040627353\n",
      "train loss:0.0058187535872209015\n",
      "train loss:0.02753992968124321\n",
      "train loss:0.02416983459328836\n",
      "train loss:0.020351947000606732\n",
      "train loss:0.0024082445935086037\n",
      "train loss:0.003964918209305205\n",
      "train loss:0.005365850167019087\n",
      "train loss:0.0027833702739712817\n",
      "train loss:0.004299093496432668\n",
      "train loss:0.017797353446774265\n",
      "train loss:0.017249659235465887\n",
      "train loss:0.017905808075325977\n",
      "train loss:0.024183140202783383\n",
      "train loss:0.0026952672718326526\n",
      "train loss:0.011190033372393542\n",
      "train loss:0.007986082869564887\n",
      "train loss:0.023539441304005652\n",
      "train loss:0.01970279496941141\n",
      "train loss:0.008774433641765755\n",
      "train loss:0.01941759258750435\n",
      "train loss:0.023030687290722347\n",
      "train loss:0.008421758751403618\n",
      "train loss:0.0038273595342664464\n",
      "train loss:0.026125663002620994\n",
      "train loss:0.007992034411556435\n",
      "train loss:0.10606700571746709\n",
      "train loss:0.0022251651408631682\n",
      "train loss:0.028119998285076497\n",
      "train loss:0.009473329476903103\n",
      "train loss:0.04204297397722132\n",
      "train loss:0.023893514598891635\n",
      "train loss:0.008036912004307168\n",
      "train loss:0.005113902319114679\n",
      "train loss:0.009448753238625074\n",
      "train loss:0.021329418712685372\n",
      "train loss:0.01036592391418716\n",
      "train loss:0.001511973776506168\n",
      "train loss:0.0025923545136328483\n",
      "train loss:0.03533595391688702\n",
      "train loss:0.008672880566466323\n",
      "train loss:0.004539365142298933\n",
      "train loss:0.02446462263872239\n",
      "train loss:0.010011365471660157\n",
      "train loss:0.0021783682475169162\n",
      "train loss:0.015406711196765534\n",
      "train loss:0.015047084129670617\n",
      "train loss:0.023670212297685574\n",
      "train loss:0.009819309972820631\n",
      "train loss:0.0036591846527056658\n",
      "train loss:0.007928499424847673\n",
      "train loss:0.02735230619686445\n",
      "train loss:0.02511751096486056\n",
      "train loss:0.01441177345668036\n",
      "train loss:0.00222862642096352\n",
      "train loss:0.05579747625850727\n",
      "train loss:0.0018494023178399794\n",
      "train loss:0.019191897065324096\n",
      "train loss:0.06963719949150433\n",
      "train loss:0.007825115070341332\n",
      "train loss:0.004597538513327217\n",
      "train loss:0.011696361071388972\n",
      "train loss:0.010080898634116707\n",
      "train loss:0.005504204398258361\n",
      "train loss:0.001990116190564611\n",
      "train loss:0.02316638324468179\n",
      "train loss:0.013913326545198645\n",
      "train loss:0.003194408770237138\n",
      "train loss:0.003385602388628733\n",
      "train loss:0.009508080553195353\n",
      "train loss:0.011076133350124645\n",
      "train loss:0.016179992641452662\n",
      "train loss:0.06116338659896679\n",
      "train loss:0.039363582703017846\n",
      "train loss:0.010890693470757202\n",
      "train loss:0.014741535945888095\n",
      "train loss:0.005560665410160932\n",
      "train loss:0.00751337496919255\n",
      "train loss:0.035276673754573926\n",
      "train loss:0.02480465741390295\n",
      "train loss:0.028668518468670294\n",
      "train loss:0.028706140436708858\n",
      "train loss:0.012626620786190722\n",
      "train loss:0.006015195872320611\n",
      "train loss:0.006574101222409204\n",
      "train loss:0.010481931007442316\n",
      "train loss:0.002794195160453122\n",
      "train loss:0.004467309993949688\n",
      "train loss:0.004147727867400758\n",
      "train loss:0.011087911236496653\n",
      "train loss:0.005225320011370145\n",
      "train loss:0.009879060471952803\n",
      "train loss:0.01791747534623948\n",
      "train loss:0.0004476247497004123\n",
      "train loss:0.005250825302160388\n",
      "train loss:0.0048602722985378885\n",
      "train loss:0.003419340004282675\n",
      "train loss:0.005694513071008516\n",
      "train loss:0.0029788054852874835\n",
      "train loss:0.006635333750948895\n",
      "train loss:0.0007160189810272498\n",
      "train loss:0.003028336355687073\n",
      "train loss:0.0016835540010990552\n",
      "train loss:0.020968021812525012\n",
      "train loss:0.005874707857793873\n",
      "train loss:0.01406218574922933\n",
      "train loss:0.01266874396247466\n",
      "train loss:0.007005472963006116\n",
      "train loss:0.007783449672237195\n",
      "train loss:0.031242364893642148\n",
      "train loss:0.003968024439613876\n",
      "train loss:0.0006821108930380813\n",
      "train loss:0.018823261337544847\n",
      "train loss:0.014239982245111632\n",
      "train loss:0.002312251152553389\n",
      "train loss:0.014398297368926212\n",
      "train loss:0.01600358563989475\n",
      "train loss:0.0025348505383716025\n",
      "train loss:0.02215211333611796\n",
      "train loss:0.0028852285641301333\n",
      "train loss:0.005205373177836064\n",
      "train loss:0.012311740597606419\n",
      "train loss:0.0043238485245942855\n",
      "train loss:0.0015592104191360543\n",
      "train loss:0.009953920685767595\n",
      "train loss:0.048643987391017746\n",
      "train loss:0.04063661325294098\n",
      "train loss:0.006625441856201802\n",
      "train loss:0.06371233268633358\n",
      "train loss:0.006374693491900011\n",
      "train loss:0.009390377039662768\n",
      "train loss:0.007752778638663448\n",
      "train loss:0.004704665106249407\n",
      "train loss:0.013848973786188903\n",
      "train loss:0.01663386062813523\n",
      "train loss:0.013661249805658762\n",
      "train loss:0.013948053442111885\n",
      "train loss:0.009996737540864754\n",
      "train loss:0.008073406940720748\n",
      "train loss:0.005138075191618246\n",
      "train loss:0.0032721852688827866\n",
      "train loss:0.0010787363633282636\n",
      "train loss:0.0020245716772223435\n",
      "train loss:0.0020890636942494157\n",
      "train loss:0.01674012793003069\n",
      "train loss:0.01913305683683105\n",
      "train loss:0.03516093936396658\n",
      "train loss:0.026155678716167613\n",
      "train loss:0.009639288046836635\n",
      "train loss:0.0041872070975286285\n",
      "train loss:0.003830241655804812\n",
      "train loss:0.0030709917735259958\n",
      "train loss:0.002247165575674823\n",
      "train loss:0.009141843163983863\n",
      "train loss:0.1034328036386423\n",
      "train loss:0.015534331804107017\n",
      "train loss:0.009329327046643967\n",
      "train loss:0.011111172800374745\n",
      "train loss:0.029523789520080147\n",
      "train loss:0.01871832688560158\n",
      "train loss:0.010054258136158085\n",
      "train loss:0.024932146848199045\n",
      "train loss:0.004079482623206532\n",
      "train loss:0.041205051925280796\n",
      "train loss:0.013261078624789726\n",
      "train loss:0.007464957124743189\n",
      "train loss:0.0045991143374292795\n",
      "train loss:0.019087452758735343\n",
      "train loss:0.0027983756300191754\n",
      "train loss:0.009032253883994451\n",
      "train loss:0.009271007421805439\n",
      "train loss:0.011656192882112695\n",
      "train loss:0.003589427118234721\n",
      "=== epoch:9, train acc:0.99, test acc:0.988 ===\n",
      "train loss:0.014100082284929729\n",
      "train loss:0.01547323484741387\n",
      "train loss:0.0034423731318016697\n",
      "train loss:0.007515062170886644\n",
      "train loss:0.00923723742522145\n",
      "train loss:0.016387507462235062\n",
      "train loss:0.030484199512363218\n",
      "train loss:0.02157666332496542\n",
      "train loss:0.0031399668292142065\n",
      "train loss:0.022872302182675986\n",
      "train loss:0.028996743457011354\n",
      "train loss:0.0063598030823148076\n",
      "train loss:0.00892453060812319\n",
      "train loss:0.00964768345246738\n",
      "train loss:0.008862216772825381\n",
      "train loss:0.006720890499055044\n",
      "train loss:0.0008362470824937362\n",
      "train loss:0.002486582996278681\n",
      "train loss:0.007508711496707774\n",
      "train loss:0.013011077360284402\n",
      "train loss:0.00439659921224758\n",
      "train loss:0.012756374327231257\n",
      "train loss:0.009674915088576467\n",
      "train loss:0.061419016658865094\n",
      "train loss:0.0021770475707237067\n",
      "train loss:0.013436954954665908\n",
      "train loss:0.019603204593712956\n",
      "train loss:0.004322205797167181\n",
      "train loss:0.011563854959341818\n",
      "train loss:0.009909725327171943\n",
      "train loss:0.01179893624646668\n",
      "train loss:0.0026476063606872308\n",
      "train loss:0.00679958753739447\n",
      "train loss:0.015319124235536823\n",
      "train loss:0.006625825539762257\n",
      "train loss:0.005629382014175036\n",
      "train loss:0.009033085167983592\n",
      "train loss:0.0030993556897641206\n",
      "train loss:0.009137150472822275\n",
      "train loss:0.044212616599933925\n",
      "train loss:0.006045677350285964\n",
      "train loss:0.002153940378853184\n",
      "train loss:0.007969056997258707\n",
      "train loss:0.011916615263207156\n",
      "train loss:0.014796771613689919\n",
      "train loss:0.05028359328330362\n",
      "train loss:0.0013230364128029512\n",
      "train loss:0.007050584835570668\n",
      "train loss:0.029325913079934724\n",
      "train loss:0.01583736274117102\n",
      "train loss:0.08295005428312009\n",
      "train loss:0.024966163580905837\n",
      "train loss:0.01187960731060676\n",
      "train loss:0.07652982366980271\n",
      "train loss:0.020337488521914887\n",
      "train loss:0.014660507271099563\n",
      "train loss:0.03390159245309036\n",
      "train loss:0.011693128581282797\n",
      "train loss:0.014155865560332461\n",
      "train loss:0.005783848739033711\n",
      "train loss:0.004861854007136251\n",
      "train loss:0.023492069706819274\n",
      "train loss:0.009575708528696672\n",
      "train loss:0.01714150142631805\n",
      "train loss:0.00803003878854572\n",
      "train loss:0.009777053847973595\n",
      "train loss:0.013865933572089897\n",
      "train loss:0.00738471971998529\n",
      "train loss:0.017242377156395098\n",
      "train loss:0.01820054595853982\n",
      "train loss:0.009311682991285099\n",
      "train loss:0.014616115664258167\n",
      "train loss:0.008271643291323324\n",
      "train loss:0.008249400030645226\n",
      "train loss:0.027876576718540463\n",
      "train loss:0.024081121771534097\n",
      "train loss:0.024750992266531272\n",
      "train loss:0.0015426933995804797\n",
      "train loss:0.030264583257285555\n",
      "train loss:0.013396309364575197\n",
      "train loss:0.001734007076441959\n",
      "train loss:0.025277264904917184\n",
      "train loss:0.002487372017915023\n",
      "train loss:0.007938894300309663\n",
      "train loss:0.018405265188664435\n",
      "train loss:0.0075308927624595035\n",
      "train loss:0.012596229464898303\n",
      "train loss:0.019522349174407996\n",
      "train loss:0.031074027376673148\n",
      "train loss:0.005501314204924165\n",
      "train loss:0.0064494806627821595\n",
      "train loss:0.0323026961257252\n",
      "train loss:0.003301648456582147\n",
      "train loss:0.08571159461114927\n",
      "train loss:0.007155272633207043\n",
      "train loss:0.004740046955659604\n",
      "train loss:0.032294506008013625\n",
      "train loss:0.004504694056170361\n",
      "train loss:0.02876193735350725\n",
      "train loss:0.005648047757143084\n",
      "train loss:0.010861222180213395\n",
      "train loss:0.0052783462793961535\n",
      "train loss:0.007562962162970639\n",
      "train loss:0.0025372639583707028\n",
      "train loss:0.02036173548650842\n",
      "train loss:0.006991692154005174\n",
      "train loss:0.016897098424266666\n",
      "train loss:0.040261706224525816\n",
      "train loss:0.019401348377686745\n",
      "train loss:0.014484402626887605\n",
      "train loss:0.0253477769265273\n",
      "train loss:0.019798904563374943\n",
      "train loss:0.0025012318069497085\n",
      "train loss:0.0023127513222338477\n",
      "train loss:0.003907001997283186\n",
      "train loss:0.003898054956123092\n",
      "train loss:0.01649236592036578\n",
      "train loss:0.016760829486272662\n",
      "train loss:0.019996747866745282\n",
      "train loss:0.0016078263612649103\n",
      "train loss:0.007704668538519644\n",
      "train loss:0.0012716182309420255\n",
      "train loss:0.03477170347975887\n",
      "train loss:0.006610630688042759\n",
      "train loss:0.004495024738589371\n",
      "train loss:0.0053278488855175166\n",
      "train loss:0.006548596307935178\n",
      "train loss:0.012444867297406428\n",
      "train loss:0.02770581850860057\n",
      "train loss:0.0017046683311308002\n",
      "train loss:0.012142026334552946\n",
      "train loss:0.1164678078898047\n",
      "train loss:0.003572328028557454\n",
      "train loss:0.08130869240160335\n",
      "train loss:0.01118067568773235\n",
      "train loss:0.05143094729242136\n",
      "train loss:0.0019915369147933486\n",
      "train loss:0.004217886130353022\n",
      "train loss:0.03361073523083974\n",
      "train loss:0.006992124051396601\n",
      "train loss:0.015177267386890769\n",
      "train loss:0.029667270845204274\n",
      "train loss:0.0618653963588342\n",
      "train loss:0.03725064582321802\n",
      "train loss:0.012336715677669302\n",
      "train loss:0.009307502659147706\n",
      "train loss:0.0068884121174914185\n",
      "train loss:0.004818257518858754\n",
      "train loss:0.003099642799506212\n",
      "train loss:0.004143934599745262\n",
      "train loss:0.012305672537494204\n",
      "train loss:0.015660445280886556\n",
      "train loss:0.009181724447783179\n",
      "train loss:0.023671107291972925\n",
      "train loss:0.010123023964062608\n",
      "train loss:0.010829444755382039\n",
      "train loss:0.013348206290943333\n",
      "train loss:0.013965573822090875\n",
      "train loss:0.0062784635576945145\n",
      "train loss:0.004907597163534913\n",
      "train loss:0.008367196737045201\n",
      "train loss:0.005608169337178107\n",
      "train loss:0.01095416260780509\n",
      "train loss:0.009947616197766957\n",
      "train loss:0.0244103063688264\n",
      "train loss:0.019643486379194632\n",
      "train loss:0.005193312194370525\n",
      "train loss:0.007321420736191606\n",
      "train loss:0.06308770383977551\n",
      "train loss:0.0077513371824065045\n",
      "train loss:0.008677179863420773\n",
      "train loss:0.01818915526081123\n",
      "train loss:0.03326980348898874\n",
      "train loss:0.006031994574310135\n",
      "train loss:0.01902608576980457\n",
      "train loss:0.014652421695859674\n",
      "train loss:0.01996189907265467\n",
      "train loss:0.001444234862705972\n",
      "train loss:0.02518515017544121\n",
      "train loss:0.01893834104512252\n",
      "train loss:0.0021077076497938553\n",
      "train loss:0.030930047204027925\n",
      "train loss:0.03140336836585955\n",
      "train loss:0.037508850254806735\n",
      "train loss:0.01664498943315822\n",
      "train loss:0.004161726247595787\n",
      "train loss:0.009115404522221059\n",
      "train loss:0.008322063996032525\n",
      "train loss:0.014490486473402954\n",
      "train loss:0.0179111696437768\n",
      "train loss:0.0034447907366136356\n",
      "train loss:0.004236160057517937\n",
      "train loss:0.006073783143300673\n",
      "train loss:0.0022751607045833106\n",
      "train loss:0.006174441877493405\n",
      "train loss:0.007162616292634261\n",
      "train loss:0.003052557776536942\n",
      "train loss:0.0012220017323327204\n",
      "train loss:0.01725832126611766\n",
      "train loss:0.01942465014485945\n",
      "train loss:0.0011394104472840466\n",
      "train loss:0.014445231974425179\n",
      "train loss:0.002416439223107712\n",
      "train loss:0.0029144635132957834\n",
      "train loss:0.009685383108188313\n",
      "train loss:0.0127970698913691\n",
      "train loss:0.009728905425952094\n",
      "train loss:0.008934877670106875\n",
      "train loss:0.008529327462906983\n",
      "train loss:0.007436242863362501\n",
      "train loss:0.006762445283590299\n",
      "train loss:0.005396443942467336\n",
      "train loss:0.001948141467751698\n",
      "train loss:0.03178490069760318\n",
      "train loss:0.00540277559420353\n",
      "train loss:0.00850573883295212\n",
      "train loss:0.017281967499623692\n",
      "train loss:0.00154959909210124\n",
      "train loss:0.006962140756328256\n",
      "train loss:0.01268793102445124\n",
      "train loss:0.009497856627541353\n",
      "train loss:0.0110801225352932\n",
      "train loss:0.0018184826399069867\n",
      "train loss:0.005758789823852654\n",
      "train loss:0.0186700734076427\n",
      "train loss:0.0023240998052523436\n",
      "train loss:0.0017015623670394845\n",
      "train loss:0.0047289623091780995\n",
      "train loss:0.003922564484873755\n",
      "train loss:0.0022762416984374724\n",
      "train loss:0.005082405821253537\n",
      "train loss:0.009228899793409993\n",
      "train loss:0.002635938846138729\n",
      "train loss:0.001026989764597235\n",
      "train loss:0.020970377840442485\n",
      "train loss:0.0023478786636894904\n",
      "train loss:0.0020909865332432655\n",
      "train loss:0.010016857074003481\n",
      "train loss:0.003303454708779681\n",
      "train loss:0.015352872726725572\n",
      "train loss:0.021677232797238145\n",
      "train loss:0.04098547190268117\n",
      "train loss:0.006085445982894883\n",
      "train loss:0.009287484517656637\n",
      "train loss:0.005282225088792454\n",
      "train loss:0.007353022464605615\n",
      "train loss:0.02267384889826356\n",
      "train loss:0.02304511899985538\n",
      "train loss:0.03358238867002678\n",
      "train loss:0.006681729031345661\n",
      "train loss:0.003185766219844509\n",
      "train loss:0.04199630601599577\n",
      "train loss:0.00772000270626737\n",
      "train loss:0.008505457905675237\n",
      "train loss:0.0071974046711784445\n",
      "train loss:0.010005100469973587\n",
      "train loss:0.00870437623133129\n",
      "train loss:0.05224985909622979\n",
      "train loss:0.005013385111988258\n",
      "train loss:0.009153486992112235\n",
      "train loss:0.01358106334041255\n",
      "train loss:0.002098256324027563\n",
      "train loss:0.008136520241244186\n",
      "train loss:0.002763302557847917\n",
      "train loss:0.00886278516077691\n",
      "train loss:0.004569555529479528\n",
      "train loss:0.007651938469653524\n",
      "train loss:0.010755936267499329\n",
      "train loss:0.009532373086508407\n",
      "train loss:0.006585156952612182\n",
      "train loss:0.006827452830947323\n",
      "train loss:0.016964513444344565\n",
      "train loss:0.0040723039001627805\n",
      "train loss:0.02568900228479275\n",
      "train loss:0.0010428427418458653\n",
      "train loss:0.015472484193458423\n",
      "train loss:0.003614284311944443\n",
      "train loss:0.004331535794572353\n",
      "train loss:0.02218807045347737\n",
      "train loss:0.012988917423792292\n",
      "train loss:0.004283127557116814\n",
      "train loss:0.006497007982774437\n",
      "train loss:0.00781511875113896\n",
      "train loss:0.002994214982009423\n",
      "train loss:0.002752853450605662\n",
      "train loss:0.01191784140973956\n",
      "train loss:0.007741596135945705\n",
      "train loss:0.04812997240256967\n",
      "train loss:0.006156598166444166\n",
      "train loss:0.018317989361196295\n",
      "train loss:0.034141377501291294\n",
      "train loss:0.02543098797235234\n",
      "train loss:0.011180760395075264\n",
      "train loss:0.0019495710672700814\n",
      "train loss:0.0024045398885512346\n",
      "train loss:0.05031896437289366\n",
      "train loss:0.0023776274837901735\n",
      "train loss:0.007586078428273912\n",
      "train loss:0.00974390315976936\n",
      "train loss:0.07571392964129968\n",
      "train loss:0.005532147014413773\n",
      "train loss:0.007633853312248704\n",
      "train loss:0.006080045266760484\n",
      "train loss:0.002978801881509396\n",
      "train loss:0.006929680680889323\n",
      "train loss:0.00530255845823888\n",
      "train loss:0.02990939099946402\n",
      "train loss:0.0040708041157827834\n",
      "train loss:0.0054953736170354475\n",
      "train loss:0.07015432870500203\n",
      "train loss:0.004697897792520115\n",
      "train loss:0.0017572017150861057\n",
      "train loss:0.016829093242758643\n",
      "train loss:0.030090904391179996\n",
      "train loss:0.0008330453654474555\n",
      "train loss:0.003162235212856561\n",
      "train loss:0.05151630416929395\n",
      "train loss:0.015847577875830073\n",
      "train loss:0.0034361860352397936\n",
      "train loss:0.01966971537785555\n",
      "train loss:0.046263538344618126\n",
      "train loss:0.002230038529082834\n",
      "train loss:0.001826758184472925\n",
      "train loss:0.0039838882563240354\n",
      "train loss:0.004301742089513739\n",
      "train loss:0.04906537309682504\n",
      "train loss:0.005436029889140569\n",
      "train loss:0.024906925074402238\n",
      "train loss:0.0023391695577120587\n",
      "train loss:0.033224554701513795\n",
      "train loss:0.0030241605953895846\n",
      "train loss:0.007615548927748527\n",
      "train loss:0.023499846363319067\n",
      "train loss:0.016336082780066475\n",
      "train loss:0.016460121523996184\n",
      "train loss:0.004150151787140207\n",
      "train loss:0.0028615862498085924\n",
      "train loss:0.016144454090577017\n",
      "train loss:0.00657929866921075\n",
      "train loss:0.006687236531291497\n",
      "train loss:0.02258942110361722\n",
      "train loss:0.003983713014980735\n",
      "train loss:0.005824694914280287\n",
      "train loss:0.0015977040214837255\n",
      "train loss:0.00806434597436945\n",
      "train loss:0.004583115474204546\n",
      "train loss:0.0260775287277357\n",
      "train loss:0.045891706350403745\n",
      "train loss:0.0029682678679823996\n",
      "train loss:0.11193377971719105\n",
      "train loss:0.010825689011390893\n",
      "train loss:0.010266570611170298\n",
      "train loss:0.005639079323452071\n",
      "train loss:0.005621620595629806\n",
      "train loss:0.050248044587049644\n",
      "train loss:0.009700846454956436\n",
      "train loss:0.013829115246299633\n",
      "train loss:0.0030005577711239547\n",
      "train loss:0.0029145622986020796\n",
      "train loss:0.000767907417893005\n",
      "train loss:0.007018616592657418\n",
      "train loss:0.009379565704184632\n",
      "train loss:0.06412264962683119\n",
      "train loss:0.007886285930819836\n",
      "train loss:0.0013859989289267396\n",
      "train loss:0.004256400197289258\n",
      "train loss:0.0024578333176147413\n",
      "train loss:0.00704087777574841\n",
      "train loss:0.003492474491684898\n",
      "train loss:0.011300402620072647\n",
      "train loss:0.011325464430545447\n",
      "train loss:0.011592724525706716\n",
      "train loss:0.011211346691274887\n",
      "train loss:0.0030087388157373166\n",
      "train loss:0.03775339508533102\n",
      "train loss:0.0011854315311731875\n",
      "train loss:0.02892221840415745\n",
      "train loss:0.03047872798625639\n",
      "train loss:0.008139700068522368\n",
      "train loss:0.00804770423436587\n",
      "train loss:0.01775123040648612\n",
      "train loss:0.0038216794423876085\n",
      "train loss:0.02022943537481876\n",
      "train loss:0.007694673868966741\n",
      "train loss:0.008567551739600715\n",
      "train loss:0.007255816619424562\n",
      "train loss:0.021386409841587604\n",
      "train loss:0.009390662436132373\n",
      "train loss:0.006718664257445138\n",
      "train loss:0.006042836146292322\n",
      "train loss:0.010271174337842097\n",
      "train loss:0.0013773995257702766\n",
      "train loss:0.0074152701322312145\n",
      "train loss:0.003110916139927812\n",
      "train loss:0.006694677164114222\n",
      "train loss:0.010077036147021394\n",
      "train loss:0.010761964279022701\n",
      "train loss:0.004021700245094173\n",
      "train loss:0.007728406100668708\n",
      "train loss:0.011957022823429371\n",
      "train loss:0.004185184616286597\n",
      "train loss:0.0032757795546022373\n",
      "train loss:0.008127994458919891\n",
      "train loss:0.008405925472802951\n",
      "train loss:0.003038758976889027\n",
      "train loss:0.006094631657998276\n",
      "train loss:0.011192610186050986\n",
      "train loss:0.0018482615185031349\n",
      "train loss:0.010452367848557213\n",
      "train loss:0.0010290684774772348\n",
      "train loss:0.0036223620551650336\n",
      "train loss:0.030571692432123525\n",
      "train loss:0.004344515405328194\n",
      "train loss:0.003373787811497471\n",
      "train loss:0.013363660147500296\n",
      "train loss:0.001548657859996308\n",
      "train loss:0.004038591357361227\n",
      "train loss:0.0016366986239981023\n",
      "train loss:0.012820414574913487\n",
      "train loss:0.004688351699633815\n",
      "train loss:0.0033572431497591775\n",
      "train loss:0.0016019433155847942\n",
      "train loss:0.01079250774686174\n",
      "train loss:0.030426425261357876\n",
      "train loss:0.00911590051524501\n",
      "train loss:0.0027156469188334975\n",
      "train loss:0.004419869425209796\n",
      "train loss:0.008818137023578249\n",
      "train loss:0.001205322278347426\n",
      "train loss:0.011879631519910939\n",
      "train loss:0.011389037868711774\n",
      "train loss:0.01116158753853759\n",
      "train loss:0.03998113289004983\n",
      "train loss:0.002064234798176112\n",
      "train loss:0.005482388479428467\n",
      "train loss:0.004081284004077619\n",
      "train loss:0.0033981884336870953\n",
      "train loss:0.0073536345692595305\n",
      "train loss:0.010787392425356111\n",
      "train loss:0.030321675377067682\n",
      "train loss:0.002734510868251777\n",
      "train loss:0.014709795609144104\n",
      "train loss:0.0020661943408548837\n",
      "train loss:0.0071804510048915785\n",
      "train loss:0.00241097063131284\n",
      "train loss:0.010806170414608393\n",
      "train loss:0.0010175177957234879\n",
      "train loss:0.007226768131308145\n",
      "train loss:0.018415042643183843\n",
      "train loss:0.026419842505990516\n",
      "train loss:0.013617493513724804\n",
      "train loss:0.013692676071067684\n",
      "train loss:0.004959294109157295\n",
      "train loss:0.006785591256786912\n",
      "train loss:0.0020003827259376153\n",
      "train loss:0.03486417240053668\n",
      "train loss:0.01160640898845237\n",
      "train loss:0.03981660055365546\n",
      "train loss:0.02044607045788558\n",
      "train loss:0.025293834238760478\n",
      "train loss:0.0057976813686271824\n",
      "train loss:0.004731226493439765\n",
      "train loss:0.009279802967284458\n",
      "train loss:0.01664673953958847\n",
      "train loss:0.011011855409659594\n",
      "train loss:0.004763827694643664\n",
      "train loss:0.011096725743741225\n",
      "train loss:0.04219759152968349\n",
      "train loss:0.0025809733774275335\n",
      "train loss:0.007886274710978686\n",
      "train loss:0.007507767818819813\n",
      "train loss:0.0065506488676957235\n",
      "train loss:0.04572132183347911\n",
      "train loss:0.013249014471433484\n",
      "train loss:0.011301899720451756\n",
      "train loss:0.021539225229599456\n",
      "train loss:0.005286994384991053\n",
      "train loss:0.05921838121497151\n",
      "train loss:0.0065497062335934855\n",
      "train loss:0.11100428879215983\n",
      "train loss:0.0009223774860953951\n",
      "train loss:0.0012765862566126234\n",
      "train loss:0.028144401703513844\n",
      "train loss:0.01789985712194187\n",
      "train loss:0.013857669235318582\n",
      "train loss:0.009001921416767054\n",
      "train loss:0.0031462728616123386\n",
      "train loss:0.0026840546094603086\n",
      "train loss:0.005520058166809378\n",
      "train loss:0.0016487686823955334\n",
      "train loss:0.004865829764057518\n",
      "train loss:0.0019082275897526332\n",
      "train loss:0.003830228978338644\n",
      "train loss:0.014333958701076312\n",
      "train loss:0.0027916362639987995\n",
      "train loss:0.008111873114662986\n",
      "train loss:0.004371346912998053\n",
      "train loss:0.02138684971444998\n",
      "train loss:0.0167919558844725\n",
      "train loss:0.004531730917635044\n",
      "train loss:0.0029968653364637545\n",
      "train loss:0.014718550258162271\n",
      "train loss:0.0096453160922931\n",
      "train loss:0.01072296400312072\n",
      "train loss:0.02763922827382429\n",
      "train loss:0.009176093097736737\n",
      "train loss:0.0009635808334579319\n",
      "train loss:0.00918834560153285\n",
      "train loss:0.014933595372387452\n",
      "train loss:0.005742712880577579\n",
      "train loss:0.00537245043789504\n",
      "train loss:0.004241161097946558\n",
      "train loss:0.007493730389852552\n",
      "train loss:0.005902802628710807\n",
      "train loss:0.004029772907585827\n",
      "train loss:0.00494543741118004\n",
      "train loss:0.011273282575282319\n",
      "train loss:0.03278978074494121\n",
      "train loss:0.02447383341746028\n",
      "train loss:0.002180496838166022\n",
      "train loss:0.0018845682087370267\n",
      "train loss:0.006033027998707884\n",
      "train loss:0.05939734730836863\n",
      "train loss:0.003155957238109596\n",
      "train loss:0.02605956123562657\n",
      "train loss:0.014990717257806836\n",
      "train loss:0.006979879579302748\n",
      "train loss:0.016160049753128146\n",
      "train loss:0.011131535902431733\n",
      "train loss:0.024773476295078055\n",
      "train loss:0.008478707046480911\n",
      "train loss:0.02113820886419944\n",
      "train loss:0.011326271421240723\n",
      "train loss:0.014262477612352574\n",
      "train loss:0.003555045907959875\n",
      "train loss:0.005736014271720312\n",
      "train loss:0.006714042719464352\n",
      "train loss:0.0042481816484551994\n",
      "train loss:0.002028243823783405\n",
      "train loss:0.0036934845420066448\n",
      "train loss:0.018698994028677404\n",
      "train loss:0.007267721357035028\n",
      "train loss:0.01875171587706973\n",
      "train loss:0.0013261925856971025\n",
      "train loss:0.004360060517698205\n",
      "train loss:0.016790824074707212\n",
      "train loss:0.011028761244462407\n",
      "train loss:0.003226732378570435\n",
      "train loss:0.003467679686301471\n",
      "train loss:0.0052063588455474145\n",
      "train loss:0.00032132524585777914\n",
      "train loss:0.014266010068931993\n",
      "train loss:0.005420099944782789\n",
      "train loss:0.009986690368023153\n",
      "train loss:0.019152883202555912\n",
      "train loss:0.0044548251077782845\n",
      "train loss:0.0012913861225849845\n",
      "train loss:0.014938500065565989\n",
      "train loss:0.0015944645985865391\n",
      "train loss:0.0007126588248559998\n",
      "train loss:0.007290238013662007\n",
      "train loss:0.0030876449185652122\n",
      "train loss:0.003181412864777779\n",
      "train loss:0.004128286874523281\n",
      "train loss:0.001954995490844675\n",
      "train loss:0.002094233361786874\n",
      "train loss:0.006349783773511597\n",
      "train loss:0.01088590832666011\n",
      "train loss:0.013784350897316755\n",
      "train loss:0.06991322360852051\n",
      "train loss:0.0015023330122788494\n",
      "train loss:0.0010371136481926253\n",
      "train loss:0.0014345916574259243\n",
      "train loss:0.026573357922356183\n",
      "train loss:0.016532937017584518\n",
      "train loss:0.010642074573119551\n",
      "train loss:0.010711113293848766\n",
      "train loss:0.02611064160174585\n",
      "train loss:0.015968878454252412\n",
      "train loss:0.0028447431816258513\n",
      "train loss:0.002103528004508214\n",
      "train loss:0.004157917738744082\n",
      "train loss:0.02265402848038781\n",
      "train loss:0.012292944100164484\n",
      "train loss:0.013495545670172754\n",
      "train loss:0.004805000011789536\n",
      "train loss:0.005438867110165984\n",
      "train loss:0.005839626674336307\n",
      "train loss:0.0032736120995743617\n",
      "train loss:0.004366372484734412\n",
      "train loss:0.02337820659417251\n",
      "train loss:0.017434041271957336\n",
      "train loss:0.017170753052707866\n",
      "train loss:0.011780228667989145\n",
      "train loss:0.018749788412536286\n",
      "train loss:0.005833651909027157\n",
      "train loss:0.003574952492906051\n",
      "train loss:0.003431320358563657\n",
      "train loss:0.016898481680439587\n",
      "train loss:0.0060770154788036095\n",
      "=== epoch:10, train acc:0.993, test acc:0.985 ===\n",
      "train loss:0.006737190440520338\n",
      "train loss:0.01039502564224935\n",
      "train loss:0.02829156212505358\n",
      "train loss:0.020045279556217602\n",
      "train loss:0.016209309568446687\n",
      "train loss:0.0026672277603262858\n",
      "train loss:0.015592849050377167\n",
      "train loss:0.022702315553310246\n",
      "train loss:0.00742935570993218\n",
      "train loss:0.031262998005097714\n",
      "train loss:0.018645275266861647\n",
      "train loss:0.0018315208102151171\n",
      "train loss:0.004238192019320167\n",
      "train loss:0.0005361195251108653\n",
      "train loss:0.01103570526500272\n",
      "train loss:0.022664009547094098\n",
      "train loss:0.06462287422775935\n",
      "train loss:0.028551967887845645\n",
      "train loss:0.0021047054497763703\n",
      "train loss:0.03921630889334408\n",
      "train loss:0.043898625981104375\n",
      "train loss:0.0023377372236406345\n",
      "train loss:0.0077872068564190725\n",
      "train loss:0.003768266997917013\n",
      "train loss:0.021975162274731275\n",
      "train loss:0.02988922460327269\n",
      "train loss:0.021867453542341174\n",
      "train loss:0.05053900061627473\n",
      "train loss:0.013234006484879659\n",
      "train loss:0.00820414623422503\n",
      "train loss:0.0029555483918958835\n",
      "train loss:0.008943512956042805\n",
      "train loss:0.005420757137552738\n",
      "train loss:0.006495381004594823\n",
      "train loss:0.004361367735363867\n",
      "train loss:0.031179958447324435\n",
      "train loss:0.0051617739351362355\n",
      "train loss:0.023437459868335054\n",
      "train loss:0.0005335230874897648\n",
      "train loss:0.0049244703571248785\n",
      "train loss:0.0054456602792436635\n",
      "train loss:0.0029663059231176403\n",
      "train loss:0.004707153469068721\n",
      "train loss:0.023927445919556325\n",
      "train loss:0.0074547248032539185\n",
      "train loss:0.03143713246016175\n",
      "train loss:0.006762240095254756\n",
      "train loss:0.01306941681128714\n",
      "train loss:0.002456489025509711\n",
      "train loss:0.024141328682394034\n",
      "train loss:0.004142054124090249\n",
      "train loss:0.006795892298627532\n",
      "train loss:0.0037948480696797125\n",
      "train loss:0.03654102453458771\n",
      "train loss:0.014113281931517061\n",
      "train loss:0.024789862030494155\n",
      "train loss:0.015652083683129456\n",
      "train loss:0.003429633087568718\n",
      "train loss:0.0025965448602697063\n",
      "train loss:0.00872212150918996\n",
      "train loss:0.005340474356671391\n",
      "train loss:0.003068280077712522\n",
      "train loss:0.0019798669350341524\n",
      "train loss:0.037052886004426015\n",
      "train loss:0.004821356465985216\n",
      "train loss:0.0037240725483476305\n",
      "train loss:0.013892565683708396\n",
      "train loss:0.004014681490696468\n",
      "train loss:0.039158628299816554\n",
      "train loss:0.016051386505157715\n",
      "train loss:0.011381699525915903\n",
      "train loss:0.01142863568234493\n",
      "train loss:0.0038355437822175475\n",
      "train loss:0.01218601609998468\n",
      "train loss:0.003746073316648922\n",
      "train loss:0.005214696480053361\n",
      "train loss:0.010110060257967622\n",
      "train loss:0.029039083725757107\n",
      "train loss:0.011124559994689693\n",
      "train loss:0.0018210403444680495\n",
      "train loss:0.015355975099805842\n",
      "train loss:0.0051999196093082765\n",
      "train loss:0.020799156704307195\n",
      "train loss:0.011498550037371925\n",
      "train loss:0.013071651464333844\n",
      "train loss:0.00392852859776703\n",
      "train loss:0.004015115403676104\n",
      "train loss:0.011759881329878756\n",
      "train loss:0.006401404027979726\n",
      "train loss:0.0072984903527398325\n",
      "train loss:0.005748365436918056\n",
      "train loss:0.002679067865835094\n",
      "train loss:0.07122793051200077\n",
      "train loss:0.004566793629190381\n",
      "train loss:0.029971200610724467\n",
      "train loss:0.012778285070881992\n",
      "train loss:0.007961239523197942\n",
      "train loss:0.04339092213118444\n",
      "train loss:0.009303198020613735\n",
      "train loss:0.003040520843399358\n",
      "train loss:0.015792220852606457\n",
      "train loss:0.006216622143327497\n",
      "train loss:0.0036028179437867748\n",
      "train loss:0.01519635459105166\n",
      "train loss:0.029988285222598363\n",
      "train loss:0.007151728471728692\n",
      "train loss:0.003127930481615209\n",
      "train loss:0.0013623967384799845\n",
      "train loss:0.014501148063004968\n",
      "train loss:0.007506517735313044\n",
      "train loss:0.005778969591465669\n",
      "train loss:0.007518585993890192\n",
      "train loss:0.03252674452806067\n",
      "train loss:0.00036211750862455254\n",
      "train loss:0.008174159867359608\n",
      "train loss:0.008076304704543923\n",
      "train loss:0.0017105140701895807\n",
      "train loss:0.003938156282370473\n",
      "train loss:0.012795438780119541\n",
      "train loss:0.010582518583428607\n",
      "train loss:0.01673473487405995\n",
      "train loss:0.005691275238726047\n",
      "train loss:0.002325477616968305\n",
      "train loss:0.016407400920253553\n",
      "train loss:0.01095889350706056\n",
      "train loss:0.032117868466708936\n",
      "train loss:0.0031691779064289293\n",
      "train loss:0.024240578650469375\n",
      "train loss:0.008307454506195593\n",
      "train loss:0.007715132651783857\n",
      "train loss:0.07553599111103733\n",
      "train loss:0.006383583252756478\n",
      "train loss:0.029876692167465867\n",
      "train loss:0.0055805130110081255\n",
      "train loss:0.0037839665509659445\n",
      "train loss:0.0043919202987363504\n",
      "train loss:0.0030324974014300505\n",
      "train loss:0.0008136958287732437\n",
      "train loss:0.0012321919984600417\n",
      "train loss:0.013467727470712293\n",
      "train loss:0.003877431040190044\n",
      "train loss:0.01819231887223019\n",
      "train loss:0.003226972357974281\n",
      "train loss:0.010315702762319787\n",
      "train loss:0.005881452416483178\n",
      "train loss:0.007517773181290067\n",
      "train loss:0.0031511368965686988\n",
      "train loss:0.004980888794530565\n",
      "train loss:0.006482529605234051\n",
      "train loss:0.003923268490622358\n",
      "train loss:0.005111727502581325\n",
      "train loss:0.0059523813276889746\n",
      "train loss:0.0012753617235816096\n",
      "train loss:0.015000692828540345\n",
      "train loss:0.011265115051486483\n",
      "train loss:0.005767383900485085\n",
      "train loss:0.010399426716076554\n",
      "train loss:0.028671764498396497\n",
      "train loss:0.013947235344281706\n",
      "train loss:0.001912235081635427\n",
      "train loss:0.009742721648603314\n",
      "train loss:0.021349045601393\n",
      "train loss:0.006618904970160246\n",
      "train loss:0.003120836434815372\n",
      "train loss:0.01163069738123443\n",
      "train loss:0.011114827956982671\n",
      "train loss:0.006961880487597224\n",
      "train loss:0.012477280773010807\n",
      "train loss:0.009841280639151259\n",
      "train loss:0.0066155161263482766\n",
      "train loss:0.10610214792026275\n",
      "train loss:0.001349681664163657\n",
      "train loss:0.0020790777908554544\n",
      "train loss:0.016010803386393165\n",
      "train loss:0.00555295322475284\n",
      "train loss:0.010734087860383091\n",
      "train loss:0.0011978165519233811\n",
      "train loss:0.02278951624010146\n",
      "train loss:0.02605715303116767\n",
      "train loss:0.0019562395274417203\n",
      "train loss:0.07341049953045582\n",
      "train loss:0.008538868214663842\n",
      "train loss:0.009746895385558554\n",
      "train loss:0.019371893107743433\n",
      "train loss:0.008149207168561682\n",
      "train loss:0.024268233660313863\n",
      "train loss:0.0027841846426220128\n",
      "train loss:0.008374147866169432\n",
      "train loss:0.025985620626356808\n",
      "train loss:0.011170093339177239\n",
      "train loss:0.012580840110608045\n",
      "train loss:0.002778797893342802\n",
      "train loss:0.0014850937800223716\n",
      "train loss:0.002129467749704709\n",
      "train loss:0.0012739727152934247\n",
      "train loss:0.016491514154406337\n",
      "train loss:0.006205418208861873\n",
      "train loss:0.0018005530508264879\n",
      "train loss:0.0072226115568296565\n",
      "train loss:0.02690113216291931\n",
      "train loss:0.0036948391134393526\n",
      "train loss:0.009814180990475983\n",
      "train loss:0.007864605301871976\n",
      "train loss:0.026238191452594695\n",
      "train loss:0.008026133064401537\n",
      "train loss:0.001544859034452067\n",
      "train loss:0.0019744093906817126\n",
      "train loss:0.004095885906101744\n",
      "train loss:0.005651141306888581\n",
      "train loss:0.006696614186492986\n",
      "train loss:0.003544775057426114\n",
      "train loss:0.007049128903121745\n",
      "train loss:0.019722950408354433\n",
      "train loss:0.002185452817799198\n",
      "train loss:0.023379931586206965\n",
      "train loss:0.02462022274909659\n",
      "train loss:0.013932183504842398\n",
      "train loss:0.004789769224323528\n",
      "train loss:0.008877320499555813\n",
      "train loss:0.0017702668687859862\n",
      "train loss:0.005947834764171798\n",
      "train loss:0.008823760191384717\n",
      "train loss:0.0021997442609629574\n",
      "train loss:0.005328697436919491\n",
      "train loss:0.004195266141575207\n",
      "train loss:0.008741495910026046\n",
      "train loss:0.04355867170376122\n",
      "train loss:0.0010292772798122475\n",
      "train loss:0.0018149833372487223\n",
      "train loss:0.0070651654318942755\n",
      "train loss:0.0031403291800175034\n",
      "train loss:0.008680023621125502\n",
      "train loss:0.0023171700468546334\n",
      "train loss:0.007629145890948415\n",
      "train loss:0.005677435915377988\n",
      "train loss:0.0065339606750699185\n",
      "train loss:0.0016050832310035123\n",
      "train loss:0.0032271153000807806\n",
      "train loss:0.011024544955971336\n",
      "train loss:0.002609213414465683\n",
      "train loss:0.007067604271195282\n",
      "train loss:0.00478543028967366\n",
      "train loss:0.0035124899675466202\n",
      "train loss:0.02029476848423826\n",
      "train loss:0.0038400588292895077\n",
      "train loss:0.006643816950071652\n",
      "train loss:0.03477732370253818\n",
      "train loss:0.0063204467193217866\n",
      "train loss:0.018295774197221278\n",
      "train loss:0.0018641162725940052\n",
      "train loss:0.01733866257789126\n",
      "train loss:0.04388012285160586\n",
      "train loss:0.0030343549122166923\n",
      "train loss:0.0027011883956975986\n",
      "train loss:0.008729905511461999\n",
      "train loss:0.014122220894139049\n",
      "train loss:0.009359324884569888\n",
      "train loss:0.0060703564164822045\n",
      "train loss:0.0014674441577429364\n",
      "train loss:0.012658424023456323\n",
      "train loss:0.008447845549443993\n",
      "train loss:0.0046116623273985856\n",
      "train loss:0.0006207481643050514\n",
      "train loss:0.001442182047640712\n",
      "train loss:0.002782807176595883\n",
      "train loss:0.004012237120871836\n",
      "train loss:0.0019260297211686293\n",
      "train loss:0.020739646532705828\n",
      "train loss:0.00624276459574767\n",
      "train loss:0.0007715327014890358\n",
      "train loss:0.0011559048831071871\n",
      "train loss:0.0028417909988767477\n",
      "train loss:0.0016116508952768599\n",
      "train loss:0.006601436409691459\n",
      "train loss:0.024404197850480404\n",
      "train loss:0.00103554747790199\n",
      "train loss:0.014540211362557016\n",
      "train loss:0.007869108131353185\n",
      "train loss:0.007157690508920003\n",
      "train loss:0.0063118914453363685\n",
      "train loss:0.0036421398445800218\n",
      "train loss:0.022256518930334236\n",
      "train loss:0.004391393578326443\n",
      "train loss:0.019155867692162834\n",
      "train loss:0.0018184447224301\n",
      "train loss:0.005622900895448434\n",
      "train loss:0.005410121945064112\n",
      "train loss:0.005115780773128679\n",
      "train loss:0.009010668881896791\n",
      "train loss:0.001413409146016681\n",
      "train loss:0.000509007562415095\n",
      "train loss:0.021960937579754276\n",
      "train loss:0.0015093981719491616\n",
      "train loss:0.016488829748498565\n",
      "train loss:0.0038219580807681936\n",
      "train loss:0.008811589716234228\n",
      "train loss:0.0007148868794545284\n",
      "train loss:0.0034278949029092546\n",
      "train loss:0.0394826078071333\n",
      "train loss:0.0013107065217885638\n",
      "train loss:0.0005905579603244255\n",
      "train loss:0.001962649536153845\n",
      "train loss:0.0083307538632646\n",
      "train loss:0.003175971766692082\n",
      "train loss:0.005654845876014625\n",
      "train loss:0.02547731369135343\n",
      "train loss:0.0066500495241032034\n",
      "train loss:0.0045795121503642185\n",
      "train loss:0.003385414741606059\n",
      "train loss:0.005182619609964716\n",
      "train loss:0.013638762464476342\n",
      "train loss:0.020481494033570562\n",
      "train loss:0.00030435772440918205\n",
      "train loss:0.006076992769635677\n",
      "train loss:0.014841296118030398\n",
      "train loss:0.010437872832009965\n",
      "train loss:0.03162955391309462\n",
      "train loss:0.003534869190435849\n",
      "train loss:0.010354111531468552\n",
      "train loss:0.01167606780329669\n",
      "train loss:0.015865678603914274\n",
      "train loss:0.017155630885142006\n",
      "train loss:0.00773213924301234\n",
      "train loss:0.0043077492990864705\n",
      "train loss:0.004840119158903139\n",
      "train loss:0.00221217213026885\n",
      "train loss:0.002210010564573916\n",
      "train loss:0.003088640444117061\n",
      "train loss:0.005991581907689367\n",
      "train loss:0.0046130083407626326\n",
      "train loss:0.00307073858042372\n",
      "train loss:0.006508681681131989\n",
      "train loss:0.0018163240751406674\n",
      "train loss:0.006924487943464657\n",
      "train loss:0.0037742321797406696\n",
      "train loss:0.00905567011263664\n",
      "train loss:0.00458322628570387\n",
      "train loss:0.0003460816945481854\n",
      "train loss:0.029609456029293574\n",
      "train loss:0.0012854544834326436\n",
      "train loss:0.018893361437448627\n",
      "train loss:0.013781290619019799\n",
      "train loss:0.03022961362461516\n",
      "train loss:0.0029824887907419833\n",
      "train loss:0.011304726706242616\n",
      "train loss:0.0020494674449239234\n",
      "train loss:0.01146825609292327\n",
      "train loss:0.0017150511525754494\n",
      "train loss:0.008104342710105463\n",
      "train loss:0.017230492263303004\n",
      "train loss:0.007894191525795326\n",
      "train loss:0.007531843050261627\n",
      "train loss:0.004295013966357466\n",
      "train loss:0.011234186090565946\n",
      "train loss:0.007185809375895246\n",
      "train loss:0.006049249408063182\n",
      "train loss:0.002918265751790272\n",
      "train loss:0.005321789445391427\n",
      "train loss:0.004181909442892813\n",
      "train loss:0.014257548986708446\n",
      "train loss:0.011951541771316696\n",
      "train loss:0.04752592719267316\n",
      "train loss:0.009080939098304288\n",
      "train loss:0.00596302055148089\n",
      "train loss:0.005336455236535092\n",
      "train loss:0.003152536856896629\n",
      "train loss:0.0009547644116076918\n",
      "train loss:0.0031867697230854248\n",
      "train loss:0.011943144114957654\n",
      "train loss:0.0014615299846563062\n",
      "train loss:0.0014532113882826808\n",
      "train loss:0.001245641985886914\n",
      "train loss:0.0026578144878851677\n",
      "train loss:0.019890706997190238\n",
      "train loss:0.020931095089987684\n",
      "train loss:0.005042655276967326\n",
      "train loss:0.0031454619924254214\n",
      "train loss:0.0063927289806631924\n",
      "train loss:0.02193036142325867\n",
      "train loss:0.007018830664527003\n",
      "train loss:0.0035407770398671796\n",
      "train loss:0.0028331334138731795\n",
      "train loss:0.006855086792207266\n",
      "train loss:0.006288408172283009\n",
      "train loss:0.02477952361831439\n",
      "train loss:0.0057425875019455905\n",
      "train loss:0.002653437803138142\n",
      "train loss:0.011445422569896446\n",
      "train loss:0.029273122947396275\n",
      "train loss:0.004532249760516488\n",
      "train loss:0.005253997927207055\n",
      "train loss:0.0016055225137583382\n",
      "train loss:0.005685067500671928\n",
      "train loss:0.00905813584359303\n",
      "train loss:0.013930035199116857\n",
      "train loss:0.012004663433819635\n",
      "train loss:0.004829272557730688\n",
      "train loss:0.009043842942470303\n",
      "train loss:0.004924768194511067\n",
      "train loss:0.00432939835019051\n",
      "train loss:0.005507432395605693\n",
      "train loss:0.0027607928870542003\n",
      "train loss:0.004499020004985119\n",
      "train loss:0.0017585908779376516\n",
      "train loss:0.006947904563834663\n",
      "train loss:0.006179028849026512\n",
      "train loss:0.00182065729523313\n",
      "train loss:0.006149156344419396\n",
      "train loss:0.049413854771104965\n",
      "train loss:0.002926015846161547\n",
      "train loss:0.006294123750714056\n",
      "train loss:0.06059661070565493\n",
      "train loss:0.0008469273062255325\n",
      "train loss:0.0007599224529502871\n",
      "train loss:0.0014236593163740071\n",
      "train loss:0.004283711437253731\n",
      "train loss:0.009447128377312701\n",
      "train loss:0.0022153629628444316\n",
      "train loss:0.025168404703144246\n",
      "train loss:0.0010012630367066332\n",
      "train loss:0.0022103962474834486\n",
      "train loss:0.0010658193681834442\n",
      "train loss:0.0027337842568116084\n",
      "train loss:0.003213358300742393\n",
      "train loss:0.00498298919178396\n",
      "train loss:0.01641776727826142\n",
      "train loss:0.04328571180934773\n",
      "train loss:0.010583044814339842\n",
      "train loss:0.005271709418601159\n",
      "train loss:0.0038407684295356957\n",
      "train loss:0.010784908352166462\n",
      "train loss:0.013190510887273093\n",
      "train loss:0.0012140428541944958\n",
      "train loss:0.005228783207870221\n",
      "train loss:0.003772094523772479\n",
      "train loss:0.012320977038876819\n",
      "train loss:0.0010791804563650203\n",
      "train loss:0.011307776506953344\n",
      "train loss:0.0033342484791638533\n",
      "train loss:0.004678669226561501\n",
      "train loss:0.007937387016186731\n",
      "train loss:0.002549131216698657\n",
      "train loss:0.0013815355013625566\n",
      "train loss:0.007093202749120103\n",
      "train loss:0.002708702483314911\n",
      "train loss:0.0010321324995336111\n",
      "train loss:0.011959658013065184\n",
      "train loss:0.005878516227217607\n",
      "train loss:0.015406887933426509\n",
      "train loss:0.003001654579791461\n",
      "train loss:0.008522664006562626\n",
      "train loss:0.009869084321585614\n",
      "train loss:0.01063795897748555\n",
      "train loss:0.004983407688738058\n",
      "train loss:0.0016840816439524884\n",
      "train loss:0.0032290681234248466\n",
      "train loss:0.016841496833751825\n",
      "train loss:0.0029542424701846813\n",
      "train loss:0.004830024591281897\n",
      "train loss:0.004026441643497403\n",
      "train loss:0.012922939230543884\n",
      "train loss:0.003992322325378036\n",
      "train loss:0.003370392085724544\n",
      "train loss:0.003971791665880984\n",
      "train loss:0.1236791132471503\n",
      "train loss:0.00578846209467933\n",
      "train loss:0.0061102035169924305\n",
      "train loss:0.002247973432144095\n",
      "train loss:0.00526806123782446\n",
      "train loss:0.0024027363058697973\n",
      "train loss:0.007547320727870952\n",
      "train loss:0.0045181442192193\n",
      "train loss:0.0015408435369439583\n",
      "train loss:0.01930105406127733\n",
      "train loss:0.005749349630637227\n",
      "train loss:0.014997805164798783\n",
      "train loss:0.0063453265110732715\n",
      "train loss:0.004595857468832851\n",
      "train loss:0.005803158337707487\n",
      "train loss:0.00436553047400435\n",
      "train loss:0.007346749852120356\n",
      "train loss:0.002278255096881625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 22\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m# 3!\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    def __init__(self, input_dim=(1, 28, 28), \u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m                 hidden_size=100, output_size=10, weight_init_std=0.01):\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     18\u001b[0m trainer_3 \u001b[38;5;241m=\u001b[39m Trainer(network, x_train, t_train, x_test, t_test,\n\u001b[1;32m     19\u001b[0m                   epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, mini_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     20\u001b[0m                   optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer_param\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0.001\u001b[39m},\n\u001b[1;32m     21\u001b[0m                   evaluate_sample_num_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mtrainer_3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/2024/DeepLearningFromScratch/1/ch08/../common/trainer.py:73\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter):\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;241m-\u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/2024/DeepLearningFromScratch/1/ch08/../common/trainer.py:48\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mgradient(x_batch, t_batch)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mparams, grads)\n\u001b[0;32m---> 48\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loss_list\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(loss))\n",
      "File \u001b[0;32m~/2024/DeepLearningFromScratch/1/ch08/../ch07/simple_convnet.py:75\u001b[0m, in \u001b[0;36mSimpleConvNet.loss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m     68\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"  .\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    t :  \u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_layer\u001b[38;5;241m.\u001b[39mforward(y, t)\n",
      "File \u001b[0;32m~/2024/DeepLearningFromScratch/1/ch08/../ch07/simple_convnet.py:63\u001b[0m, in \u001b[0;36mSimpleConvNet.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m---> 63\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/2024/DeepLearningFromScratch/1/ch08/../common/layers.py:223\u001b[0m, in \u001b[0;36mConvolution.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    220\u001b[0m col \u001b[38;5;241m=\u001b[39m im2col(x, FH, FW, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad)\n\u001b[1;32m    221\u001b[0m col_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW\u001b[38;5;241m.\u001b[39mreshape(FN, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m--> 223\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_W\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb\u001b[49m\n\u001b[1;32m    224\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mreshape(N, out_h, out_w, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from ch07.simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = SimpleConvNet()  \n",
    "'''\n",
    "# 3!\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "'''\n",
    "trainer_3 = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=20, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer_3.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8318525-0442-4ff1-b178-a7dc325b546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100], output_size=10,\n",
    "                             activation='relu', weight_init_std=0.01)  \n",
    "\n",
    "trainer_4 = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=20, mini_batch_size=100, # 60000  // 100 = 600 * 20\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000) #  12000!!\n",
    "\n",
    "trainer_4.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83338cb2-7378-4905-a914-409ca02aee47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caluculate accuracy (float64) ... \n",
      "0.9926\n",
      "caluculate accuracy (float16) ... \n",
      "0.9926\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  #       \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deep_convnet import DeepConvNet\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = DeepConvNet()\n",
    "network.load_params(\"deep_convnet_params.pkl\")\n",
    "\n",
    "sampled = 10000 #   \n",
    "x_test = x_test[:sampled]\n",
    "t_test = t_test[:sampled]\n",
    "\n",
    "print(\"caluculate accuracy (float64) ... \")\n",
    "print(network.accuracy(x_test, t_test))\n",
    "\n",
    "# float16() \n",
    "x_test = x_test.astype(np.float16)\n",
    "for param in network.params.values():\n",
    "    param[...] = param.astype(np.float16)\n",
    "\n",
    "print(\"caluculate accuracy (float16) ... \")\n",
    "print(network.accuracy(x_test, t_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfce25fc-c622-4417-9708-e813fdf13ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating test accuracy ... \n",
      "test accuracy:0.9926\n",
      "======= misclassified result =======\n",
      "{view index: (label, inference), ...}\n",
      "{1: (4, 9), 2: (6, 5), 3: (6, 0), 4: (4, 9), 5: (3, 5), 6: (8, 2), 7: (6, 4), 8: (2, 1), 9: (1, 7), 10: (7, 4), 11: (3, 5), 12: (3, 5), 13: (8, 9), 14: (6, 5), 15: (9, 4), 16: (7, 1), 17: (1, 6), 18: (6, 0), 19: (2, 7), 20: (9, 4)}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  #       \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deep_convnet import DeepConvNet\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = DeepConvNet()\n",
    "network.load_params(\"deep_convnet_params.pkl\")\n",
    "\n",
    "print(\"calculating test accuracy ... \")\n",
    "#sampled = 1000\n",
    "#x_test = x_test[:sampled]\n",
    "#t_test = t_test[:sampled]\n",
    "\n",
    "classified_ids = []\n",
    "\n",
    "acc = 0.0\n",
    "batch_size = 100\n",
    "\n",
    "for i in range(int(x_test.shape[0] / batch_size)):\n",
    "    tx = x_test[i*batch_size:(i+1)*batch_size]\n",
    "    tt = t_test[i*batch_size:(i+1)*batch_size]\n",
    "    y = network.predict(tx, train_flg=False)\n",
    "    y = np.argmax(y, axis=1)\n",
    "    classified_ids.append(y)\n",
    "    acc += np.sum(y == tt)\n",
    "    \n",
    "acc = acc / x_test.shape[0]\n",
    "print(\"test accuracy:\" + str(acc))\n",
    "\n",
    "classified_ids = np.array(classified_ids)\n",
    "classified_ids = classified_ids.flatten()\n",
    " \n",
    "max_view = 20\n",
    "current_view = 1\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.2, wspace=0.2)\n",
    "\n",
    "mis_pairs = {}\n",
    "for i, val in enumerate(classified_ids == t_test):\n",
    "    if not val:\n",
    "        ax = fig.add_subplot(4, 5, current_view, xticks=[], yticks=[])\n",
    "        ax.imshow(x_test[i].reshape(28, 28), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "        mis_pairs[current_view] = (t_test[i], classified_ids[i])\n",
    "            \n",
    "        current_view += 1\n",
    "        if current_view > max_view:\n",
    "            break\n",
    "\n",
    "print(\"======= misclassified result =======\")\n",
    "print(\"{view index: (label, inference), ...}\")\n",
    "print(mis_pairs)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7c1dfa-ebe3-45ee-86cb-29c7f8d1e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=False, flatten=False)\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2b91b1-32a5-4b04-bf89-6afe40f13be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=False)\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b5eb44-0e46-4e8f-9c4e-9dc3a0ec9851",
   "metadata": {},
   "source": [
    "    if normalize:\n",
    "        for key in ('train_img', 'test_img'):\n",
    "            dataset[key] = dataset[key].astype(np.float32)\n",
    "            dataset[key] /= 255.0\n",
    "            ->      elment-wise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7cd328-edb6-48cb-871a-df4e0fdbe42f",
   "metadata": {},
   "source": [
    "    def _change_one_hot_label(X):\n",
    "        T = np.zeros((X.size, 10))\n",
    "        for idx, row in enumerate(T):\n",
    "            row[X[idx]] = 1\n",
    "            \n",
    "        return T    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbddaf62-d132-4405-9825-d317f11ec003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2bce48d-f010-40bb-b7e4-92f5cff86317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = np.zeros((t_train.size, 10))\n",
    "T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8043c05c-1adb-4ef3-a3ce-ba3fbb7f55d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "867c4fb5-7b40-4c7d-9f2c-3b9597dfad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in enumerate(T):\n",
    "    row[t_train[idx]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad193ee9-5dde-4e25-b1cd-9f7e85fbd84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afe86f65-c21f-44c9-9c74-c0fce344312b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3741a6-b6f4-4be6-8f61-0a9c6c7e7a45",
   "metadata": {},
   "source": [
    "    if not flatten:  ->     data flatten  \r\n",
    "         for key in ('train_img', 'test_img'):\r\n",
    "            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\r\n",
    "\r\n",
    "    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd7c6a84-378c-4303-8fe5-87aecd4089a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 1, 28, 28)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=False)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9a76555-7291-4f0b-a486-eec8dce08312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7a673b90-d020-491d-81c9-8d834b263cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import pickle\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "# from collections import OrderedDict\n",
    "\n",
    "class DeepConvNet:\n",
    "    \"\"\"  99%    CNN\n",
    "    (   ,     )\n",
    "    \n",
    "    conv - relu - conv - relu - pool -\n",
    "    conv - relu - conv - relu - pool -\n",
    "    conv - relu - conv - relu - pool - \n",
    "    affine - relu - dropout - affine - dropout - softmax\n",
    "\n",
    "    He  & ReLU & Adam optimizer & dropout\n",
    "     3x3 (small)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), # (N,1,28,28)\n",
    "                 #      (FN,C,FH,FW) -> (N,FN,OH,OW)\n",
    "                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1}, # conv1\n",
    "                 conv_param_2 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1}, # conv2\n",
    "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1}, # conv3\n",
    "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1}, # conv4\n",
    "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1}, # conv5\n",
    "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1}, # conv6\n",
    "                 hidden_size=50, output_size=10): # affine(7,8)\n",
    "    \n",
    "        # 1. filter ==============================================================================\n",
    "        #           (N)\n",
    "        # 6   2         (C/FN,FH,FW)\n",
    "        \"\"\"\n",
    "        [Input] --(1*3*3)--> [Conv1] --(16*3*3)--> [Conv2] --(16*3*3)--> [Conv3] --(32*3*3)--> [Conv4] --(32*3*3)--> [Conv5] --(64*3*3)--> [Conv6] \n",
    "        --(64*4*4)--> [Hidden] --(hidden_size)--> [Output]\n",
    "        \"\"\"\n",
    "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n",
    "        weight_init_scales = np.sqrt(2.0 / pre_node_nums) # He  (ReLu  )\n",
    "\n",
    "        # 2.  ==================================================================================\n",
    "        self.params = {}\n",
    "        pre_channel_num = input_dim[0] #(1,28,28)\n",
    "        # 2(1)   (FN,C,FH,FW), (FN,)\n",
    "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
    "            self.params['W' + str(idx+1)] = weight_init_scales[idx] \\\n",
    "                                            * np.random.rands(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
    "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
    "            pre_channel_num = conv_param['filter_num']\n",
    "        # 2(2)   (input/filter,hidden/output)\n",
    "        self.params['W7'] = weight_init_scales[6] * np.random.randn(64*4*4, hidden_size)\n",
    "        self.params['b7'] = np.zeros(hidden_size)\n",
    "        self.params['W8'] = weight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b8'] = np.zeros(output_size)\n",
    "\n",
    "        # 3. layer ======================================================================================\n",
    "        self.layers = [] # OrderedDict()\n",
    "        # 3(1) (self, W, b, stride=1, pad=0) / (self, pool_h, pool_w, stride=1, pad=0)\n",
    "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], conv_param_1['stride'], conv_param_1['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        \n",
    "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], conv_param_2['stride'], conv_param_2['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        \n",
    "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], conv_param_3['stride'], conv_param_3['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        \n",
    "        self.layers.append(Convolution(self.params['W4'], self.params['b4'], conv_param_4['stride'], conv_param_4['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        \n",
    "        self.layers.append(Convolution(self.params['W5'], self.params['b5'], conv_param_5['stride'], conv_param_5['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        \n",
    "        self.layers.append(Convolution(self.params['W6'], self.params['b6'], conv_param_6['stride'], conv_param_6['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))         \n",
    "        \n",
    "        # 3(2) (self, W, b) / (self, dropout_ratio=0.5)\n",
    "        self.layers.append(Affine(self.params['W7'], self, params['b7']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Dropout(0.5))\n",
    "\n",
    "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
    "        self.layers.append(Dropout(0.5))\n",
    "                           \n",
    "        # 3(3)\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    # 4. predict(forward)=====================================================================================\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout): #    / \n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "    # 5. loss(forward~loss)====================================================================================\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x, train_flg=True) # train true default\n",
    "        return self.last_layer.forward(y, t)\n",
    "        \n",
    "    # 6. accuracy==============================================================================================\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        # ->label\n",
    "        if t.dim != 1 : t = np.argmax(t, axis=1)             \n",
    "        #  \n",
    "        acc = 0.0\n",
    "        # 1 epoch!! (    )\n",
    "        for i in range(int(x.shape[0] / batch_size)): # \n",
    "            tx = x[i*batch_size:(i+1)*batch_size] # 0*100 ~ 1*100\n",
    "            tt = t[i*batch_size:(i+1)*batch_size] # -> slicing\n",
    "            y = self.predict(tx, train_flag=False) # regularization \n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    # 7. for&back&grads=========================================================================================\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        tmp_layers = self.layers.copy() # layers = list(self.layers.values())\n",
    "        tmp_layers.reverse()\n",
    "        for layer in tmp_layers:\n",
    "            dout = layer.backward(dout)\n",
    "        # save grads\n",
    "        grads = {}                    # c-r - c-r-p - c-r - c-r-p - c-r - c-r-p - a-r-d - a-d - s\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            grads['w' + str(i+1)] = self.layers[layer_idx].dW\n",
    "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
    "        return grads\n",
    "\n",
    "    # 9.  (pickle)======================================================================================\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        #  dict \n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        # \n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "            \n",
    "    # 9.  (pickle)======================================================================================\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        # self.params \n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "        # layer     \n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
    "            self.layers[layer_idx].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a2d8be-7416-4d99-a35e-b496c859609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from deep_convnet import DeepConvNet \n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = DeepConvNet()\n",
    "# Trainer : optimizer, epoch acc,  test acc\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=20, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "\n",
    "triner.train()\n",
    "\n",
    "#  \n",
    "network.save_params(\"deep_convnet_params.pkl\")\n",
    "print(\"Saved Network Parameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fe81802d-97c6-4a65-bf20-cf300d0f414e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating test accuracy ... \n",
      "test accuracy:0.9926\n",
      "======= misclassified result =======\n",
      "{view index: (label, inference), ...}\n",
      "{1: (4, 9), 2: (6, 5), 3: (6, 0), 4: (4, 9), 5: (3, 5), 6: (8, 2), 7: (6, 4), 8: (2, 1), 9: (1, 7), 10: (7, 4), 11: (3, 5), 12: (3, 5), 13: (8, 9), 14: (6, 5), 15: (9, 4), 16: (7, 1), 17: (1, 6), 18: (6, 0), 19: (2, 7), 20: (9, 4)}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deep_convnet import DeepConvNet\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# trainer !  test\n",
    "network = DeepConvNet()\n",
    "network.load_params(\"deep_convnet_params.pkl\")\n",
    "print(\"calculating test accuracy ... \")\n",
    "\n",
    "#  \n",
    "classified_ids = []\n",
    "\n",
    "acc = 0.0\n",
    "batch_size = 100\n",
    "# 1 epoch ( ) : 100 (0~99)\n",
    "for i in range(int(x_test.shape[0] / batch_size)):\n",
    "    tx = x_test[i*batch_size:(i+1)*batch_size]\n",
    "    tt = t_test[i*batch_size:(i+1)*batch_size]\n",
    "    y = network.predict(tx, train_flg=False) # logits(score)\n",
    "    y = np.argmax(y, axis=1)\n",
    "    classified_ids.append(y) # 1 100 set \n",
    "    acc += np.sum(y == tt)\n",
    "\n",
    "acc = acc / x_test.shape[0]\n",
    "print(\"test accuracy:\" + str(acc))\n",
    "\n",
    "classified_ids = np.array(classified_ids) # list to array\n",
    "classified_ids = classified_ids.flatten() # (10000,) !\n",
    "                                          #  (100,100)\n",
    "\n",
    "#   20  \n",
    "max_view = 20\n",
    "current_view = 1\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.2, wspace=0.2)\n",
    "\n",
    "mis_pairs = {}\n",
    "for i, val in enumerate(classified_ids == t_test): # 1. boolean   (classified_ids == t_test)\n",
    "                                                   # 2.   1~1000(i)\n",
    "    if not val: # False,  \n",
    "        ax = fig.add_subplot(4, 5, current_view, xticks=[], yticks=[]) # add_subplot(r,c,idx)\n",
    "        ax.imshow(x_test[i].reshape(28, 28), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "        mis_pairs[current_view] = (t_test[i], classified_ids[i])\n",
    "\n",
    "        current_view += 1\n",
    "        if current_view > max_view:\n",
    "            break\n",
    "\n",
    "print(\"======= misclassified result =======\")\n",
    "print(\"{view index: (label, inference), ...}\")\n",
    "print(mis_pairs)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "20ebce1f-bd21-4f07-b7c3-b3bb120e994c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating test accuracy ... \n",
      "test accuracy:0.9926\n",
      "======= misclassified result =======\n",
      "{view index: (label, inference), ...}\n",
      "{1: (4, 9), 2: (6, 5), 3: (6, 0), 4: (4, 9), 5: (3, 5), 6: (8, 2), 7: (6, 4), 8: (2, 1), 9: (1, 7), 10: (7, 4), 11: (3, 5), 12: (3, 5), 13: (8, 9), 14: (6, 5), 15: (9, 4), 16: (7, 1), 17: (1, 6), 18: (6, 0), 19: (2, 7), 20: (9, 4)}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deep_convnet import DeepConvNet\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# trainer !  test\n",
    "network = DeepConvNet()\n",
    "network.load_params(\"deep_convnet_params.pkl\")\n",
    "print(\"calculating test accuracy ... \")\n",
    "\n",
    "#  \n",
    "classified_ids = []\n",
    "\n",
    "acc = 0.0\n",
    "batch_size = 100\n",
    "# 1 epoch ( ) : 100 (0~99)\n",
    "for i in range(int(x_test.shape[0] / batch_size)):\n",
    "    tx = x_test[i*batch_size:(i+1)*batch_size]\n",
    "    tt = t_test[i*batch_size:(i+1)*batch_size]\n",
    "    y = network.predict(tx, train_flg=False) # logits(score)\n",
    "    y = np.argmax(y, axis=1)\n",
    "    classified_ids.append(y) # 1 100 set \n",
    "    acc += np.sum(y == tt)\n",
    "\n",
    "acc = acc / x_test.shape[0]\n",
    "print(\"test accuracy:\" + str(acc))\n",
    "\n",
    "classified_ids = np.array(classified_ids) # list to array\n",
    "classified_ids = classified_ids.flatten() # (10000,) !\n",
    "                                          #  (100,100)\n",
    "\n",
    "#   20  \n",
    "max_view = 20\n",
    "current_view = 1\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.2, wspace=0.2)\n",
    "\n",
    "mis_pairs = {}\n",
    "for i, val in enumerate(classified_ids == t_test):\n",
    "    if not val:\n",
    "        ax = fig.add_subplot(4, 5, current_view, xticks=[], yticks=[])\n",
    "        ax.imshow(x_test[i].reshape(28, 28), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "        \n",
    "        #     (label)  (   , 0~1)\n",
    "        ax.text(0.05, 0.95, t_test[i], ha='left', va='top', transform=ax.transAxes) # x/y  \n",
    "        #     (inference) \n",
    "        ax.text(0.95, 0.05, classified_ids[i], ha='right', va='bottom', transform=ax.transAxes)\n",
    "        \n",
    "        mis_pairs[current_view] = (t_test[i], classified_ids[i])\n",
    "\n",
    "        current_view += 1\n",
    "        if current_view > max_view:\n",
    "            break\n",
    "\n",
    "print(\"======= misclassified result =======\")\n",
    "print(\"{view index: (label, inference), ...}\")\n",
    "print(mis_pairs)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fb3ea8dd-dd52-48f5-a7cf-b213d41a26d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classified_ids = np.array(classified_ids)\n",
    "classified_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "410f9e0c-f797-41e2-ab1f-12a0f3daff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_ids = classified_ids.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3de88fbb-e0d6-40bf-b127-ae2d916a7075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 False\n",
      "1 False\n",
      "2 False\n",
      "3 False\n",
      "4 False\n"
     ]
    }
   ],
   "source": [
    "t_test = [3, 4, 7, 3, 2]\n",
    "for i, val in enumerate(classified_ids == t_test):\n",
    "    print(i,val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "314b0bc6-61b2-4911-8331-64ff49015c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating test accuracy ... \n",
      "test accuracy:0.9926\n",
      "[[7 2 1 ... 7 6 9]\n",
      " [6 0 5 ... 6 4 2]\n",
      " [3 6 1 ... 0 2 8]\n",
      " ...\n",
      " [2 9 5 ... 3 2 1]\n",
      " [0 0 1 ... 0 1 7]\n",
      " [8 9 0 ... 4 5 6]]\n",
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  #       \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deep_convnet import DeepConvNet\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = DeepConvNet()\n",
    "network.load_params(\"deep_convnet_params.pkl\")\n",
    "\n",
    "print(\"calculating test accuracy ... \")\n",
    "#sampled = 1000\n",
    "#x_test = x_test[:sampled]\n",
    "#t_test = t_test[:sampled]\n",
    "\n",
    "classified_ids = []\n",
    "\n",
    "acc = 0.0\n",
    "batch_size = 100\n",
    "\n",
    "for i in range(int(x_test.shape[0] / batch_size)):\n",
    "    tx = x_test[i*batch_size:(i+1)*batch_size]\n",
    "    tt = t_test[i*batch_size:(i+1)*batch_size]\n",
    "    y = network.predict(tx, train_flg=False)\n",
    "    y = np.argmax(y, axis=1)\n",
    "    classified_ids.append(y)\n",
    "    acc += np.sum(y == tt)\n",
    "    \n",
    "acc = acc / x_test.shape[0]\n",
    "print(\"test accuracy:\" + str(acc))\n",
    "\n",
    "classified_ids = np.array(classified_ids)\n",
    "print(classified_ids)\n",
    "print(classified_ids.shape)\n",
    "#classified_ids = classified_ids.flatten() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c526107e-db05-4e25-b7a9-848f8f358071",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(int(x_test.shape[0] / batch_size)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd109e8f-1f62-4cd3-a10b-6ceb423e59d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(int(x_test.shape[0] / batch_size)):\n",
    "    print(i*batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677af0c1-7f84-4fbb-8645-b4a0427d224e",
   "metadata": {},
   "source": [
    "  Matplotlib subplot     .   `subplots_adjust()`  subplot      .\n",
    "\n",
    "      :\n",
    "\n",
    "- `left`: subplot    figure      .   .\n",
    "- `right`: subplot    figure      .   .\n",
    "- `bottom`: subplot    figure      .   .\n",
    "- `top`: subplot    figure      .   .\n",
    "- `hspace`:   . subplot    .\n",
    "- `wspace`:   . subplot    .\n",
    "\n",
    "   subplot    `left`, `right`, `bottom`, `top` 0 , subplot     `hspace`, `wspace` 0.2  .   subplot         ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90aae73-43a8-4ce2-b013-3ded97df2325",
   "metadata": {},
   "source": [
    "transform=ax.transAxes     .    x y        .\n",
    "\n",
    "ax.transAxes      ,   (0, 0)   , (1, 1)   . ,   (0, 1)   , (1, 0)    .\n",
    "\n",
    "              .        ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
