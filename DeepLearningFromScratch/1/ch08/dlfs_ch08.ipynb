{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d476befa-20d3-4104-b70a-2048da8db4c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.302327301857172\n",
      "=== epoch:1, train acc:0.109, test acc:0.088 ===\n",
      "train loss:2.3024350168863017\n",
      "train loss:2.302999776383075\n",
      "train loss:2.303685682128154\n",
      "train loss:2.3020597794836926\n",
      "train loss:2.3025446987890272\n",
      "train loss:2.3026421219653184\n",
      "train loss:2.3013832706687603\n",
      "train loss:2.302978278464221\n",
      "train loss:2.302997774624338\n",
      "train loss:2.301174223504152\n",
      "train loss:2.302991421689626\n",
      "train loss:2.3023600709352987\n",
      "train loss:2.3025371596204165\n",
      "train loss:2.3020926553534284\n",
      "train loss:2.302811409616474\n",
      "train loss:2.3015391551655044\n",
      "train loss:2.3022224323800464\n",
      "train loss:2.3019468607646196\n",
      "train loss:2.3034405485838665\n",
      "train loss:2.2995766390949433\n",
      "train loss:2.302882485714448\n",
      "train loss:2.30324387650021\n",
      "train loss:2.303214063990147\n",
      "train loss:2.301711316726939\n",
      "train loss:2.3020787640244054\n",
      "train loss:2.3022636534730414\n",
      "train loss:2.304731370142651\n",
      "train loss:2.300921146024305\n",
      "train loss:2.301859448637565\n",
      "train loss:2.3016803518681384\n",
      "train loss:2.3007414558983057\n",
      "train loss:2.301450772862166\n",
      "train loss:2.302568135792512\n",
      "train loss:2.303011355871179\n",
      "train loss:2.3018308201603754\n",
      "train loss:2.303002300762557\n",
      "train loss:2.3021827669727557\n",
      "train loss:2.3019030334641695\n",
      "train loss:2.2991266572157274\n",
      "train loss:2.298714262859594\n",
      "train loss:2.2996965659765696\n",
      "train loss:2.298316838374196\n",
      "train loss:2.3004163286139003\n",
      "train loss:2.3024512243068553\n",
      "train loss:2.299792487049118\n",
      "train loss:2.298115659677342\n",
      "train loss:2.30057597100343\n",
      "train loss:2.2996637125798145\n",
      "train loss:2.2987264257153743\n",
      "train loss:2.297967738462778\n",
      "train loss:2.297896585522851\n",
      "train loss:2.295502689976636\n",
      "train loss:2.2994925593784403\n",
      "train loss:2.297636550498463\n",
      "train loss:2.296241770474934\n",
      "train loss:2.287815737436499\n",
      "train loss:2.296333915678386\n",
      "train loss:2.296253838526907\n",
      "train loss:2.2912069358817906\n",
      "train loss:2.2904869180176863\n",
      "train loss:2.2859780669577354\n",
      "train loss:2.283731842947997\n",
      "train loss:2.286509731479491\n",
      "train loss:2.2843230237902943\n",
      "train loss:2.281590010061771\n",
      "train loss:2.2771330947372412\n",
      "train loss:2.2719881529230346\n",
      "train loss:2.2719137451598117\n",
      "train loss:2.273166420713092\n",
      "train loss:2.2753443242570075\n",
      "train loss:2.2681523781006034\n",
      "train loss:2.262111794568187\n",
      "train loss:2.238684249912894\n",
      "train loss:2.265366301521124\n",
      "train loss:2.2164919071485194\n",
      "train loss:2.2520165300355086\n",
      "train loss:2.234137212723695\n",
      "train loss:2.2152403936583944\n",
      "train loss:2.2415528048160267\n",
      "train loss:2.230049484497827\n",
      "train loss:2.214138173351136\n",
      "train loss:2.1922695918547763\n",
      "train loss:2.179215585431418\n",
      "train loss:2.157986300067487\n",
      "train loss:2.1759021314520854\n",
      "train loss:2.1542814800416696\n",
      "train loss:2.143957648926783\n",
      "train loss:2.148950287220499\n",
      "train loss:2.1854398128898227\n",
      "train loss:2.135269784343642\n",
      "train loss:2.132293164931027\n",
      "train loss:2.1180738671885964\n",
      "train loss:2.1334824940890096\n",
      "train loss:2.0651141629015397\n",
      "train loss:2.0900291054776265\n",
      "train loss:2.078926714958739\n",
      "train loss:2.1695273844985863\n",
      "train loss:2.175276757632173\n",
      "train loss:2.126792180272326\n",
      "train loss:2.1469003199239025\n",
      "train loss:2.122746205070913\n",
      "train loss:2.103407097146024\n",
      "train loss:2.1227404478797274\n",
      "train loss:2.0805952923665827\n",
      "train loss:1.9818640692866407\n",
      "train loss:2.080514281754576\n",
      "train loss:2.083371219539622\n",
      "train loss:2.0812502430574766\n",
      "train loss:2.1039068595969126\n",
      "train loss:2.051418557274935\n",
      "train loss:2.028944892811935\n",
      "train loss:2.0117506243215377\n",
      "train loss:2.049272003336898\n",
      "train loss:2.0563123497199034\n",
      "train loss:2.0684305995263657\n",
      "train loss:2.0543968053814403\n",
      "train loss:2.0541202765922595\n",
      "train loss:2.080756192071971\n",
      "train loss:2.080938068069179\n",
      "train loss:2.016822100338005\n",
      "train loss:2.065270417802282\n",
      "train loss:2.052707226492107\n",
      "train loss:2.0420548367554674\n",
      "train loss:2.090584321316314\n",
      "train loss:2.0567464987997828\n",
      "train loss:2.085311920525038\n",
      "train loss:2.0775526536830835\n",
      "train loss:2.080380220504308\n",
      "train loss:2.0654414613089673\n",
      "train loss:2.045556376174074\n",
      "train loss:2.0267350927205747\n",
      "train loss:2.0502741688729653\n",
      "train loss:2.023232067612069\n",
      "train loss:2.0086385871982735\n",
      "train loss:2.0637378198978786\n",
      "train loss:2.012502047614257\n",
      "train loss:2.0840114986058835\n",
      "train loss:2.0425498044586115\n",
      "train loss:2.0633178932778837\n",
      "train loss:2.009488344084128\n",
      "train loss:2.0010071686241533\n",
      "train loss:1.9569107654599731\n",
      "train loss:1.989229636554913\n",
      "train loss:2.026385588600444\n",
      "train loss:1.9887503237989541\n",
      "train loss:1.992621762301255\n",
      "train loss:2.049685820038139\n",
      "train loss:2.0870200888139405\n",
      "train loss:1.997987890456952\n",
      "train loss:1.9243487686535559\n",
      "train loss:2.0946798007916745\n",
      "train loss:2.014609154324644\n",
      "train loss:2.068796404622576\n",
      "train loss:2.0535678916068285\n",
      "train loss:2.0597983840233085\n",
      "train loss:2.035398601526932\n",
      "train loss:2.0219304727154723\n",
      "train loss:2.0257080831581344\n",
      "train loss:2.0077704961739626\n",
      "train loss:2.0340586863939816\n",
      "train loss:2.0068190162534365\n",
      "train loss:1.9172239724298663\n",
      "train loss:2.0260108403024883\n",
      "train loss:1.9855429075796973\n",
      "train loss:2.03305050934389\n",
      "train loss:2.0161769490060615\n",
      "train loss:1.8482021644160576\n",
      "train loss:1.974170267511061\n",
      "train loss:1.966799779173337\n",
      "train loss:2.0209570453780916\n",
      "train loss:1.8956432509281296\n",
      "train loss:1.9913510097871978\n",
      "train loss:2.02748020111005\n",
      "train loss:1.9509117677911354\n",
      "train loss:1.8840222146674188\n",
      "train loss:1.9890598465972524\n",
      "train loss:1.993847871432211\n",
      "train loss:2.0181182744824984\n",
      "train loss:1.931537308534591\n",
      "train loss:1.9947526970263474\n",
      "train loss:2.0317698260795214\n",
      "train loss:1.966863575561408\n",
      "train loss:1.9796101623539528\n",
      "train loss:1.9843736339419875\n",
      "train loss:2.0263651319173537\n",
      "train loss:2.033534191070077\n",
      "train loss:2.0281032561565113\n",
      "train loss:2.072422050935744\n",
      "train loss:1.9197677618445972\n",
      "train loss:1.9892494810655095\n",
      "train loss:2.030378872643789\n",
      "train loss:2.0012565230614543\n",
      "train loss:1.9281732608086615\n",
      "train loss:1.977226703422094\n",
      "train loss:2.0717475491798165\n",
      "train loss:1.9563597372205905\n",
      "train loss:1.8755839533939107\n",
      "train loss:1.9341831601844723\n",
      "train loss:2.0259540142890855\n",
      "train loss:1.9112734076230538\n",
      "train loss:1.8816054936893465\n",
      "train loss:1.915138942575525\n",
      "train loss:2.019141628835714\n",
      "train loss:1.9470229004865607\n",
      "train loss:1.8940231652789634\n",
      "train loss:1.9973962536488545\n",
      "train loss:2.022929287939622\n",
      "train loss:1.9758077826523277\n",
      "train loss:1.9401682610443165\n",
      "train loss:2.0447631506661104\n",
      "train loss:2.017302074387139\n",
      "train loss:1.9250534526881058\n",
      "train loss:1.9422996790822444\n",
      "train loss:1.9135760545648484\n",
      "train loss:1.9336977994196696\n",
      "train loss:1.9828434262937895\n",
      "train loss:1.9375160955986095\n",
      "train loss:1.83741934063932\n",
      "train loss:1.9755344815995384\n",
      "train loss:1.983918551701931\n",
      "train loss:1.956400179941638\n",
      "train loss:1.9275127121140372\n",
      "train loss:1.9235441265300022\n",
      "train loss:1.8267467154935013\n",
      "train loss:1.9709375022316706\n",
      "train loss:1.9220978747476687\n",
      "train loss:1.9683569769033193\n",
      "train loss:1.865499466282617\n",
      "train loss:1.9626744401726528\n",
      "train loss:1.962590634694265\n",
      "train loss:1.9183490653828543\n",
      "train loss:1.8979733075033132\n",
      "train loss:1.8347588384652591\n",
      "train loss:1.8891721173319853\n",
      "train loss:1.8564555972600225\n",
      "train loss:1.879703991406847\n",
      "train loss:1.8227137142828593\n",
      "train loss:1.881647541273548\n",
      "train loss:1.915634556510434\n",
      "train loss:1.914513332877242\n",
      "train loss:1.8902376732792092\n",
      "train loss:1.885860423939056\n",
      "train loss:1.8263068947347958\n",
      "train loss:1.8955431278263821\n",
      "train loss:1.8976140573223514\n",
      "train loss:1.8419999598473935\n",
      "train loss:1.8863608533499698\n",
      "train loss:1.9790724022804975\n",
      "train loss:1.9177403478160295\n",
      "train loss:1.8892922173090023\n",
      "train loss:1.8077221826182868\n",
      "train loss:1.8691706843510678\n",
      "train loss:1.8888138685829632\n",
      "train loss:1.9714285526573716\n",
      "train loss:1.9104827121104904\n",
      "train loss:1.9030512154126538\n",
      "train loss:1.8554538160439042\n",
      "train loss:1.8955409674431203\n",
      "train loss:1.9004288049002351\n",
      "train loss:1.9277918012790751\n",
      "train loss:1.9066299565697526\n",
      "train loss:1.786058627530881\n",
      "train loss:1.8522353638562175\n",
      "train loss:1.8686591709812281\n",
      "train loss:1.9205188842522165\n",
      "train loss:1.8139232017732327\n",
      "train loss:1.8094199290688302\n",
      "train loss:1.8423725287364883\n",
      "train loss:1.8674017162812888\n",
      "train loss:1.8821488858073712\n",
      "train loss:2.0043045171202927\n",
      "train loss:1.8155089778978952\n",
      "train loss:1.9137898077611661\n",
      "train loss:1.8970050324375776\n",
      "train loss:1.864553500914775\n",
      "train loss:1.891799899452864\n",
      "train loss:1.8108759925482372\n",
      "train loss:1.7986895017898394\n",
      "train loss:1.8504091662666395\n",
      "train loss:1.8530347186393485\n",
      "train loss:1.8010092679185237\n",
      "train loss:1.8621000005597237\n",
      "train loss:1.8304952282676885\n",
      "train loss:1.8328641276651831\n",
      "train loss:1.824316925378117\n",
      "train loss:1.8352174312585214\n",
      "train loss:1.8732842367446063\n",
      "train loss:1.8072906506020112\n",
      "train loss:1.8337460394454737\n",
      "train loss:1.7928574521795346\n",
      "train loss:1.7770987837428418\n",
      "train loss:1.8061574302807804\n",
      "train loss:1.7770211393745605\n",
      "train loss:1.7848526146179995\n",
      "train loss:1.7864319439493672\n",
      "train loss:1.7239532512505193\n",
      "train loss:1.8323313131748618\n",
      "train loss:1.8014041249299055\n",
      "train loss:1.7468764788860056\n",
      "train loss:1.7574098876176052\n",
      "train loss:1.8932878095422225\n",
      "train loss:1.7349741980138063\n",
      "train loss:1.8593955403551194\n",
      "train loss:1.8554258596600863\n",
      "train loss:1.775755140292472\n",
      "train loss:1.8844548078100112\n",
      "train loss:1.8611310845655717\n",
      "train loss:1.9110512786603286\n",
      "train loss:1.8602522840169444\n",
      "train loss:1.8602714947096515\n",
      "train loss:1.9336811053380274\n",
      "train loss:1.8733731671606566\n",
      "train loss:1.8623050437382906\n",
      "train loss:1.8382967125741627\n",
      "train loss:1.7729437123230911\n",
      "train loss:1.7861232125238822\n",
      "train loss:1.780595861420879\n",
      "train loss:1.7768181879094962\n",
      "train loss:1.77978969723264\n",
      "train loss:1.7632092328929272\n",
      "train loss:1.6939501637924588\n",
      "train loss:1.8945229158126824\n",
      "train loss:1.8118607319857774\n",
      "train loss:1.8344470891312634\n",
      "train loss:1.7914434456332504\n",
      "train loss:1.8958506048889692\n",
      "train loss:1.8118284995161562\n",
      "train loss:1.8936382350480716\n",
      "train loss:1.7272139238860533\n",
      "train loss:1.714459554177358\n",
      "train loss:1.811571428146258\n",
      "train loss:1.754482991985397\n",
      "train loss:1.7356443651281146\n",
      "train loss:1.810584392779678\n",
      "train loss:1.79169595799252\n",
      "train loss:1.7649153442815377\n",
      "train loss:1.8486950058876166\n",
      "train loss:1.747976891963465\n",
      "train loss:1.8579138304274148\n",
      "train loss:1.7098024563462693\n",
      "train loss:1.7441173579309188\n",
      "train loss:1.7712695094052444\n",
      "train loss:1.859341870027958\n",
      "train loss:1.7790521983295013\n",
      "train loss:1.821221507590203\n",
      "train loss:1.6416526831846177\n",
      "train loss:1.7036669251866365\n",
      "train loss:1.7674349914925833\n",
      "train loss:1.6662971833820794\n",
      "train loss:1.7072079828780398\n",
      "train loss:1.6761515448452389\n",
      "train loss:1.7132691910984659\n",
      "train loss:1.7011314880093484\n",
      "train loss:1.6575865868780466\n",
      "train loss:1.798082644997171\n",
      "train loss:1.7661667564284773\n",
      "train loss:1.7675666652959856\n",
      "train loss:1.66900731171275\n",
      "train loss:1.7760483540176717\n",
      "train loss:1.6914994612807923\n",
      "train loss:1.7232022345819331\n",
      "train loss:1.7172280993657616\n",
      "train loss:1.7443190291449986\n",
      "train loss:1.73577303109909\n",
      "train loss:1.7420740595302269\n",
      "train loss:1.633348700266464\n",
      "train loss:1.6673690897985562\n",
      "train loss:1.7436062725952148\n",
      "train loss:1.7505015125902559\n",
      "train loss:1.7589929946727865\n",
      "train loss:1.709667948032511\n",
      "train loss:1.6304616531099003\n",
      "train loss:1.579511000119327\n",
      "train loss:1.6136162807544137\n",
      "train loss:1.695956029893798\n",
      "train loss:1.5679112720591208\n",
      "train loss:1.7249362989768784\n",
      "train loss:1.6300231730147277\n",
      "train loss:1.7213022390688493\n",
      "train loss:1.6599070342559086\n",
      "train loss:1.6596563452179693\n",
      "train loss:1.6281417614819198\n",
      "train loss:1.6026888334510756\n",
      "train loss:1.5846845898495976\n",
      "train loss:1.5454060822826658\n",
      "train loss:1.6593323939501243\n",
      "train loss:1.6145523960570785\n",
      "train loss:1.6354142871377886\n",
      "train loss:1.6312051523007611\n",
      "train loss:1.6693760502378256\n",
      "train loss:1.7181448972015694\n",
      "train loss:1.5882225233544498\n",
      "train loss:1.6707265469110792\n",
      "train loss:1.7404616618733382\n",
      "train loss:1.6989984494578008\n",
      "train loss:1.6745658066117324\n",
      "train loss:1.607516946929551\n",
      "train loss:1.5751287112281647\n",
      "train loss:1.6505367773755888\n",
      "train loss:1.5968326130677348\n",
      "train loss:1.507121314829953\n",
      "train loss:1.6566568004644453\n",
      "train loss:1.5987986907276275\n",
      "train loss:1.6712908649949383\n",
      "train loss:1.5682963502165967\n",
      "train loss:1.6600901723217805\n",
      "train loss:1.5880839133942926\n",
      "train loss:1.6347027073397888\n",
      "train loss:1.5838964072695743\n",
      "train loss:1.5267973308948544\n",
      "train loss:1.5366777158786025\n",
      "train loss:1.5804013218288406\n",
      "train loss:1.5908127917530777\n",
      "train loss:1.6074644247856127\n",
      "train loss:1.597870211807808\n",
      "train loss:1.5280471897339052\n",
      "train loss:1.4239987680473083\n",
      "train loss:1.5321454067835796\n",
      "train loss:1.4906889640933927\n",
      "train loss:1.552183779659419\n",
      "train loss:1.5179735451593879\n",
      "train loss:1.4849708733965272\n",
      "train loss:1.503390423649314\n",
      "train loss:1.5017052637833166\n",
      "train loss:1.6024122472904794\n",
      "train loss:1.5542976305582477\n",
      "train loss:1.4143848336767193\n",
      "train loss:1.3556428461671879\n",
      "train loss:1.4857671271030088\n",
      "train loss:1.576039876118243\n",
      "train loss:1.5302584338083023\n",
      "train loss:1.4795715986895168\n",
      "train loss:1.4007004163179468\n",
      "train loss:1.4910074166692064\n",
      "train loss:1.6273269457352348\n",
      "train loss:1.4703044288002747\n",
      "train loss:1.402093180356314\n",
      "train loss:1.4177422389743273\n",
      "train loss:1.413430432768216\n",
      "train loss:1.4368386984422719\n",
      "train loss:1.3900293176082723\n",
      "train loss:1.4246115113843205\n",
      "train loss:1.368699136330236\n",
      "train loss:1.2957244425276764\n",
      "train loss:1.2936512528243913\n",
      "train loss:1.4866275524103318\n",
      "train loss:1.5154041858223808\n",
      "train loss:1.506434676570584\n",
      "train loss:1.5310099673642514\n",
      "train loss:1.3633739975539803\n",
      "train loss:1.4120210445163572\n",
      "train loss:1.356550060044479\n",
      "train loss:1.3432632082495046\n",
      "train loss:1.4571801465126586\n",
      "train loss:1.4304055097068897\n",
      "train loss:1.396231953895392\n",
      "train loss:1.4978594893171802\n",
      "train loss:1.4581908096621021\n",
      "train loss:1.2727997036868282\n",
      "train loss:1.256088573411207\n",
      "train loss:1.4220597406843336\n",
      "train loss:1.3590568587991556\n",
      "train loss:1.4124897171327269\n",
      "train loss:1.3498734244272712\n",
      "train loss:1.3324232650529921\n",
      "train loss:1.4495048694465043\n",
      "train loss:1.2611879499956018\n",
      "train loss:1.3693704157639273\n",
      "train loss:1.2553484967577229\n",
      "train loss:1.4488641286031738\n",
      "train loss:1.3497540815045201\n",
      "train loss:1.2113138887411787\n",
      "train loss:1.2327953405880907\n",
      "train loss:1.2593183645471542\n",
      "train loss:1.1748701975735303\n",
      "train loss:1.3323676891960199\n",
      "train loss:1.3111193280012259\n",
      "train loss:1.3079337649650387\n",
      "train loss:1.4436514729056376\n",
      "train loss:1.170867120073651\n",
      "train loss:1.149446395848689\n",
      "train loss:1.5155159508533318\n",
      "train loss:1.3450540149017933\n",
      "train loss:1.4695693135564452\n",
      "train loss:1.3796785419450046\n",
      "train loss:1.4750848178612583\n",
      "train loss:1.3762228632816147\n",
      "train loss:1.3692115919365329\n",
      "train loss:1.3300785733637026\n",
      "train loss:1.4226257702778526\n",
      "train loss:1.3832180955183246\n",
      "train loss:1.2456867646103853\n",
      "train loss:1.3054432343980666\n",
      "train loss:1.2100055581665932\n",
      "train loss:1.1922812460947285\n",
      "train loss:1.2671520134555336\n",
      "train loss:1.306660331816495\n",
      "train loss:1.2931671888397105\n",
      "train loss:1.2028234047744126\n",
      "train loss:1.2785772223770244\n",
      "train loss:1.3155186502365925\n",
      "train loss:1.3581991928471575\n",
      "train loss:1.278826203173499\n",
      "train loss:1.1870198089978827\n",
      "train loss:1.1751942284455539\n",
      "train loss:1.2767922199935844\n",
      "train loss:1.2027003411215522\n",
      "train loss:1.2455844536341507\n",
      "train loss:1.290006564723299\n",
      "train loss:1.3816033619451786\n",
      "train loss:1.3615791460411006\n",
      "train loss:1.4561335984493036\n",
      "train loss:1.33831636531781\n",
      "train loss:1.2723518023229554\n",
      "train loss:1.2407639463844597\n",
      "train loss:1.3227819881269056\n",
      "train loss:1.2276942844984517\n",
      "train loss:1.2346384208120513\n",
      "train loss:1.2577854516555311\n",
      "train loss:1.2094239972205214\n",
      "train loss:1.1719391213386465\n",
      "train loss:1.2437881967230797\n",
      "train loss:1.1586431624087032\n",
      "train loss:1.3458658404641064\n",
      "train loss:1.2136217065081416\n",
      "train loss:1.3162037592553737\n",
      "train loss:1.1915913233001776\n",
      "train loss:1.1537020574874493\n",
      "train loss:1.2657317810228859\n",
      "train loss:1.2733204275648522\n",
      "train loss:1.180360906297791\n",
      "train loss:1.2890104168575995\n",
      "train loss:1.2501548319130722\n",
      "train loss:1.2188293294596673\n",
      "train loss:1.253345808554826\n",
      "train loss:1.239789868887315\n",
      "train loss:1.2095186342723856\n",
      "train loss:1.179177084519376\n",
      "train loss:1.3906590982713056\n",
      "train loss:1.2790712122719194\n",
      "train loss:1.453106142065976\n",
      "train loss:1.2433920448711921\n",
      "train loss:1.4034453147835202\n",
      "train loss:1.4141179157849293\n",
      "train loss:1.3415869097795712\n",
      "train loss:1.1316089133283902\n",
      "train loss:1.2725597363086079\n",
      "train loss:1.34480924922321\n",
      "train loss:1.2503403990027815\n",
      "train loss:1.3374707637524548\n",
      "train loss:1.2846968601229156\n",
      "train loss:1.2628870921568391\n",
      "train loss:1.2024033418930662\n",
      "train loss:1.2279144229593344\n",
      "train loss:1.3391660160413272\n",
      "train loss:1.1572571795162263\n",
      "train loss:1.3019965335717187\n",
      "train loss:1.1691931248566365\n",
      "train loss:1.238055918240659\n",
      "train loss:1.2693501215683003\n",
      "train loss:1.1944118592604407\n",
      "train loss:1.2405212299554431\n",
      "train loss:1.184386177353008\n",
      "train loss:1.2169652235126163\n",
      "train loss:1.1921416736865382\n",
      "train loss:1.2680291358706246\n",
      "train loss:1.1991468583336722\n",
      "train loss:1.177055265985016\n",
      "train loss:1.184691431609175\n",
      "train loss:1.3044336925579603\n",
      "train loss:1.1626641090835041\n",
      "train loss:1.2561949846537448\n",
      "train loss:1.243431412815655\n",
      "train loss:1.3316633029077514\n",
      "train loss:1.2839046141414534\n",
      "train loss:1.2886876129046334\n",
      "train loss:1.2192349213447258\n",
      "train loss:1.173706913151038\n",
      "train loss:1.2524885438560682\n",
      "train loss:1.1859332073375124\n",
      "train loss:1.1115779007513984\n",
      "train loss:1.1012060083131583\n",
      "train loss:1.316562388830596\n",
      "train loss:1.1551086274564437\n",
      "train loss:1.3056927021341542\n",
      "train loss:1.2108338339044078\n",
      "train loss:1.1873099373283953\n",
      "train loss:1.1769417150561043\n",
      "train loss:1.2658893460380216\n",
      "train loss:1.2262636723732765\n",
      "train loss:1.1555808487776833\n",
      "train loss:1.4469363454055568\n",
      "train loss:1.2995060761940598\n",
      "train loss:1.2766183143925227\n",
      "train loss:1.1422293145302178\n",
      "train loss:1.2874911567441993\n",
      "train loss:1.243273195327187\n",
      "train loss:1.1511495385308361\n",
      "train loss:1.198177219714919\n",
      "train loss:1.1447712054601824\n",
      "=== epoch:2, train acc:0.446, test acc:0.457 ===\n",
      "train loss:1.2751550905322129\n",
      "train loss:1.2230653587259674\n",
      "train loss:1.2429789975035364\n",
      "train loss:1.2578659377315775\n",
      "train loss:1.3236487134608372\n",
      "train loss:1.166517118067006\n",
      "train loss:1.1927071566378624\n",
      "train loss:1.1669578330276276\n",
      "train loss:1.177842971124799\n",
      "train loss:1.2215167345658382\n",
      "train loss:1.0231560529447687\n",
      "train loss:1.128063267212277\n",
      "train loss:1.127067892859699\n",
      "train loss:1.1589841732152537\n",
      "train loss:1.2555411877537934\n",
      "train loss:1.2894990553365515\n",
      "train loss:1.2728682304261603\n",
      "train loss:1.2423975112541263\n",
      "train loss:1.1302396824681806\n",
      "train loss:1.2845447813943205\n",
      "train loss:1.3220278116949922\n",
      "train loss:1.4014502279616443\n",
      "train loss:1.2360201133222501\n",
      "train loss:1.098773303019741\n",
      "train loss:1.295037744672743\n",
      "train loss:1.3276243196579478\n",
      "train loss:1.2026451254735304\n",
      "train loss:1.3362704459508463\n",
      "train loss:1.2038645278989109\n",
      "train loss:1.1558823720660631\n",
      "train loss:1.1332984420670358\n",
      "train loss:1.261549642574227\n",
      "train loss:1.1533796975201163\n",
      "train loss:1.081199597542667\n",
      "train loss:1.2147470640726683\n",
      "train loss:1.1488780677948995\n",
      "train loss:1.1603764615445085\n",
      "train loss:1.1671489697079875\n",
      "train loss:1.077754879256768\n",
      "train loss:1.138087695749561\n",
      "train loss:1.209071300613668\n",
      "train loss:1.1020851309851816\n",
      "train loss:1.2980451042551306\n",
      "train loss:1.2034507313258793\n",
      "train loss:1.0915747357066734\n",
      "train loss:1.154075671004018\n",
      "train loss:1.1457034325867463\n",
      "train loss:1.179402900265575\n",
      "train loss:1.1428165203181657\n",
      "train loss:1.3679517426536585\n",
      "train loss:1.2362373857258204\n",
      "train loss:1.183045389166927\n",
      "train loss:1.1985420108550608\n",
      "train loss:1.2271506863376322\n",
      "train loss:1.3251761469275387\n",
      "train loss:1.1969005505328507\n",
      "train loss:1.179088349812823\n",
      "train loss:1.2607184687039241\n",
      "train loss:1.3610460760421352\n",
      "train loss:1.237050963377229\n",
      "train loss:1.1521760032754056\n",
      "train loss:1.1571407766219854\n",
      "train loss:1.271224060306582\n",
      "train loss:1.3081442611301255\n",
      "train loss:1.2076919146314524\n",
      "train loss:1.1831224728501508\n",
      "train loss:1.137397730602964\n",
      "train loss:1.2040088390894883\n",
      "train loss:1.1268218689145464\n",
      "train loss:1.227772983864971\n",
      "train loss:1.1574702734600406\n",
      "train loss:1.198156669220042\n",
      "train loss:1.3169161264401887\n",
      "train loss:1.054750453424658\n",
      "train loss:1.323301345247594\n",
      "train loss:1.1446013490592202\n",
      "train loss:1.1224138296261637\n",
      "train loss:1.1113514372883102\n",
      "train loss:1.1514672156182848\n",
      "train loss:1.198815657475247\n",
      "train loss:1.152367887577266\n",
      "train loss:1.2430626711377548\n",
      "train loss:1.1688244613969423\n",
      "train loss:1.21546209196286\n",
      "train loss:1.1775680034307456\n",
      "train loss:1.1771542148304943\n",
      "train loss:1.2375301885488676\n",
      "train loss:1.2143191779206364\n",
      "train loss:1.1105661238653002\n",
      "train loss:1.0863348278917717\n",
      "train loss:1.1979299310734013\n",
      "train loss:1.2875943225544282\n",
      "train loss:1.1660635037709028\n",
      "train loss:1.1461156925825495\n",
      "train loss:1.1167792510942534\n",
      "train loss:1.2296284466331\n",
      "train loss:1.211792064724593\n",
      "train loss:1.1567306310289938\n",
      "train loss:1.2510760579493008\n",
      "train loss:1.1961253461319785\n",
      "train loss:1.1656163461935984\n",
      "train loss:1.2102120226094366\n",
      "train loss:1.3052554948223005\n",
      "train loss:1.2173274161478067\n",
      "train loss:1.1209303528074712\n",
      "train loss:1.2345028514935674\n",
      "train loss:1.239783685011248\n",
      "train loss:1.2379321122872762\n",
      "train loss:1.1624582965037753\n",
      "train loss:1.180676667330959\n",
      "train loss:1.316764520704942\n",
      "train loss:1.152446741951319\n",
      "train loss:1.1366406198224426\n",
      "train loss:1.103592456712414\n",
      "train loss:1.1119633577624999\n",
      "train loss:1.1071358371391633\n",
      "train loss:1.1890230145921525\n",
      "train loss:1.143305150467966\n",
      "train loss:1.2470271611535586\n",
      "train loss:1.2302784927291182\n",
      "train loss:1.217285708279554\n",
      "train loss:1.071358951421749\n",
      "train loss:1.1102325854168664\n",
      "train loss:1.1431005737680338\n",
      "train loss:1.160657265171428\n",
      "train loss:1.1655664307880398\n",
      "train loss:1.1470836269462117\n",
      "train loss:1.3107069305309753\n",
      "train loss:1.1816530146484272\n",
      "train loss:1.0731396032984828\n",
      "train loss:1.1961337388848978\n",
      "train loss:1.1421772478205752\n",
      "train loss:1.1728290090901337\n",
      "train loss:1.1424276009648395\n",
      "train loss:1.2600850128337109\n",
      "train loss:1.2026807214623352\n",
      "train loss:1.2272501383723107\n",
      "train loss:1.3845364749250917\n",
      "train loss:1.1372551926513785\n",
      "train loss:1.2274123823413245\n",
      "train loss:1.294361437270716\n",
      "train loss:1.0804671951784475\n",
      "train loss:1.2526348117925559\n",
      "train loss:1.21039736269262\n",
      "train loss:1.2897739884794985\n",
      "train loss:1.218921784787609\n",
      "train loss:1.2786575646696339\n",
      "train loss:1.2663386869811895\n",
      "train loss:1.28081618667406\n",
      "train loss:1.1322931307056543\n",
      "train loss:1.1711519396590264\n",
      "train loss:1.2080512063148736\n",
      "train loss:1.1505398387184433\n",
      "train loss:1.139478759258767\n",
      "train loss:1.0759268767031427\n",
      "train loss:1.0828330488401106\n",
      "train loss:1.1286920805940392\n",
      "train loss:1.2503874699867863\n",
      "train loss:1.068911819433853\n",
      "train loss:1.198171373763541\n",
      "train loss:1.1442698721053417\n",
      "train loss:1.1274986256296358\n",
      "train loss:1.1367715075504354\n",
      "train loss:1.2033365345814058\n",
      "train loss:1.1274416495070916\n",
      "train loss:1.4553718289732809\n",
      "train loss:1.2510739744300963\n",
      "train loss:1.2539846360763691\n",
      "train loss:1.1444567178837313\n",
      "train loss:1.3128438329766197\n",
      "train loss:1.2030136616583151\n",
      "train loss:1.1246356159778506\n",
      "train loss:1.1242040556892794\n",
      "train loss:1.087825990003696\n",
      "train loss:1.1843863358444215\n",
      "train loss:1.173143023269014\n",
      "train loss:1.1131333988970429\n",
      "train loss:1.126998670078202\n",
      "train loss:1.1745058419269079\n",
      "train loss:1.1775448148696854\n",
      "train loss:1.0663631101955076\n",
      "train loss:1.0763414394909845\n",
      "train loss:1.1726213442950288\n",
      "train loss:1.193745333093664\n",
      "train loss:1.149650420482223\n",
      "train loss:1.1938219262208378\n",
      "train loss:1.0756507488510862\n",
      "train loss:1.1436128652231314\n",
      "train loss:1.1509151697057072\n",
      "train loss:1.367088802640388\n",
      "train loss:1.1704249412963994\n",
      "train loss:1.2718436511375517\n",
      "train loss:1.2320120581210223\n",
      "train loss:1.2141270782165106\n",
      "train loss:1.1696789608594442\n",
      "train loss:1.0195734714127629\n",
      "train loss:1.1107512411940517\n",
      "train loss:1.013982010621693\n",
      "train loss:1.1408374016070502\n",
      "train loss:1.1156564906151347\n",
      "train loss:1.038930636925051\n",
      "train loss:1.166796187720097\n",
      "train loss:1.079647840680719\n",
      "train loss:1.133315949554257\n",
      "train loss:1.1493632342204843\n",
      "train loss:1.1735949251240176\n",
      "train loss:1.1230636728736327\n",
      "train loss:1.1334709761584316\n",
      "train loss:1.1709577492806043\n",
      "train loss:0.9784697837042452\n",
      "train loss:1.2266083431926147\n",
      "train loss:1.0958764719558989\n",
      "train loss:1.2410550836462133\n",
      "train loss:1.1206752026375038\n",
      "train loss:1.2147802536785466\n",
      "train loss:1.1253899527146236\n",
      "train loss:1.2002485925460116\n",
      "train loss:1.1259305238026136\n",
      "train loss:1.1001996771913978\n",
      "train loss:1.1676723865275918\n",
      "train loss:1.0508012562252569\n",
      "train loss:1.1080743639427642\n",
      "train loss:1.1246762144962255\n",
      "train loss:1.044756554412401\n",
      "train loss:1.1583258572195683\n",
      "train loss:1.0447825611717767\n",
      "train loss:1.0943073148467617\n",
      "train loss:1.3244871569949412\n",
      "train loss:1.0937162032932508\n",
      "train loss:1.1428679229108036\n",
      "train loss:0.9908948086172381\n",
      "train loss:1.208142445948344\n",
      "train loss:1.1449384229099702\n",
      "train loss:1.1668994486119537\n",
      "train loss:1.087609454051877\n",
      "train loss:1.3338155283665682\n",
      "train loss:1.1168149056575452\n",
      "train loss:1.1322208132403748\n",
      "train loss:1.1936145977002013\n",
      "train loss:1.160822604703959\n",
      "train loss:1.1925231010901902\n",
      "train loss:1.022119479084721\n",
      "train loss:1.0530685190001972\n",
      "train loss:1.1727810795180735\n",
      "train loss:1.0681111508683907\n",
      "train loss:1.1862541727679683\n",
      "train loss:1.1643116491811467\n",
      "train loss:1.1763538753848612\n",
      "train loss:1.1521866705848902\n",
      "train loss:1.0671256241836127\n",
      "train loss:1.0402016954510005\n",
      "train loss:1.3762054850776477\n",
      "train loss:1.1080128462358028\n",
      "train loss:1.2406668103786485\n",
      "train loss:1.1674888422604015\n",
      "train loss:1.0457135772329733\n",
      "train loss:1.1408263824528182\n",
      "train loss:1.0697861794750818\n",
      "train loss:1.0723011034093093\n",
      "train loss:1.1545725845749162\n",
      "train loss:1.133565078258849\n",
      "train loss:0.9767828380717887\n",
      "train loss:1.0706789179295675\n",
      "train loss:1.0105056808314483\n",
      "train loss:1.0809347631641952\n",
      "train loss:1.0553522102080375\n",
      "train loss:1.0995209065461977\n",
      "train loss:1.1457470360259396\n",
      "train loss:1.112254422207336\n",
      "train loss:1.1582596529425577\n",
      "train loss:1.2045884590107634\n",
      "train loss:1.1340521857854258\n",
      "train loss:1.1094649653276354\n",
      "train loss:1.3455552796475632\n",
      "train loss:1.2016057535959461\n",
      "train loss:1.2396915080478004\n",
      "train loss:1.1495681318458986\n",
      "train loss:1.2485332089300767\n",
      "train loss:1.2052327685765234\n",
      "train loss:1.2983735210494844\n",
      "train loss:1.336255140586145\n",
      "train loss:1.0287989451061612\n",
      "train loss:1.235858828465865\n",
      "train loss:1.1159381238077584\n",
      "train loss:1.2323850037754054\n",
      "train loss:1.207232830644216\n",
      "train loss:1.1468853858450256\n",
      "train loss:1.1328710141278686\n",
      "train loss:1.183474051849866\n",
      "train loss:1.070405386749803\n",
      "train loss:1.0690039667263884\n",
      "train loss:1.0668358210542142\n",
      "train loss:1.0914619572500386\n",
      "train loss:1.3187717560118486\n",
      "train loss:1.1820931104935426\n",
      "train loss:1.051469220064079\n",
      "train loss:0.9762961196681976\n",
      "train loss:1.0903755872466898\n",
      "train loss:0.9976891023308062\n",
      "train loss:1.1547869020556698\n",
      "train loss:1.1442819650351397\n",
      "train loss:1.4154757643538982\n",
      "train loss:1.156166195827747\n",
      "train loss:1.1755530244969556\n",
      "train loss:1.122140591574376\n",
      "train loss:1.2187235981239288\n",
      "train loss:1.1728925783210264\n",
      "train loss:1.0699710593174556\n",
      "train loss:1.185257726345894\n",
      "train loss:1.0959828834541836\n",
      "train loss:1.2620621548280984\n",
      "train loss:1.1267129541163732\n",
      "train loss:1.0503577601624834\n",
      "train loss:1.0591997483058122\n",
      "train loss:1.2344122960455441\n",
      "train loss:1.034486677831663\n",
      "train loss:1.2324665666304453\n",
      "train loss:1.0943306228797633\n",
      "train loss:1.2139016862104806\n",
      "train loss:1.1309105143937175\n",
      "train loss:1.2047612785921509\n",
      "train loss:1.1868624646893928\n",
      "train loss:1.1011023748371975\n",
      "train loss:1.0691884401608773\n",
      "train loss:1.278651297371825\n",
      "train loss:1.1393892416525138\n",
      "train loss:1.110267494875561\n",
      "train loss:1.1077664778397611\n",
      "train loss:1.1079458587284285\n",
      "train loss:1.1006919371700212\n",
      "train loss:1.1356302016898911\n",
      "train loss:1.26714082794652\n",
      "train loss:1.1521092805914068\n",
      "train loss:1.146268470376994\n",
      "train loss:1.1266180523217262\n",
      "train loss:1.0819755835796792\n",
      "train loss:1.174079803654658\n",
      "train loss:1.0477562304517642\n",
      "train loss:0.9890039636880675\n",
      "train loss:1.1639241627299954\n",
      "train loss:1.175118289075275\n",
      "train loss:1.2175617232602038\n",
      "train loss:1.1140423635847376\n",
      "train loss:1.284757992904517\n",
      "train loss:1.247956007718331\n",
      "train loss:1.0015498629446165\n",
      "train loss:1.1407001491372786\n",
      "train loss:1.134871658885725\n",
      "train loss:1.1212757163024052\n",
      "train loss:1.0861905583445826\n",
      "train loss:1.110815636108375\n",
      "train loss:1.0402454501195044\n",
      "train loss:1.190480045916327\n",
      "train loss:1.1249967433594146\n",
      "train loss:1.0858040628926329\n",
      "train loss:1.198814666155261\n",
      "train loss:1.1130808866531348\n",
      "train loss:0.9482159366521791\n",
      "train loss:1.0816124987641005\n",
      "train loss:1.2737572419247767\n",
      "train loss:1.2238984478213781\n",
      "train loss:1.2070719152265896\n",
      "train loss:1.2079568901652282\n",
      "train loss:1.1030376009380392\n",
      "train loss:1.1434408682291035\n",
      "train loss:1.0476197243866232\n",
      "train loss:1.127758297617726\n",
      "train loss:1.0499143270268625\n",
      "train loss:1.178955703522815\n",
      "train loss:1.3114294758284442\n",
      "train loss:1.3633481952539623\n",
      "train loss:1.0867698984249288\n",
      "train loss:1.1417562406817994\n",
      "train loss:1.0308235518476947\n",
      "train loss:1.1618013752040084\n",
      "train loss:1.0375028091571166\n",
      "train loss:0.959697348994728\n",
      "train loss:1.1416086206218319\n",
      "train loss:1.2451771424504006\n",
      "train loss:1.0133436823398614\n",
      "train loss:1.0488572325627368\n",
      "train loss:1.0638460637169505\n",
      "train loss:1.0388912894484041\n",
      "train loss:1.084866742062643\n",
      "train loss:1.1477716939286982\n",
      "train loss:1.1293184580919435\n",
      "train loss:1.2523305755117888\n",
      "train loss:1.075753203040602\n",
      "train loss:1.1814235137947473\n",
      "train loss:1.292827122265441\n",
      "train loss:1.1714515837440997\n",
      "train loss:1.1243677068364963\n",
      "train loss:1.1002897246799963\n",
      "train loss:0.9969709346660296\n",
      "train loss:1.103830289606805\n",
      "train loss:1.2107887088512934\n",
      "train loss:1.0675173953505739\n",
      "train loss:1.0551455144769306\n",
      "train loss:1.2166892051846203\n",
      "train loss:1.1711888230061323\n",
      "train loss:1.031342784970906\n",
      "train loss:1.0979254984513604\n",
      "train loss:1.0965554185713247\n",
      "train loss:1.0987033163594078\n",
      "train loss:0.9703468404249931\n",
      "train loss:1.1093221076518223\n",
      "train loss:1.0987797294653443\n",
      "train loss:1.06141134214496\n",
      "train loss:1.0493290186244228\n",
      "train loss:1.1418813244864565\n",
      "train loss:0.9850804556462035\n",
      "train loss:1.163252273652396\n",
      "train loss:0.9849724490371448\n",
      "train loss:1.2930200036887312\n",
      "train loss:1.1680080289305226\n",
      "train loss:0.8919638274081941\n",
      "train loss:1.0646873536936439\n",
      "train loss:0.9955588084216523\n",
      "train loss:1.065725490045899\n",
      "train loss:0.9929135450670931\n",
      "train loss:0.9436521131179174\n",
      "train loss:1.1139258092722133\n",
      "train loss:1.0209341494025326\n",
      "train loss:1.2456557577682679\n",
      "train loss:1.0923256757562207\n",
      "train loss:1.2755143563376028\n",
      "train loss:1.0836269097569131\n",
      "train loss:1.1947684075650893\n",
      "train loss:1.1959009259268383\n",
      "train loss:0.9451363284162333\n",
      "train loss:1.0196194354567687\n",
      "train loss:1.2197384691963828\n",
      "train loss:1.089593450948944\n",
      "train loss:1.2781871736425097\n",
      "train loss:1.1042604756640235\n",
      "train loss:1.0630719209075097\n",
      "train loss:1.1435050445970274\n",
      "train loss:1.1776101853774688\n",
      "train loss:1.0675753522349058\n",
      "train loss:0.9572119091824092\n",
      "train loss:1.2678338288911088\n",
      "train loss:1.1103126366489988\n",
      "train loss:1.134574668836163\n",
      "train loss:1.34703922328132\n",
      "train loss:1.075900643960721\n",
      "train loss:1.0366687074752536\n",
      "train loss:1.197962218190992\n",
      "train loss:1.2887990741463262\n",
      "train loss:1.0965670644328656\n",
      "train loss:0.9431450452336989\n",
      "train loss:1.0702032198869356\n",
      "train loss:1.1891753721005753\n",
      "train loss:0.9952058561805122\n",
      "train loss:1.0715802650400954\n",
      "train loss:1.0693899933727098\n",
      "train loss:1.300725215990992\n",
      "train loss:1.1340387078727128\n",
      "train loss:0.9618196177626887\n",
      "train loss:1.0311458033922616\n",
      "train loss:0.95392621956358\n",
      "train loss:1.040241961651665\n",
      "train loss:1.0683710465449783\n",
      "train loss:1.0690688689498127\n",
      "train loss:0.9546663545833307\n",
      "train loss:0.8903852469605034\n",
      "train loss:1.0449727070758725\n",
      "train loss:1.2177795641144828\n",
      "train loss:1.1250002039469846\n",
      "train loss:1.026247260439346\n",
      "train loss:1.0041880098630824\n",
      "train loss:1.2197289867904404\n",
      "train loss:1.265121207693026\n",
      "train loss:1.1330136383948328\n",
      "train loss:1.0372453692029497\n",
      "train loss:1.070351413481748\n",
      "train loss:0.9945414144339928\n",
      "train loss:1.1354224254490672\n",
      "train loss:1.2481019310758348\n",
      "train loss:1.0397853477829806\n",
      "train loss:1.0206869485886496\n",
      "train loss:1.1838419529758644\n",
      "train loss:1.1190805464822746\n",
      "train loss:1.1891199206162448\n",
      "train loss:0.9074674153116767\n",
      "train loss:1.0451580732240877\n",
      "train loss:0.9854889557939447\n",
      "train loss:0.931478558952188\n",
      "train loss:0.9022032067229059\n",
      "train loss:1.010599468454374\n",
      "train loss:0.9432906789400672\n",
      "train loss:0.9498686337449124\n",
      "train loss:1.267188193784556\n",
      "train loss:1.0284182010520828\n",
      "train loss:1.0469899893480175\n",
      "train loss:1.1795121673642022\n",
      "train loss:1.1064412467266507\n",
      "train loss:1.2918053109770862\n",
      "train loss:1.0125242206593414\n",
      "train loss:1.1941262283170753\n",
      "train loss:1.4585699675616357\n",
      "train loss:1.0860555909699903\n",
      "train loss:1.128979340527274\n",
      "train loss:1.082679830879513\n",
      "train loss:1.0198219330727356\n",
      "train loss:0.9620463697254419\n",
      "train loss:1.0166205958084582\n",
      "train loss:1.1147040814567049\n",
      "train loss:1.0164732346638685\n",
      "train loss:1.1048854968223045\n",
      "train loss:1.1043957801289015\n",
      "train loss:1.0956135498853632\n",
      "train loss:1.1855899036585849\n",
      "train loss:1.0710428698487728\n",
      "train loss:0.9731767885969282\n",
      "train loss:1.095472939820023\n",
      "train loss:1.1028319440301415\n",
      "train loss:0.8495317252281186\n",
      "train loss:1.0595329524269037\n",
      "train loss:0.975876325086729\n",
      "train loss:1.1807038643528194\n",
      "train loss:0.9880866277504259\n",
      "train loss:1.0437797593912153\n",
      "train loss:1.0418204635157997\n",
      "train loss:0.9958956033867448\n",
      "train loss:1.1695576280308688\n",
      "train loss:1.0799412865066917\n",
      "train loss:1.1322675123271981\n",
      "train loss:1.0951565289877383\n",
      "train loss:1.123777412989652\n",
      "train loss:1.14952183211149\n",
      "train loss:0.9445961217816433\n",
      "train loss:1.079070440152394\n",
      "train loss:1.1778461997992589\n",
      "train loss:1.0202148918436242\n",
      "train loss:1.0055318979899432\n",
      "train loss:1.0089720433324934\n",
      "train loss:1.0130459472716544\n",
      "train loss:1.0790269505028298\n",
      "train loss:1.0153609469687053\n",
      "train loss:0.9501847058634734\n",
      "train loss:1.0522582502212565\n",
      "train loss:0.9554872563636811\n",
      "train loss:0.9574971558355287\n",
      "train loss:1.0269743759748733\n",
      "train loss:1.1501354624508207\n",
      "train loss:0.9793070032956159\n",
      "train loss:1.152498552339474\n",
      "train loss:1.0272221489997582\n",
      "train loss:1.0276061429350722\n",
      "train loss:1.0892399484448039\n",
      "train loss:1.1129154060425792\n",
      "train loss:1.009944040272396\n",
      "train loss:0.8950209728938952\n",
      "train loss:1.0961699114801005\n",
      "train loss:1.076903382837743\n",
      "train loss:1.0459340778380835\n",
      "train loss:0.9165132097852016\n",
      "train loss:0.9889958972509367\n",
      "train loss:0.9760235753628191\n",
      "train loss:0.945517716271836\n",
      "train loss:1.0402893998929807\n",
      "train loss:1.0453190498535803\n",
      "train loss:0.9711977498024308\n",
      "train loss:0.9771996749745724\n",
      "train loss:1.0321759285143888\n",
      "train loss:1.0283771307645047\n",
      "train loss:1.0952581180596233\n",
      "train loss:1.024467656229539\n",
      "train loss:1.0507311555084902\n",
      "train loss:0.9039396548897932\n",
      "train loss:1.0801759496023202\n",
      "train loss:1.1185087425928524\n",
      "train loss:0.9811727659386861\n",
      "train loss:0.9838231623674496\n",
      "train loss:0.9119186669313816\n",
      "train loss:0.9283059185275156\n",
      "train loss:0.9507623354024689\n",
      "train loss:0.8510900479652416\n",
      "train loss:0.8488160270762619\n",
      "train loss:1.1136673786423024\n",
      "train loss:1.0127443732451546\n",
      "train loss:0.9172074035122187\n",
      "train loss:1.0404716810304502\n",
      "train loss:1.0850511260684277\n",
      "train loss:1.157203895768675\n",
      "train loss:1.0238363616114527\n",
      "train loss:1.1510925634427192\n",
      "train loss:1.0326305854510878\n",
      "train loss:1.089666730577648\n",
      "train loss:0.9289063014214493\n",
      "train loss:1.0433118319865395\n",
      "train loss:1.0698685528535672\n",
      "train loss:1.114475122339821\n",
      "train loss:0.9454160769626277\n",
      "train loss:0.8153747016857331\n",
      "train loss:0.8770523773449844\n",
      "train loss:1.1914715879780464\n",
      "train loss:1.0106230448861373\n",
      "train loss:1.1586602331213778\n",
      "train loss:0.9577236777421768\n",
      "=== epoch:3, train acc:0.561, test acc:0.555 ===\n",
      "train loss:1.121756336907923\n",
      "train loss:0.9758399492539859\n",
      "train loss:1.0316883628749585\n",
      "train loss:1.0282320848783797\n",
      "train loss:1.088160257919962\n",
      "train loss:1.0308148457852142\n",
      "train loss:0.9813036136146716\n",
      "train loss:0.9966857340650104\n",
      "train loss:1.068361841415207\n",
      "train loss:0.9002664963720934\n",
      "train loss:1.0552033898247493\n",
      "train loss:0.9042322081887303\n",
      "train loss:0.983503355121693\n",
      "train loss:0.9709226841181712\n",
      "train loss:0.937454273552718\n",
      "train loss:0.950460564328586\n",
      "train loss:0.9968575109637619\n",
      "train loss:1.0591036535591223\n",
      "train loss:0.9031023293058776\n",
      "train loss:0.9351253303000668\n",
      "train loss:0.9643339771635041\n",
      "train loss:0.9710931255501376\n",
      "train loss:1.1402225812898248\n",
      "train loss:0.9994146758093337\n",
      "train loss:1.0614304594091615\n",
      "train loss:0.9640018913135806\n",
      "train loss:1.053430793622308\n",
      "train loss:1.0713925070157166\n",
      "train loss:1.0429540186059543\n",
      "train loss:1.25877332335644\n",
      "train loss:1.0372598925220287\n",
      "train loss:1.1908587114588929\n",
      "train loss:1.0446861132616705\n",
      "train loss:1.0066111959996775\n",
      "train loss:0.9931342070559294\n",
      "train loss:0.9449401456693947\n",
      "train loss:1.163077166108518\n",
      "train loss:1.0086505266044334\n",
      "train loss:0.9891287334836031\n",
      "train loss:1.124182461222833\n",
      "train loss:1.0175674000791246\n",
      "train loss:1.0528170285383016\n",
      "train loss:0.9807873505140114\n",
      "train loss:0.9553786450958244\n",
      "train loss:0.9302811795875934\n",
      "train loss:0.9377253480373637\n",
      "train loss:0.9965500796865371\n",
      "train loss:0.9998571908109831\n",
      "train loss:0.9384929278869812\n",
      "train loss:0.9544738522855917\n",
      "train loss:0.9642434086243066\n",
      "train loss:0.9914701749229229\n",
      "train loss:0.8898269014945638\n",
      "train loss:0.8807686596586572\n",
      "train loss:1.0520949562776785\n",
      "train loss:0.8418529665942907\n",
      "train loss:0.8985067729195881\n",
      "train loss:0.907367402738258\n",
      "train loss:1.0272711865018287\n",
      "train loss:0.8418738761060273\n",
      "train loss:1.1893211807055308\n",
      "train loss:1.1312596912640331\n",
      "train loss:1.0925219532967436\n",
      "train loss:1.0083906640999631\n",
      "train loss:0.9254205644016998\n",
      "train loss:1.0392490982001663\n",
      "train loss:1.0193412281969167\n",
      "train loss:0.8802333210104948\n",
      "train loss:1.0183591071752187\n",
      "train loss:1.1027632986073046\n",
      "train loss:0.8695356925863159\n",
      "train loss:0.9901081865263734\n",
      "train loss:1.0103381768431658\n",
      "train loss:1.1354941622473391\n",
      "train loss:1.2472557419672978\n",
      "train loss:1.1446249750218858\n",
      "train loss:1.1119184857696278\n",
      "train loss:1.215206166851395\n",
      "train loss:0.99788092007072\n",
      "train loss:1.0012506355807798\n",
      "train loss:1.0087115764523293\n",
      "train loss:1.1285732032477929\n",
      "train loss:0.8984247617627555\n",
      "train loss:1.0097379172236018\n",
      "train loss:0.9479379762475617\n",
      "train loss:0.9432681489399503\n",
      "train loss:1.0327223145559983\n",
      "train loss:1.1318640475552948\n",
      "train loss:0.9617118909210726\n",
      "train loss:1.027896659546707\n",
      "train loss:0.929899175836507\n",
      "train loss:1.0911568771969369\n",
      "train loss:0.9610601572487671\n",
      "train loss:0.9017136661039676\n",
      "train loss:1.1552038892330434\n",
      "train loss:1.0930081246059669\n",
      "train loss:1.0033162101237145\n",
      "train loss:0.8436387464224804\n",
      "train loss:0.8159543225829835\n",
      "train loss:1.1030814865389176\n",
      "train loss:0.8780149242750926\n",
      "train loss:1.1047434766617863\n",
      "train loss:0.9345416778237152\n",
      "train loss:0.9522789088178394\n",
      "train loss:0.8443103425408391\n",
      "train loss:0.9729435186753642\n",
      "train loss:1.084397343402643\n",
      "train loss:0.9297274603234814\n",
      "train loss:0.8409105927826943\n",
      "train loss:0.9252123049002448\n",
      "train loss:0.8557613159764013\n",
      "train loss:1.130633258785347\n",
      "train loss:1.0045744044318359\n",
      "train loss:0.9375584854920591\n",
      "train loss:0.7685392947974664\n",
      "train loss:0.9034216696793926\n",
      "train loss:0.9770001702534011\n",
      "train loss:1.0152499872480818\n",
      "train loss:0.9038817566120546\n",
      "train loss:0.9478617629496994\n",
      "train loss:0.9277640615669326\n",
      "train loss:0.9551527392269693\n",
      "train loss:0.9503886452604253\n",
      "train loss:1.207153911235955\n",
      "train loss:1.0240026601903074\n",
      "train loss:0.9845564174110046\n",
      "train loss:0.8851112178752694\n",
      "train loss:1.2156119171047421\n",
      "train loss:0.8162982680101571\n",
      "train loss:0.918236818326751\n",
      "train loss:0.9010545506842947\n",
      "train loss:0.9809539977769428\n",
      "train loss:1.0128782355368384\n",
      "train loss:1.0777886443726943\n",
      "train loss:0.9006587325839501\n",
      "train loss:0.8676055394719251\n",
      "train loss:0.8912690121004446\n",
      "train loss:0.881230302164492\n",
      "train loss:0.9998157134725548\n",
      "train loss:0.8853574217602913\n",
      "train loss:0.8828669107914614\n",
      "train loss:0.9161136992133118\n",
      "train loss:0.9201352098196112\n",
      "train loss:0.8641117422614522\n",
      "train loss:0.8353719033572777\n",
      "train loss:0.7664278578778208\n",
      "train loss:1.0195881351686829\n",
      "train loss:1.0489984702133366\n",
      "train loss:1.0103962829330198\n",
      "train loss:1.0850163210775288\n",
      "train loss:1.0851470697971313\n",
      "train loss:0.9165663627745598\n",
      "train loss:0.9340082141893764\n",
      "train loss:1.050099402165936\n",
      "train loss:1.0176163505342557\n",
      "train loss:0.8826484876511461\n",
      "train loss:0.9610136527023141\n",
      "train loss:0.9822477166626716\n",
      "train loss:0.8485205448883228\n",
      "train loss:1.0004743946200993\n",
      "train loss:0.9717936344989184\n",
      "train loss:0.9279398421794112\n",
      "train loss:1.0201877116891949\n",
      "train loss:0.9294948072094412\n",
      "train loss:0.9116531753045538\n",
      "train loss:0.9416730668737077\n",
      "train loss:0.8826468581101352\n",
      "train loss:0.8809314210238081\n",
      "train loss:0.8618643061839579\n",
      "train loss:0.9953064345147362\n",
      "train loss:0.9990165590949905\n",
      "train loss:0.9873541151046539\n",
      "train loss:0.8875402212858625\n",
      "train loss:0.9979075910076932\n",
      "train loss:1.0480975294837664\n",
      "train loss:0.9285380422131594\n",
      "train loss:1.096237410945378\n",
      "train loss:0.8703857952939432\n",
      "train loss:1.0755892071608222\n",
      "train loss:0.9100144891202868\n",
      "train loss:1.0161585579314407\n",
      "train loss:1.23993460276685\n",
      "train loss:1.0755997180489067\n",
      "train loss:0.9434575372489165\n",
      "train loss:1.0333881681354269\n",
      "train loss:0.9260160537667139\n",
      "train loss:1.0016110232795805\n",
      "train loss:0.972918550679833\n",
      "train loss:0.8870346765560748\n",
      "train loss:0.8523498371720402\n",
      "train loss:0.7733681403155618\n",
      "train loss:0.9509884980558735\n",
      "train loss:0.9694232504573396\n",
      "train loss:0.987623285488213\n",
      "train loss:0.9078797427842514\n",
      "train loss:0.9413723711688309\n",
      "train loss:0.885862599716197\n",
      "train loss:0.9891831159386274\n",
      "train loss:0.9037881310127625\n",
      "train loss:0.8519615640981395\n",
      "train loss:0.9298369728402582\n",
      "train loss:0.9504616570478059\n",
      "train loss:0.9770761431992713\n",
      "train loss:0.8577635369701077\n",
      "train loss:0.8607440767345358\n",
      "train loss:0.9233937461863252\n",
      "train loss:1.0465265907520394\n",
      "train loss:0.9542096031911906\n",
      "train loss:0.9062750499967623\n",
      "train loss:0.8584263437426426\n",
      "train loss:0.8590553424514011\n",
      "train loss:0.8815498326589679\n",
      "train loss:0.9628276612485207\n",
      "train loss:0.8553455833441838\n",
      "train loss:0.865628525797122\n",
      "train loss:0.8318162639326095\n",
      "train loss:0.915964011372377\n",
      "train loss:0.7912326630486818\n",
      "train loss:0.9960359144032651\n",
      "train loss:0.9922768377161226\n",
      "train loss:0.9644149596326715\n",
      "train loss:0.8525386139656878\n",
      "train loss:0.9872428046333837\n",
      "train loss:1.046972773559156\n",
      "train loss:0.9556611578455282\n",
      "train loss:1.0899556642006099\n",
      "train loss:0.9095971638472962\n",
      "train loss:0.9332579038171215\n",
      "train loss:0.9721602178260227\n",
      "train loss:0.8769296494265281\n",
      "train loss:0.9613665333190604\n",
      "train loss:0.799347349634187\n",
      "train loss:0.9053299046991128\n",
      "train loss:1.001614264416866\n",
      "train loss:0.9400459060104189\n",
      "train loss:0.7974366300695032\n",
      "train loss:0.9912473665249142\n",
      "train loss:0.9226111404347751\n",
      "train loss:1.029224753049059\n",
      "train loss:0.917556016152857\n",
      "train loss:1.045103610401496\n",
      "train loss:1.1019310300659304\n",
      "train loss:0.8243466150323858\n",
      "train loss:0.9155861641104427\n",
      "train loss:0.8608679243643373\n",
      "train loss:0.8422632294184526\n",
      "train loss:0.8836198275580004\n",
      "train loss:0.9352855383505614\n",
      "train loss:0.9239397591917939\n",
      "train loss:1.1506491854420993\n",
      "train loss:0.9755804106935392\n",
      "train loss:0.9087194333931743\n",
      "train loss:0.8775300990520906\n",
      "train loss:0.9974194426245984\n",
      "train loss:0.8235200802704964\n",
      "train loss:0.9636371867790193\n",
      "train loss:0.9223541851822887\n",
      "train loss:0.8739808661982623\n",
      "train loss:0.8423942885531462\n",
      "train loss:0.8126688850716672\n",
      "train loss:0.8803102054770914\n",
      "train loss:0.8650181716330725\n",
      "train loss:0.8247951954045054\n",
      "train loss:0.8113257162206625\n",
      "train loss:0.9105613263806239\n",
      "train loss:0.929333735626713\n",
      "train loss:1.056875087315026\n",
      "train loss:0.9172268129272669\n",
      "train loss:0.9010229154665749\n",
      "train loss:0.9246842796522534\n",
      "train loss:0.7962722727690843\n",
      "train loss:1.1715341681554228\n",
      "train loss:1.0286374837528882\n",
      "train loss:0.9801229370476452\n",
      "train loss:0.785299047129396\n",
      "train loss:0.9463784109808441\n",
      "train loss:0.8257887084522114\n",
      "train loss:0.9101065162946195\n",
      "train loss:0.7547299544906179\n",
      "train loss:1.111893631542459\n",
      "train loss:0.8990975080882937\n",
      "train loss:0.9881377234328577\n",
      "train loss:1.0360276780634903\n",
      "train loss:0.8284514756712963\n",
      "train loss:1.0336297919482842\n",
      "train loss:0.8663588173690029\n",
      "train loss:0.965887790876835\n",
      "train loss:0.9078403622777325\n",
      "train loss:0.9815679056516288\n",
      "train loss:0.9448309857826364\n",
      "train loss:0.9816059753005303\n",
      "train loss:0.9726815465715133\n",
      "train loss:0.9419975178770862\n",
      "train loss:0.9926201582021572\n",
      "train loss:0.9020053673583027\n",
      "train loss:0.8645616701114143\n",
      "train loss:1.052666076349257\n",
      "train loss:0.7566770372848355\n",
      "train loss:0.8809382125550554\n",
      "train loss:0.9560318934007584\n",
      "train loss:0.8666586018917402\n",
      "train loss:1.07288829563867\n",
      "train loss:0.7115598572852815\n",
      "train loss:1.0299753675408234\n",
      "train loss:1.025728387071972\n",
      "train loss:0.8718366345035526\n",
      "train loss:0.9099690660668494\n",
      "train loss:0.8929192420909068\n",
      "train loss:0.9144652690903523\n",
      "train loss:0.9056351037220458\n",
      "train loss:0.9129330584795068\n",
      "train loss:0.8754028743486317\n",
      "train loss:0.8578331018533444\n",
      "train loss:0.852873447013889\n",
      "train loss:0.8952961210340266\n",
      "train loss:0.8643408614945711\n",
      "train loss:0.895696862322399\n",
      "train loss:0.828518829423551\n",
      "train loss:0.8216728647144366\n",
      "train loss:0.7692599956090134\n",
      "train loss:0.8607380927227029\n",
      "train loss:0.711886846807965\n",
      "train loss:0.8227268244406376\n",
      "train loss:0.796945121931345\n",
      "train loss:0.9215882905237861\n",
      "train loss:0.9044275129022737\n",
      "train loss:0.9200181530335414\n",
      "train loss:0.9309983978112827\n",
      "train loss:0.9020234475907623\n",
      "train loss:0.9749502603450307\n",
      "train loss:0.9643518222552193\n",
      "train loss:1.0127117811357806\n",
      "train loss:0.8510641568966925\n",
      "train loss:0.9380439962981362\n",
      "train loss:0.96757197328622\n",
      "train loss:0.9044551816909089\n",
      "train loss:0.8688833554933959\n",
      "train loss:0.8539140648012347\n",
      "train loss:1.047968650360196\n",
      "train loss:0.8473831491547675\n",
      "train loss:0.8807781384623057\n",
      "train loss:0.8599961711546332\n",
      "train loss:0.9034494092973011\n",
      "train loss:0.8617324396496265\n",
      "train loss:0.8844138986337119\n",
      "train loss:0.9577776558570045\n",
      "train loss:0.9771709336826307\n",
      "train loss:1.0057971571979636\n",
      "train loss:0.8107151273000943\n",
      "train loss:0.9259513726206201\n",
      "train loss:0.922125709977894\n",
      "train loss:0.8794026556943672\n",
      "train loss:0.8966950024634451\n",
      "train loss:0.8832540641741647\n",
      "train loss:0.9986862290260913\n",
      "train loss:1.0293563099976477\n",
      "train loss:0.9358993493031582\n",
      "train loss:0.7756720882129948\n",
      "train loss:0.997915900521273\n",
      "train loss:0.782468173491927\n",
      "train loss:0.877146921407825\n",
      "train loss:0.7370034129484212\n",
      "train loss:0.8926286281293007\n",
      "train loss:0.8877810161287356\n",
      "train loss:0.953710901607545\n",
      "train loss:0.8193797679944934\n",
      "train loss:0.9673236697196362\n",
      "train loss:0.8051045761132894\n",
      "train loss:0.8263656388349205\n",
      "train loss:0.8513764264057667\n",
      "train loss:0.9012819418065118\n",
      "train loss:0.8857487614940243\n",
      "train loss:0.8621337037648198\n",
      "train loss:0.8484499625048388\n",
      "train loss:0.8604123196655009\n",
      "train loss:0.8168248241804481\n",
      "train loss:0.9455381418473812\n",
      "train loss:0.9220243213654026\n",
      "train loss:0.796030017056366\n",
      "train loss:0.7045202582503194\n",
      "train loss:0.8252833910801088\n",
      "train loss:0.878223817731765\n",
      "train loss:0.8432653207629021\n",
      "train loss:0.9915535549557477\n",
      "train loss:0.9023136636610493\n",
      "train loss:0.7494816775331273\n",
      "train loss:0.9522597595384282\n",
      "train loss:0.976446761480371\n",
      "train loss:0.765869576889967\n",
      "train loss:0.7664495113214229\n",
      "train loss:1.0375746008283488\n",
      "train loss:1.0229634223641504\n",
      "train loss:0.8728898979092561\n",
      "train loss:0.7887922123003164\n",
      "train loss:0.8951521000375106\n",
      "train loss:0.833169821729255\n",
      "train loss:0.9475610139915469\n",
      "train loss:0.864802814150678\n",
      "train loss:0.7412887984917425\n",
      "train loss:0.8360046552879968\n",
      "train loss:0.7758644939943984\n",
      "train loss:0.8491422522506186\n",
      "train loss:0.8311960711371555\n",
      "train loss:0.7902493647306226\n",
      "train loss:0.8138486987736744\n",
      "train loss:0.8205149822205078\n",
      "train loss:0.897109150581832\n",
      "train loss:0.9262832074677783\n",
      "train loss:0.8689076446496756\n",
      "train loss:0.9130773713738572\n",
      "train loss:0.8915036968076354\n",
      "train loss:1.0481616206238011\n",
      "train loss:0.9194502186354583\n",
      "train loss:0.9817561521074974\n",
      "train loss:0.7340972153209075\n",
      "train loss:0.9041041935984648\n",
      "train loss:0.8737860175672307\n",
      "train loss:0.757869949337981\n",
      "train loss:0.8668072849889501\n",
      "train loss:0.8342313679488182\n",
      "train loss:0.9210976521653872\n",
      "train loss:0.8511038233647871\n",
      "train loss:0.9852734602509164\n",
      "train loss:1.0955617183708772\n",
      "train loss:0.8534255839036468\n",
      "train loss:0.7160321092886227\n",
      "train loss:0.796113008961536\n",
      "train loss:0.8546505342221965\n",
      "train loss:0.7213405112577594\n",
      "train loss:0.8157614316454904\n",
      "train loss:0.9131126581135811\n",
      "train loss:0.8338220621645087\n",
      "train loss:0.7988258318153934\n",
      "train loss:0.8426093323731729\n",
      "train loss:0.8773757102318785\n",
      "train loss:0.8018619253399223\n",
      "train loss:0.8793346626063141\n",
      "train loss:0.9070544157276895\n",
      "train loss:0.9049470962233381\n",
      "train loss:0.8591938901374889\n",
      "train loss:0.7858365546536069\n",
      "train loss:0.78275373147826\n",
      "train loss:0.7891768470414223\n",
      "train loss:0.8071387810128988\n",
      "train loss:0.8618791887655937\n",
      "train loss:0.8240894795866359\n",
      "train loss:0.7454699682196225\n",
      "train loss:0.7056018189050429\n",
      "train loss:0.8432067794804992\n",
      "train loss:0.7254254544255305\n",
      "train loss:0.8073241264208322\n",
      "train loss:0.9013067450382012\n",
      "train loss:0.6594758084437997\n",
      "train loss:0.7564192014617597\n",
      "train loss:0.7278899456233573\n",
      "train loss:0.85179691453253\n",
      "train loss:0.7635362003721056\n",
      "train loss:0.8854258072938755\n",
      "train loss:0.7706257830656871\n",
      "train loss:0.7213147231613781\n",
      "train loss:0.986649971520938\n",
      "train loss:0.7793710070207815\n",
      "train loss:0.6946899460839069\n",
      "train loss:0.8516587701050093\n",
      "train loss:0.8927740674448259\n",
      "train loss:0.8804856045436763\n",
      "train loss:0.8745963112206698\n",
      "train loss:0.854554942364479\n",
      "train loss:0.9115113065795256\n",
      "train loss:0.9610973958921528\n",
      "train loss:0.9438686475052975\n",
      "train loss:0.7212494536399273\n",
      "train loss:0.8251605294074712\n",
      "train loss:0.88339574230588\n",
      "train loss:0.7843787779772449\n",
      "train loss:0.893769203416626\n",
      "train loss:0.9113857805280565\n",
      "train loss:0.7649644506233\n",
      "train loss:0.7355806060034012\n",
      "train loss:0.907597824147159\n",
      "train loss:0.6329660477730328\n",
      "train loss:0.9085235501822695\n",
      "train loss:0.7068626697070437\n",
      "train loss:0.7416065667686813\n",
      "train loss:0.8128974953377001\n",
      "train loss:0.8502363682163796\n",
      "train loss:0.7571620408163249\n",
      "train loss:0.7398974111423214\n",
      "train loss:0.7625916497596735\n",
      "train loss:0.9622933482185114\n",
      "train loss:0.8441863485286352\n",
      "train loss:0.7499817840825829\n",
      "train loss:0.918351924513425\n",
      "train loss:0.7950234702714625\n",
      "train loss:0.8411167250416212\n",
      "train loss:1.0432550939511538\n",
      "train loss:0.9035458242309233\n",
      "train loss:0.846633314482633\n",
      "train loss:0.8175830396088948\n",
      "train loss:0.8327649546474288\n",
      "train loss:0.9240025766799326\n",
      "train loss:0.8893148491769145\n",
      "train loss:0.8604090306836867\n",
      "train loss:0.9764735346449511\n",
      "train loss:0.8951680999761124\n",
      "train loss:0.8897818223777679\n",
      "train loss:0.8954896625273411\n",
      "train loss:0.8952165417824078\n",
      "train loss:0.8802944713801243\n",
      "train loss:0.8408957891315277\n",
      "train loss:0.9904993579602275\n",
      "train loss:0.7986052389325792\n",
      "train loss:0.7655586748804534\n",
      "train loss:0.8357187054287148\n",
      "train loss:0.874471688552471\n",
      "train loss:0.8367061318823273\n",
      "train loss:0.9951636370590353\n",
      "train loss:0.7445258055345216\n",
      "train loss:0.8041605097347864\n",
      "train loss:0.7634680759055802\n",
      "train loss:0.7359715414292977\n",
      "train loss:0.6638410469351343\n",
      "train loss:0.8445512313024198\n",
      "train loss:0.8991678563628933\n",
      "train loss:0.7617270461057146\n",
      "train loss:0.8449306364864604\n",
      "train loss:0.740089299074029\n",
      "train loss:0.8378829274492305\n",
      "train loss:0.8192318837298309\n",
      "train loss:0.7606956136973697\n",
      "train loss:0.8064347195109802\n",
      "train loss:0.9530032013761067\n",
      "train loss:0.8486779796951063\n",
      "train loss:0.9310609081062818\n",
      "train loss:0.8020124305682428\n",
      "train loss:0.7130563605118955\n",
      "train loss:0.8093649693503354\n",
      "train loss:0.746563193605326\n",
      "train loss:0.8563695180440312\n",
      "train loss:0.8654748122047683\n",
      "train loss:0.8213248153814747\n",
      "train loss:0.7803563353296948\n",
      "train loss:0.8424547612412014\n",
      "train loss:0.7551189011253114\n",
      "train loss:0.8898539123616463\n",
      "train loss:0.7820942622393876\n",
      "train loss:0.7322796613053753\n",
      "train loss:0.8301360364584505\n",
      "train loss:0.6937367017874955\n",
      "train loss:0.6866139551178381\n",
      "train loss:0.8297108215410599\n",
      "train loss:0.8518578444747463\n",
      "train loss:0.847608873836489\n",
      "train loss:0.8003579834705526\n",
      "train loss:0.7229313981194284\n",
      "train loss:0.8066658798587527\n",
      "train loss:0.7624608408299761\n",
      "train loss:0.7952100380563151\n",
      "train loss:0.6992183131722582\n",
      "train loss:0.8017617391317927\n",
      "train loss:0.7822374656084192\n",
      "train loss:0.838967175780145\n",
      "train loss:0.7245710637713094\n",
      "train loss:0.8007426580395791\n",
      "train loss:0.8312750113697376\n",
      "train loss:0.8754851597312845\n",
      "train loss:0.8011354400271624\n",
      "train loss:0.8194808112947879\n",
      "train loss:0.7095757688464868\n",
      "train loss:0.6252733041903007\n",
      "train loss:0.8742381548351753\n",
      "train loss:0.7029491725324911\n",
      "train loss:0.7895761672089953\n",
      "train loss:1.030970494033499\n",
      "train loss:0.8861639824593679\n",
      "train loss:1.0246454175507211\n",
      "train loss:0.7373563120754987\n",
      "train loss:0.7533834195652435\n",
      "train loss:0.6749612244510139\n",
      "train loss:0.9045298697639573\n",
      "train loss:0.8601408638797102\n",
      "train loss:0.8759186179301378\n",
      "train loss:0.9523047811317776\n",
      "train loss:0.6691997347321571\n",
      "train loss:0.8808327888397349\n",
      "train loss:0.7515691866266552\n",
      "train loss:0.7880351387372386\n",
      "train loss:0.843193419932155\n",
      "train loss:0.7856145038499579\n",
      "train loss:0.8534979423853397\n",
      "train loss:0.7966486227984068\n",
      "train loss:0.8558631998397176\n",
      "train loss:0.8874819617848385\n",
      "train loss:0.6732967709904194\n",
      "train loss:0.8177667718614297\n",
      "train loss:0.8448590796441134\n",
      "train loss:0.8755708085024431\n",
      "train loss:0.9048166948399847\n",
      "train loss:0.8513106218825732\n",
      "train loss:0.8894906669257786\n",
      "=== epoch:4, train acc:0.68, test acc:0.676 ===\n",
      "train loss:0.9169786526019397\n",
      "train loss:0.6510450924790929\n",
      "train loss:0.6871714752235486\n",
      "train loss:0.9600647344965024\n",
      "train loss:0.7168858940639947\n",
      "train loss:0.8612383306471552\n",
      "train loss:0.8277061907528492\n",
      "train loss:0.9755102015769119\n",
      "train loss:0.7318470541302747\n",
      "train loss:0.7189372692975171\n",
      "train loss:0.823689833725298\n",
      "train loss:1.0149844790510436\n",
      "train loss:0.9104488386833448\n",
      "train loss:0.8051744745262498\n",
      "train loss:0.7985441217021301\n",
      "train loss:0.738612593269642\n",
      "train loss:1.000099610080414\n",
      "train loss:0.9021311090078237\n",
      "train loss:0.7378747311742518\n",
      "train loss:0.7392654305792054\n",
      "train loss:0.8732284037708026\n",
      "train loss:0.9530631173171465\n",
      "train loss:0.7702384955178403\n",
      "train loss:0.7998513225070367\n",
      "train loss:0.8003128645727342\n",
      "train loss:0.6999251298940231\n",
      "train loss:0.814026854218134\n",
      "train loss:0.80601886233076\n",
      "train loss:0.713862403318059\n",
      "train loss:0.6790909048772613\n",
      "train loss:0.7165007710659773\n",
      "train loss:0.7276773427036659\n",
      "train loss:0.8031107806818847\n",
      "train loss:0.7456199828431651\n",
      "train loss:0.7159908465852163\n",
      "train loss:0.8775068805982781\n",
      "train loss:0.718842335290181\n",
      "train loss:0.8394495635335639\n",
      "train loss:0.8622604804165355\n",
      "train loss:0.7606368825481056\n",
      "train loss:0.8064495727182694\n",
      "train loss:0.664740318659314\n",
      "train loss:0.6081678207182981\n",
      "train loss:0.7991777459728717\n",
      "train loss:0.7031724528282389\n",
      "train loss:0.7470935507034762\n",
      "train loss:0.7931827504892139\n",
      "train loss:0.8012774357042811\n",
      "train loss:0.7799703323350069\n",
      "train loss:0.7988975340947175\n",
      "train loss:0.8006882035677053\n",
      "train loss:0.8392555186679784\n",
      "train loss:0.9643287711536834\n",
      "train loss:0.8184043861165429\n",
      "train loss:0.7695153379700266\n",
      "train loss:0.7081229434315481\n",
      "train loss:0.6405631758933861\n",
      "train loss:0.8005543852588748\n",
      "train loss:0.7012679049949169\n",
      "train loss:0.7441014133389349\n",
      "train loss:0.7348824424817999\n",
      "train loss:0.8645799893071772\n",
      "train loss:0.8774072334318859\n",
      "train loss:0.7285005910030512\n",
      "train loss:0.6596620054055792\n",
      "train loss:0.7911566733909886\n",
      "train loss:0.748317869383932\n",
      "train loss:0.8150689471906628\n",
      "train loss:0.7539120368148662\n",
      "train loss:0.8034154318838042\n",
      "train loss:0.6993496042789907\n",
      "train loss:0.8453135113539265\n",
      "train loss:0.7144057879714744\n",
      "train loss:0.6811561060038386\n",
      "train loss:0.7788649921772068\n",
      "train loss:0.7198372417448321\n",
      "train loss:0.7307427489088272\n",
      "train loss:0.8144905055542583\n",
      "train loss:0.6149473757955006\n",
      "train loss:0.9291419449159161\n",
      "train loss:0.8315387812470633\n",
      "train loss:0.8173184219415127\n",
      "train loss:0.8923856130329537\n",
      "train loss:0.6646727472105184\n",
      "train loss:0.7986569483754578\n",
      "train loss:0.6656920091761819\n",
      "train loss:0.8233552373806425\n",
      "train loss:0.6574774891972666\n",
      "train loss:0.755586690444117\n",
      "train loss:0.7750667492035878\n",
      "train loss:0.827114061570947\n",
      "train loss:0.8194728584302376\n",
      "train loss:0.756302378643381\n",
      "train loss:0.7033384773618093\n",
      "train loss:0.8370712761470288\n",
      "train loss:0.6915187205517042\n",
      "train loss:0.8128216250206661\n",
      "train loss:0.7371929125934382\n",
      "train loss:0.6577658217642832\n",
      "train loss:1.080751227327026\n",
      "train loss:0.8131889108808269\n",
      "train loss:1.0516496958675043\n",
      "train loss:0.6526036480599429\n",
      "train loss:0.6553150748430093\n",
      "train loss:0.708596360429428\n",
      "train loss:0.796335927027046\n",
      "train loss:0.7864028584445993\n",
      "train loss:0.6735548346808359\n",
      "train loss:0.6976815664947299\n",
      "train loss:0.7728254787415065\n",
      "train loss:0.8726734740700809\n",
      "train loss:0.9423338531511702\n",
      "train loss:0.656190775674569\n",
      "train loss:0.7638868903716483\n",
      "train loss:0.7956020384268955\n",
      "train loss:0.718223399370419\n",
      "train loss:0.97936522078552\n",
      "train loss:0.7375060904058863\n",
      "train loss:0.8592870643021665\n",
      "train loss:0.8299080261669181\n",
      "train loss:0.7567821829696026\n",
      "train loss:0.7697042057492969\n",
      "train loss:0.9033228890642309\n",
      "train loss:0.9869107966400931\n",
      "train loss:0.7683567087023994\n",
      "train loss:0.7724429239780606\n",
      "train loss:0.7473166660348929\n",
      "train loss:0.7000344003915686\n",
      "train loss:0.6203034567785535\n",
      "train loss:0.8581858807993246\n",
      "train loss:0.7354011394248373\n",
      "train loss:0.8210909613802202\n",
      "train loss:0.6959646998849371\n",
      "train loss:0.7595182398380613\n",
      "train loss:0.7914562812061391\n",
      "train loss:0.7441320966720488\n",
      "train loss:0.9663821729591335\n",
      "train loss:0.6909563307811963\n",
      "train loss:0.8591289864617961\n",
      "train loss:0.7233571461434559\n",
      "train loss:0.5746321062992173\n",
      "train loss:0.7638492183797584\n",
      "train loss:0.6516959560026139\n",
      "train loss:0.7956540709281189\n",
      "train loss:0.786013503210522\n",
      "train loss:0.8283431240756495\n",
      "train loss:0.6506339185170471\n",
      "train loss:0.7590495632458226\n",
      "train loss:0.6902559816722167\n",
      "train loss:0.8542447349841717\n",
      "train loss:0.8870071319605941\n",
      "train loss:0.6591857032343285\n",
      "train loss:0.8012816328034954\n",
      "train loss:0.7883076726118547\n",
      "train loss:0.7050483908716508\n",
      "train loss:0.7472527021140679\n",
      "train loss:0.7779064311968924\n",
      "train loss:0.8274701205708527\n",
      "train loss:0.9694054667057103\n",
      "train loss:0.7569595449557793\n",
      "train loss:0.8520431808607433\n",
      "train loss:0.7220011227825209\n",
      "train loss:0.6756581144763365\n",
      "train loss:0.7425970780182315\n",
      "train loss:0.7337137658416695\n",
      "train loss:0.8246901254759587\n",
      "train loss:0.7642920256037778\n",
      "train loss:0.8600476080358358\n",
      "train loss:0.7261945271569737\n",
      "train loss:0.769307744977678\n",
      "train loss:0.7721247154606388\n",
      "train loss:0.7884298330732995\n",
      "train loss:0.7055249599115556\n",
      "train loss:0.6779805906883758\n",
      "train loss:0.7476129583864539\n",
      "train loss:0.7352456988031664\n",
      "train loss:0.8591735190144522\n",
      "train loss:0.746311628411085\n",
      "train loss:0.7976154924857513\n",
      "train loss:0.6830026827021735\n",
      "train loss:0.6098203923408008\n",
      "train loss:0.8372701501919415\n",
      "train loss:0.5823948052487672\n",
      "train loss:0.6159353726651087\n",
      "train loss:0.6626684959856419\n",
      "train loss:0.7869457250112376\n",
      "train loss:0.7422289795020676\n",
      "train loss:0.8258212496976064\n",
      "train loss:0.6902400852496661\n",
      "train loss:0.7086421591918889\n",
      "train loss:0.637190247825182\n",
      "train loss:0.7232658419361238\n",
      "train loss:0.6734769995346538\n",
      "train loss:0.7352517334992678\n",
      "train loss:0.7055770607741291\n",
      "train loss:0.7472715653931097\n",
      "train loss:0.8104830070644167\n",
      "train loss:0.7673359620007774\n",
      "train loss:0.5832156412236852\n",
      "train loss:0.6759162297322066\n",
      "train loss:0.6975265658602114\n",
      "train loss:0.6752215324787976\n",
      "train loss:0.6770826509947874\n",
      "train loss:0.7642834330574919\n",
      "train loss:0.7615652212825934\n",
      "train loss:0.7055925640591753\n",
      "train loss:0.7451735598652826\n",
      "train loss:0.749681652816689\n",
      "train loss:0.6944655922345562\n",
      "train loss:0.6828921378247074\n",
      "train loss:0.6854948832245726\n",
      "train loss:0.7618472381648815\n",
      "train loss:0.7544614864527495\n",
      "train loss:0.6833216433341885\n",
      "train loss:0.7110817685390978\n",
      "train loss:0.7125558126037943\n",
      "train loss:0.8609853715595416\n",
      "train loss:0.7607730671355037\n",
      "train loss:0.6750710157608271\n",
      "train loss:0.657102702993963\n",
      "train loss:0.7475449742840118\n",
      "train loss:0.6945715368641728\n",
      "train loss:0.819925742264369\n",
      "train loss:0.6662478814785697\n",
      "train loss:0.7397244264070072\n",
      "train loss:0.7317684473484333\n",
      "train loss:0.6663454250988577\n",
      "train loss:0.7583594524207544\n",
      "train loss:0.6314270022905035\n",
      "train loss:0.6476340975033757\n",
      "train loss:0.7436803033345908\n",
      "train loss:0.6549938655341887\n",
      "train loss:0.6919888604470088\n",
      "train loss:0.7628333817011425\n",
      "train loss:0.7952616540552827\n",
      "train loss:0.6588307269517988\n",
      "train loss:0.6032212544635138\n",
      "train loss:0.8117262154615125\n",
      "train loss:0.8656071763773484\n",
      "train loss:0.716712266161553\n",
      "train loss:0.6816410529867565\n",
      "train loss:0.7967712442995676\n",
      "train loss:0.6693450592148428\n",
      "train loss:0.789353380509422\n",
      "train loss:0.772714442070993\n",
      "train loss:0.77004229478462\n",
      "train loss:0.7353050600695481\n",
      "train loss:0.7142816892001607\n",
      "train loss:0.7227506460633312\n",
      "train loss:0.685675836595356\n",
      "train loss:0.820741651726738\n",
      "train loss:0.5850437515152412\n",
      "train loss:0.7747383180335957\n",
      "train loss:0.7798423999086012\n",
      "train loss:0.7611939117480034\n",
      "train loss:0.7357975814450113\n",
      "train loss:0.6287246681147398\n",
      "train loss:0.7716191375447906\n",
      "train loss:0.6949782178233045\n",
      "train loss:0.6172071848362133\n",
      "train loss:0.7327083394876404\n",
      "train loss:0.7179076857808013\n",
      "train loss:0.6426178830891327\n",
      "train loss:0.6612358062652166\n",
      "train loss:0.7796997281340368\n",
      "train loss:0.82812386132756\n",
      "train loss:0.7240211776985644\n",
      "train loss:0.6353106389212266\n",
      "train loss:0.6235572655893186\n",
      "train loss:0.7853149489964634\n",
      "train loss:0.5493840924068998\n",
      "train loss:0.7104469343155132\n",
      "train loss:0.7657660092235732\n",
      "train loss:0.8548867512773164\n",
      "train loss:0.6965432469685398\n",
      "train loss:0.644267739371432\n",
      "train loss:0.6614145089587155\n",
      "train loss:0.735351761558291\n",
      "train loss:0.8547878110760746\n",
      "train loss:0.6999465625175939\n",
      "train loss:0.663047941021576\n",
      "train loss:0.7177353230305592\n",
      "train loss:0.7352476406852726\n",
      "train loss:0.774323083132176\n",
      "train loss:0.8695110293115169\n",
      "train loss:0.6767010451486226\n",
      "train loss:0.8371309097141539\n",
      "train loss:0.6813962713504312\n",
      "train loss:0.6182081862291657\n",
      "train loss:0.6285950629143499\n",
      "train loss:0.6369210137686462\n",
      "train loss:0.6002163172267099\n",
      "train loss:0.713609372555479\n",
      "train loss:0.5619034717824068\n",
      "train loss:0.6885676043113793\n",
      "train loss:0.7962430350682598\n",
      "train loss:0.7032944387815298\n",
      "train loss:0.699444843461306\n",
      "train loss:0.6500918871383409\n",
      "train loss:0.6294817671439334\n",
      "train loss:0.7924078571520637\n",
      "train loss:0.83533441698956\n",
      "train loss:0.6556312852961206\n",
      "train loss:0.7457460222543291\n",
      "train loss:0.7133884516156348\n",
      "train loss:0.6351429882007195\n",
      "train loss:0.6212609135660903\n",
      "train loss:0.6670715455011017\n",
      "train loss:0.8036598183411477\n",
      "train loss:0.6851283716605319\n",
      "train loss:0.6689535117528537\n",
      "train loss:0.7293030283742781\n",
      "train loss:0.709613763473514\n",
      "train loss:0.6313521300951233\n",
      "train loss:0.8477657199834365\n",
      "train loss:0.80788379790043\n",
      "train loss:0.7224913896526148\n",
      "train loss:0.7234492612130323\n",
      "train loss:0.6577792097022312\n",
      "train loss:0.6547675342062875\n",
      "train loss:0.7504006448183609\n",
      "train loss:0.6306477413639764\n",
      "train loss:0.6577698385649617\n",
      "train loss:0.7023668737608786\n",
      "train loss:0.7228865311458506\n",
      "train loss:0.6637388754312893\n",
      "train loss:0.6157092540408248\n",
      "train loss:0.8492183510256709\n",
      "train loss:0.7525988991798872\n",
      "train loss:0.6601115928105272\n",
      "train loss:0.6387601897471553\n",
      "train loss:0.8012881369619009\n",
      "train loss:0.7068214669771468\n",
      "train loss:0.7364476276607981\n",
      "train loss:0.7845117026652981\n",
      "train loss:0.6409333069674733\n",
      "train loss:0.7122411403540997\n",
      "train loss:0.6723906223541768\n",
      "train loss:0.7086445039276321\n",
      "train loss:0.6791536404002705\n",
      "train loss:0.5630565370501306\n",
      "train loss:0.8670084551533555\n",
      "train loss:0.7133245620891151\n",
      "train loss:0.7724162469690125\n",
      "train loss:0.6915128375469297\n",
      "train loss:0.5968325458510044\n",
      "train loss:0.743612178714951\n",
      "train loss:0.6891072138240181\n",
      "train loss:0.7738928673403338\n",
      "train loss:0.7516479914198341\n",
      "train loss:0.7342108489948161\n",
      "train loss:0.6676594828482496\n",
      "train loss:0.6658533593854737\n",
      "train loss:0.7527545485257745\n",
      "train loss:0.5863450812671724\n",
      "train loss:0.7000092362904663\n",
      "train loss:0.8751713701663735\n",
      "train loss:0.7469374422356393\n",
      "train loss:0.6900007353308228\n",
      "train loss:0.7684468847626724\n",
      "train loss:0.7804861429950501\n",
      "train loss:0.6455106798117625\n",
      "train loss:0.6976328153542045\n",
      "train loss:0.6761959534773616\n",
      "train loss:0.6750879636457783\n",
      "train loss:0.7645255512117446\n",
      "train loss:0.7305253404845904\n",
      "train loss:0.6441362654992844\n",
      "train loss:0.6407210332501815\n",
      "train loss:0.625224010060664\n",
      "train loss:0.74282296769875\n",
      "train loss:0.5988668024191175\n",
      "train loss:0.6788994678793477\n",
      "train loss:0.6048559695719715\n",
      "train loss:0.8492656217796231\n",
      "train loss:0.5766759594335299\n",
      "train loss:0.7205474137024455\n",
      "train loss:0.7966131853030377\n",
      "train loss:0.7361442585851045\n",
      "train loss:0.7598891124185966\n",
      "train loss:0.6548934648075327\n",
      "train loss:0.6511877420099225\n",
      "train loss:0.681347840431851\n",
      "train loss:0.5876152028549094\n",
      "train loss:0.784343818125726\n",
      "train loss:0.7447872409253548\n",
      "train loss:0.673430518283014\n",
      "train loss:0.6540251703285379\n",
      "train loss:0.6362717606288957\n",
      "train loss:0.6089761621084424\n",
      "train loss:0.8284943343321425\n",
      "train loss:0.7351808760666003\n",
      "train loss:0.639638106362846\n",
      "train loss:0.7604190824109128\n",
      "train loss:0.6114814098343515\n",
      "train loss:0.5931201221804606\n",
      "train loss:0.6956321484056754\n",
      "train loss:0.7458888186675039\n",
      "train loss:0.6332935042231361\n",
      "train loss:0.6528293661339489\n",
      "train loss:0.6130965110933151\n",
      "train loss:0.6472452452736899\n",
      "train loss:0.5754940807721112\n",
      "train loss:0.7413324108448589\n",
      "train loss:0.7494785727268867\n",
      "train loss:0.6850061337369078\n",
      "train loss:0.6304697166038454\n",
      "train loss:0.7989004684456468\n",
      "train loss:0.7154105186222252\n",
      "train loss:0.5655383001722808\n",
      "train loss:0.5421464661414713\n",
      "train loss:0.7711010419500316\n",
      "train loss:0.8508920445095957\n",
      "train loss:0.6081948945092478\n",
      "train loss:0.9232548365518349\n",
      "train loss:0.6839624546701459\n",
      "train loss:0.6254185676074862\n",
      "train loss:0.7753993315934735\n",
      "train loss:0.6582054148267928\n",
      "train loss:0.7309090522300391\n",
      "train loss:0.7340567190301611\n",
      "train loss:0.6251035935647432\n",
      "train loss:0.9231152042053177\n",
      "train loss:0.5855953238711696\n",
      "train loss:0.6302709301058542\n",
      "train loss:0.8645267190310022\n",
      "train loss:0.6895035484352374\n",
      "train loss:0.755334914060605\n",
      "train loss:0.9717155736362915\n",
      "train loss:0.6233006325066648\n",
      "train loss:0.6524000385033092\n",
      "train loss:0.6464176282386235\n",
      "train loss:0.7812031831914841\n",
      "train loss:0.8175845789304286\n",
      "train loss:0.6585123932723868\n",
      "train loss:0.6025222621131627\n",
      "train loss:0.6416059286450302\n",
      "train loss:0.6195092215207969\n",
      "train loss:0.6838593726596414\n",
      "train loss:0.5569446710415338\n",
      "train loss:0.8109254044769046\n",
      "train loss:0.70108037024278\n",
      "train loss:0.6381042203706004\n",
      "train loss:0.7624625672207699\n",
      "train loss:0.5721664312736727\n",
      "train loss:0.795567142499167\n",
      "train loss:0.9070139705421988\n",
      "train loss:0.8327691921203326\n",
      "train loss:0.8435770972848837\n",
      "train loss:0.8053374655806275\n",
      "train loss:0.6826395567453661\n",
      "train loss:0.714978527450782\n",
      "train loss:0.7390673857842388\n",
      "train loss:0.8014824535013202\n",
      "train loss:0.7515401770910815\n",
      "train loss:0.7751525564645967\n",
      "train loss:0.7804896912316573\n",
      "train loss:0.6536479128757549\n",
      "train loss:0.6414288451308863\n",
      "train loss:0.6176828830166179\n",
      "train loss:0.7316231089765368\n",
      "train loss:0.7252980541704228\n",
      "train loss:0.7002701463488434\n",
      "train loss:0.6519080739814074\n",
      "train loss:0.6864315703344168\n",
      "train loss:0.5669544322799587\n",
      "train loss:0.5427705248859364\n",
      "train loss:0.6278965392309483\n",
      "train loss:0.5821762753897463\n",
      "train loss:0.6565040404695783\n",
      "train loss:0.6827883538493229\n",
      "train loss:0.8245203301154995\n",
      "train loss:0.7750169059997758\n",
      "train loss:0.8746614272333069\n",
      "train loss:0.7433216093712852\n",
      "train loss:0.7199908120821206\n",
      "train loss:0.7979804585811288\n",
      "train loss:0.669540208893702\n",
      "train loss:0.6822048617282634\n",
      "train loss:0.8034734059762972\n",
      "train loss:0.6100454362614474\n",
      "train loss:0.7548209166536434\n",
      "train loss:0.6581894734853555\n",
      "train loss:0.7764448535640366\n",
      "train loss:0.5878595365992035\n",
      "train loss:0.5519253251093966\n",
      "train loss:0.7085723423359569\n",
      "train loss:0.6529981099078221\n",
      "train loss:0.6455030633545868\n",
      "train loss:0.6962432788679859\n",
      "train loss:0.6969054694784076\n",
      "train loss:0.7327546446924066\n",
      "train loss:0.8275844572561921\n",
      "train loss:0.7742437798671956\n",
      "train loss:0.703260961467834\n",
      "train loss:0.6205196835665978\n",
      "train loss:0.7220830175109395\n",
      "train loss:0.6157688469085013\n",
      "train loss:0.6060874734425856\n",
      "train loss:0.7443446217643276\n",
      "train loss:0.7548634896427404\n",
      "train loss:0.5485836765185387\n",
      "train loss:0.5535941890517059\n",
      "train loss:0.6422248937450437\n",
      "train loss:0.5917209005987082\n",
      "train loss:0.5593180993463767\n",
      "train loss:0.8199001249551197\n",
      "train loss:0.5629930977521667\n",
      "train loss:0.5547058110099418\n",
      "train loss:0.884163176684647\n",
      "train loss:0.7633851754576121\n",
      "train loss:0.7143153183830242\n",
      "train loss:0.6228646249066524\n",
      "train loss:0.7293178729705506\n",
      "train loss:0.6624340522329071\n",
      "train loss:0.768043051499966\n",
      "train loss:1.0026309598299994\n",
      "train loss:0.7932837428596805\n",
      "train loss:0.584909428263988\n",
      "train loss:0.7618884152404024\n",
      "train loss:0.5543334290604705\n",
      "train loss:0.732952080795114\n",
      "train loss:0.7283922816179114\n",
      "train loss:0.6602612269157203\n",
      "train loss:0.7306737926781715\n",
      "train loss:0.6980519765659483\n",
      "train loss:0.6275918791095644\n",
      "train loss:0.5960325484224203\n",
      "train loss:0.7475246888506826\n",
      "train loss:0.8420346289284925\n",
      "train loss:0.7600735287686752\n",
      "train loss:0.6685225493387351\n",
      "train loss:0.722506424334751\n",
      "train loss:0.6919171216753933\n",
      "train loss:0.7716584357776405\n",
      "train loss:0.7276495601462466\n",
      "train loss:0.6776003396690977\n",
      "train loss:0.6883005609146573\n",
      "train loss:0.8552136997607894\n",
      "train loss:0.8550259410895137\n",
      "train loss:0.6758855769014203\n",
      "train loss:0.7269597109727214\n",
      "train loss:0.6327418370222806\n",
      "train loss:0.6746078284417707\n",
      "train loss:0.7515298683818572\n",
      "train loss:0.6807550745698133\n",
      "train loss:0.6251042325678658\n",
      "train loss:0.6623140486444796\n",
      "train loss:0.7699933681276424\n",
      "train loss:0.7127141035400137\n",
      "train loss:0.5686120418275386\n",
      "train loss:0.7217926213750505\n",
      "train loss:0.7109770126151492\n",
      "train loss:0.744188184694076\n",
      "train loss:0.687575356409702\n",
      "train loss:0.6311295739106576\n",
      "train loss:0.7336584720687314\n",
      "train loss:0.7361544466541736\n",
      "train loss:0.6601424097627855\n",
      "train loss:0.6392064282760915\n",
      "train loss:0.6908196248238184\n",
      "train loss:0.7259203196491407\n",
      "train loss:0.627605702013043\n",
      "train loss:0.6165530414174452\n",
      "train loss:0.6752867736678805\n",
      "train loss:0.741658003709325\n",
      "train loss:0.6860989564754931\n",
      "train loss:0.7608583428365779\n",
      "train loss:0.5614138582173634\n",
      "train loss:0.6366639952048114\n",
      "train loss:0.682905486825971\n",
      "train loss:0.7904535349129024\n",
      "train loss:0.7008816302676365\n",
      "train loss:0.5975595223473091\n",
      "train loss:0.741936379231386\n",
      "train loss:0.8205754975850287\n",
      "train loss:0.6320605206858608\n",
      "train loss:0.6764209724622741\n",
      "train loss:0.6442482619870722\n",
      "train loss:0.6041581659480603\n",
      "train loss:0.6086560475556009\n",
      "train loss:0.5926285468001026\n",
      "train loss:0.6463325443107526\n",
      "train loss:0.8107731517229538\n",
      "train loss:0.7492698020438044\n",
      "train loss:0.7608037543320353\n",
      "train loss:0.6871428306872701\n",
      "train loss:0.6988716811476927\n",
      "train loss:0.606611158439922\n",
      "train loss:0.7568781991447721\n",
      "train loss:0.6705916611631608\n",
      "train loss:0.7450709374902172\n",
      "train loss:0.621944813068939\n",
      "train loss:0.6576585363847731\n",
      "train loss:0.6196048771648908\n",
      "train loss:0.6281873652030585\n",
      "train loss:0.6613258622289887\n",
      "train loss:0.6160644139025147\n",
      "train loss:0.680397679211784\n",
      "train loss:0.5846423644210461\n",
      "=== epoch:5, train acc:0.705, test acc:0.708 ===\n",
      "train loss:0.6133638023791727\n",
      "train loss:0.6405848189566568\n",
      "train loss:0.7968503923392798\n",
      "train loss:0.6370796757264575\n",
      "train loss:0.7799699595328297\n",
      "train loss:0.8674115065493999\n",
      "train loss:0.7526526900589139\n",
      "train loss:0.787326664731253\n",
      "train loss:0.645948275277105\n",
      "train loss:0.6519054572658787\n",
      "train loss:0.5761963273347482\n",
      "train loss:0.7056219620081248\n",
      "train loss:0.8708732541892146\n",
      "train loss:0.6706813648122992\n",
      "train loss:0.6320639769634295\n",
      "train loss:0.6975141818157418\n",
      "train loss:0.6912538232810725\n",
      "train loss:0.7217286245864554\n",
      "train loss:0.7786945314753723\n",
      "train loss:0.6327605335882616\n",
      "train loss:0.7165669001738693\n",
      "train loss:0.7456689758543112\n",
      "train loss:0.6594106518856182\n",
      "train loss:0.6877929100455493\n",
      "train loss:0.600486971683121\n",
      "train loss:0.611121863828271\n",
      "train loss:0.6916732109262327\n",
      "train loss:0.6505469035782017\n",
      "train loss:0.8639042894545769\n",
      "train loss:0.7235253420426058\n",
      "train loss:0.6882982877228889\n",
      "train loss:0.6612808340613957\n",
      "train loss:0.6516725441017907\n",
      "train loss:0.6596602076820862\n",
      "train loss:0.6364685294157947\n",
      "train loss:0.6948621331751061\n",
      "train loss:0.5659685383327245\n",
      "train loss:0.686194949380162\n",
      "train loss:0.7173101412222741\n",
      "train loss:0.6205406496864475\n",
      "train loss:0.7544678564389907\n",
      "train loss:0.6776304072369611\n",
      "train loss:0.6104508611810371\n",
      "train loss:0.5885826411158721\n",
      "train loss:0.7020318236349015\n",
      "train loss:0.5201607826600232\n",
      "train loss:0.5009638643919802\n",
      "train loss:0.6242167040163771\n",
      "train loss:0.7405611932261154\n",
      "train loss:0.7026503869128241\n",
      "train loss:0.6071471215984426\n",
      "train loss:0.7743369482364817\n",
      "train loss:0.6453088658151316\n",
      "train loss:0.6543844342370164\n",
      "train loss:0.6346570631924026\n",
      "train loss:0.6704237660451936\n",
      "train loss:0.5439518047657944\n",
      "train loss:0.658289922073444\n",
      "train loss:0.6845160224139734\n",
      "train loss:0.5962788371553005\n",
      "train loss:0.5683934560978648\n",
      "train loss:0.688454319442923\n",
      "train loss:0.809085619670507\n",
      "train loss:0.6251836472419419\n",
      "train loss:0.6187755118762261\n",
      "train loss:0.7093078937741505\n",
      "train loss:0.6886753268714435\n",
      "train loss:0.7085882435892421\n",
      "train loss:0.6417963895249763\n",
      "train loss:0.5855552243548405\n",
      "train loss:0.7122505190576414\n",
      "train loss:0.6289130111384045\n",
      "train loss:0.5731336688408747\n",
      "train loss:0.6140492041559314\n",
      "train loss:0.7673989337400413\n",
      "train loss:0.7024569901093591\n",
      "train loss:0.5905503529306325\n",
      "train loss:0.7090531278271953\n",
      "train loss:0.5575517729964179\n",
      "train loss:0.6566253332983001\n",
      "train loss:0.5733451659255758\n",
      "train loss:0.697206668928133\n",
      "train loss:0.669715549546531\n",
      "train loss:0.5516129765469961\n",
      "train loss:0.7842664485242341\n",
      "train loss:0.6746968393596836\n",
      "train loss:0.6878242830232325\n",
      "train loss:0.6738874659536563\n",
      "train loss:0.754801126207044\n",
      "train loss:0.666588438327207\n",
      "train loss:0.7771144153995039\n",
      "train loss:0.8045347860137337\n",
      "train loss:0.8066444467814408\n",
      "train loss:0.6705147200693985\n",
      "train loss:0.6877741847901531\n",
      "train loss:0.6108077007794537\n",
      "train loss:0.5483638164908886\n",
      "train loss:0.6148550603064433\n",
      "train loss:0.6008987090506676\n",
      "train loss:0.6635697945674494\n",
      "train loss:0.5166731933661766\n",
      "train loss:0.7522306527114219\n",
      "train loss:0.6185098849630997\n",
      "train loss:0.6203938314996962\n",
      "train loss:0.6074980910311772\n",
      "train loss:0.5174754214383208\n",
      "train loss:0.6123593694656018\n",
      "train loss:0.765716951868349\n",
      "train loss:0.6227124671370013\n",
      "train loss:0.5679654970262411\n",
      "train loss:0.6764205794026468\n",
      "train loss:0.7095278701530515\n",
      "train loss:0.5891784342423174\n",
      "train loss:0.6379723623703132\n",
      "train loss:0.6051089683873176\n",
      "train loss:0.5958058832703257\n",
      "train loss:0.6179248748799033\n",
      "train loss:0.6608738726689384\n",
      "train loss:0.7954214979623389\n",
      "train loss:0.6134354512239518\n",
      "train loss:0.6863991397081048\n",
      "train loss:0.7494493150588248\n",
      "train loss:0.6292112947080799\n",
      "train loss:0.6885742290967066\n",
      "train loss:0.6176892934554816\n",
      "train loss:0.6301563462078998\n",
      "train loss:0.6494340524294381\n",
      "train loss:0.7950639431981348\n",
      "train loss:0.626979945395929\n",
      "train loss:0.6200868132291234\n",
      "train loss:0.6394058937999927\n",
      "train loss:0.7940719798334152\n",
      "train loss:0.698871343143706\n",
      "train loss:0.607929070732938\n",
      "train loss:0.6688816898811264\n",
      "train loss:0.8925119471724949\n",
      "train loss:0.7718467968718707\n",
      "train loss:0.6862996687639342\n",
      "train loss:0.6797419709212703\n",
      "train loss:0.5499490150862699\n",
      "train loss:0.6648104704430564\n",
      "train loss:0.6336745482248103\n",
      "train loss:0.6829177933011463\n",
      "train loss:0.7024674245638092\n",
      "train loss:0.5628469307111602\n",
      "train loss:0.5649758511125879\n",
      "train loss:0.7189352477684681\n",
      "train loss:0.7241011087789877\n",
      "train loss:0.7920163525896955\n",
      "train loss:0.5653148267684742\n",
      "train loss:0.6595719580201075\n",
      "train loss:0.6029456527328971\n",
      "train loss:0.6145982508894914\n",
      "train loss:0.6743933656454683\n",
      "train loss:0.6775899839604247\n",
      "train loss:0.6466799939624411\n",
      "train loss:0.524936257068115\n",
      "train loss:0.5046582827313129\n",
      "train loss:0.5639283514831126\n",
      "train loss:0.6794877751793004\n",
      "train loss:0.5546436033251517\n",
      "train loss:0.7009886377818557\n",
      "train loss:0.6557982507558022\n",
      "train loss:0.6664575135771357\n",
      "train loss:0.676466895196032\n",
      "train loss:0.6519257856839908\n",
      "train loss:0.7281384887295035\n",
      "train loss:0.5510007103679524\n",
      "train loss:0.5480177736222109\n",
      "train loss:0.8244986656150624\n",
      "train loss:0.6147283491112809\n",
      "train loss:0.6464588020109109\n",
      "train loss:0.5788031597178116\n",
      "train loss:0.6406209300862288\n",
      "train loss:0.7292900311662646\n",
      "train loss:0.6830078372716168\n",
      "train loss:0.5758724096221534\n",
      "train loss:0.8072431759962926\n",
      "train loss:0.6457138079222589\n",
      "train loss:0.4388918892296054\n",
      "train loss:0.5652062182320962\n",
      "train loss:0.7253718530668307\n",
      "train loss:0.6878686147188907\n",
      "train loss:0.6736462073020413\n",
      "train loss:0.781396085994585\n",
      "train loss:0.6715893136368853\n",
      "train loss:0.6571251293802313\n",
      "train loss:0.748385741895448\n",
      "train loss:0.5715223765343741\n",
      "train loss:0.8001087378670836\n",
      "train loss:0.6529385782089375\n",
      "train loss:0.5617256726953732\n",
      "train loss:0.6587106463145534\n",
      "train loss:0.7365626791324046\n",
      "train loss:0.6637228277182275\n",
      "train loss:0.6780346295409039\n",
      "train loss:0.5289469041069566\n",
      "train loss:0.627589655171856\n",
      "train loss:0.6848051191190513\n",
      "train loss:0.6967635947648684\n",
      "train loss:0.6568330780535864\n",
      "train loss:0.6300618178927844\n",
      "train loss:0.5623150681767488\n",
      "train loss:0.6631961045180055\n",
      "train loss:0.6828265046434687\n",
      "train loss:0.5848110610110849\n",
      "train loss:0.6569021155797873\n",
      "train loss:0.7226221126742602\n",
      "train loss:0.6551360876497182\n",
      "train loss:0.7212429435205727\n",
      "train loss:0.7135960107251528\n",
      "train loss:0.6609137268185208\n",
      "train loss:0.6292822641157015\n",
      "train loss:0.6024398787401997\n",
      "train loss:0.6032531715569357\n",
      "train loss:0.8135048713043149\n",
      "train loss:0.6041394660053953\n",
      "train loss:0.6222699565745294\n",
      "train loss:0.6398700785690328\n",
      "train loss:0.6196652389832066\n",
      "train loss:0.6130899310995145\n",
      "train loss:0.8495029534169573\n",
      "train loss:0.6084811261845674\n",
      "train loss:0.8079268929438335\n",
      "train loss:0.6499299310676468\n",
      "train loss:0.6289004180108394\n",
      "train loss:0.6966225089032057\n",
      "train loss:0.7310455591811222\n",
      "train loss:0.6614747191675413\n",
      "train loss:0.67164395445342\n",
      "train loss:0.5658161527035523\n",
      "train loss:0.8577381349892089\n",
      "train loss:0.7267797883083691\n",
      "train loss:0.6643618982712637\n",
      "train loss:0.7062881674075371\n",
      "train loss:0.6867362600023165\n",
      "train loss:0.6749638586203358\n",
      "train loss:0.6857863585225463\n",
      "train loss:0.6304776022805048\n",
      "train loss:0.6777683742415178\n",
      "train loss:0.7357638890640555\n",
      "train loss:0.5404822844339114\n",
      "train loss:0.6335069840250044\n",
      "train loss:0.6326210215376146\n",
      "train loss:0.7283106771568064\n",
      "train loss:0.6637334165197055\n",
      "train loss:0.5203962234974621\n",
      "train loss:0.6114963571843683\n",
      "train loss:0.6194547653967144\n",
      "train loss:0.7661919658281966\n",
      "train loss:0.619002609796858\n",
      "train loss:0.6251078094700775\n",
      "train loss:0.5888513245530849\n",
      "train loss:0.6051622291247474\n",
      "train loss:0.6109751429878224\n",
      "train loss:0.6545188290592445\n",
      "train loss:0.6063360152257451\n",
      "train loss:0.5456587301915641\n",
      "train loss:0.6616505889032418\n",
      "train loss:0.63989888039217\n",
      "train loss:0.5251510640126255\n",
      "train loss:0.6823214273835864\n",
      "train loss:0.7598023420268583\n",
      "train loss:0.6592504282034183\n",
      "train loss:0.5935192121303101\n",
      "train loss:0.5556974655250926\n",
      "train loss:0.7169768814963909\n",
      "train loss:0.6668125765557292\n",
      "train loss:0.5090114903407301\n",
      "train loss:0.5816823453931853\n",
      "train loss:0.5948363157803831\n",
      "train loss:0.6459760616565824\n",
      "train loss:0.6164799224970399\n",
      "train loss:0.7235602266012284\n",
      "train loss:0.6780574796399962\n",
      "train loss:0.8346960184962497\n",
      "train loss:0.5934153861748824\n",
      "train loss:0.5939341263005756\n",
      "train loss:0.6571910622449191\n",
      "train loss:0.5301774737585747\n",
      "train loss:0.5550231477138534\n",
      "train loss:0.7854375921646164\n",
      "train loss:0.5603933756972398\n",
      "train loss:0.6808121152029502\n",
      "train loss:0.8510738125928787\n",
      "train loss:0.6627097009863963\n",
      "train loss:0.6242706381865386\n",
      "train loss:0.5711304427741648\n",
      "train loss:0.6140866304416412\n",
      "train loss:0.6897367207637524\n",
      "train loss:0.6579033834773115\n",
      "train loss:0.7537385083174181\n",
      "train loss:0.6368591334977729\n",
      "train loss:0.7268033724115459\n",
      "train loss:0.6236084613725628\n",
      "train loss:0.6173155960238172\n",
      "train loss:0.5795433436023427\n",
      "train loss:0.6533764786414896\n",
      "train loss:0.6102811489530461\n",
      "train loss:0.5223436539938319\n",
      "train loss:0.7370586390986513\n",
      "train loss:0.5417773445513838\n",
      "train loss:0.624746221662034\n",
      "train loss:0.6802054975635434\n",
      "train loss:0.8101916257994639\n",
      "train loss:0.8005697290917072\n",
      "train loss:0.8224781076805161\n",
      "train loss:0.6005020263456815\n",
      "train loss:0.5751694058960093\n",
      "train loss:0.5109909522670424\n",
      "train loss:0.6666664289605241\n",
      "train loss:0.535866069978881\n",
      "train loss:0.6858105140157256\n",
      "train loss:0.713887618694479\n",
      "train loss:0.4928830847342514\n",
      "train loss:0.5678964365980141\n",
      "train loss:0.6041146036480367\n",
      "train loss:0.4657693032130625\n",
      "train loss:0.7395530507913594\n",
      "train loss:0.6763696613945693\n",
      "train loss:0.5459714693444007\n",
      "train loss:0.5383556195678783\n",
      "train loss:0.6654872449521123\n",
      "train loss:0.5822865695570498\n",
      "train loss:0.7269763870395329\n",
      "train loss:0.6788926820562047\n",
      "train loss:0.605918209443448\n",
      "train loss:0.6019307620051277\n",
      "train loss:0.7201946986246474\n",
      "train loss:0.6381163431239164\n",
      "train loss:0.5704665290507959\n",
      "train loss:0.6962532731202002\n",
      "train loss:0.7116733736623732\n",
      "train loss:0.6254486781021337\n",
      "train loss:0.7667662925052461\n",
      "train loss:0.7571303366718075\n",
      "train loss:0.6742087793641127\n",
      "train loss:0.6445865342885816\n",
      "train loss:0.6786145252849155\n",
      "train loss:0.6746508221492061\n",
      "train loss:0.6087630229014855\n",
      "train loss:0.6516873326314481\n",
      "train loss:0.8019159632570283\n",
      "train loss:0.6470950681105555\n",
      "train loss:0.6599939660500792\n",
      "train loss:0.5407019933291075\n",
      "train loss:0.802990356192261\n",
      "train loss:0.5827946239832683\n",
      "train loss:0.6014067867451621\n",
      "train loss:0.6235955727099933\n",
      "train loss:0.6091994457752518\n",
      "train loss:0.6235249666753124\n",
      "train loss:0.8627853717706943\n",
      "train loss:0.5434008753230973\n",
      "train loss:0.5981134643043426\n",
      "train loss:0.5530731067897087\n",
      "train loss:0.6392651329862733\n",
      "train loss:0.6053926816273316\n",
      "train loss:0.6605779311013383\n",
      "train loss:0.5983576690866177\n",
      "train loss:0.6214815092975157\n",
      "train loss:0.6061633971590557\n",
      "train loss:0.5829665292844866\n",
      "train loss:0.6496603967489206\n",
      "train loss:0.6206092207013392\n",
      "train loss:0.6611932428028134\n",
      "train loss:0.6852807256248903\n",
      "train loss:0.7105607573693407\n",
      "train loss:0.5597634010435968\n",
      "train loss:0.662125747770134\n",
      "train loss:0.5510510764589907\n",
      "train loss:0.7148834803640198\n",
      "train loss:0.5579448536194861\n",
      "train loss:0.6222232796389369\n",
      "train loss:0.5028949321399179\n",
      "train loss:0.6381416341221591\n",
      "train loss:0.5036048744133435\n",
      "train loss:0.5499837467291296\n",
      "train loss:0.6442506186280841\n",
      "train loss:0.637248189413943\n",
      "train loss:0.6252433346423723\n",
      "train loss:0.6149528636382766\n",
      "train loss:0.6832719462197572\n",
      "train loss:0.5687466701860902\n",
      "train loss:0.6598819598361126\n",
      "train loss:0.6479444328024729\n",
      "train loss:0.572112037699243\n",
      "train loss:0.5565261922589955\n",
      "train loss:0.5407506908122233\n",
      "train loss:0.7572729850435261\n",
      "train loss:0.5929903013298579\n",
      "train loss:0.6684530755700447\n",
      "train loss:0.7513572925977405\n",
      "train loss:0.5787978975921411\n",
      "train loss:0.6291334873550948\n",
      "train loss:0.5434967364348482\n",
      "train loss:0.7426884004248397\n",
      "train loss:0.6535598925172005\n",
      "train loss:0.5482725465357202\n",
      "train loss:0.5850442578756724\n",
      "train loss:0.6722078451534296\n",
      "train loss:0.6880495410484195\n",
      "train loss:0.7531146536158089\n",
      "train loss:0.6998316893241061\n",
      "train loss:0.7943280152707511\n",
      "train loss:0.4938574583421421\n",
      "train loss:0.4716785408894713\n",
      "train loss:0.5529513657876808\n",
      "train loss:0.578089033581752\n",
      "train loss:0.591521330083514\n",
      "train loss:0.6418953209774115\n",
      "train loss:0.7307754073496906\n",
      "train loss:0.6415861550080001\n",
      "train loss:0.5556887681531274\n",
      "train loss:0.5302934357815855\n",
      "train loss:0.5810286523078924\n",
      "train loss:0.8073247626831477\n",
      "train loss:0.5305181349809251\n",
      "train loss:0.49990096695015707\n",
      "train loss:0.7076067252098507\n",
      "train loss:0.5463496560891471\n",
      "train loss:0.6635890465984527\n",
      "train loss:0.7098231802669792\n",
      "train loss:0.6100661567044161\n",
      "train loss:0.6263764718941831\n",
      "train loss:0.5089227742527955\n",
      "train loss:0.6168260512294953\n",
      "train loss:0.5622344581934438\n",
      "train loss:0.59029257944095\n",
      "train loss:0.5792413033440065\n",
      "train loss:0.9124245097846252\n",
      "train loss:0.619845981908208\n",
      "train loss:0.6514154031256869\n",
      "train loss:0.6436477397424661\n",
      "train loss:0.676335829174039\n",
      "train loss:0.6168758861940054\n",
      "train loss:0.6465536969854777\n",
      "train loss:0.5683412850918559\n",
      "train loss:0.6455161658440689\n",
      "train loss:0.8246183182204608\n",
      "train loss:0.5299806047047168\n",
      "train loss:0.638372492614415\n",
      "train loss:0.4694718849066363\n",
      "train loss:0.514742166560561\n",
      "train loss:0.6628407487142661\n",
      "train loss:0.7543610099672736\n",
      "train loss:0.5709750232957457\n",
      "train loss:0.579991637596414\n",
      "train loss:0.6095296800344331\n",
      "train loss:0.4393486286905046\n",
      "train loss:0.6504074931817008\n",
      "train loss:0.5640741344462769\n",
      "train loss:0.59883634290996\n",
      "train loss:0.6811317072739824\n",
      "train loss:0.7420860464435409\n",
      "train loss:0.7060354689744577\n",
      "train loss:0.7872343352793126\n",
      "train loss:0.7394252378205808\n",
      "train loss:0.6567901159054286\n",
      "train loss:0.6693226174893024\n",
      "train loss:0.800398474850549\n",
      "train loss:0.5873318078473634\n",
      "train loss:0.5592723645839306\n",
      "train loss:0.6676733238240152\n",
      "train loss:0.6390087591566943\n",
      "train loss:0.7500990085363318\n",
      "train loss:0.9059380199438262\n",
      "train loss:0.5435116548173842\n",
      "train loss:0.7417933699342217\n",
      "train loss:0.7257636632115841\n",
      "train loss:0.553209907103415\n",
      "train loss:0.6073309736977017\n",
      "train loss:0.570672116965915\n",
      "train loss:0.5806720958099977\n",
      "train loss:0.6289160255931902\n",
      "train loss:0.7607296844263238\n",
      "train loss:0.6307637353096532\n",
      "train loss:0.603300126820431\n",
      "train loss:0.7444967280022317\n",
      "train loss:0.6405464863914194\n",
      "train loss:0.701765977063389\n",
      "train loss:0.6607721958806096\n",
      "train loss:0.5997975028338288\n",
      "train loss:0.6382531349692971\n",
      "train loss:0.580109999221578\n",
      "train loss:0.5849541473823081\n",
      "train loss:0.7543085388926554\n",
      "train loss:0.712844425326859\n",
      "train loss:0.6412531093734987\n",
      "train loss:0.6339690308773829\n",
      "train loss:0.6341839980866376\n",
      "train loss:0.6394403027065602\n",
      "train loss:0.575363852033823\n",
      "train loss:0.679953058919373\n",
      "train loss:0.7769824722522849\n",
      "train loss:0.592033442213406\n",
      "train loss:0.5113127261344418\n",
      "train loss:0.5556465616192505\n",
      "train loss:0.563904522484262\n",
      "train loss:0.5170199980203997\n",
      "train loss:0.5647047310107732\n",
      "train loss:0.7895421071013226\n",
      "train loss:0.5405916610882298\n",
      "train loss:0.6366720717012645\n",
      "train loss:0.627514512113261\n",
      "train loss:0.5547234454883424\n",
      "train loss:0.5718193269471324\n",
      "train loss:0.5653309842349582\n",
      "train loss:0.5828766247700995\n",
      "train loss:0.5472405772891568\n",
      "train loss:0.6339123761778069\n",
      "train loss:0.5857994094234932\n",
      "train loss:0.5992101985918264\n",
      "train loss:0.5368614823761834\n",
      "train loss:0.745770522345242\n",
      "train loss:0.5594247803681822\n",
      "train loss:0.5890143903493398\n",
      "train loss:0.561270679177015\n",
      "train loss:0.462070506196937\n",
      "train loss:0.5326271125434967\n",
      "train loss:0.6195188206628607\n",
      "train loss:0.5180980474384898\n",
      "train loss:0.5805171344441827\n",
      "train loss:0.6558316132094062\n",
      "train loss:0.6490367401627927\n",
      "train loss:0.5365421987463909\n",
      "train loss:0.5681689463651661\n",
      "train loss:0.4704048000081356\n",
      "train loss:0.6808554334795316\n",
      "train loss:0.6189193813531106\n",
      "train loss:0.575335855530107\n",
      "train loss:0.4826114455824008\n",
      "train loss:0.6519561436066723\n",
      "train loss:0.5109838841592648\n",
      "train loss:0.6200343315894145\n",
      "train loss:0.5142045605744638\n",
      "train loss:0.7546905495100401\n",
      "train loss:0.636625290444484\n",
      "train loss:0.5169308846515619\n",
      "train loss:0.5768313081302386\n",
      "train loss:0.6664806823844655\n",
      "train loss:0.6531464868507376\n",
      "train loss:0.6095011869057292\n",
      "train loss:0.5570491218816195\n",
      "train loss:0.5901855166995617\n",
      "train loss:0.7873676805617263\n",
      "train loss:0.6128451899739682\n",
      "train loss:0.7506791890308488\n",
      "train loss:0.5462040207414078\n",
      "train loss:0.6930423436962999\n",
      "train loss:0.8513046818334412\n",
      "train loss:0.6170135177663155\n",
      "train loss:0.6257159626310579\n",
      "train loss:0.6379292707164627\n",
      "train loss:0.6731742745037296\n",
      "train loss:0.6470946778031771\n",
      "train loss:0.6128508974631722\n",
      "train loss:0.5570056161365436\n",
      "train loss:0.5747407857795307\n",
      "train loss:0.6647484535017308\n",
      "train loss:0.7032686397477452\n",
      "train loss:0.6691765336167353\n",
      "train loss:0.7454048905317407\n",
      "train loss:0.6689463766354049\n",
      "train loss:0.6484458692277311\n",
      "train loss:0.5982850648860526\n",
      "train loss:0.5786978676738622\n",
      "train loss:0.5543929047180918\n",
      "train loss:0.7136612947552653\n",
      "train loss:0.5747011457185782\n",
      "train loss:0.7334849664478292\n",
      "train loss:0.5610052760015061\n",
      "train loss:0.59169940884792\n",
      "train loss:0.5787910184721873\n",
      "train loss:0.5780582038646638\n",
      "train loss:0.4540471080012203\n",
      "train loss:0.5688052316158605\n",
      "train loss:0.447866069730544\n",
      "train loss:0.5575541386657615\n",
      "train loss:0.6502854241994056\n",
      "train loss:0.6291500678105844\n",
      "train loss:0.646028570468927\n",
      "train loss:0.6288762214739484\n",
      "train loss:0.5397161388540214\n",
      "train loss:0.6260846539791872\n",
      "train loss:0.5671212451711487\n",
      "train loss:0.5654753363668452\n",
      "train loss:0.659064808898861\n",
      "train loss:0.6982624717207438\n",
      "train loss:0.7187469024600063\n",
      "train loss:0.6487504890947551\n",
      "train loss:0.6026956026319703\n",
      "train loss:0.6157093804562129\n",
      "train loss:0.5718528475277375\n",
      "train loss:0.6993379984794426\n",
      "train loss:0.5341532175219429\n",
      "train loss:0.5136096860032708\n",
      "train loss:0.5232675495910806\n",
      "train loss:0.5334687885716323\n",
      "train loss:0.6547181442290296\n",
      "=== epoch:6, train acc:0.726, test acc:0.733 ===\n",
      "train loss:0.8229860280826102\n",
      "train loss:0.5009057122008389\n",
      "train loss:0.6042883368413265\n",
      "train loss:0.6718080873977946\n",
      "train loss:0.706519925218163\n",
      "train loss:0.8924675696124724\n",
      "train loss:0.6077919590072397\n",
      "train loss:0.7150710252808526\n",
      "train loss:0.7643374250702502\n",
      "train loss:0.5468143278908493\n",
      "train loss:0.6217707789209603\n",
      "train loss:0.6210868246100236\n",
      "train loss:0.6355150616930572\n",
      "train loss:0.6895813665231855\n",
      "train loss:0.644770210975075\n",
      "train loss:0.6108979269506762\n",
      "train loss:0.7835992409936702\n",
      "train loss:0.5359457690502571\n",
      "train loss:0.5237521901664641\n",
      "train loss:0.517613210350548\n",
      "train loss:0.565901972376909\n",
      "train loss:0.6631018612862689\n",
      "train loss:0.6477715661316827\n",
      "train loss:0.5449346536937355\n",
      "train loss:0.5651144822903434\n",
      "train loss:0.5313965374182976\n",
      "train loss:0.6260152297566113\n",
      "train loss:0.6047250561417115\n",
      "train loss:0.6124801406944647\n",
      "train loss:0.7871549209277086\n",
      "train loss:0.5073258810920664\n",
      "train loss:0.6098958797568872\n",
      "train loss:0.5959106580636463\n",
      "train loss:0.5975948906610804\n",
      "train loss:0.7602521007915051\n",
      "train loss:0.6049940901719641\n",
      "train loss:0.6291216107702068\n",
      "train loss:0.6774971131171635\n",
      "train loss:0.6150258820189008\n",
      "train loss:0.5071133774436395\n",
      "train loss:0.6391443417281697\n",
      "train loss:0.6956948812070967\n",
      "train loss:0.5751489336387994\n",
      "train loss:0.5449378505031411\n",
      "train loss:0.5989359762932436\n",
      "train loss:0.5345924514486048\n",
      "train loss:0.8736126487017783\n",
      "train loss:0.5382555276658789\n",
      "train loss:0.532815246465433\n",
      "train loss:0.5789438559142145\n",
      "train loss:0.5059608645389909\n",
      "train loss:0.5905563179312238\n",
      "train loss:0.508390184290352\n",
      "train loss:0.5702424357473047\n",
      "train loss:0.7388299594914126\n",
      "train loss:0.5171181374626883\n",
      "train loss:0.5018945514581984\n",
      "train loss:0.6086734259159214\n",
      "train loss:0.582700838155813\n",
      "train loss:0.5825921856404465\n",
      "train loss:0.4915483799882503\n",
      "train loss:0.5284142775804969\n",
      "train loss:0.4301391858811033\n",
      "train loss:0.7887026932536826\n",
      "train loss:0.6451878967695667\n",
      "train loss:0.5811817406706318\n",
      "train loss:0.6599392424458209\n",
      "train loss:0.47509655081029406\n",
      "train loss:0.6443440987357693\n",
      "train loss:0.7098330648552801\n",
      "train loss:0.5535324305034502\n",
      "train loss:0.5175926826264861\n",
      "train loss:0.6392585151586162\n",
      "train loss:0.4990941168021044\n",
      "train loss:0.5409132631108358\n",
      "train loss:0.5898402672407852\n",
      "train loss:0.5843336358033788\n",
      "train loss:0.7323935918487123\n",
      "train loss:0.6793300872579231\n",
      "train loss:0.5599544622532154\n",
      "train loss:0.6314266391352461\n",
      "train loss:0.5404366267153151\n",
      "train loss:0.5759023188887681\n",
      "train loss:0.7054327312426575\n",
      "train loss:0.6760312138740708\n",
      "train loss:0.44113451483135285\n",
      "train loss:0.509399477266501\n",
      "train loss:0.7094733994750259\n",
      "train loss:0.6122941956157101\n",
      "train loss:0.6461356415509348\n",
      "train loss:0.6604094383485801\n",
      "train loss:0.5210405630601432\n",
      "train loss:0.6720945017082665\n",
      "train loss:0.636673299580166\n",
      "train loss:0.5706741891035995\n",
      "train loss:0.6766368843499722\n",
      "train loss:0.472450168033077\n",
      "train loss:0.5969605652793443\n",
      "train loss:0.6415873722452398\n",
      "train loss:0.5736696102647562\n",
      "train loss:0.6537863914557088\n",
      "train loss:0.6319147966241657\n",
      "train loss:0.48810705331992515\n",
      "train loss:0.5519565521389899\n",
      "train loss:0.6375577863942204\n",
      "train loss:0.6420738082818196\n",
      "train loss:0.6012385208764096\n",
      "train loss:0.6913552277094329\n",
      "train loss:0.5474526421719169\n",
      "train loss:0.76145541221655\n",
      "train loss:0.587653682018362\n",
      "train loss:0.546108674315056\n",
      "train loss:0.5736166515063065\n",
      "train loss:0.5024723691962434\n",
      "train loss:0.5208233559325212\n",
      "train loss:0.730513946566135\n",
      "train loss:0.5583512728470698\n",
      "train loss:0.5714577182162448\n",
      "train loss:0.6888441193776904\n",
      "train loss:0.6051288068094152\n",
      "train loss:0.561137098633603\n",
      "train loss:0.6165266062031439\n",
      "train loss:0.5500741330876323\n",
      "train loss:0.47002657829504246\n",
      "train loss:0.6293029674115956\n",
      "train loss:0.5055379286634832\n",
      "train loss:0.5768559835675222\n",
      "train loss:0.6772718730508409\n",
      "train loss:0.7331855388854552\n",
      "train loss:0.5286561496291075\n",
      "train loss:0.5223159949920897\n",
      "train loss:0.6729396286053638\n",
      "train loss:0.4875003524912036\n",
      "train loss:0.49383125022456126\n",
      "train loss:0.4581174958168323\n",
      "train loss:0.6854084637133597\n",
      "train loss:0.4741734757180809\n",
      "train loss:0.5392203115842261\n",
      "train loss:0.5990482076168744\n",
      "train loss:0.5562168178057241\n",
      "train loss:0.5172684340039622\n",
      "train loss:0.6570682060953904\n",
      "train loss:0.7786382037772998\n",
      "train loss:0.6055497307827488\n",
      "train loss:0.518943586277463\n",
      "train loss:0.5883449787662263\n",
      "train loss:0.7038099195713308\n",
      "train loss:0.6810350686806501\n",
      "train loss:0.45432243224266416\n",
      "train loss:0.4495661563468859\n",
      "train loss:0.49990526987130224\n",
      "train loss:0.554155775788383\n",
      "train loss:0.4666664453950095\n",
      "train loss:0.528169075819759\n",
      "train loss:0.5404451313386321\n",
      "train loss:0.608708549296566\n",
      "train loss:0.5914980774735146\n",
      "train loss:0.5644254744107332\n",
      "train loss:0.5365009793865864\n",
      "train loss:0.5777110368347638\n",
      "train loss:0.4903670892849399\n",
      "train loss:0.676784528570827\n",
      "train loss:0.5753453718626298\n",
      "train loss:0.7117726959261019\n",
      "train loss:0.6086841483775257\n",
      "train loss:0.5632431117050609\n",
      "train loss:0.5384117828993235\n",
      "train loss:0.7091636168078774\n",
      "train loss:0.614357280648511\n",
      "train loss:0.5323090213418176\n",
      "train loss:0.6452484859992599\n",
      "train loss:0.4925395633187005\n",
      "train loss:0.62036395584202\n",
      "train loss:0.656958728376222\n",
      "train loss:0.47057046372451466\n",
      "train loss:0.5662098338000333\n",
      "train loss:0.5214056107486654\n",
      "train loss:0.580599505897978\n",
      "train loss:0.4876585212063281\n",
      "train loss:0.6280394578826778\n",
      "train loss:0.6036259222721511\n",
      "train loss:0.7577795827255409\n",
      "train loss:0.5254723795537619\n",
      "train loss:0.4045745257638626\n",
      "train loss:0.5805610456332969\n",
      "train loss:0.6144793235703261\n",
      "train loss:0.5673880583646695\n",
      "train loss:0.46418950728568126\n",
      "train loss:0.6461853434705381\n",
      "train loss:0.5680490443883485\n",
      "train loss:0.6266237861421382\n",
      "train loss:0.5820821309931098\n",
      "train loss:0.5811518466250244\n",
      "train loss:0.652558570177066\n",
      "train loss:0.5346673277519416\n",
      "train loss:0.5288632258247434\n",
      "train loss:0.5850427821674689\n",
      "train loss:0.7163022446309764\n",
      "train loss:0.5899815054449145\n",
      "train loss:0.6572253153507538\n",
      "train loss:0.6676313478129471\n",
      "train loss:0.5898645842158946\n",
      "train loss:0.5127302064814899\n",
      "train loss:0.5694438867388416\n",
      "train loss:0.7781155559087278\n",
      "train loss:0.6650405368539745\n",
      "train loss:0.6983283268264358\n",
      "train loss:0.5433541654879184\n",
      "train loss:0.8432797685196879\n",
      "train loss:0.621191169295678\n",
      "train loss:0.6457677509701699\n",
      "train loss:0.6440783004965508\n",
      "train loss:0.6156188851630002\n",
      "train loss:0.5272312103352602\n",
      "train loss:0.5525737590528716\n",
      "train loss:0.6534396562712488\n",
      "train loss:0.628704652938262\n",
      "train loss:0.4104968428682345\n",
      "train loss:0.502481541800529\n",
      "train loss:0.5641253649296015\n",
      "train loss:0.6358724284566799\n",
      "train loss:0.5410088847094142\n",
      "train loss:0.6770036996571177\n",
      "train loss:0.6584990352137189\n",
      "train loss:0.6181713161974481\n",
      "train loss:0.87652103969688\n",
      "train loss:0.5234100430969266\n",
      "train loss:0.626581840929111\n",
      "train loss:0.6297760430683936\n",
      "train loss:0.5024512792834822\n",
      "train loss:0.5075984331171612\n",
      "train loss:0.687295179040339\n",
      "train loss:0.600372082117096\n",
      "train loss:0.604333865746806\n",
      "train loss:0.5714201452823299\n",
      "train loss:0.4415665479595873\n",
      "train loss:0.4054417374804513\n",
      "train loss:0.5634167299008004\n",
      "train loss:0.5564114125059557\n",
      "train loss:0.832070726698406\n",
      "train loss:0.5869097026145753\n",
      "train loss:0.6435391156813125\n",
      "train loss:0.5941374304855351\n",
      "train loss:0.599215584667554\n",
      "train loss:0.5404245676911399\n",
      "train loss:0.7658084477591224\n",
      "train loss:0.4112711419238701\n",
      "train loss:0.6023004071826719\n",
      "train loss:0.5261108787277637\n",
      "train loss:0.46551753364840825\n",
      "train loss:0.5412665456707622\n",
      "train loss:0.5856797489224399\n",
      "train loss:0.7180160008381752\n",
      "train loss:0.6368747317950088\n",
      "train loss:0.5605784320135289\n",
      "train loss:0.7877960651331445\n",
      "train loss:0.6216988924078545\n",
      "train loss:0.7493783428087617\n",
      "train loss:0.6013568840622865\n",
      "train loss:0.5544726101673461\n",
      "train loss:0.5045046053630293\n",
      "train loss:0.7694583067782039\n",
      "train loss:0.47572358678820764\n",
      "train loss:0.48333279568228454\n",
      "train loss:0.723853659696666\n",
      "train loss:0.6290830132958475\n",
      "train loss:0.6015322961591519\n",
      "train loss:0.5896951514238977\n",
      "train loss:0.5649516835494777\n",
      "train loss:0.5472194239258275\n",
      "train loss:0.6175750936269336\n",
      "train loss:0.7946863854897721\n",
      "train loss:0.5063368964367727\n",
      "train loss:0.5981394127212054\n",
      "train loss:0.6110165080175608\n",
      "train loss:0.49324061138097286\n",
      "train loss:0.6397589860896626\n",
      "train loss:0.5846899059364351\n",
      "train loss:0.5057760164653423\n",
      "train loss:0.585994771644648\n",
      "train loss:0.5278565649505996\n",
      "train loss:0.5736174814422625\n",
      "train loss:0.5564030391726827\n",
      "train loss:0.657467578719861\n",
      "train loss:0.5665202495551648\n",
      "train loss:0.6377172542673828\n",
      "train loss:0.48940800649226834\n",
      "train loss:0.6713015290901718\n",
      "train loss:0.6779819593890841\n",
      "train loss:0.5209437050740141\n",
      "train loss:0.47908272029375765\n",
      "train loss:0.5488880196157857\n",
      "train loss:0.5416491131847674\n",
      "train loss:0.5725491004002812\n",
      "train loss:0.6665350787911533\n",
      "train loss:0.6927804312260153\n",
      "train loss:0.5805849338319622\n",
      "train loss:0.5771438482841107\n",
      "train loss:0.494078128355245\n",
      "train loss:0.594006708338878\n",
      "train loss:0.6375177709944934\n",
      "train loss:0.5293320440946822\n",
      "train loss:0.5101393167599544\n",
      "train loss:0.6164699327978564\n",
      "train loss:0.4971729690916485\n",
      "train loss:0.5900934506123852\n",
      "train loss:0.5695862614818731\n",
      "train loss:0.5686667879976852\n",
      "train loss:0.5377474008052736\n",
      "train loss:0.5352515891355083\n",
      "train loss:0.6059078245072924\n",
      "train loss:0.558501017237489\n",
      "train loss:0.5867310934432528\n",
      "train loss:0.548284459908328\n",
      "train loss:0.5242307668943027\n",
      "train loss:0.5109775077720662\n",
      "train loss:0.6012129976514665\n",
      "train loss:0.7041086656796697\n",
      "train loss:0.5556922494441796\n",
      "train loss:0.6724899473900632\n",
      "train loss:0.5219751167730258\n",
      "train loss:0.7349477120562307\n",
      "train loss:0.49054545978376185\n",
      "train loss:0.6887021698451828\n",
      "train loss:0.7146347834185376\n",
      "train loss:0.45926687271640837\n",
      "train loss:0.7257324461514492\n",
      "train loss:0.6671369696948365\n",
      "train loss:0.5508961919508486\n",
      "train loss:0.6017185237355804\n",
      "train loss:0.6472403895585227\n",
      "train loss:0.514323279152009\n",
      "train loss:0.6685362275159801\n",
      "train loss:0.5471106616475436\n",
      "train loss:0.614615404499014\n",
      "train loss:0.531891266099666\n",
      "train loss:0.7004085405466519\n",
      "train loss:0.5271683109526596\n",
      "train loss:0.5674349857763695\n",
      "train loss:0.5540182368335348\n",
      "train loss:0.4321831222621279\n",
      "train loss:0.5705536416248522\n",
      "train loss:0.5283656035207396\n",
      "train loss:0.5353808936848472\n",
      "train loss:0.7554280768465409\n",
      "train loss:0.5839609364361268\n",
      "train loss:0.47227672887898975\n",
      "train loss:0.6573769261610326\n",
      "train loss:0.5616254452286147\n",
      "train loss:0.5629989029725442\n",
      "train loss:0.7149724375645363\n",
      "train loss:0.5055182887596523\n",
      "train loss:0.5992070886301343\n",
      "train loss:0.5752796239251304\n",
      "train loss:0.5498987051208103\n",
      "train loss:0.6537455281139395\n",
      "train loss:0.6619766862987353\n",
      "train loss:0.46181118866557824\n",
      "train loss:0.49524892649414576\n",
      "train loss:0.5283102600421816\n",
      "train loss:0.5506182668019902\n",
      "train loss:0.6993946958368141\n",
      "train loss:0.6278947310763183\n",
      "train loss:0.4930854901532324\n",
      "train loss:0.5807182581274603\n",
      "train loss:0.48351995422836347\n",
      "train loss:0.4978342310587865\n",
      "train loss:0.4131562341831719\n",
      "train loss:0.47434057025981397\n",
      "train loss:0.7047205810726702\n",
      "train loss:0.5095287584430929\n",
      "train loss:0.6156909451117996\n",
      "train loss:0.6816345866190394\n",
      "train loss:0.5971251571542103\n",
      "train loss:0.6579834243975172\n",
      "train loss:0.6388014535252834\n",
      "train loss:0.6098538362578473\n",
      "train loss:0.5621145940986496\n",
      "train loss:0.6020077178798444\n",
      "train loss:0.5103137119098817\n",
      "train loss:0.5579865019038172\n",
      "train loss:0.5763376298680766\n",
      "train loss:0.47676035576643067\n",
      "train loss:0.5733245575019449\n",
      "train loss:0.6118450938467649\n",
      "train loss:0.7251981286508556\n",
      "train loss:0.6271119836348502\n",
      "train loss:0.5364889361312964\n",
      "train loss:0.4934461741510496\n",
      "train loss:0.5328817676743666\n",
      "train loss:0.4761045317313811\n",
      "train loss:0.59298416214919\n",
      "train loss:0.5956539752656516\n",
      "train loss:0.5104691334490301\n",
      "train loss:0.5754079331021233\n",
      "train loss:0.5876173305239847\n",
      "train loss:0.5993876541427215\n",
      "train loss:0.6347562851401743\n",
      "train loss:0.5089642929537772\n",
      "train loss:0.5459199180967529\n",
      "train loss:0.5329056526664331\n",
      "train loss:0.5972541470414463\n",
      "train loss:0.518419525631101\n",
      "train loss:0.5579172066818417\n",
      "train loss:0.4755845840092405\n",
      "train loss:0.5373204009288216\n",
      "train loss:0.6194537063622524\n",
      "train loss:0.5577386604575237\n",
      "train loss:0.7008493176539479\n",
      "train loss:0.4705757813270588\n",
      "train loss:0.7887280541571902\n",
      "train loss:0.8381557035798677\n",
      "train loss:0.5132482867209291\n",
      "train loss:0.5618962899600384\n",
      "train loss:0.5682742285279855\n",
      "train loss:0.6526289128446576\n",
      "train loss:0.6842766464557897\n",
      "train loss:0.5361848759670447\n",
      "train loss:0.6090798116335288\n",
      "train loss:0.6168584524316394\n",
      "train loss:0.4673721073619836\n",
      "train loss:0.47592113842380995\n",
      "train loss:0.6690652058684026\n",
      "train loss:0.659726944371993\n",
      "train loss:0.7344452130855622\n",
      "train loss:0.5696993149539583\n",
      "train loss:0.7064866458740159\n",
      "train loss:0.5596253492174731\n",
      "train loss:0.6058836966020487\n",
      "train loss:0.574854288583495\n",
      "train loss:0.5695308415716002\n",
      "train loss:0.5106254618433758\n",
      "train loss:0.4855279693971488\n",
      "train loss:0.4767358718068909\n",
      "train loss:0.6210768364858137\n",
      "train loss:0.5886605611820507\n",
      "train loss:0.5460044361877572\n",
      "train loss:0.6519567462327305\n",
      "train loss:0.5185161559776773\n",
      "train loss:0.44532387480525676\n",
      "train loss:0.6151629443020465\n",
      "train loss:0.5852713219789293\n",
      "train loss:0.5168898088484001\n",
      "train loss:0.4548829627711012\n",
      "train loss:0.5695948541998935\n",
      "train loss:0.5069301904593617\n",
      "train loss:0.5064538744400712\n",
      "train loss:0.40480664029772645\n",
      "train loss:0.5391479222910561\n",
      "train loss:0.6461942316917362\n",
      "train loss:0.4439496161936707\n",
      "train loss:0.536119381204752\n",
      "train loss:0.6694473509401548\n",
      "train loss:0.5697183642393091\n",
      "train loss:0.5392560459055912\n",
      "train loss:0.527921607855362\n",
      "train loss:0.6240892587372442\n",
      "train loss:0.5574495081552039\n",
      "train loss:0.45175051577694864\n",
      "train loss:0.8278898669019238\n",
      "train loss:0.7444561579774494\n",
      "train loss:0.5461803757191903\n",
      "train loss:0.6194205728942633\n",
      "train loss:0.4998007517253271\n",
      "train loss:0.5368079565760354\n",
      "train loss:0.5848199663656294\n",
      "train loss:0.5166088413099879\n",
      "train loss:0.6051942364045979\n",
      "train loss:0.5771463101441575\n",
      "train loss:0.4593406592877377\n",
      "train loss:0.4673180241908167\n",
      "train loss:0.5077992903875633\n",
      "train loss:0.5014157131756029\n",
      "train loss:0.71595179744297\n",
      "train loss:0.5773020986638653\n",
      "train loss:0.4463917596656885\n",
      "train loss:0.5761137090372095\n",
      "train loss:0.5879230085642942\n",
      "train loss:0.5583228412811941\n",
      "train loss:0.4497810270663478\n",
      "train loss:0.5860286678924838\n",
      "train loss:0.6164206264851386\n",
      "train loss:0.4505076800347004\n",
      "train loss:0.4790334207943468\n",
      "train loss:0.542043469060005\n",
      "train loss:0.5320676360576977\n",
      "train loss:0.6287623819441671\n",
      "train loss:0.4533294098522657\n",
      "train loss:0.6478013898234507\n",
      "train loss:0.6452286975894254\n",
      "train loss:0.5811199570117976\n",
      "train loss:0.45323977831893\n",
      "train loss:0.5954636428668963\n",
      "train loss:0.5239185234138982\n",
      "train loss:0.5697036825073456\n",
      "train loss:0.624945202091698\n",
      "train loss:0.5800187259066507\n",
      "train loss:0.4877563445746075\n",
      "train loss:0.4969250437842272\n",
      "train loss:0.5435270551290757\n",
      "train loss:0.4723978629961075\n",
      "train loss:0.4754549405792183\n",
      "train loss:0.44296830070359183\n",
      "train loss:0.5390726848614047\n",
      "train loss:0.5121090056860897\n",
      "train loss:0.529109324907068\n",
      "train loss:0.4246779645765837\n",
      "train loss:0.5617844602542916\n",
      "train loss:0.5820774279480156\n",
      "train loss:0.6722293502728559\n",
      "train loss:0.6506298751405984\n",
      "train loss:0.6890864354492808\n",
      "train loss:0.6514215011470099\n",
      "train loss:0.3876971804622605\n",
      "train loss:0.5941350208963878\n",
      "train loss:0.4490986480520106\n",
      "train loss:0.4517111866771399\n",
      "train loss:0.6125154550488445\n",
      "train loss:0.5111443256875006\n",
      "train loss:0.40805868691816827\n",
      "train loss:0.525204926882975\n",
      "train loss:0.4823470852240299\n",
      "train loss:0.5124716086205457\n",
      "train loss:0.6416001042968423\n",
      "train loss:0.5383072114369375\n",
      "train loss:0.5199778167132209\n",
      "train loss:0.5132833621941365\n",
      "train loss:0.6197714127926767\n",
      "train loss:0.5353529846296667\n",
      "train loss:0.6058708669989815\n",
      "train loss:0.6895598755551936\n",
      "train loss:0.6014680200568912\n",
      "train loss:0.5334140458433773\n",
      "train loss:0.6091784873302383\n",
      "train loss:0.5643790544543521\n",
      "train loss:0.5758047207089882\n",
      "train loss:0.573102314451249\n",
      "train loss:0.5376384815084255\n",
      "train loss:0.551093579619064\n",
      "train loss:0.4915379044327803\n",
      "train loss:0.685837381430435\n",
      "train loss:0.5851479189942321\n",
      "train loss:0.4627571058178427\n",
      "train loss:0.5305548820158853\n",
      "train loss:0.5468168986768235\n",
      "train loss:0.5339486714360451\n",
      "train loss:0.5086522526201286\n",
      "train loss:0.5164308703624834\n",
      "train loss:0.6988457143909135\n",
      "train loss:0.5975606774766233\n",
      "train loss:0.5208895895067277\n",
      "train loss:0.6668126602027953\n",
      "train loss:0.5164154569853155\n",
      "train loss:0.6395093769608295\n",
      "train loss:0.4680285524955861\n",
      "train loss:0.5403751591727343\n",
      "train loss:0.42455110268985374\n",
      "train loss:0.5020435620390648\n",
      "train loss:0.7863041105074474\n",
      "train loss:0.6682142011244452\n",
      "train loss:0.43888920284536925\n",
      "train loss:0.48110022628331245\n",
      "train loss:0.6688833191401904\n",
      "train loss:0.570743970007921\n",
      "train loss:0.5879740766526246\n",
      "train loss:0.5668148678302469\n",
      "train loss:0.5653032416513637\n",
      "train loss:0.5569700959385561\n",
      "train loss:0.48606545755467884\n",
      "train loss:0.40870189016897784\n",
      "train loss:0.5253508733809489\n",
      "train loss:0.49033679262143903\n",
      "train loss:0.5039320307363587\n",
      "train loss:0.40302333886256636\n",
      "train loss:0.5555851009088671\n",
      "train loss:0.39425740147841437\n",
      "train loss:0.5065542738251613\n",
      "train loss:0.5089498730012161\n",
      "train loss:0.5677542287343245\n",
      "train loss:0.5020872541570511\n",
      "train loss:0.43856212972033404\n",
      "train loss:0.5738238725036688\n",
      "train loss:0.5889754031553405\n",
      "train loss:0.601034319979934\n",
      "train loss:0.44201673518456663\n",
      "train loss:0.5431894650312824\n",
      "train loss:0.665249364592149\n",
      "train loss:0.520067760483609\n",
      "train loss:0.5882767616189739\n",
      "train loss:0.5737995864367861\n",
      "train loss:0.626039014987948\n",
      "train loss:0.45687342337352155\n",
      "train loss:0.6245403892372522\n",
      "train loss:0.5197375173782786\n",
      "train loss:0.6098707050220614\n",
      "train loss:0.5335494748859815\n",
      "train loss:0.4863210720358462\n",
      "train loss:0.5707585315538879\n",
      "train loss:0.46870330570409535\n",
      "train loss:0.44730968308345664\n",
      "=== epoch:7, train acc:0.734, test acc:0.743 ===\n",
      "train loss:0.6655085597166756\n",
      "train loss:0.530944694359581\n",
      "train loss:0.575368311573907\n",
      "train loss:0.5509758323531605\n",
      "train loss:0.48009252428665855\n",
      "train loss:0.6403041140275463\n",
      "train loss:0.6459919173666847\n",
      "train loss:0.524122787787798\n",
      "train loss:0.7158598272078038\n",
      "train loss:0.42369684391406637\n",
      "train loss:0.5969810090764421\n",
      "train loss:0.4738849989121928\n",
      "train loss:0.7328477208062218\n",
      "train loss:0.7584686161276321\n",
      "train loss:0.6777459419370203\n",
      "train loss:0.4746446634510816\n",
      "train loss:0.5861688864031351\n",
      "train loss:0.5830785671316827\n",
      "train loss:0.4718223064051913\n",
      "train loss:0.5401167799136598\n",
      "train loss:0.4008617995641542\n",
      "train loss:0.44924965746539547\n",
      "train loss:0.6254734638501488\n",
      "train loss:0.5785106580100106\n",
      "train loss:0.4768804736307318\n",
      "train loss:0.630372198180082\n",
      "train loss:0.4877319725053113\n",
      "train loss:0.5790521975329779\n",
      "train loss:0.586802518507538\n",
      "train loss:0.5912563978658281\n",
      "train loss:0.4726226007124965\n",
      "train loss:0.5700840716126405\n",
      "train loss:0.5261684467821357\n",
      "train loss:0.4864002837671336\n",
      "train loss:0.6090200132780502\n",
      "train loss:0.585588955913469\n",
      "train loss:0.5644324135465504\n",
      "train loss:0.508174530196054\n",
      "train loss:0.5215381810094775\n",
      "train loss:0.44654757605668416\n",
      "train loss:0.6462723864240144\n",
      "train loss:0.5431050649318493\n",
      "train loss:0.5364723349448907\n",
      "train loss:0.5591026331687885\n",
      "train loss:0.49717586641010103\n",
      "train loss:0.5515042154080141\n",
      "train loss:0.4732893481525873\n",
      "train loss:0.5859817623772609\n",
      "train loss:0.45576526590042166\n",
      "train loss:0.5770570227012827\n",
      "train loss:0.7051572756411844\n",
      "train loss:0.49701847805466903\n",
      "train loss:0.4517526997516501\n",
      "train loss:0.5005384480361937\n",
      "train loss:0.4725670904357897\n",
      "train loss:0.514731245715097\n",
      "train loss:0.6222367511179339\n",
      "train loss:0.6257778461349455\n",
      "train loss:0.5949092726693676\n",
      "train loss:0.5571623054891163\n",
      "train loss:0.5580844063585965\n",
      "train loss:0.4211116772945116\n",
      "train loss:0.6026547776293613\n",
      "train loss:0.7326413358257099\n",
      "train loss:0.48533594734384494\n",
      "train loss:0.6056777197002482\n",
      "train loss:0.6546222069487965\n",
      "train loss:0.5487683840490486\n",
      "train loss:0.41573018916011556\n",
      "train loss:0.5091541810073544\n",
      "train loss:0.3720600988996554\n",
      "train loss:0.4967035679207872\n",
      "train loss:0.7627957009595073\n",
      "train loss:0.6055827382942437\n",
      "train loss:0.46611590984065354\n",
      "train loss:0.62167173489747\n",
      "train loss:0.7128570887611402\n",
      "train loss:0.4418695809431651\n",
      "train loss:0.5669286944807439\n",
      "train loss:0.5185495012175392\n",
      "train loss:0.5567215228477924\n",
      "train loss:0.552983160414333\n",
      "train loss:0.6465136016043672\n",
      "train loss:0.677435344199234\n",
      "train loss:0.5996914185538053\n",
      "train loss:0.5478887514028807\n",
      "train loss:0.532066196498792\n",
      "train loss:0.5405818745400373\n",
      "train loss:0.5210350845041158\n",
      "train loss:0.49405645573553053\n",
      "train loss:0.5506328674836065\n",
      "train loss:0.5380870200823694\n",
      "train loss:0.36711429442196675\n",
      "train loss:0.5965491873506208\n",
      "train loss:0.4646384249745236\n",
      "train loss:0.5059348982667461\n",
      "train loss:0.53419649033432\n",
      "train loss:0.5269694213529822\n",
      "train loss:0.4700039204248265\n",
      "train loss:0.5229645570406554\n",
      "train loss:0.5299933073467924\n",
      "train loss:0.5094378537777883\n",
      "train loss:0.5136335553957209\n",
      "train loss:0.589453016653333\n",
      "train loss:0.6399808590130122\n",
      "train loss:0.651943548312752\n",
      "train loss:0.6087818165629316\n",
      "train loss:0.46882029027287714\n",
      "train loss:0.46426794104612024\n",
      "train loss:0.5372354480540509\n",
      "train loss:0.6168731459221651\n",
      "train loss:0.660485299836679\n",
      "train loss:0.5993923353579926\n",
      "train loss:0.5744119845845134\n",
      "train loss:0.5648272990793296\n",
      "train loss:0.6194206283061582\n",
      "train loss:0.48149391782893536\n",
      "train loss:0.581013957484926\n",
      "train loss:0.5044065463795313\n",
      "train loss:0.5381076503320256\n",
      "train loss:0.5959806540542553\n",
      "train loss:0.4450217022963212\n",
      "train loss:0.4042607896904664\n",
      "train loss:0.5413051772292016\n",
      "train loss:0.565988421962712\n",
      "train loss:0.5564401759942643\n",
      "train loss:0.5743317444053588\n",
      "train loss:0.5717094054252532\n",
      "train loss:0.5685648074781702\n",
      "train loss:0.5589784862418383\n",
      "train loss:0.5736037872408476\n",
      "train loss:0.41416701320067856\n",
      "train loss:0.4859377000459248\n",
      "train loss:0.49116496379893376\n",
      "train loss:0.5692793674649437\n",
      "train loss:0.5607446407195614\n",
      "train loss:0.4456077794314017\n",
      "train loss:0.5594322708133934\n",
      "train loss:0.4917323389112573\n",
      "train loss:0.46621929836245635\n",
      "train loss:0.5126094504751273\n",
      "train loss:0.5066910504869178\n",
      "train loss:0.5205191468353673\n",
      "train loss:0.6278681789858751\n",
      "train loss:0.6191591092444314\n",
      "train loss:0.45043873514790567\n",
      "train loss:0.46648914595701074\n",
      "train loss:0.5950524078475293\n",
      "train loss:0.5280470234650622\n",
      "train loss:0.6056239544725495\n",
      "train loss:0.6465881362769273\n",
      "train loss:0.6379326586144144\n",
      "train loss:0.7441400808099998\n",
      "train loss:0.49145742405365406\n",
      "train loss:0.5258809304482718\n",
      "train loss:0.6550569077692883\n",
      "train loss:0.5742141778783233\n",
      "train loss:0.5460860277278264\n",
      "train loss:0.47971823086278886\n",
      "train loss:0.5596648037100789\n",
      "train loss:0.5613420700097165\n",
      "train loss:0.6269244863201128\n",
      "train loss:0.5473578698171636\n",
      "train loss:0.4772065006450555\n",
      "train loss:0.6125645440972706\n",
      "train loss:0.5676055604685406\n",
      "train loss:0.5353250553236449\n",
      "train loss:0.4808893502074488\n",
      "train loss:0.4536475801247186\n",
      "train loss:0.5906861172549217\n",
      "train loss:0.5045621567108783\n",
      "train loss:0.5487380244961199\n",
      "train loss:0.5752967052510707\n",
      "train loss:0.4836894780714043\n",
      "train loss:0.5407264287920891\n",
      "train loss:0.41808064288368113\n",
      "train loss:0.5793156090762569\n",
      "train loss:0.56318828349236\n",
      "train loss:0.43843342511536315\n",
      "train loss:0.6532931064559319\n",
      "train loss:0.6218590113359849\n",
      "train loss:0.4705685938449043\n",
      "train loss:0.526984302851051\n",
      "train loss:0.48749089560434905\n",
      "train loss:0.6129673451835952\n",
      "train loss:0.44521624746246397\n",
      "train loss:0.4951542200472371\n",
      "train loss:0.6071360109703626\n",
      "train loss:0.5091604768172615\n",
      "train loss:0.45117426912879594\n",
      "train loss:0.5286403747101344\n",
      "train loss:0.6915102774227353\n",
      "train loss:0.4820316927534767\n",
      "train loss:0.4333043945984874\n",
      "train loss:0.5091848707649049\n",
      "train loss:0.5791405770922158\n",
      "train loss:0.5302510600434284\n",
      "train loss:0.519759199844837\n",
      "train loss:0.5256382699408281\n",
      "train loss:0.6104442539459608\n",
      "train loss:0.46698587489892746\n",
      "train loss:0.4970264315816597\n",
      "train loss:0.46993573712057746\n",
      "train loss:0.5991786931433778\n",
      "train loss:0.479000939688856\n",
      "train loss:0.5353250107593869\n",
      "train loss:0.5180695265147136\n",
      "train loss:0.732318547828748\n",
      "train loss:0.5027664250639967\n",
      "train loss:0.6736586111487359\n",
      "train loss:0.6018483193270224\n",
      "train loss:0.5909268399027148\n",
      "train loss:0.5436211720371177\n",
      "train loss:0.7394446606778919\n",
      "train loss:0.4961270047483425\n",
      "train loss:0.5560571426166395\n",
      "train loss:0.6124977997828254\n",
      "train loss:0.5008069503172617\n",
      "train loss:0.508777845322727\n",
      "train loss:0.45214078199182495\n",
      "train loss:0.6414686221101582\n",
      "train loss:0.5511536931618857\n",
      "train loss:0.503002149690095\n",
      "train loss:0.6315519609385408\n",
      "train loss:0.4108387752596529\n",
      "train loss:0.5544043816359623\n",
      "train loss:0.5606532477758835\n",
      "train loss:0.44868647481482865\n",
      "train loss:0.5314436189574768\n",
      "train loss:0.5697990257764354\n",
      "train loss:0.5672208936726983\n",
      "train loss:0.4829264487303977\n",
      "train loss:0.5098175030949699\n",
      "train loss:0.5739937091157422\n",
      "train loss:0.4236126671752418\n",
      "train loss:0.5249462387877967\n",
      "train loss:0.5494317824535777\n",
      "train loss:0.7060928307147339\n",
      "train loss:0.5923390388084301\n",
      "train loss:0.33789824920145983\n",
      "train loss:0.4016090828089681\n",
      "train loss:0.6064928265061874\n",
      "train loss:0.5999649466103821\n",
      "train loss:0.5400847990223768\n",
      "train loss:0.5615707332449742\n",
      "train loss:0.5609532930528637\n",
      "train loss:0.4672649403826469\n",
      "train loss:0.5324787096762388\n",
      "train loss:0.4812167874730776\n",
      "train loss:0.5502602170165333\n",
      "train loss:0.7076067055133453\n",
      "train loss:0.5364909068067991\n",
      "train loss:0.42770370574581223\n",
      "train loss:0.3999279943779153\n",
      "train loss:0.48659532219792195\n",
      "train loss:0.4446284052953489\n",
      "train loss:0.5561222929847375\n",
      "train loss:0.4626662148292731\n",
      "train loss:0.7102732723037297\n",
      "train loss:0.536691591474707\n",
      "train loss:0.553754329071605\n",
      "train loss:0.5658561274197274\n",
      "train loss:0.5505258913110519\n",
      "train loss:0.6366854819976938\n",
      "train loss:0.5899843705694545\n",
      "train loss:0.6154701891574663\n",
      "train loss:0.5252740697050108\n",
      "train loss:0.7515505174194845\n",
      "train loss:0.6555006004038572\n",
      "train loss:0.5413560648167386\n",
      "train loss:0.5579770388953872\n",
      "train loss:0.6009629236208964\n",
      "train loss:0.5313960354247035\n",
      "train loss:0.44631798475544365\n",
      "train loss:0.4350946083594667\n",
      "train loss:0.5487731686821533\n",
      "train loss:0.4575022985077424\n",
      "train loss:0.6783157197392733\n",
      "train loss:0.5241346608736581\n",
      "train loss:0.5064311213850117\n",
      "train loss:0.45881143542962244\n",
      "train loss:0.4914450374849673\n",
      "train loss:0.519154487535092\n",
      "train loss:0.686308113366823\n",
      "train loss:0.53621155872947\n",
      "train loss:0.5234779275190324\n",
      "train loss:0.5410841629175027\n",
      "train loss:0.4449191883611849\n",
      "train loss:0.5645822779450449\n",
      "train loss:0.4799463871416754\n",
      "train loss:0.4459143832668398\n",
      "train loss:0.5119541448893876\n",
      "train loss:0.6334540927621681\n",
      "train loss:0.417449850751314\n",
      "train loss:0.44300377828652115\n",
      "train loss:0.39955502969759005\n",
      "train loss:0.6208214830347656\n",
      "train loss:0.45943151855383635\n",
      "train loss:0.45033106476782736\n",
      "train loss:0.7082716553113155\n",
      "train loss:0.5458997235468857\n",
      "train loss:0.5868628219222034\n",
      "train loss:0.567470674022969\n",
      "train loss:0.5242979711404895\n",
      "train loss:0.6681747827349243\n",
      "train loss:0.5136956175549643\n",
      "train loss:0.6615195839517889\n",
      "train loss:0.6250212073661071\n",
      "train loss:0.5052428787521902\n",
      "train loss:0.5400684830294241\n",
      "train loss:0.6018581423174727\n",
      "train loss:0.5003375353603683\n",
      "train loss:0.5469251937722663\n",
      "train loss:0.5278306516466226\n",
      "train loss:0.6633064950652232\n",
      "train loss:0.6088787316125615\n",
      "train loss:0.5344795436634782\n",
      "train loss:0.5059345071158957\n",
      "train loss:0.5055712944358083\n",
      "train loss:0.47207782579115753\n",
      "train loss:0.680526620062066\n",
      "train loss:0.48863064827729713\n",
      "train loss:0.6377485580808419\n",
      "train loss:0.4116920607391075\n",
      "train loss:0.40295796693749125\n",
      "train loss:0.6025893050867106\n",
      "train loss:0.6422024850183162\n",
      "train loss:0.7846162959845087\n",
      "train loss:0.5093490508298675\n",
      "train loss:0.6156193405159223\n",
      "train loss:0.5672346684278605\n",
      "train loss:0.5201540511401006\n",
      "train loss:0.3917160856747911\n",
      "train loss:0.5245024585545136\n",
      "train loss:0.6510650974625359\n",
      "train loss:0.4972738735325355\n",
      "train loss:0.45606471457579234\n",
      "train loss:0.49482252037978697\n",
      "train loss:0.4636866253415416\n",
      "train loss:0.6448284162582487\n",
      "train loss:0.6818250514929389\n",
      "train loss:0.6015954591165908\n",
      "train loss:0.4475598447138395\n",
      "train loss:0.48967018170658766\n",
      "train loss:0.5807586191746329\n",
      "train loss:0.594376365322842\n",
      "train loss:0.6155184015361574\n",
      "train loss:0.602724958075575\n",
      "train loss:0.5541253230652012\n",
      "train loss:0.49951331347505834\n",
      "train loss:0.4490468207957733\n",
      "train loss:0.6333106183265085\n",
      "train loss:0.4886344875549422\n",
      "train loss:0.4301898343684076\n",
      "train loss:0.5713560141132963\n",
      "train loss:0.5903521638353912\n",
      "train loss:0.6971135419945993\n",
      "train loss:0.67026418184199\n",
      "train loss:0.5369178384402463\n",
      "train loss:0.5576792098172987\n",
      "train loss:0.39447648640555194\n",
      "train loss:0.4190511691052655\n",
      "train loss:0.47527611403379055\n",
      "train loss:0.4686633532407327\n",
      "train loss:0.556731694917589\n",
      "train loss:0.5527186831056138\n",
      "train loss:0.6854863755380736\n",
      "train loss:0.5858508374876277\n",
      "train loss:0.6486838195497954\n",
      "train loss:0.6706334144676027\n",
      "train loss:0.5615481607715463\n",
      "train loss:0.51477269285626\n",
      "train loss:0.44993577361689774\n",
      "train loss:0.5062787562496347\n",
      "train loss:0.45922905119462404\n",
      "train loss:0.6754566433563005\n",
      "train loss:0.6261302589004313\n",
      "train loss:0.5237850386896502\n",
      "train loss:0.506521645753683\n",
      "train loss:0.5521232630918521\n",
      "train loss:0.6889292410389476\n",
      "train loss:0.550525296159876\n",
      "train loss:0.4405377151026075\n",
      "train loss:0.4745364353644461\n",
      "train loss:0.706816158565025\n",
      "train loss:0.6009273463217542\n",
      "train loss:0.3652409631590116\n",
      "train loss:0.5420667193957214\n",
      "train loss:0.43511495932939176\n",
      "train loss:0.5470175570510025\n",
      "train loss:0.4038332001173637\n",
      "train loss:0.43152928310901506\n",
      "train loss:0.3835175235444369\n",
      "train loss:0.6334037655671497\n",
      "train loss:0.5503176421663379\n",
      "train loss:0.5338505504519498\n",
      "train loss:0.481140038540789\n",
      "train loss:0.4768324892556057\n",
      "train loss:0.5147179378761223\n",
      "train loss:0.5379667347368012\n",
      "train loss:0.5138568681120109\n",
      "train loss:0.4526025046876134\n",
      "train loss:0.4824419401354454\n",
      "train loss:0.3989829218132296\n",
      "train loss:0.5005623583360957\n",
      "train loss:0.5805278441047924\n",
      "train loss:0.463226713926685\n",
      "train loss:0.6060681052613174\n",
      "train loss:0.6290926934336266\n",
      "train loss:0.661936026083827\n",
      "train loss:0.4054737294091331\n",
      "train loss:0.5021431947381818\n",
      "train loss:0.5603704776853832\n",
      "train loss:0.5449871122139299\n",
      "train loss:0.600541695193173\n",
      "train loss:0.5143018140965244\n",
      "train loss:0.5874677769007228\n",
      "train loss:0.5129748990556284\n",
      "train loss:0.5474423601702588\n",
      "train loss:0.6031699066805323\n",
      "train loss:0.6286183241186335\n",
      "train loss:0.611594891196028\n",
      "train loss:0.5071881873735005\n",
      "train loss:0.49109098325643075\n",
      "train loss:0.5000842121499076\n",
      "train loss:0.439137643281861\n",
      "train loss:0.4887589710168773\n",
      "train loss:0.4450140698720299\n",
      "train loss:0.5604287148653838\n",
      "train loss:0.5772897493408591\n",
      "train loss:0.46566850738000826\n",
      "train loss:0.4463254090511502\n",
      "train loss:0.5048317928582746\n",
      "train loss:0.5373377766490999\n",
      "train loss:0.6541137640408397\n",
      "train loss:0.5685038371138587\n",
      "train loss:0.46678755329840316\n",
      "train loss:0.6067659165612771\n",
      "train loss:0.49143125175535096\n",
      "train loss:0.5207276220921679\n",
      "train loss:0.6591907541621264\n",
      "train loss:0.4944900839661092\n",
      "train loss:0.5928420921964187\n",
      "train loss:0.5274071583085047\n",
      "train loss:0.6435586402347091\n",
      "train loss:0.49948066342947023\n",
      "train loss:0.5607708948672196\n",
      "train loss:0.42321991421796734\n",
      "train loss:0.507600450856868\n",
      "train loss:0.5179837052289361\n",
      "train loss:0.4811594944257327\n",
      "train loss:0.5285970794424457\n",
      "train loss:0.6640705395614078\n",
      "train loss:0.4888128939001277\n",
      "train loss:0.4789805303403295\n",
      "train loss:0.45137551130885634\n",
      "train loss:0.6221976284395015\n",
      "train loss:0.5475035852912894\n",
      "train loss:0.5155629756357887\n",
      "train loss:0.5304343835504572\n",
      "train loss:0.5635992389295051\n",
      "train loss:0.5975386144153529\n",
      "train loss:0.47667627155295117\n",
      "train loss:0.5224767359322172\n",
      "train loss:0.5304246734740758\n",
      "train loss:0.5072684277234606\n",
      "train loss:0.5700351433649545\n",
      "train loss:0.5352738433671569\n",
      "train loss:0.5379266368708356\n",
      "train loss:0.634013017137052\n",
      "train loss:0.5010464228649508\n",
      "train loss:0.5670924587464339\n",
      "train loss:0.6122595583814285\n",
      "train loss:0.6496384423938004\n",
      "train loss:0.4166689810971957\n",
      "train loss:0.5114693838392985\n",
      "train loss:0.4505236029856659\n",
      "train loss:0.6856902589610543\n",
      "train loss:0.542770109585315\n",
      "train loss:0.5506313342790335\n",
      "train loss:0.6372556055157694\n",
      "train loss:0.5510179078934925\n",
      "train loss:0.5161659043206911\n",
      "train loss:0.492784246266203\n",
      "train loss:0.5056104333789537\n",
      "train loss:0.5332613137741554\n",
      "train loss:0.5485291087628368\n",
      "train loss:0.5851548383640026\n",
      "train loss:0.4742601335371556\n",
      "train loss:0.514994685156365\n",
      "train loss:0.7339869180582896\n",
      "train loss:0.4972351706886926\n",
      "train loss:0.493659201555545\n",
      "train loss:0.4887410924740356\n",
      "train loss:0.607850372325619\n",
      "train loss:0.46940780464015314\n",
      "train loss:0.6410359083694211\n",
      "train loss:0.4939727215516465\n",
      "train loss:0.6246917107488222\n",
      "train loss:0.6199026170128856\n",
      "train loss:0.41639423491005245\n",
      "train loss:0.5438008517355745\n",
      "train loss:0.6732932823491592\n",
      "train loss:0.4722747904372199\n",
      "train loss:0.6411771196060078\n",
      "train loss:0.5608123356529419\n",
      "train loss:0.39284265819825764\n",
      "train loss:0.5895185716291775\n",
      "train loss:0.6795561659067143\n",
      "train loss:0.5570751070001196\n",
      "train loss:0.44391755281512085\n",
      "train loss:0.4465772531848692\n",
      "train loss:0.5550719585277415\n",
      "train loss:0.6389763211603171\n",
      "train loss:0.6032759164566255\n",
      "train loss:0.5859113572668083\n",
      "train loss:0.562248618877601\n",
      "train loss:0.5828249445220801\n",
      "train loss:0.49175502231451146\n",
      "train loss:0.5875195613200951\n",
      "train loss:0.6279572761620195\n",
      "train loss:0.5858675883936546\n",
      "train loss:0.6334643285561989\n",
      "train loss:0.682518664342815\n",
      "train loss:0.48734496051064125\n",
      "train loss:0.47983832862329967\n",
      "train loss:0.45386665244982133\n",
      "train loss:0.5083766960618419\n",
      "train loss:0.5717044735083798\n",
      "train loss:0.4688732492114185\n",
      "train loss:0.5113493818637592\n",
      "train loss:0.3921562317226029\n",
      "train loss:0.5293132020852551\n",
      "train loss:0.6739362619119815\n",
      "train loss:0.6176493901444631\n",
      "train loss:0.39876310082083427\n",
      "train loss:0.5267332389376764\n",
      "train loss:0.643115921235193\n",
      "train loss:0.6314190973393925\n",
      "train loss:0.5423691662192762\n",
      "train loss:0.725792518899289\n",
      "train loss:0.5831479923421642\n",
      "train loss:0.5530258745078495\n",
      "train loss:0.5240525480977146\n",
      "train loss:0.5336707002367791\n",
      "train loss:0.46557357434088886\n",
      "train loss:0.4306505292106161\n",
      "train loss:0.812204865618495\n",
      "train loss:0.5072621599113275\n",
      "train loss:0.5764503504683252\n",
      "train loss:0.4694995858051637\n",
      "train loss:0.5870696483870752\n",
      "train loss:0.525400636871156\n",
      "train loss:0.668803830777406\n",
      "train loss:0.6318093599772133\n",
      "train loss:0.49492156526472153\n",
      "train loss:0.6394743656253814\n",
      "train loss:0.5506149622773228\n",
      "train loss:0.5993466514721288\n",
      "train loss:0.43713852549859405\n",
      "train loss:0.5290110611577016\n",
      "train loss:0.6392155989023673\n",
      "train loss:0.5187945975463709\n",
      "train loss:0.3923738995284112\n",
      "train loss:0.5404925043327885\n",
      "train loss:0.7316524424595171\n",
      "train loss:0.7154031969844364\n",
      "train loss:0.5539175256464853\n",
      "train loss:0.6570651276907976\n",
      "train loss:0.6963161363220226\n",
      "train loss:0.5729894524155826\n",
      "train loss:0.49388542007466546\n",
      "train loss:0.49388613012625143\n",
      "train loss:0.5344682792771486\n",
      "train loss:0.6421573671626785\n",
      "train loss:0.5103202020715492\n",
      "train loss:0.6534267893519156\n",
      "train loss:0.4892629647911543\n",
      "train loss:0.5092973449962076\n",
      "train loss:0.4688242207819371\n",
      "train loss:0.6469018850907241\n",
      "train loss:0.6862232007390971\n",
      "train loss:0.51971930915419\n",
      "train loss:0.49585412464952516\n",
      "train loss:0.47815174309331554\n",
      "train loss:0.5182259818921043\n",
      "train loss:0.5297594361476169\n",
      "train loss:0.48002328540701705\n",
      "train loss:0.5931051352730713\n",
      "train loss:0.4627178204937248\n",
      "train loss:0.5767928334198598\n",
      "train loss:0.572168679147356\n",
      "train loss:0.47147910959641964\n",
      "train loss:0.5334214733469439\n",
      "train loss:0.44044297404414523\n",
      "train loss:0.47035568483304474\n",
      "train loss:0.5017404604955304\n",
      "train loss:0.39848538942020034\n",
      "train loss:0.432125909460556\n",
      "train loss:0.7761916532457456\n",
      "=== epoch:8, train acc:0.745, test acc:0.758 ===\n",
      "train loss:0.5783297004435012\n",
      "train loss:0.5363893495262101\n",
      "train loss:0.5145235346829586\n",
      "train loss:0.5097320322542672\n",
      "train loss:0.555844405231379\n",
      "train loss:0.5878433153369375\n",
      "train loss:0.6481899360633934\n",
      "train loss:0.5387176770145407\n",
      "train loss:0.6120136072348753\n",
      "train loss:0.5062180328918503\n",
      "train loss:0.5130309469892641\n",
      "train loss:0.4972771242707499\n",
      "train loss:0.5707392200342771\n",
      "train loss:0.46696934620303177\n",
      "train loss:0.7025129294141172\n",
      "train loss:0.47018947517007403\n",
      "train loss:0.44533576235119504\n",
      "train loss:0.4052567002560744\n",
      "train loss:0.5399745165719162\n",
      "train loss:0.559076990626894\n",
      "train loss:0.5756224646527328\n",
      "train loss:0.5270637963812955\n",
      "train loss:0.5273638773477152\n",
      "train loss:0.5621386031062443\n",
      "train loss:0.45927007670968434\n",
      "train loss:0.5763000298961122\n",
      "train loss:0.517027151909003\n",
      "train loss:0.42332061010769756\n",
      "train loss:0.570149211870055\n",
      "train loss:0.5340134498403319\n",
      "train loss:0.5347542815946451\n",
      "train loss:0.7933050747578492\n",
      "train loss:0.5121383131683791\n",
      "train loss:0.46603586889525866\n",
      "train loss:0.6025717411152286\n",
      "train loss:0.37724735833331224\n",
      "train loss:0.6197683130431728\n",
      "train loss:0.5762860542052447\n",
      "train loss:0.6131788514865119\n",
      "train loss:0.5051479856632421\n",
      "train loss:0.5934175809757828\n",
      "train loss:0.45039553035353225\n",
      "train loss:0.5650366871242813\n",
      "train loss:0.5074489802108004\n",
      "train loss:0.5586934945838578\n",
      "train loss:0.47252788415183145\n",
      "train loss:0.5380660222959212\n",
      "train loss:0.6780377399166622\n",
      "train loss:0.5271741790902694\n",
      "train loss:0.46946971597399473\n",
      "train loss:0.5294907389525725\n",
      "train loss:0.49904871263871764\n",
      "train loss:0.5343405779221948\n",
      "train loss:0.5483773526350906\n",
      "train loss:0.6366299674843905\n",
      "train loss:0.45460239818951337\n",
      "train loss:0.6566573609849935\n",
      "train loss:0.5194816377837762\n",
      "train loss:0.417919210970734\n",
      "train loss:0.5839571452102702\n",
      "train loss:0.4938041625350069\n",
      "train loss:0.5499663751744022\n",
      "train loss:0.5390487457887972\n",
      "train loss:0.3941216800413831\n",
      "train loss:0.4217476537668604\n",
      "train loss:0.5706218682953405\n",
      "train loss:0.45917999611524235\n",
      "train loss:0.5048013784298833\n",
      "train loss:0.44139927476100793\n",
      "train loss:0.41369015304309237\n",
      "train loss:0.5247343123528632\n",
      "train loss:0.5034244219798518\n",
      "train loss:0.44451787408703\n",
      "train loss:0.5242079443914547\n",
      "train loss:0.4926803873354874\n",
      "train loss:0.5750975693690649\n",
      "train loss:0.5187603474773913\n",
      "train loss:0.5989907482389504\n",
      "train loss:0.438097749419237\n",
      "train loss:0.5295281034186585\n",
      "train loss:0.4245040796480419\n",
      "train loss:0.5037022585583223\n",
      "train loss:0.4580485245776035\n",
      "train loss:0.48994808352996744\n",
      "train loss:0.627807664944076\n",
      "train loss:0.521842503398138\n",
      "train loss:0.575258119275352\n",
      "train loss:0.48561504279083406\n",
      "train loss:0.48353627794661025\n",
      "train loss:0.4846592999824385\n",
      "train loss:0.6687691533483487\n",
      "train loss:0.4660284953153714\n",
      "train loss:0.5684963365844681\n",
      "train loss:0.6139052377344253\n",
      "train loss:0.5147447549062086\n",
      "train loss:0.48026756143594973\n",
      "train loss:0.556860591936903\n",
      "train loss:0.46541412616825034\n",
      "train loss:0.6113619192084238\n",
      "train loss:0.6639775651378996\n",
      "train loss:0.5219386318926418\n",
      "train loss:0.682500306927893\n",
      "train loss:0.44483521018903205\n",
      "train loss:0.5953284126347106\n",
      "train loss:0.5961990860257607\n",
      "train loss:0.5281413202209808\n",
      "train loss:0.3915910713479471\n",
      "train loss:0.41339722491789416\n",
      "train loss:0.43606296358470453\n",
      "train loss:0.47518345763225867\n",
      "train loss:0.5044473997077095\n",
      "train loss:0.4044829039408615\n",
      "train loss:0.46014132569884125\n",
      "train loss:0.49392519262645157\n",
      "train loss:0.6424104023919684\n",
      "train loss:0.49507060200742686\n",
      "train loss:0.4879316169397913\n",
      "train loss:0.4858787057818537\n",
      "train loss:0.5001561935563698\n",
      "train loss:0.7227128182178265\n",
      "train loss:0.5349377912673183\n",
      "train loss:0.5085154617557748\n",
      "train loss:0.5219800156927683\n",
      "train loss:0.4050889132745092\n",
      "train loss:0.5829519368716286\n",
      "train loss:0.34612968280074186\n",
      "train loss:0.5914892895690788\n",
      "train loss:0.5567756182260282\n",
      "train loss:0.5864111813865343\n",
      "train loss:0.5295894422564388\n",
      "train loss:0.5197516195252236\n",
      "train loss:0.6263515763298378\n",
      "train loss:0.6428211751327741\n",
      "train loss:0.4112845387176253\n",
      "train loss:0.5508093210223232\n",
      "train loss:0.6919347860481386\n",
      "train loss:0.4957067473331365\n",
      "train loss:0.5224483994594676\n",
      "train loss:0.4771904614239859\n",
      "train loss:0.49191319901486225\n",
      "train loss:0.528304305642014\n",
      "train loss:0.5001404023815302\n",
      "train loss:0.6525174074483985\n",
      "train loss:0.4760417558578722\n",
      "train loss:0.5237233236814681\n",
      "train loss:0.5197750102900357\n",
      "train loss:0.5010588127366927\n",
      "train loss:0.5782497760426601\n",
      "train loss:0.634623389885872\n",
      "train loss:0.558503071164371\n",
      "train loss:0.4239278317685889\n",
      "train loss:0.5093214908476555\n",
      "train loss:0.5261647872826702\n",
      "train loss:0.44092096721352436\n",
      "train loss:0.5159546318600434\n",
      "train loss:0.6542044499124647\n",
      "train loss:0.5451923285351712\n",
      "train loss:0.5528867404540091\n",
      "train loss:0.4695658297495536\n",
      "train loss:0.5302422378119007\n",
      "train loss:0.4864412869657049\n",
      "train loss:0.5650712077244762\n",
      "train loss:0.5785478060081352\n",
      "train loss:0.5221006840920343\n",
      "train loss:0.5662664920697996\n",
      "train loss:0.48366061438987024\n",
      "train loss:0.5813180558471854\n",
      "train loss:0.7352457643306303\n",
      "train loss:0.45309542248667917\n",
      "train loss:0.5059839036197432\n",
      "train loss:0.6173917857374308\n",
      "train loss:0.4545638904551142\n",
      "train loss:0.5624408514585022\n",
      "train loss:0.5293815494546119\n",
      "train loss:0.43586928413466663\n",
      "train loss:0.5850812709533606\n",
      "train loss:0.49617800337877926\n",
      "train loss:0.4077409762524471\n",
      "train loss:0.457093171857754\n",
      "train loss:0.6173185212512395\n",
      "train loss:0.4079538389897251\n",
      "train loss:0.5374581262872213\n",
      "train loss:0.4361806932565979\n",
      "train loss:0.5425250336616473\n",
      "train loss:0.47595457473513997\n",
      "train loss:0.5264888814879957\n",
      "train loss:0.4892730303069653\n",
      "train loss:0.4394935301916529\n",
      "train loss:0.5665210966878209\n",
      "train loss:0.5430750072042937\n",
      "train loss:0.5466938019118784\n",
      "train loss:0.5123231471743364\n",
      "train loss:0.48663063037810694\n",
      "train loss:0.694574400294078\n",
      "train loss:0.5766340339201694\n",
      "train loss:0.48855771962103794\n",
      "train loss:0.519665044145464\n",
      "train loss:0.5333834708078858\n",
      "train loss:0.6783325411569582\n",
      "train loss:0.6021520754660448\n",
      "train loss:0.4390632234998691\n",
      "train loss:0.4934281895869904\n",
      "train loss:0.6487259318102987\n",
      "train loss:0.6758495558502353\n",
      "train loss:0.598972666047089\n",
      "train loss:0.5349705653464512\n",
      "train loss:0.5338081538364902\n",
      "train loss:0.5616436973289877\n",
      "train loss:0.6230529091610212\n",
      "train loss:0.31900355805707625\n",
      "train loss:0.5496669429272584\n",
      "train loss:0.5399373557100698\n",
      "train loss:0.5046848913332183\n",
      "train loss:0.6908082254209157\n",
      "train loss:0.5063718902281069\n",
      "train loss:0.46851620675008554\n",
      "train loss:0.4580016041284043\n",
      "train loss:0.5553682819466799\n",
      "train loss:0.3666384030387412\n",
      "train loss:0.6943024044904468\n",
      "train loss:0.5500836214149486\n",
      "train loss:0.5461415269203035\n",
      "train loss:0.573200339061419\n",
      "train loss:0.4774380471189415\n",
      "train loss:0.6142151588062066\n",
      "train loss:0.4570348556934535\n",
      "train loss:0.5823192635350237\n",
      "train loss:0.616240019662748\n",
      "train loss:0.31899262991256855\n",
      "train loss:0.4180802078970312\n",
      "train loss:0.5000260324141245\n",
      "train loss:0.4720830195769549\n",
      "train loss:0.4512562042864753\n",
      "train loss:0.4136678574783515\n",
      "train loss:0.5356006308784917\n",
      "train loss:0.5095734051423446\n",
      "train loss:0.6898122031300864\n",
      "train loss:0.5148225002656618\n",
      "train loss:0.4418091637878934\n",
      "train loss:0.5302984903719531\n",
      "train loss:0.43718274533348445\n",
      "train loss:0.49800130341166216\n",
      "train loss:0.5555516393214969\n",
      "train loss:0.6087652510417602\n",
      "train loss:0.5813069693508389\n",
      "train loss:0.6598469058675428\n",
      "train loss:0.6875577564639187\n",
      "train loss:0.5901758544648039\n",
      "train loss:0.5796101928225379\n",
      "train loss:0.5426897692648999\n",
      "train loss:0.5897910719647834\n",
      "train loss:0.4590911288316805\n",
      "train loss:0.5183936567598899\n",
      "train loss:0.5618985590990443\n",
      "train loss:0.4388490683589473\n",
      "train loss:0.6071632944958281\n",
      "train loss:0.5182618702843222\n",
      "train loss:0.5051855624172988\n",
      "train loss:0.518110728870063\n",
      "train loss:0.599713966662532\n",
      "train loss:0.6534912716299464\n",
      "train loss:0.43554474577594005\n",
      "train loss:0.468194922718142\n",
      "train loss:0.5072229220344248\n",
      "train loss:0.5883675790414734\n",
      "train loss:0.48195865946730565\n",
      "train loss:0.41012157236828833\n",
      "train loss:0.577111132832925\n",
      "train loss:0.43506500704359796\n",
      "train loss:0.4117532023711557\n",
      "train loss:0.6552619017619299\n",
      "train loss:0.47242257954712275\n",
      "train loss:0.5128515113931732\n",
      "train loss:0.5335830275307601\n",
      "train loss:0.4966422420543827\n",
      "train loss:0.411489671234085\n",
      "train loss:0.4589913487280207\n",
      "train loss:0.44773674969445165\n",
      "train loss:0.6472854230176643\n",
      "train loss:0.665879248916609\n",
      "train loss:0.49011605091716054\n",
      "train loss:0.6001226445629328\n",
      "train loss:0.36823956772473304\n",
      "train loss:0.43105223169642143\n",
      "train loss:0.6998664231191704\n",
      "train loss:0.41690021807467903\n",
      "train loss:0.48588100038374343\n",
      "train loss:0.5018943209883568\n",
      "train loss:0.398506620510097\n",
      "train loss:0.5766661976207391\n",
      "train loss:0.5476189314161513\n",
      "train loss:0.4356661839593274\n",
      "train loss:0.6724595484395657\n",
      "train loss:0.40852559231043933\n",
      "train loss:0.3856011941998671\n",
      "train loss:0.5025731064453323\n",
      "train loss:0.5027058282022168\n",
      "train loss:0.4656326786153654\n",
      "train loss:0.5575059632458504\n",
      "train loss:0.501017842272447\n",
      "train loss:0.45108681220336033\n",
      "train loss:0.5083956910790551\n",
      "train loss:0.47178249892691704\n",
      "train loss:0.5240732084384725\n",
      "train loss:0.4902092122417298\n",
      "train loss:0.5273505571017445\n",
      "train loss:0.5444962996518119\n",
      "train loss:0.6005763675089232\n",
      "train loss:0.552242052144442\n",
      "train loss:0.6332724138811957\n",
      "train loss:0.5176609436034295\n",
      "train loss:0.4783895570974241\n",
      "train loss:0.44611152836931356\n",
      "train loss:0.5081655562632357\n",
      "train loss:0.49666866347853555\n",
      "train loss:0.5028590975112499\n",
      "train loss:0.7554472914103871\n",
      "train loss:0.5012506111788347\n",
      "train loss:0.5663589482181063\n",
      "train loss:0.4854892063465676\n",
      "train loss:0.4123271351064834\n",
      "train loss:0.4942268189242337\n",
      "train loss:0.4033880061161138\n",
      "train loss:0.4638850581009215\n",
      "train loss:0.6728841626044431\n",
      "train loss:0.5006677511494297\n",
      "train loss:0.5626975427046874\n",
      "train loss:0.5370120013576659\n",
      "train loss:0.4284169806956485\n",
      "train loss:0.46358882076171226\n",
      "train loss:0.34620359443037446\n",
      "train loss:0.4958516994994286\n",
      "train loss:0.539887086545046\n",
      "train loss:0.4910547667749355\n",
      "train loss:0.6277474240427893\n",
      "train loss:0.46219660523434136\n",
      "train loss:0.5342375308405061\n",
      "train loss:0.48691811628087317\n",
      "train loss:0.5856371083384938\n",
      "train loss:0.5731782162267285\n",
      "train loss:0.5375167202367809\n",
      "train loss:0.5231493998098881\n",
      "train loss:0.5013202352775433\n",
      "train loss:0.5299583386152252\n",
      "train loss:0.4626750782002847\n",
      "train loss:0.44314692708964176\n",
      "train loss:0.5787191724722106\n",
      "train loss:0.4853695727129292\n",
      "train loss:0.4281467259721825\n",
      "train loss:0.49815993007849435\n",
      "train loss:0.47426990416678955\n",
      "train loss:0.6331310071941769\n",
      "train loss:0.5085558357354144\n",
      "train loss:0.4772535928067285\n",
      "train loss:0.5535720414571945\n",
      "train loss:0.6074974972865299\n",
      "train loss:0.5020678839441488\n",
      "train loss:0.48219703071614217\n",
      "train loss:0.4657004878569946\n",
      "train loss:0.42600102180034793\n",
      "train loss:0.45367133631661866\n",
      "train loss:0.3861142963974822\n",
      "train loss:0.5273902816039672\n",
      "train loss:0.5200286598918404\n",
      "train loss:0.5616677284552157\n",
      "train loss:0.4730176874755332\n",
      "train loss:0.49713416634537905\n",
      "train loss:0.38969588590848153\n",
      "train loss:0.7302132019733166\n",
      "train loss:0.479172312902277\n",
      "train loss:0.3912990053858789\n",
      "train loss:0.5402629350663049\n",
      "train loss:0.4118631799806893\n",
      "train loss:0.4248084175325662\n",
      "train loss:0.4379475467202974\n",
      "train loss:0.5833053318019935\n",
      "train loss:0.5141628878940485\n",
      "train loss:0.6900834460294921\n",
      "train loss:0.3987025697452431\n",
      "train loss:0.4598254617403146\n",
      "train loss:0.8046936639241186\n",
      "train loss:0.5081218541535703\n",
      "train loss:0.3746226599233405\n",
      "train loss:0.7019189785945079\n",
      "train loss:0.5886949836788152\n",
      "train loss:0.6937254658289683\n",
      "train loss:0.6175916061936663\n",
      "train loss:0.44800760844308646\n",
      "train loss:0.38477005810796183\n",
      "train loss:0.4754450505425814\n",
      "train loss:0.3584542455960046\n",
      "train loss:0.5429070916003509\n",
      "train loss:0.6572579537668806\n",
      "train loss:0.6222281872257343\n",
      "train loss:0.4054972979797871\n",
      "train loss:0.5363597365188345\n",
      "train loss:0.5000457386340832\n",
      "train loss:0.5412739700319367\n",
      "train loss:0.5044680704767486\n",
      "train loss:0.6849190250197223\n",
      "train loss:0.5453654049639747\n",
      "train loss:0.47186457159842954\n",
      "train loss:0.6072987522693417\n",
      "train loss:0.582969762777525\n",
      "train loss:0.5172594498737901\n",
      "train loss:0.41072069532353156\n",
      "train loss:0.3691729865155418\n",
      "train loss:0.4350267434353123\n",
      "train loss:0.4852052685030516\n",
      "train loss:0.5206297694695474\n",
      "train loss:0.5663629191599373\n",
      "train loss:0.4772494489926439\n",
      "train loss:0.4880145233433529\n",
      "train loss:0.5821633641169127\n",
      "train loss:0.37434779758389164\n",
      "train loss:0.5449982880324383\n",
      "train loss:0.41985573633174006\n",
      "train loss:0.3953434429168841\n",
      "train loss:0.509337567721731\n",
      "train loss:0.45050033677976414\n",
      "train loss:0.47125884088950015\n",
      "train loss:0.42869680646881797\n",
      "train loss:0.42331546485331295\n",
      "train loss:0.4091115154079414\n",
      "train loss:0.560107219466521\n",
      "train loss:0.4766812116335216\n",
      "train loss:0.6890066321333858\n",
      "train loss:0.5539408726926979\n",
      "train loss:0.4734588117228573\n",
      "train loss:0.6518239671466044\n",
      "train loss:0.5356492897745903\n",
      "train loss:0.6115389033225596\n",
      "train loss:0.35047931452963915\n",
      "train loss:0.4690318971328522\n",
      "train loss:0.5192644781622929\n",
      "train loss:0.45014636555430576\n",
      "train loss:0.49032027148247775\n",
      "train loss:0.49493466016811505\n",
      "train loss:0.46815059152367994\n",
      "train loss:0.4582649886993284\n",
      "train loss:0.5808288529617625\n",
      "train loss:0.5828001428318678\n",
      "train loss:0.4886815660322995\n",
      "train loss:0.413170099839124\n",
      "train loss:0.5314511399099893\n",
      "train loss:0.3406150064896729\n",
      "train loss:0.39640990939979226\n",
      "train loss:0.48240755372968347\n",
      "train loss:0.5755453827377548\n",
      "train loss:0.5683399932523547\n",
      "train loss:0.4125736305666115\n",
      "train loss:0.734062976124883\n",
      "train loss:0.41087501993321385\n",
      "train loss:0.5381416310250742\n",
      "train loss:0.519903262406784\n",
      "train loss:0.5392614556508394\n",
      "train loss:0.4640789097664818\n",
      "train loss:0.6441575117703222\n",
      "train loss:0.6443486703490872\n",
      "train loss:0.48006023466125725\n",
      "train loss:0.6725537338385618\n",
      "train loss:0.4517212298043436\n",
      "train loss:0.5186311938571737\n",
      "train loss:0.5373628760609055\n",
      "train loss:0.517197379895275\n",
      "train loss:0.5993937091931734\n",
      "train loss:0.4294213126932657\n",
      "train loss:0.5517066489324266\n",
      "train loss:0.5088682087980744\n",
      "train loss:0.4796026844400672\n",
      "train loss:0.6621454837157686\n",
      "train loss:0.4661120350450134\n",
      "train loss:0.5711997787745958\n",
      "train loss:0.5364917696951312\n",
      "train loss:0.4680332517206057\n",
      "train loss:0.4937345005833106\n",
      "train loss:0.5168538043016013\n",
      "train loss:0.6467196275201412\n",
      "train loss:0.49155009895990676\n",
      "train loss:0.5441730033401516\n",
      "train loss:0.5317230569935958\n",
      "train loss:0.5774737059257771\n",
      "train loss:0.5002207277900272\n",
      "train loss:0.4482658152903449\n",
      "train loss:0.41812631117527965\n",
      "train loss:0.5238733424632022\n",
      "train loss:0.4974587723780388\n",
      "train loss:0.7354756638733193\n",
      "train loss:0.4996497032663034\n",
      "train loss:0.4283258792238898\n",
      "train loss:0.5428137078347202\n",
      "train loss:0.4658167379447577\n",
      "train loss:0.47769501078970683\n",
      "train loss:0.3934329716326944\n",
      "train loss:0.45139388123185775\n",
      "train loss:0.6350907713712571\n",
      "train loss:0.4784125411860367\n",
      "train loss:0.5321854172944621\n",
      "train loss:0.6414052658694935\n",
      "train loss:0.35845420221006535\n",
      "train loss:0.44859637599008795\n",
      "train loss:0.5370825381442423\n",
      "train loss:0.5172587755771851\n",
      "train loss:0.47631878998757043\n",
      "train loss:0.5002803054449714\n",
      "train loss:0.43981708197427216\n",
      "train loss:0.3631242073294224\n",
      "train loss:0.47955575495418357\n",
      "train loss:0.5513861264851201\n",
      "train loss:0.481639390855239\n",
      "train loss:0.5863877513166842\n",
      "train loss:0.6056351670742165\n",
      "train loss:0.46172970153156584\n",
      "train loss:0.5257171088468819\n",
      "train loss:0.40062948784234825\n",
      "train loss:0.408699197512582\n",
      "train loss:0.5468817519413045\n",
      "train loss:0.5490597575059705\n",
      "train loss:0.728164741706213\n",
      "train loss:0.6053384620771963\n",
      "train loss:0.46059891601156944\n",
      "train loss:0.42880322211930944\n",
      "train loss:0.38826069961934523\n",
      "train loss:0.47148979838312277\n",
      "train loss:0.5735592101000273\n",
      "train loss:0.6335518403360464\n",
      "train loss:0.489446362252755\n",
      "train loss:0.6103207311141606\n",
      "train loss:0.5310024081570413\n",
      "train loss:0.37301781625337677\n",
      "train loss:0.5060140102263884\n",
      "train loss:0.4660693576214402\n",
      "train loss:0.47118668888437304\n",
      "train loss:0.5025741677416241\n",
      "train loss:0.5017460799733913\n",
      "train loss:0.40786199987523075\n",
      "train loss:0.4279913373916907\n",
      "train loss:0.4436990421093576\n",
      "train loss:0.4872431371785603\n",
      "train loss:0.4501193281196956\n",
      "train loss:0.5605476172131877\n",
      "train loss:0.5059337028124792\n",
      "train loss:0.45333286819785484\n",
      "train loss:0.4041098640094172\n",
      "train loss:0.4317985211787852\n",
      "train loss:0.47958154893219884\n",
      "train loss:0.5174144426699674\n",
      "train loss:0.6713806064571994\n",
      "train loss:0.66706947575315\n",
      "train loss:0.4449985492399233\n",
      "train loss:0.5320385787956746\n",
      "train loss:0.5247894135173654\n",
      "train loss:0.5658287212825617\n",
      "train loss:0.4853626483134505\n",
      "train loss:0.4964767013822729\n",
      "train loss:0.46000600030233907\n",
      "train loss:0.6045680527992788\n",
      "train loss:0.5138657534931625\n",
      "train loss:0.5309554659419287\n",
      "train loss:0.39723259087656443\n",
      "train loss:0.3625731962012303\n",
      "train loss:0.5408756832676653\n",
      "train loss:0.43997556242099173\n",
      "train loss:0.4860390314222041\n",
      "train loss:0.4078366432630541\n",
      "train loss:0.5539230148610972\n",
      "train loss:0.6433944824851097\n",
      "train loss:0.4543244743984484\n",
      "train loss:0.47952776351198517\n",
      "train loss:0.5762866063581449\n",
      "train loss:0.34834397704300707\n",
      "train loss:0.4132179197884102\n",
      "train loss:0.4698290123973136\n",
      "train loss:0.7241769221948363\n",
      "train loss:0.4521914795854636\n",
      "train loss:0.5303664980399302\n",
      "train loss:0.3901599884849508\n",
      "train loss:0.36183253230191537\n",
      "train loss:0.5566287347605718\n",
      "train loss:0.5143279380842851\n",
      "train loss:0.5747615957350853\n",
      "train loss:0.4977072595528096\n",
      "train loss:0.40876295200120416\n",
      "train loss:0.6266605638814713\n",
      "train loss:0.5260604760026079\n",
      "train loss:0.5172216443810993\n",
      "train loss:0.49554716250468883\n",
      "train loss:0.45258192437530026\n",
      "train loss:0.5762263280053356\n",
      "train loss:0.37560957185818133\n",
      "train loss:0.47888921166829385\n",
      "train loss:0.5473259434386984\n",
      "train loss:0.5303966712678436\n",
      "train loss:0.3685043786979189\n",
      "train loss:0.47837030636034084\n",
      "train loss:0.5613478742968366\n",
      "train loss:0.38085155013405336\n",
      "train loss:0.5241572262856496\n",
      "train loss:0.43966600814403606\n",
      "train loss:0.3797358921385265\n",
      "=== epoch:9, train acc:0.759, test acc:0.765 ===\n",
      "train loss:0.609826616614301\n",
      "train loss:0.502736193224878\n",
      "train loss:0.6277229773986027\n",
      "train loss:0.3994826815117863\n",
      "train loss:0.4626606850248127\n",
      "train loss:0.4758843991122598\n",
      "train loss:0.5231598454131775\n",
      "train loss:0.558596464745126\n",
      "train loss:0.49392686683415166\n",
      "train loss:0.6132938803335981\n",
      "train loss:0.4014764130310249\n",
      "train loss:0.4696470271053844\n",
      "train loss:0.3992261265575134\n",
      "train loss:0.48757886516764115\n",
      "train loss:0.546820332464706\n",
      "train loss:0.5791928875821268\n",
      "train loss:0.3810502820934802\n",
      "train loss:0.5585397191603187\n",
      "train loss:0.5024007843287568\n",
      "train loss:0.5784783876492938\n",
      "train loss:0.3678240561641929\n",
      "train loss:0.42734397631816934\n",
      "train loss:0.5208688834618896\n",
      "train loss:0.41797778519896445\n",
      "train loss:0.6244418839098295\n",
      "train loss:0.5586553527609194\n",
      "train loss:0.5590049550013135\n",
      "train loss:0.39723189552430205\n",
      "train loss:0.37055507908529106\n",
      "train loss:0.6045704333168885\n",
      "train loss:0.5050937100637752\n",
      "train loss:0.5565305405327267\n",
      "train loss:0.47288128110667643\n",
      "train loss:0.48733308974332756\n",
      "train loss:0.5111667637063784\n",
      "train loss:0.5964189685320234\n",
      "train loss:0.475599944328364\n",
      "train loss:0.5179771144964957\n",
      "train loss:0.5735716164505884\n",
      "train loss:0.527228236808583\n",
      "train loss:0.5192914954824179\n",
      "train loss:0.5720475105784468\n",
      "train loss:0.5908065239931569\n",
      "train loss:0.5890081396877319\n",
      "train loss:0.43601821420577047\n",
      "train loss:0.5881503506395399\n",
      "train loss:0.4218928554720494\n",
      "train loss:0.5918457391308448\n",
      "train loss:0.7832938130211012\n",
      "train loss:0.4032192902877311\n",
      "train loss:0.5268644158850918\n",
      "train loss:0.5705531851585954\n",
      "train loss:0.4380671275153602\n",
      "train loss:0.5804148886850033\n",
      "train loss:0.5656981534090769\n",
      "train loss:0.5007502862325205\n",
      "train loss:0.521182371581363\n",
      "train loss:0.46091269942540714\n",
      "train loss:0.47254143570869034\n",
      "train loss:0.5942103259998723\n",
      "train loss:0.45442336689038226\n",
      "train loss:0.43865909211911874\n",
      "train loss:0.5447555188392618\n",
      "train loss:0.6628594425411563\n",
      "train loss:0.5140282977632448\n",
      "train loss:0.4692094187292865\n",
      "train loss:0.44141153434594493\n",
      "train loss:0.59258763911748\n",
      "train loss:0.5100397958347278\n",
      "train loss:0.47904124354907274\n",
      "train loss:0.5577264322633563\n",
      "train loss:0.6025455471446748\n",
      "train loss:0.5308662246781262\n",
      "train loss:0.7015966822826334\n",
      "train loss:0.33489505516870816\n",
      "train loss:0.6089327320200928\n",
      "train loss:0.49082370165305844\n",
      "train loss:0.47538065068507757\n",
      "train loss:0.61770479704479\n",
      "train loss:0.4330874935807414\n",
      "train loss:0.5219775394592268\n",
      "train loss:0.4938711427503017\n",
      "train loss:0.44668291243402436\n",
      "train loss:0.5516042034677416\n",
      "train loss:0.6037093376836041\n",
      "train loss:0.60885553820522\n",
      "train loss:0.5419726224112754\n",
      "train loss:0.5208774268563847\n",
      "train loss:0.4702237057731976\n",
      "train loss:0.49801532340186566\n",
      "train loss:0.4825632026790816\n",
      "train loss:0.5244994248323281\n",
      "train loss:0.6105875928533195\n",
      "train loss:0.5468577079634956\n",
      "train loss:0.5976091755995562\n",
      "train loss:0.5563656824916935\n",
      "train loss:0.6142458954646074\n",
      "train loss:0.527348578858021\n",
      "train loss:0.3092627318187159\n",
      "train loss:0.5001477825526969\n",
      "train loss:0.5915462745821009\n",
      "train loss:0.3842802157373356\n",
      "train loss:0.40307082204519096\n",
      "train loss:0.6978382119807081\n",
      "train loss:0.4625252289326102\n",
      "train loss:0.5516449925548197\n",
      "train loss:0.43990509144125356\n",
      "train loss:0.5273849866748928\n",
      "train loss:0.5431765276999567\n",
      "train loss:0.3920794372466745\n",
      "train loss:0.4758789755948582\n",
      "train loss:0.5667949492620693\n",
      "train loss:0.4011758783913627\n",
      "train loss:0.5345337489728683\n",
      "train loss:0.5916723773127104\n",
      "train loss:0.5297661959335977\n",
      "train loss:0.3848068465708152\n",
      "train loss:0.37845453307022403\n",
      "train loss:0.6372357508748845\n",
      "train loss:0.3827870451968011\n",
      "train loss:0.4658798492763422\n",
      "train loss:0.4832131558469107\n",
      "train loss:0.5924452452259349\n",
      "train loss:0.45623783416455965\n",
      "train loss:0.5319895689280405\n",
      "train loss:0.691734104464041\n",
      "train loss:0.6746934232013478\n",
      "train loss:0.5360905429098756\n",
      "train loss:0.5534544247324592\n",
      "train loss:0.5607327111727963\n",
      "train loss:0.4696487321794864\n",
      "train loss:0.4193552287575153\n",
      "train loss:0.5103893725707255\n",
      "train loss:0.4108858432154834\n",
      "train loss:0.4921118008264358\n",
      "train loss:0.5267354065523533\n",
      "train loss:0.43944064364811963\n",
      "train loss:0.4258592518964123\n",
      "train loss:0.6128458630858772\n",
      "train loss:0.4715415172211045\n",
      "train loss:0.47587622887443565\n",
      "train loss:0.5723433474825426\n",
      "train loss:0.404923810785728\n",
      "train loss:0.5835318298746228\n",
      "train loss:0.40642172318777203\n",
      "train loss:0.39971720029408603\n",
      "train loss:0.5742735085896701\n",
      "train loss:0.3740967693319189\n",
      "train loss:0.4333109515120496\n",
      "train loss:0.5395382294771207\n",
      "train loss:0.3826139764583638\n",
      "train loss:0.4617242755180445\n",
      "train loss:0.5292959189117775\n",
      "train loss:0.593345107219944\n",
      "train loss:0.5061284283598163\n",
      "train loss:0.4982947597004101\n",
      "train loss:0.41029780924247794\n",
      "train loss:0.5086964394669624\n",
      "train loss:0.6232849739338591\n",
      "train loss:0.6675618721534734\n",
      "train loss:0.5182817700973851\n",
      "train loss:0.468388373137427\n",
      "train loss:0.5365520939446276\n",
      "train loss:0.4059942869937833\n",
      "train loss:0.5360822604069806\n",
      "train loss:0.48460160418960413\n",
      "train loss:0.5614883080479326\n",
      "train loss:0.4144897452402151\n",
      "train loss:0.5285282187710333\n",
      "train loss:0.4590405997595528\n",
      "train loss:0.5564360889796569\n",
      "train loss:0.5681305461369843\n",
      "train loss:0.4649609029256922\n",
      "train loss:0.6126872509160062\n",
      "train loss:0.5342942095094517\n",
      "train loss:0.3801269640254295\n",
      "train loss:0.44595822273345964\n",
      "train loss:0.5698112552676937\n",
      "train loss:0.5484219472514674\n",
      "train loss:0.4506049326482403\n",
      "train loss:0.6213129334825697\n",
      "train loss:0.43664213881921277\n",
      "train loss:0.5554629903775049\n",
      "train loss:0.4699477470788366\n",
      "train loss:0.4775420779643806\n",
      "train loss:0.6164944126396429\n",
      "train loss:0.5220912217713279\n",
      "train loss:0.4946274436484545\n",
      "train loss:0.46482155741325804\n",
      "train loss:0.5986950540180046\n",
      "train loss:0.6061494531442555\n",
      "train loss:0.5933029730860193\n",
      "train loss:0.35101561010782084\n",
      "train loss:0.49970789258310766\n",
      "train loss:0.5244137313795706\n",
      "train loss:0.4445854991376664\n",
      "train loss:0.525820869596827\n",
      "train loss:0.5543300530783102\n",
      "train loss:0.5660401855526775\n",
      "train loss:0.43940777171287676\n",
      "train loss:0.5312627268709765\n",
      "train loss:0.4032587874426233\n",
      "train loss:0.32028455871001876\n",
      "train loss:0.5765648453661367\n",
      "train loss:0.5443237881953751\n",
      "train loss:0.4921581817588118\n",
      "train loss:0.5930428397332813\n",
      "train loss:0.5320445390251128\n",
      "train loss:0.49281657752841945\n",
      "train loss:0.5459003710310536\n",
      "train loss:0.4740178101318652\n",
      "train loss:0.4466924648666212\n",
      "train loss:0.43255807659358825\n",
      "train loss:0.36165940788856105\n",
      "train loss:0.47937532287761336\n",
      "train loss:0.5750316070522562\n",
      "train loss:0.5666649638008794\n",
      "train loss:0.4884708204954324\n",
      "train loss:0.6301047085665192\n",
      "train loss:0.4527395692511454\n",
      "train loss:0.44771578462536\n",
      "train loss:0.4185394110260258\n",
      "train loss:0.4899591404878823\n",
      "train loss:0.3938296881944545\n",
      "train loss:0.3670476610040134\n",
      "train loss:0.3407811843723277\n",
      "train loss:0.46666225592987837\n",
      "train loss:0.3971596713849707\n",
      "train loss:0.5004367336122272\n",
      "train loss:0.5224106126556516\n",
      "train loss:0.5204844255114471\n",
      "train loss:0.4945997082747735\n",
      "train loss:0.5277605056360958\n",
      "train loss:0.6117926865817473\n",
      "train loss:0.44115610759143115\n",
      "train loss:0.4029201256014555\n",
      "train loss:0.5190053883155352\n",
      "train loss:0.5296486151334263\n",
      "train loss:0.632245746509538\n",
      "train loss:0.3885959186532524\n",
      "train loss:0.4193827401728877\n",
      "train loss:0.5251447465782201\n",
      "train loss:0.4977283269409289\n",
      "train loss:0.46504631995845086\n",
      "train loss:0.41961368245633685\n",
      "train loss:0.3605452797854344\n",
      "train loss:0.45427856811831296\n",
      "train loss:0.49580004487965296\n",
      "train loss:0.44632194259873004\n",
      "train loss:0.634290635628563\n",
      "train loss:0.5326674294803059\n",
      "train loss:0.438886483628821\n",
      "train loss:0.48972656410433246\n",
      "train loss:0.456890449370704\n",
      "train loss:0.49453633894732113\n",
      "train loss:0.4426944035074789\n",
      "train loss:0.5369111360001837\n",
      "train loss:0.5621485495234692\n",
      "train loss:0.38518242543743236\n",
      "train loss:0.5425141439204908\n",
      "train loss:0.6145490463103634\n",
      "train loss:0.5283042372027971\n",
      "train loss:0.6121354082048952\n",
      "train loss:0.45181084390590553\n",
      "train loss:0.39157950255545365\n",
      "train loss:0.5550773630484996\n",
      "train loss:0.5084905671181559\n",
      "train loss:0.43961253626375685\n",
      "train loss:0.46055789003172315\n",
      "train loss:0.45792315200645617\n",
      "train loss:0.5918461811088227\n",
      "train loss:0.39021320044295904\n",
      "train loss:0.41603968233292093\n",
      "train loss:0.3489243075646153\n",
      "train loss:0.4883473562974297\n",
      "train loss:0.4151281691594159\n",
      "train loss:0.5755124932610438\n",
      "train loss:0.6161189943225299\n",
      "train loss:0.514680194009965\n",
      "train loss:0.3879997121629263\n",
      "train loss:0.5602059892418203\n",
      "train loss:0.5166057122970118\n",
      "train loss:0.5323023278609554\n",
      "train loss:0.5126766395526612\n",
      "train loss:0.4987286868093386\n",
      "train loss:0.6074451197251832\n",
      "train loss:0.5852041586311683\n",
      "train loss:0.8249701933857599\n",
      "train loss:0.537836786052429\n",
      "train loss:0.5537110140479689\n",
      "train loss:0.5012596259884604\n",
      "train loss:0.4529290795916085\n",
      "train loss:0.5190912513639716\n",
      "train loss:0.7743618763923454\n",
      "train loss:0.4715628463410731\n",
      "train loss:0.581792274633775\n",
      "train loss:0.4303847655723244\n",
      "train loss:0.4334420083918867\n",
      "train loss:0.35220228825575284\n",
      "train loss:0.5322036451505651\n",
      "train loss:0.4793424917372221\n",
      "train loss:0.4781352375261345\n",
      "train loss:0.6158947653099268\n",
      "train loss:0.45825436398686725\n",
      "train loss:0.5329533721274202\n",
      "train loss:0.5380065540839073\n",
      "train loss:0.40321926447638634\n",
      "train loss:0.558248991025641\n",
      "train loss:0.5748600720177885\n",
      "train loss:0.5549965490069524\n",
      "train loss:0.5596408496329129\n",
      "train loss:0.48442991987725587\n",
      "train loss:0.40860854567907157\n",
      "train loss:0.48854028395284343\n",
      "train loss:0.546398074510041\n",
      "train loss:0.5594218528783572\n",
      "train loss:0.3721677000519221\n",
      "train loss:0.5904497811895538\n",
      "train loss:0.4663571312302202\n",
      "train loss:0.5544351937489919\n",
      "train loss:0.47311447661107986\n",
      "train loss:0.5541716566455613\n",
      "train loss:0.5484835985553367\n",
      "train loss:0.5330975589583383\n",
      "train loss:0.579078087521183\n",
      "train loss:0.5068704133102461\n",
      "train loss:0.4688795156449065\n",
      "train loss:0.5160362011674257\n",
      "train loss:0.5951478054601409\n",
      "train loss:0.6308364063492842\n",
      "train loss:0.426278669680128\n",
      "train loss:0.36343539900413757\n",
      "train loss:0.5465987801956504\n",
      "train loss:0.6098472097724986\n",
      "train loss:0.5363051604626056\n",
      "train loss:0.40665894124607893\n",
      "train loss:0.6366653330337091\n",
      "train loss:0.552202429310625\n",
      "train loss:0.5481568340306637\n",
      "train loss:0.622519107423026\n",
      "train loss:0.3891361796204265\n",
      "train loss:0.3836927688068382\n",
      "train loss:0.4299512532333302\n",
      "train loss:0.5298877225947954\n",
      "train loss:0.5043658268093199\n",
      "train loss:0.3017570632719404\n",
      "train loss:0.45518097618979647\n",
      "train loss:0.5219647494632665\n",
      "train loss:0.49609108296635546\n",
      "train loss:0.3882998459503586\n",
      "train loss:0.5931011119563944\n",
      "train loss:0.5666254269170458\n",
      "train loss:0.38818494628667494\n",
      "train loss:0.5103750386723059\n",
      "train loss:0.45843515070699914\n",
      "train loss:0.43575597042996655\n",
      "train loss:0.5200033769483556\n",
      "train loss:0.5293579098446223\n",
      "train loss:0.5194276294023864\n",
      "train loss:0.4536873219224107\n",
      "train loss:0.4218664267966781\n",
      "train loss:0.6060918386300923\n",
      "train loss:0.5692488431372044\n",
      "train loss:0.45086289948245684\n",
      "train loss:0.5084630079063339\n",
      "train loss:0.41954396306953584\n",
      "train loss:0.8871205519805998\n",
      "train loss:0.43045293670655516\n",
      "train loss:0.37401043841846365\n",
      "train loss:0.5156758504950956\n",
      "train loss:0.5288196222462411\n",
      "train loss:0.5317648585501246\n",
      "train loss:0.5107761041621789\n",
      "train loss:0.4643717920819328\n",
      "train loss:0.4647978036622781\n",
      "train loss:0.5302470084753172\n",
      "train loss:0.6504579609794783\n",
      "train loss:0.3466024661497265\n",
      "train loss:0.6322167725480119\n",
      "train loss:0.4101359714420777\n",
      "train loss:0.44775829339801454\n",
      "train loss:0.4984237474010168\n",
      "train loss:0.626447977097113\n",
      "train loss:0.4130558906719034\n",
      "train loss:0.36871982601473313\n",
      "train loss:0.45400840992021263\n",
      "train loss:0.4086659581155171\n",
      "train loss:0.530837351761636\n",
      "train loss:0.4071690990063865\n",
      "train loss:0.5217413267539628\n",
      "train loss:0.4547757124341375\n",
      "train loss:0.5158397913793855\n",
      "train loss:0.42435815516141157\n",
      "train loss:0.4861075437170114\n",
      "train loss:0.47437842757374676\n",
      "train loss:0.5628098616561352\n",
      "train loss:0.5508695925744407\n",
      "train loss:0.4333738961973066\n",
      "train loss:0.5538862508865356\n",
      "train loss:0.46649335215366894\n",
      "train loss:0.4115977567299708\n",
      "train loss:0.3951420556767607\n",
      "train loss:0.3869692784007242\n",
      "train loss:0.46198465099733127\n",
      "train loss:0.4189425761880045\n",
      "train loss:0.4625706730041308\n",
      "train loss:0.3493017746003014\n",
      "train loss:0.3911368908432577\n",
      "train loss:0.5230224537890098\n",
      "train loss:0.4985324699533238\n",
      "train loss:0.5169469716084192\n",
      "train loss:0.3987295602940612\n",
      "train loss:0.40018446523310736\n",
      "train loss:0.4604781441945443\n",
      "train loss:0.477492086604285\n",
      "train loss:0.56397708135609\n",
      "train loss:0.4708269974498963\n",
      "train loss:0.5215517467813074\n",
      "train loss:0.3802159181837465\n",
      "train loss:0.4885463567363377\n",
      "train loss:0.4072176883796913\n",
      "train loss:0.5871428380829692\n",
      "train loss:0.48311564929099887\n",
      "train loss:0.47768381035862156\n",
      "train loss:0.45410413835940344\n",
      "train loss:0.38404126687305296\n",
      "train loss:0.5154877161124173\n",
      "train loss:0.5174258057639914\n",
      "train loss:0.3460194495368233\n",
      "train loss:0.3988205643982877\n",
      "train loss:0.49422068097204397\n",
      "train loss:0.60127913396057\n",
      "train loss:0.6757853802608836\n",
      "train loss:0.5153388532682933\n",
      "train loss:0.5186211084283326\n",
      "train loss:0.44597762519945566\n",
      "train loss:0.5281945374944005\n",
      "train loss:0.5083016723582741\n",
      "train loss:0.39591976579462157\n",
      "train loss:0.4610796245156411\n",
      "train loss:0.40737036216031103\n",
      "train loss:0.43382707775103235\n",
      "train loss:0.5020573777228525\n",
      "train loss:0.35225274645565113\n",
      "train loss:0.5551886639110822\n",
      "train loss:0.6064148317253014\n",
      "train loss:0.4056194972553549\n",
      "train loss:0.4233307964174443\n",
      "train loss:0.50216923630672\n",
      "train loss:0.38500517588770733\n",
      "train loss:0.4839456762054234\n",
      "train loss:0.39982024781934944\n",
      "train loss:0.5316591791264134\n",
      "train loss:0.49787700121869927\n",
      "train loss:0.3478486858179568\n",
      "train loss:0.5699387754159482\n",
      "train loss:0.5822830198129797\n",
      "train loss:0.5132603811765659\n",
      "train loss:0.42454441785929886\n",
      "train loss:0.44908820299527064\n",
      "train loss:0.5419861438531663\n",
      "train loss:0.5794487773943778\n",
      "train loss:0.5198471949997939\n",
      "train loss:0.42768719448882186\n",
      "train loss:0.48609140007587404\n",
      "train loss:0.38435508767072035\n",
      "train loss:0.5318414297139761\n",
      "train loss:0.46227368298547994\n",
      "train loss:0.46433975654159737\n",
      "train loss:0.5172174513065615\n",
      "train loss:0.5823140275148443\n",
      "train loss:0.519431907854285\n",
      "train loss:0.5663964651242995\n",
      "train loss:0.6802050534012711\n",
      "train loss:0.37555807575047573\n",
      "train loss:0.6440120430097036\n",
      "train loss:0.44903334569997705\n",
      "train loss:0.576229528700343\n",
      "train loss:0.4529322051721057\n",
      "train loss:0.38696674035051304\n",
      "train loss:0.34832413960851305\n",
      "train loss:0.44058525090491796\n",
      "train loss:0.3619535430514127\n",
      "train loss:0.49553817730394534\n",
      "train loss:0.4278018436636222\n",
      "train loss:0.35278494717341574\n",
      "train loss:0.5416616981425457\n",
      "train loss:0.6938934988861637\n",
      "train loss:0.41022276900653437\n",
      "train loss:0.49543420529812804\n",
      "train loss:0.5284522618671381\n",
      "train loss:0.4535606527755464\n",
      "train loss:0.40772362325402745\n",
      "train loss:0.4139492133287915\n",
      "train loss:0.40201655219392096\n",
      "train loss:0.6310856591264795\n",
      "train loss:0.4263733334100401\n",
      "train loss:0.4922231892693684\n",
      "train loss:0.44674069260718513\n",
      "train loss:0.47031433183435056\n",
      "train loss:0.5427789627659334\n",
      "train loss:0.3498935019299975\n",
      "train loss:0.47521667265918993\n",
      "train loss:0.3985133096498168\n",
      "train loss:0.5200686419744078\n",
      "train loss:0.45846567868078814\n",
      "train loss:0.48049446592350653\n",
      "train loss:0.47233489981095794\n",
      "train loss:0.5249946362576078\n",
      "train loss:0.45142121108891364\n",
      "train loss:0.3826177853016269\n",
      "train loss:0.4624556241934263\n",
      "train loss:0.5135216634370129\n",
      "train loss:0.44969917577420015\n",
      "train loss:0.49730162373329356\n",
      "train loss:0.3870213615018937\n",
      "train loss:0.5482464313494417\n",
      "train loss:0.4194641172273981\n",
      "train loss:0.5124085915481199\n",
      "train loss:0.3916093663002173\n",
      "train loss:0.4804733606303691\n",
      "train loss:0.49122085755367495\n",
      "train loss:0.39771079955553934\n",
      "train loss:0.4404368360111535\n",
      "train loss:0.40074085623903927\n",
      "train loss:0.5612624293462907\n",
      "train loss:0.3875027769902266\n",
      "train loss:0.4572818087406311\n",
      "train loss:0.5247059928724219\n",
      "train loss:0.39466027567911316\n",
      "train loss:0.40842368350649116\n",
      "train loss:0.5773055917966982\n",
      "train loss:0.5451498469919707\n",
      "train loss:0.4647397591257952\n",
      "train loss:0.5000802537684027\n",
      "train loss:0.4907645299087382\n",
      "train loss:0.44060255483603844\n",
      "train loss:0.5498672894929395\n",
      "train loss:0.5693540823756679\n",
      "train loss:0.41074520095558087\n",
      "train loss:0.6163308849950124\n",
      "train loss:0.6687180170363652\n",
      "train loss:0.4364077856912031\n",
      "train loss:0.4946741976042099\n",
      "train loss:0.46200510913584336\n",
      "train loss:0.4690032023988169\n",
      "train loss:0.4071991077400242\n",
      "train loss:0.3394482759544875\n",
      "train loss:0.5931944864230168\n",
      "train loss:0.4369166335564647\n",
      "train loss:0.5493137577779318\n",
      "train loss:0.4552028275344229\n",
      "train loss:0.6046155965276974\n",
      "train loss:0.5889459934091598\n",
      "train loss:0.45790805029110493\n",
      "train loss:0.4922314154514862\n",
      "train loss:0.5123938370577548\n",
      "train loss:0.5557529216078637\n",
      "train loss:0.48114640102651474\n",
      "train loss:0.514963158378875\n",
      "train loss:0.4259978032397659\n",
      "train loss:0.3738028899782782\n",
      "train loss:0.453720323964767\n",
      "train loss:0.48216983379716155\n",
      "train loss:0.3477290844092898\n",
      "train loss:0.34831794951568346\n",
      "train loss:0.3773741329836532\n",
      "train loss:0.7176977691570456\n",
      "train loss:0.4766494478043647\n",
      "train loss:0.553528529637884\n",
      "train loss:0.49062980617388113\n",
      "train loss:0.47616764473244017\n",
      "train loss:0.4580026693383965\n",
      "train loss:0.5345289806642268\n",
      "train loss:0.4920375846994285\n",
      "train loss:0.46690356020309765\n",
      "train loss:0.49196152595925674\n",
      "train loss:0.5725835703184564\n",
      "train loss:0.4042211485657768\n",
      "train loss:0.3986252220134386\n",
      "train loss:0.44512426017634404\n",
      "train loss:0.4546492783773436\n",
      "train loss:0.47195929323715385\n",
      "train loss:0.4396807732391699\n",
      "train loss:0.5197785518777593\n",
      "train loss:0.4228013993536544\n",
      "train loss:0.3882406957223505\n",
      "train loss:0.4230836199284001\n",
      "train loss:0.4109135802928639\n",
      "train loss:0.42060619111280784\n",
      "train loss:0.36164560790476685\n",
      "train loss:0.40930313315776495\n",
      "train loss:0.4318736162366544\n",
      "train loss:0.41938116070384496\n",
      "train loss:0.5121583789971407\n",
      "train loss:0.5261561281011148\n",
      "train loss:0.5386752263576268\n",
      "train loss:0.5249620616331542\n",
      "train loss:0.37881928094533235\n",
      "train loss:0.5916959515522854\n",
      "=== epoch:10, train acc:0.765, test acc:0.769 ===\n",
      "train loss:0.3307140407548006\n",
      "train loss:0.49511956419440517\n",
      "train loss:0.3958758056853691\n",
      "train loss:0.4403266427013987\n",
      "train loss:0.4707757074396832\n",
      "train loss:0.4439343386677105\n",
      "train loss:0.4731047407447096\n",
      "train loss:0.5823021712979831\n",
      "train loss:0.3467679695373666\n",
      "train loss:0.5639679770518078\n",
      "train loss:0.5976680475851373\n",
      "train loss:0.5668388334684541\n",
      "train loss:0.46798132007739957\n",
      "train loss:0.5628271286075701\n",
      "train loss:0.5249533698961875\n",
      "train loss:0.5039848148445517\n",
      "train loss:0.5869896620851665\n",
      "train loss:0.603434844475135\n",
      "train loss:0.3905292583457856\n",
      "train loss:0.4967302091758027\n",
      "train loss:0.38677052498466813\n",
      "train loss:0.46727869834462793\n",
      "train loss:0.6182973930897837\n",
      "train loss:0.5457249637580858\n",
      "train loss:0.5263831679642316\n",
      "train loss:0.45699828435792\n",
      "train loss:0.3927387138092525\n",
      "train loss:0.38432545220680014\n",
      "train loss:0.5506149345031252\n",
      "train loss:0.4384175615954756\n",
      "train loss:0.6053384748641322\n",
      "train loss:0.38554605913068923\n",
      "train loss:0.3355846547028573\n",
      "train loss:0.4699845812362203\n",
      "train loss:0.4476924066496765\n",
      "train loss:0.47055407389927933\n",
      "train loss:0.49481278001259604\n",
      "train loss:0.4290561647429288\n",
      "train loss:0.6291696492616028\n",
      "train loss:0.4953640172986051\n",
      "train loss:0.49723929871188033\n",
      "train loss:0.6760117279657878\n",
      "train loss:0.4357638590744153\n",
      "train loss:0.4391940245400216\n",
      "train loss:0.5700235237534986\n",
      "train loss:0.3471391895555408\n",
      "train loss:0.3880807300225325\n",
      "train loss:0.46521598286178145\n",
      "train loss:0.43776134498765634\n",
      "train loss:0.4368845965100856\n",
      "train loss:0.5164216873364841\n",
      "train loss:0.46069234371292545\n",
      "train loss:0.5137684737621346\n",
      "train loss:0.5731657552804311\n",
      "train loss:0.5483738297741645\n",
      "train loss:0.575751732874273\n",
      "train loss:0.537593007507172\n",
      "train loss:0.45850372105047277\n",
      "train loss:0.4840562629357759\n",
      "train loss:0.46881231666435125\n",
      "train loss:0.4574773957143018\n",
      "train loss:0.4606721451262869\n",
      "train loss:0.3902390632027671\n",
      "train loss:0.5565929122464801\n",
      "train loss:0.50527575907609\n",
      "train loss:0.607037560278563\n",
      "train loss:0.47967621244601977\n",
      "train loss:0.55025040879984\n",
      "train loss:0.6343526378936654\n",
      "train loss:0.4884682223690925\n",
      "train loss:0.4974518010919834\n",
      "train loss:0.4107018445699432\n",
      "train loss:0.596607749708265\n",
      "train loss:0.5660463497600857\n",
      "train loss:0.5348291126003288\n",
      "train loss:0.4462021777775118\n",
      "train loss:0.3404086920164104\n",
      "train loss:0.6545056016980628\n",
      "train loss:0.4868073455178398\n",
      "train loss:0.4640007615006522\n",
      "train loss:0.6117370584413492\n",
      "train loss:0.4785265851680821\n",
      "train loss:0.5000673920786802\n",
      "train loss:0.4991360369636344\n",
      "train loss:0.6015552707469729\n",
      "train loss:0.501520531561434\n",
      "train loss:0.48825833598546103\n",
      "train loss:0.4485407673308099\n",
      "train loss:0.41510461772764473\n",
      "train loss:0.42205183157963433\n",
      "train loss:0.48349964318187694\n",
      "train loss:0.36475327565384574\n",
      "train loss:0.4659275566327457\n",
      "train loss:0.465477023806071\n",
      "train loss:0.4952848718958818\n",
      "train loss:0.46246525467905975\n",
      "train loss:0.34711945405934264\n",
      "train loss:0.47181545561677124\n",
      "train loss:0.6675138925845647\n",
      "train loss:0.5868871335960827\n",
      "train loss:0.4366089014277936\n",
      "train loss:0.4280687067691343\n",
      "train loss:0.48772421167948954\n",
      "train loss:0.4610810494800277\n",
      "train loss:0.3413469805908405\n",
      "train loss:0.47195577790946686\n",
      "train loss:0.46316062490313137\n",
      "train loss:0.5353380242146387\n",
      "train loss:0.4667687503947498\n",
      "train loss:0.46868789290928475\n",
      "train loss:0.6088252596771568\n",
      "train loss:0.5450238117263763\n",
      "train loss:0.4672085272396629\n",
      "train loss:0.4749984328222156\n",
      "train loss:0.6305423861989662\n",
      "train loss:0.47607526184520327\n",
      "train loss:0.4803022886331086\n",
      "train loss:0.4780291352351012\n",
      "train loss:0.39645378893326133\n",
      "train loss:0.4764506562036727\n",
      "train loss:0.4569480582175936\n",
      "train loss:0.5346497672095658\n",
      "train loss:0.6002926404718727\n",
      "train loss:0.4144306155304156\n",
      "train loss:0.5013578054310223\n",
      "train loss:0.5153610241403447\n",
      "train loss:0.42393930787523415\n",
      "train loss:0.5256737131641835\n",
      "train loss:0.6781397080827308\n",
      "train loss:0.6099047631521868\n",
      "train loss:0.4679153373091718\n",
      "train loss:0.3586985002792752\n",
      "train loss:0.49176924877906375\n",
      "train loss:0.46786962944219695\n",
      "train loss:0.38286323175761344\n",
      "train loss:0.4451769439655879\n",
      "train loss:0.3308956193778599\n",
      "train loss:0.42527766086900487\n",
      "train loss:0.3641739042437293\n",
      "train loss:0.5576757937178972\n",
      "train loss:0.46999888628304914\n",
      "train loss:0.4833655040355215\n",
      "train loss:0.46724125248467074\n",
      "train loss:0.4840420648062031\n",
      "train loss:0.7090459373820645\n",
      "train loss:0.4729941214955278\n",
      "train loss:0.5434054197434413\n",
      "train loss:0.34670329791320526\n",
      "train loss:0.5413353056934427\n",
      "train loss:0.5324205369273906\n",
      "train loss:0.5311283396097219\n",
      "train loss:0.620603328256464\n",
      "train loss:0.4697524144604581\n",
      "train loss:0.7753531232604567\n",
      "train loss:0.5222896538556676\n",
      "train loss:0.5573139621295107\n",
      "train loss:0.522495786623167\n",
      "train loss:0.5139474167399771\n",
      "train loss:0.4075236825634799\n",
      "train loss:0.6421994177454746\n",
      "train loss:0.4073281037200942\n",
      "train loss:0.37669290080325346\n",
      "train loss:0.440666111488955\n",
      "train loss:0.4515137911985748\n",
      "train loss:0.4585534182334189\n",
      "train loss:0.4634843950569468\n",
      "train loss:0.5680590111897544\n",
      "train loss:0.37954699811059334\n",
      "train loss:0.44857224500681037\n",
      "train loss:0.37752262205862125\n",
      "train loss:0.515390577553921\n",
      "train loss:0.519669488969811\n",
      "train loss:0.556724606143306\n",
      "train loss:0.5291637573818924\n",
      "train loss:0.39245923088653956\n",
      "train loss:0.4727833766192486\n",
      "train loss:0.4649716520848763\n",
      "train loss:0.5800769641274516\n",
      "train loss:0.40043849138310345\n",
      "train loss:0.5723858972915133\n",
      "train loss:0.4372039160828156\n",
      "train loss:0.4092872293922673\n",
      "train loss:0.5407662493636699\n",
      "train loss:0.38633865530762734\n",
      "train loss:0.3911063749010441\n",
      "train loss:0.48193422966771143\n",
      "train loss:0.4698950207402515\n",
      "train loss:0.4955420475145273\n",
      "train loss:0.44220392504091577\n",
      "train loss:0.6342601202416759\n",
      "train loss:0.40207714758373847\n",
      "train loss:0.4456277958329846\n",
      "train loss:0.49219192875053425\n",
      "train loss:0.6069292494514661\n",
      "train loss:0.47729927094356756\n",
      "train loss:0.49858109825889846\n",
      "train loss:0.45126885133943945\n",
      "train loss:0.5040113182894088\n",
      "train loss:0.48260487939440305\n",
      "train loss:0.5687906817299901\n",
      "train loss:0.39326955622806664\n",
      "train loss:0.5882566885394922\n",
      "train loss:0.3990325111956684\n",
      "train loss:0.47527533548304146\n",
      "train loss:0.6286470247229983\n",
      "train loss:0.4356292531916005\n",
      "train loss:0.4362987362521485\n",
      "train loss:0.36917704189626965\n",
      "train loss:0.5730464116561453\n",
      "train loss:0.5907598036142069\n",
      "train loss:0.4658270444466066\n",
      "train loss:0.4114932511380715\n",
      "train loss:0.5681265471555075\n",
      "train loss:0.5541767806068542\n",
      "train loss:0.3685384083840322\n",
      "train loss:0.5318290008453253\n",
      "train loss:0.5390669207377947\n",
      "train loss:0.6147075560950844\n",
      "train loss:0.49978619481821185\n",
      "train loss:0.5976753901570343\n",
      "train loss:0.44635144466292537\n",
      "train loss:0.4509224169328936\n",
      "train loss:0.49606098878648863\n",
      "train loss:0.34699793003175006\n",
      "train loss:0.5029885931075808\n",
      "train loss:0.47016706284678844\n",
      "train loss:0.43551401674005297\n",
      "train loss:0.3675605511804003\n",
      "train loss:0.46832608822578015\n",
      "train loss:0.5013651338597185\n",
      "train loss:0.5143365184257971\n",
      "train loss:0.531996587986296\n",
      "train loss:0.5229346017711641\n",
      "train loss:0.5410380280676056\n",
      "train loss:0.421830666079939\n",
      "train loss:0.4654885137010226\n",
      "train loss:0.42828717449857956\n",
      "train loss:0.5420053015821326\n",
      "train loss:0.46750700399895523\n",
      "train loss:0.4041923124655217\n",
      "train loss:0.5860884629910939\n",
      "train loss:0.5150691702969821\n",
      "train loss:0.44082514151304486\n",
      "train loss:0.6409744998781547\n",
      "train loss:0.5739140301549849\n",
      "train loss:0.4084615049140022\n",
      "train loss:0.3854500589259008\n",
      "train loss:0.5720155741426285\n",
      "train loss:0.46188274819373704\n",
      "train loss:0.6441338906768181\n",
      "train loss:0.545170884953922\n",
      "train loss:0.4462890378734098\n",
      "train loss:0.5006313421239246\n",
      "train loss:0.4791532423067709\n",
      "train loss:0.5601050175397494\n",
      "train loss:0.49995242648363714\n",
      "train loss:0.46500911303175263\n",
      "train loss:0.45713552364028354\n",
      "train loss:0.54471198427866\n",
      "train loss:0.5308986230741788\n",
      "train loss:0.33928190149235626\n",
      "train loss:0.464443698152441\n",
      "train loss:0.4542701405967886\n",
      "train loss:0.34077368955487913\n",
      "train loss:0.5510870699609945\n",
      "train loss:0.5258391150843459\n",
      "train loss:0.5476264785486201\n",
      "train loss:0.31397718778841555\n",
      "train loss:0.581838227655847\n",
      "train loss:0.541786088775745\n",
      "train loss:0.5392321748593016\n",
      "train loss:0.5544305329102758\n",
      "train loss:0.6018664747727737\n",
      "train loss:0.5194978045486409\n",
      "train loss:0.42646082700291915\n",
      "train loss:0.43494243930302756\n",
      "train loss:0.4626124272802683\n",
      "train loss:0.4360531581576515\n",
      "train loss:0.4415851479466194\n",
      "train loss:0.4091458531658729\n",
      "train loss:0.369542652670629\n",
      "train loss:0.4578670341959081\n",
      "train loss:0.41307811994997906\n",
      "train loss:0.4985792856038379\n",
      "train loss:0.48836018889436167\n",
      "train loss:0.49718442887347386\n",
      "train loss:0.48893256509322813\n",
      "train loss:0.45459505102703557\n",
      "train loss:0.40954874326485013\n",
      "train loss:0.3551370990921234\n",
      "train loss:0.44458756670634\n",
      "train loss:0.4975639116002044\n",
      "train loss:0.43416633972960794\n",
      "train loss:0.5130724804523388\n",
      "train loss:0.5181443748555137\n",
      "train loss:0.48219580085554986\n",
      "train loss:0.4542297965209935\n",
      "train loss:0.42830029145794646\n",
      "train loss:0.4207379486429528\n",
      "train loss:0.45541212841522966\n",
      "train loss:0.5371177764846362\n",
      "train loss:0.4890408122650609\n",
      "train loss:0.3574625767635233\n",
      "train loss:0.32777536697543685\n",
      "train loss:0.6198996238110626\n",
      "train loss:0.4035432015681489\n",
      "train loss:0.6766805579979\n",
      "train loss:0.41652934168516126\n",
      "train loss:0.400641161792091\n",
      "train loss:0.35830059122040386\n",
      "train loss:0.4316197944544375\n",
      "train loss:0.451551067295888\n",
      "train loss:0.4473936948731192\n",
      "train loss:0.5730651396754584\n",
      "train loss:0.39681513456003903\n",
      "train loss:0.5004354832259544\n",
      "train loss:0.35564435719675197\n",
      "train loss:0.4246040269030065\n",
      "train loss:0.42845839069592495\n",
      "train loss:0.4337679511529234\n",
      "train loss:0.7793128251839779\n",
      "train loss:0.43360436581110806\n",
      "train loss:0.42871016145862584\n",
      "train loss:0.6436920853399687\n",
      "train loss:0.4632649649946207\n",
      "train loss:0.3507745738975486\n",
      "train loss:0.4792341967071729\n",
      "train loss:0.5428494184003657\n",
      "train loss:0.40131917359987573\n",
      "train loss:0.485843515696095\n",
      "train loss:0.34641440899166265\n",
      "train loss:0.4577869654648947\n",
      "train loss:0.5945700115501169\n",
      "train loss:0.41701815681672194\n",
      "train loss:0.6312631764972122\n",
      "train loss:0.33194762320153587\n",
      "train loss:0.4754466711589614\n",
      "train loss:0.7150046682833556\n",
      "train loss:0.5414127173948422\n",
      "train loss:0.42178841107589876\n",
      "train loss:0.4834811295981813\n",
      "train loss:0.5908734201880073\n",
      "train loss:0.5468354142284321\n",
      "train loss:0.4768980378129386\n",
      "train loss:0.3561865563077387\n",
      "train loss:0.615065838195298\n",
      "train loss:0.5923450964254826\n",
      "train loss:0.3080414829755419\n",
      "train loss:0.4021254648459046\n",
      "train loss:0.648065074643106\n",
      "train loss:0.45597231998068305\n",
      "train loss:0.4395437126583684\n",
      "train loss:0.4875121684644644\n",
      "train loss:0.4876206029771788\n",
      "train loss:0.4548854253571218\n",
      "train loss:0.4215162824928224\n",
      "train loss:0.47305788611878624\n",
      "train loss:0.43335639763906797\n",
      "train loss:0.38479878399106754\n",
      "train loss:0.43473811340816154\n",
      "train loss:0.456436762025259\n",
      "train loss:0.49850102362329257\n",
      "train loss:0.5101132144101352\n",
      "train loss:0.3439949128462838\n",
      "train loss:0.39597670533967944\n",
      "train loss:0.4312623009157606\n",
      "train loss:0.5109879172325452\n",
      "train loss:0.5385838766667775\n",
      "train loss:0.49423269354554944\n",
      "train loss:0.5394168814968281\n",
      "train loss:0.4752697338340699\n",
      "train loss:0.645695972327881\n",
      "train loss:0.36709676897797733\n",
      "train loss:0.48859762008923013\n",
      "train loss:0.39261553261007814\n",
      "train loss:0.45738297544034556\n",
      "train loss:0.41230247903606343\n",
      "train loss:0.5134286426755393\n",
      "train loss:0.42071952201357943\n",
      "train loss:0.5598272256557888\n",
      "train loss:0.37822744500367184\n",
      "train loss:0.44799785112521434\n",
      "train loss:0.4452570680040349\n",
      "train loss:0.4503201357556105\n",
      "train loss:0.8571265928923637\n",
      "train loss:0.5965696064213669\n",
      "train loss:0.40877396089322354\n",
      "train loss:0.5790447852840485\n",
      "train loss:0.48309539663336326\n",
      "train loss:0.5143937473766811\n",
      "train loss:0.44382059465595025\n",
      "train loss:0.39844272408841597\n",
      "train loss:0.6322783774285362\n",
      "train loss:0.6721796471143283\n",
      "train loss:0.35597031837407345\n",
      "train loss:0.42392468727881794\n",
      "train loss:0.3913017001435828\n",
      "train loss:0.4598267937117175\n",
      "train loss:0.39338454239100995\n",
      "train loss:0.351715449747333\n",
      "train loss:0.54886639484403\n",
      "train loss:0.3426435600263379\n",
      "train loss:0.4418725164282439\n",
      "train loss:0.374951352480831\n",
      "train loss:0.4802973229771102\n",
      "train loss:0.5552490863965714\n",
      "train loss:0.37334385809628556\n",
      "train loss:0.4543717316041713\n",
      "train loss:0.4661155586629904\n",
      "train loss:0.4990203333688509\n",
      "train loss:0.40189151728383493\n",
      "train loss:0.41608410504436577\n",
      "train loss:0.6553713497589823\n",
      "train loss:0.5262532029695631\n",
      "train loss:0.49106844573561304\n",
      "train loss:0.533401213126107\n",
      "train loss:0.5368573571287554\n",
      "train loss:0.46850027530629595\n",
      "train loss:0.4140668037699588\n",
      "train loss:0.47982066109992777\n",
      "train loss:0.5010326390764888\n",
      "train loss:0.5062649179748727\n",
      "train loss:0.4088171597074971\n",
      "train loss:0.5852629602707541\n",
      "train loss:0.4761127103517768\n",
      "train loss:0.5163837539855733\n",
      "train loss:0.5018611302077204\n",
      "train loss:0.4773774570992502\n",
      "train loss:0.5474242188109801\n",
      "train loss:0.42649970051577024\n",
      "train loss:0.5346224459084878\n",
      "train loss:0.3100212996086962\n",
      "train loss:0.5058541052658394\n",
      "train loss:0.34594876420305604\n",
      "train loss:0.6289316091398515\n",
      "train loss:0.597018629688005\n",
      "train loss:0.5335215315709961\n",
      "train loss:0.4328230889910572\n",
      "train loss:0.5492303741865605\n",
      "train loss:0.558536456954284\n",
      "train loss:0.5370413745836333\n",
      "train loss:0.5723006271852281\n",
      "train loss:0.4694692325885585\n",
      "train loss:0.43711034160012363\n",
      "train loss:0.6107482582228544\n",
      "train loss:0.45810965427897443\n",
      "train loss:0.6303009072887064\n",
      "train loss:0.6572325874313694\n",
      "train loss:0.5376273263433431\n",
      "train loss:0.5089068819261336\n",
      "train loss:0.4605472369225561\n",
      "train loss:0.4932611600816566\n",
      "train loss:0.5008255259491428\n",
      "train loss:0.501133052801064\n",
      "train loss:0.42770706299975153\n",
      "train loss:0.5365422609511583\n",
      "train loss:0.5165731219759752\n",
      "train loss:0.5867697147431246\n",
      "train loss:0.4838328894858811\n",
      "train loss:0.47980878697919577\n",
      "train loss:0.5670713176847352\n",
      "train loss:0.5358228607562693\n",
      "train loss:0.45567469780797576\n",
      "train loss:0.39289481761383016\n",
      "train loss:0.5335168889002211\n",
      "train loss:0.4993272080838611\n",
      "train loss:0.38079092374098805\n",
      "train loss:0.5032120083924343\n",
      "train loss:0.5967267966344799\n",
      "train loss:0.47826145075741067\n",
      "train loss:0.3879749775591744\n",
      "train loss:0.42521264577255147\n",
      "train loss:0.32498993685513555\n",
      "train loss:0.4760231463246022\n",
      "train loss:0.47548943318878295\n",
      "train loss:0.5469241400163499\n",
      "train loss:0.5447308245659761\n",
      "train loss:0.49498467892501125\n",
      "train loss:0.5616172563342473\n",
      "train loss:0.44399928763800245\n",
      "train loss:0.5808910264347673\n",
      "train loss:0.421250336975196\n",
      "train loss:0.46287073915049404\n",
      "train loss:0.5429147559620813\n",
      "train loss:0.4302940851566736\n",
      "train loss:0.5384335565886059\n",
      "train loss:0.5029667069639061\n",
      "train loss:0.528026371676238\n",
      "train loss:0.4806961510184171\n",
      "train loss:0.48942336936942205\n",
      "train loss:0.5040703090958902\n",
      "train loss:0.3183000395540593\n",
      "train loss:0.3700795366780595\n",
      "train loss:0.4768546255110367\n",
      "train loss:0.4738893340236877\n",
      "train loss:0.47793924294477824\n",
      "train loss:0.46496820270311695\n",
      "train loss:0.3709143767821306\n",
      "train loss:0.4375371868629944\n",
      "train loss:0.49041837352237194\n",
      "train loss:0.545240412583576\n",
      "train loss:0.5257410942925036\n",
      "train loss:0.4173453121997132\n",
      "train loss:0.4047644187857698\n",
      "train loss:0.4602470952533942\n",
      "train loss:0.4386984346148071\n",
      "train loss:0.42245969466850075\n",
      "train loss:0.5125386923194812\n",
      "train loss:0.49521403114918655\n",
      "train loss:0.45033707293540914\n",
      "train loss:0.5381232499748846\n",
      "train loss:0.4187972013507698\n",
      "train loss:0.4920709703399795\n",
      "train loss:0.6613950423068209\n",
      "train loss:0.5281677434988081\n",
      "train loss:0.5488147338470566\n",
      "train loss:0.5216106226850211\n",
      "train loss:0.3761539294401049\n",
      "train loss:0.5021017686270245\n",
      "train loss:0.44854770082914847\n",
      "train loss:0.47010197643512713\n",
      "train loss:0.37256860381724927\n",
      "train loss:0.4760426791478741\n",
      "train loss:0.4604847024318692\n",
      "train loss:0.5078984395834599\n",
      "train loss:0.38537591021518336\n",
      "train loss:0.41157854513298886\n",
      "train loss:0.3795596662767658\n",
      "train loss:0.37812034994770033\n",
      "train loss:0.41542581029236464\n",
      "train loss:0.49678586265553265\n",
      "train loss:0.40894643056281127\n",
      "train loss:0.41707591417292084\n",
      "train loss:0.3761516682841484\n",
      "train loss:0.6174392511038146\n",
      "train loss:0.49848207758014706\n",
      "train loss:0.40611648360835545\n",
      "train loss:0.45843042767098313\n",
      "train loss:0.4854643563411013\n",
      "train loss:0.497472218352091\n",
      "train loss:0.49992380787366497\n",
      "train loss:0.45320644512424657\n",
      "train loss:0.3546389159319394\n",
      "train loss:0.44309649132096907\n",
      "train loss:0.4341071355580323\n",
      "train loss:0.3512636327228087\n",
      "train loss:0.44443131108636896\n",
      "train loss:0.43325826551108504\n",
      "train loss:0.6687323296981343\n",
      "train loss:0.4858330336709074\n",
      "train loss:0.37569284658535546\n",
      "train loss:0.5489075995592824\n",
      "train loss:0.39319239334594697\n",
      "train loss:0.41565187379630275\n",
      "train loss:0.6293228153859757\n",
      "train loss:0.502405113492511\n",
      "train loss:0.6063392202077084\n",
      "train loss:0.46447369346081474\n",
      "train loss:0.5052786285488551\n",
      "train loss:0.6764354766705226\n",
      "train loss:0.3986279333214886\n",
      "train loss:0.5953355863384848\n",
      "train loss:0.6637603539914657\n",
      "train loss:0.465333522984977\n",
      "train loss:0.4338887065861149\n",
      "train loss:0.4159500977555637\n",
      "train loss:0.4661711584170978\n",
      "train loss:0.407686254421901\n",
      "train loss:0.5996821107415785\n",
      "train loss:0.39943111511050206\n",
      "train loss:0.45990391310943124\n",
      "train loss:0.4998311206587689\n",
      "train loss:0.5088159352397333\n",
      "train loss:0.4851539716432237\n",
      "train loss:0.42107379962798147\n",
      "train loss:0.3648919313288566\n",
      "train loss:0.7982475518054524\n",
      "train loss:0.5178434604323301\n",
      "train loss:0.36425071921308855\n",
      "train loss:0.5694842383191893\n",
      "train loss:0.5796259461075486\n",
      "train loss:0.47482180617250935\n",
      "train loss:0.454511906987543\n",
      "train loss:0.3993960366211073\n",
      "train loss:0.3655986267574953\n",
      "train loss:0.3245919056454559\n",
      "train loss:0.47679629763537323\n",
      "train loss:0.3568475365113597\n",
      "train loss:0.3895152851524751\n",
      "train loss:0.33151379562369115\n",
      "train loss:0.36642338968190624\n",
      "train loss:0.5389869741615299\n",
      "train loss:0.47226907278296415\n",
      "train loss:0.48763846366591856\n",
      "train loss:0.31054699805025543\n",
      "train loss:0.40498627447201424\n",
      "train loss:0.4367558649563202\n",
      "train loss:0.4880885066626264\n",
      "train loss:0.5419216312575197\n",
      "train loss:0.5083952412286622\n",
      "=== epoch:11, train acc:0.767, test acc:0.779 ===\n",
      "train loss:0.41808667025257323\n",
      "train loss:0.5583734991071171\n",
      "train loss:0.44427533226076654\n",
      "train loss:0.5067164510358514\n",
      "train loss:0.400048444422579\n",
      "train loss:0.39249865642794995\n",
      "train loss:0.3752737118937951\n",
      "train loss:0.47818318578791164\n",
      "train loss:0.45760303517592293\n",
      "train loss:0.3807747107477904\n",
      "train loss:0.3436217577140735\n",
      "train loss:0.4077174949248114\n",
      "train loss:0.30911462830900777\n",
      "train loss:0.4702489000964655\n",
      "train loss:0.5060433279829948\n",
      "train loss:0.47022247635732894\n",
      "train loss:0.41010236888127294\n",
      "train loss:0.4194019244135951\n",
      "train loss:0.4589798931000255\n",
      "train loss:0.5223961747248554\n",
      "train loss:0.5247176101357419\n",
      "train loss:0.531101038129298\n",
      "train loss:0.4126128488650908\n",
      "train loss:0.4566203421596039\n",
      "train loss:0.5353889427684091\n",
      "train loss:0.42179786972768196\n",
      "train loss:0.5305458622386239\n",
      "train loss:0.39101172322478683\n",
      "train loss:0.4587301063697195\n",
      "train loss:0.4173781240987173\n",
      "train loss:0.4799142026171552\n",
      "train loss:0.5383850495029026\n",
      "train loss:0.5625385310171489\n",
      "train loss:0.5013955541446107\n",
      "train loss:0.5667392531879818\n",
      "train loss:0.48927635181042495\n",
      "train loss:0.60029236582918\n",
      "train loss:0.5058710713078739\n",
      "train loss:0.5571101570816058\n",
      "train loss:0.46644789023795447\n",
      "train loss:0.5708030126342962\n",
      "train loss:0.5699783616681138\n",
      "train loss:0.44462737847371414\n",
      "train loss:0.41195776960836417\n",
      "train loss:0.47797013695726975\n",
      "train loss:0.4628473212378265\n",
      "train loss:0.4214318433627977\n",
      "train loss:0.5714079362729997\n",
      "train loss:0.39233619327873503\n",
      "train loss:0.46350061534916537\n",
      "train loss:0.6002778909369108\n",
      "train loss:0.6391637818914178\n",
      "train loss:0.4976253685945894\n",
      "train loss:0.48223698618694916\n",
      "train loss:0.5314863511872913\n",
      "train loss:0.4704723669223588\n",
      "train loss:0.5432747387823781\n",
      "train loss:0.5040365295201756\n",
      "train loss:0.43820116578573853\n",
      "train loss:0.6323211915221949\n",
      "train loss:0.45454913048876117\n",
      "train loss:0.4942459922728073\n",
      "train loss:0.5387137728945234\n",
      "train loss:0.5793269622333651\n",
      "train loss:0.43858026413380924\n",
      "train loss:0.679470119781306\n",
      "train loss:0.42202687711303194\n",
      "train loss:0.40845964984815\n",
      "train loss:0.40719922549064\n",
      "train loss:0.553519072359464\n",
      "train loss:0.4854997418419278\n",
      "train loss:0.3864434908316435\n",
      "train loss:0.45565585617314275\n",
      "train loss:0.5823527613384829\n",
      "train loss:0.5486855289539465\n",
      "train loss:0.43558219004730786\n",
      "train loss:0.4010710524213961\n",
      "train loss:0.47807657844779905\n",
      "train loss:0.4380537057192813\n",
      "train loss:0.39801362850851063\n",
      "train loss:0.41873767080496177\n",
      "train loss:0.44777170259522014\n",
      "train loss:0.5792186964432681\n",
      "train loss:0.4661063765364142\n",
      "train loss:0.35050118519305273\n",
      "train loss:0.460284723267407\n",
      "train loss:0.5268712935060197\n",
      "train loss:0.4804155063009798\n",
      "train loss:0.5949357733637065\n",
      "train loss:0.48478638432701404\n",
      "train loss:0.5862017449590281\n",
      "train loss:0.4158882944425539\n",
      "train loss:0.43626009308621677\n",
      "train loss:0.45101333743712635\n",
      "train loss:0.5961374992524746\n",
      "train loss:0.41636542835427837\n",
      "train loss:0.49133550754324096\n",
      "train loss:0.4956879920588508\n",
      "train loss:0.663688331245336\n",
      "train loss:0.501610234492541\n",
      "train loss:0.5220945520180983\n",
      "train loss:0.4283553918899781\n",
      "train loss:0.5491100029584338\n",
      "train loss:0.5091975926861809\n",
      "train loss:0.524935277593595\n",
      "train loss:0.41116819923396164\n",
      "train loss:0.4303097687720786\n",
      "train loss:0.33930998906486953\n",
      "train loss:0.2898391230611841\n",
      "train loss:0.37712554480771127\n",
      "train loss:0.47624007994234147\n",
      "train loss:0.4168848563210903\n",
      "train loss:0.48642732257384375\n",
      "train loss:0.44643760574323\n",
      "train loss:0.4437034531765409\n",
      "train loss:0.4369038214849396\n",
      "train loss:0.34866772923471934\n",
      "train loss:0.4986761526212863\n",
      "train loss:0.4176389703712655\n",
      "train loss:0.48712102486397746\n",
      "train loss:0.46323280535776395\n",
      "train loss:0.45107919317164513\n",
      "train loss:0.35794700910837723\n",
      "train loss:0.5266995011718336\n",
      "train loss:0.4528520757806861\n",
      "train loss:0.66542190235092\n",
      "train loss:0.3747313258319667\n",
      "train loss:0.3714943154834836\n",
      "train loss:0.4923352730587593\n",
      "train loss:0.4253752718028197\n",
      "train loss:0.45973061256799824\n",
      "train loss:0.4814222097438897\n",
      "train loss:0.3454820259236759\n",
      "train loss:0.40820038425618754\n",
      "train loss:0.3657488791546237\n",
      "train loss:0.5732098582498263\n",
      "train loss:0.496135990529243\n",
      "train loss:0.4591910323175912\n",
      "train loss:0.3643412150223127\n",
      "train loss:0.3893228920592467\n",
      "train loss:0.4378862697428926\n",
      "train loss:0.5364591321499207\n",
      "train loss:0.5955242860526768\n",
      "train loss:0.48374206667314323\n",
      "train loss:0.5201199079580826\n",
      "train loss:0.6961237863664808\n",
      "train loss:0.6322928414064074\n",
      "train loss:0.46865835436029507\n",
      "train loss:0.5702754163376421\n",
      "train loss:0.4250252184999022\n",
      "train loss:0.3473442425133036\n",
      "train loss:0.6036862267824918\n",
      "train loss:0.38591908964733024\n",
      "train loss:0.6060667440832229\n",
      "train loss:0.4290676804341986\n",
      "train loss:0.506272187420618\n",
      "train loss:0.5109611069536644\n",
      "train loss:0.3779795149382001\n",
      "train loss:0.3702227108051322\n",
      "train loss:0.5223412377094373\n",
      "train loss:0.5827435295826531\n",
      "train loss:0.4560024490843586\n",
      "train loss:0.5050376848541791\n",
      "train loss:0.5205835890593221\n",
      "train loss:0.3851453691533557\n",
      "train loss:0.43575903571008917\n",
      "train loss:0.5486864216214106\n",
      "train loss:0.46291024154421917\n",
      "train loss:0.517684320012196\n",
      "train loss:0.6218760275255099\n",
      "train loss:0.5016248180877568\n",
      "train loss:0.46340860287134944\n",
      "train loss:0.4828541294548635\n",
      "train loss:0.43899313799296846\n",
      "train loss:0.5411901758011177\n",
      "train loss:0.4245887968908738\n",
      "train loss:0.4927618510257967\n",
      "train loss:0.4695813912602139\n",
      "train loss:0.4533889996338928\n",
      "train loss:0.43178005089947186\n",
      "train loss:0.49891102296096074\n",
      "train loss:0.42715246159223114\n",
      "train loss:0.30858026072938477\n",
      "train loss:0.5493075409619071\n",
      "train loss:0.5212415681243712\n",
      "train loss:0.5064715647269132\n",
      "train loss:0.4245065460266267\n",
      "train loss:0.47032331556982354\n",
      "train loss:0.5395662715725742\n",
      "train loss:0.5836477513403059\n",
      "train loss:0.386786370482771\n",
      "train loss:0.3676581901246123\n",
      "train loss:0.5240783770559586\n",
      "train loss:0.5194866900492007\n",
      "train loss:0.42341077662779386\n",
      "train loss:0.457518184898477\n",
      "train loss:0.5558588406555631\n",
      "train loss:0.5785460787234791\n",
      "train loss:0.5196789030852408\n",
      "train loss:0.4609757956847792\n",
      "train loss:0.47450888988131673\n",
      "train loss:0.5492628826497171\n",
      "train loss:0.33853262687906494\n",
      "train loss:0.4569543525366406\n",
      "train loss:0.679362424956096\n",
      "train loss:0.5244644310693746\n",
      "train loss:0.3684994699200543\n",
      "train loss:0.4426017305981469\n",
      "train loss:0.5429181875133628\n",
      "train loss:0.5640345581712397\n",
      "train loss:0.4412530315899154\n",
      "train loss:0.36672296091873974\n",
      "train loss:0.36947175632253876\n",
      "train loss:0.5739080965079121\n",
      "train loss:0.5304653179162471\n",
      "train loss:0.37886229362420254\n",
      "train loss:0.5130495231164032\n",
      "train loss:0.36468941084059464\n",
      "train loss:0.5168965986498678\n",
      "train loss:0.5544289008126241\n",
      "train loss:0.48484891373446287\n",
      "train loss:0.7371422455250766\n",
      "train loss:0.5504713042570607\n",
      "train loss:0.5701078674597722\n",
      "train loss:0.4251851878230767\n",
      "train loss:0.513498633227875\n",
      "train loss:0.6399124791269338\n",
      "train loss:0.49144835592478503\n",
      "train loss:0.6042461129253426\n",
      "train loss:0.42814169785338313\n",
      "train loss:0.5132351315603246\n",
      "train loss:0.5691440865592959\n",
      "train loss:0.4632839933523804\n",
      "train loss:0.424731737832217\n",
      "train loss:0.5246513273010573\n",
      "train loss:0.3882537912123\n",
      "train loss:0.4837130140996237\n",
      "train loss:0.4939491238397843\n",
      "train loss:0.4489663440356943\n",
      "train loss:0.49371446557456466\n",
      "train loss:0.42283064408006354\n",
      "train loss:0.48276524083567834\n",
      "train loss:0.4744369487726728\n",
      "train loss:0.47485783273448284\n",
      "train loss:0.3777098909027098\n",
      "train loss:0.5584702418082387\n",
      "train loss:0.5975416598562028\n",
      "train loss:0.582962291507587\n",
      "train loss:0.43319427724277576\n",
      "train loss:0.34122288810440154\n",
      "train loss:0.5053422346932169\n",
      "train loss:0.4603227652023705\n",
      "train loss:0.3930591474292681\n",
      "train loss:0.35526997110950903\n",
      "train loss:0.4721410483517737\n",
      "train loss:0.4647235068140305\n",
      "train loss:0.4995567063935188\n",
      "train loss:0.56471702067358\n",
      "train loss:0.3964349998303134\n",
      "train loss:0.4657727091739512\n",
      "train loss:0.37869431394723535\n",
      "train loss:0.5290346720837575\n",
      "train loss:0.4761174928287017\n",
      "train loss:0.4311469067850784\n",
      "train loss:0.7006340102829219\n",
      "train loss:0.4426007040686233\n",
      "train loss:0.40283383542997453\n",
      "train loss:0.45499483418636716\n",
      "train loss:0.5365441347579354\n",
      "train loss:0.4749735400016392\n",
      "train loss:0.5949887488647175\n",
      "train loss:0.523791924564301\n",
      "train loss:0.5024502482885495\n",
      "train loss:0.3936392417868144\n",
      "train loss:0.5371544669095794\n",
      "train loss:0.391158637498027\n",
      "train loss:0.4070567730222052\n",
      "train loss:0.530157011886399\n",
      "train loss:0.5115165034155651\n",
      "train loss:0.39808171606190884\n",
      "train loss:0.46930173946606574\n",
      "train loss:0.4410876091973515\n",
      "train loss:0.6129490705770447\n",
      "train loss:0.5504124876062924\n",
      "train loss:0.5598904893362748\n",
      "train loss:0.41604138746924624\n",
      "train loss:0.4859103361668407\n",
      "train loss:0.504625445914054\n",
      "train loss:0.48907443988125593\n",
      "train loss:0.36617277031653017\n",
      "train loss:0.3745803065152423\n",
      "train loss:0.44345673323565377\n",
      "train loss:0.4724214780358381\n",
      "train loss:0.39905661690837496\n",
      "train loss:0.6135419763618589\n",
      "train loss:0.548444381590027\n",
      "train loss:0.5048263409050664\n",
      "train loss:0.38827673817985653\n",
      "train loss:0.4624062621968583\n",
      "train loss:0.4158469077352459\n",
      "train loss:0.34697737658117733\n",
      "train loss:0.37905734329168994\n",
      "train loss:0.4070657560408009\n",
      "train loss:0.5144155151845863\n",
      "train loss:0.6030832320332382\n",
      "train loss:0.42858839943995725\n",
      "train loss:0.3719418277121856\n",
      "train loss:0.35483071559099244\n",
      "train loss:0.40576471402806297\n",
      "train loss:0.6135975684076027\n",
      "train loss:0.40059225733826354\n",
      "train loss:0.5609209554357681\n",
      "train loss:0.5564611294027371\n",
      "train loss:0.3781768888555156\n",
      "train loss:0.36622651348471374\n",
      "train loss:0.3789424407088787\n",
      "train loss:0.42374678208498706\n",
      "train loss:0.47113014607733694\n",
      "train loss:0.4661042717336087\n",
      "train loss:0.617447045431614\n",
      "train loss:0.5077237403197777\n",
      "train loss:0.4339715216768474\n",
      "train loss:0.5711822317294701\n",
      "train loss:0.41846396738096286\n",
      "train loss:0.34501007332223166\n",
      "train loss:0.48057848286817373\n",
      "train loss:0.40397659267419633\n",
      "train loss:0.42345743956333337\n",
      "train loss:0.622837764659639\n",
      "train loss:0.4245842632327619\n",
      "train loss:0.4719248445067407\n",
      "train loss:0.44574304989931635\n",
      "train loss:0.3174508162203881\n",
      "train loss:0.3682575924577105\n",
      "train loss:0.543403731476145\n",
      "train loss:0.3770250546952414\n",
      "train loss:0.5349377320601888\n",
      "train loss:0.5088203014456583\n",
      "train loss:0.5228912659110514\n",
      "train loss:0.4956402471177851\n",
      "train loss:0.7254828477240868\n",
      "train loss:0.37715945123396755\n",
      "train loss:0.4967383460234131\n",
      "train loss:0.430952067859389\n",
      "train loss:0.5512972488342366\n",
      "train loss:0.45406374638283553\n",
      "train loss:0.33470543046270385\n",
      "train loss:0.4393798122269753\n",
      "train loss:0.49354586633583497\n",
      "train loss:0.45903901663226543\n",
      "train loss:0.3602196615454485\n",
      "train loss:0.5662840743760372\n",
      "train loss:0.3910088334886911\n",
      "train loss:0.43351593084011664\n",
      "train loss:0.4834507893838154\n",
      "train loss:0.44940719734675355\n",
      "train loss:0.42073950018561024\n",
      "train loss:0.46104912263756115\n",
      "train loss:0.4797948687144096\n",
      "train loss:0.38219695850463103\n",
      "train loss:0.49588819139963236\n",
      "train loss:0.48466685610445137\n",
      "train loss:0.4204139720759411\n",
      "train loss:0.4097096270577987\n",
      "train loss:0.5124092718315326\n",
      "train loss:0.3235880121244225\n",
      "train loss:0.42677447729227713\n",
      "train loss:0.36521727637041557\n",
      "train loss:0.44680150267749985\n",
      "train loss:0.4785112402389637\n",
      "train loss:0.47338622570983196\n",
      "train loss:0.3604948479617597\n",
      "train loss:0.4531129798477895\n",
      "train loss:0.49537512125380806\n",
      "train loss:0.3854025979485216\n",
      "train loss:0.44876112594307566\n",
      "train loss:0.43324920248922383\n",
      "train loss:0.4811761438744871\n",
      "train loss:0.4099019268518814\n",
      "train loss:0.4005230987329395\n",
      "train loss:0.4645988455543007\n",
      "train loss:0.46177911660427684\n",
      "train loss:0.41198955613752\n",
      "train loss:0.4806438926709828\n",
      "train loss:0.4515270916426696\n",
      "train loss:0.4114974931552353\n",
      "train loss:0.46940826674781505\n",
      "train loss:0.47038164353342254\n",
      "train loss:0.4059596008455739\n",
      "train loss:0.5414592669816037\n",
      "train loss:0.44475822952890065\n",
      "train loss:0.38539584891969797\n",
      "train loss:0.476318217363483\n",
      "train loss:0.41122281536147887\n",
      "train loss:0.3412669882540032\n",
      "train loss:0.5082401201078534\n",
      "train loss:0.3822089479181031\n",
      "train loss:0.45938498562140895\n",
      "train loss:0.49692695316107716\n",
      "train loss:0.5072847415650185\n",
      "train loss:0.4999877785378133\n",
      "train loss:0.3947276620806876\n",
      "train loss:0.484461247143896\n",
      "train loss:0.5016834102466329\n",
      "train loss:0.4743055082636408\n",
      "train loss:0.503108026739097\n",
      "train loss:0.5239278442866577\n",
      "train loss:0.6299797417064156\n",
      "train loss:0.4373898564722054\n",
      "train loss:0.506307744740594\n",
      "train loss:0.4319697067085566\n",
      "train loss:0.32033473165707155\n",
      "train loss:0.33601274036227674\n",
      "train loss:0.41245625275373143\n",
      "train loss:0.4607076714070004\n",
      "train loss:0.47121636424014307\n",
      "train loss:0.5710299040365239\n",
      "train loss:0.429767196077306\n",
      "train loss:0.40921142953468886\n",
      "train loss:0.4724714691825922\n",
      "train loss:0.4762119301721108\n",
      "train loss:0.40705938099872496\n",
      "train loss:0.5053757331227101\n",
      "train loss:0.42244313054355187\n",
      "train loss:0.5153572279276866\n",
      "train loss:0.44697958310036895\n",
      "train loss:0.39292358610788247\n",
      "train loss:0.5350364701113329\n",
      "train loss:0.42320362093098995\n",
      "train loss:0.47253521061433207\n",
      "train loss:0.4649368900921772\n",
      "train loss:0.5535693651595726\n",
      "train loss:0.48189562925834645\n",
      "train loss:0.4679433475997768\n",
      "train loss:0.3793851822740488\n",
      "train loss:0.41096444733244764\n",
      "train loss:0.5339817349935888\n",
      "train loss:0.40517535859516884\n",
      "train loss:0.5120223972447571\n",
      "train loss:0.4419668055252629\n",
      "train loss:0.36713100325074544\n",
      "train loss:0.4918504932490492\n",
      "train loss:0.42092056561756513\n",
      "train loss:0.4998142270157288\n",
      "train loss:0.4684810553167152\n",
      "train loss:0.4609807157994688\n",
      "train loss:0.35963010704482995\n",
      "train loss:0.36348482011497985\n",
      "train loss:0.39137610128245615\n",
      "train loss:0.5298193578053979\n",
      "train loss:0.5222227957346404\n",
      "train loss:0.40881579937205165\n",
      "train loss:0.47195566358766267\n",
      "train loss:0.39039572232720104\n",
      "train loss:0.47213936949176616\n",
      "train loss:0.31140021537177315\n",
      "train loss:0.4438749134866016\n",
      "train loss:0.6096063492386438\n",
      "train loss:0.49524627186707354\n",
      "train loss:0.3994111539958899\n",
      "train loss:0.5390143203473516\n",
      "train loss:0.5100144286408796\n",
      "train loss:0.511229189634467\n",
      "train loss:0.5860365043343174\n",
      "train loss:0.4106527582684853\n",
      "train loss:0.4219021910838771\n",
      "train loss:0.4164636485350512\n",
      "train loss:0.35005397177255676\n",
      "train loss:0.4345709654751368\n",
      "train loss:0.4119419316919721\n",
      "train loss:0.3796294173855217\n",
      "train loss:0.5118359919887738\n",
      "train loss:0.3966103594087001\n",
      "train loss:0.4882509991451158\n",
      "train loss:0.4343636418274407\n",
      "train loss:0.4282142101253496\n",
      "train loss:0.4013878052446067\n",
      "train loss:0.47895850381431404\n",
      "train loss:0.4384690876679286\n",
      "train loss:0.38584563351551115\n",
      "train loss:0.5355680847515486\n",
      "train loss:0.46858441140559115\n",
      "train loss:0.612001704173967\n",
      "train loss:0.5192074257704994\n",
      "train loss:0.4382396921389472\n",
      "train loss:0.49103568001008874\n",
      "train loss:0.4693125224464451\n",
      "train loss:0.5039975654713611\n",
      "train loss:0.4733659142816405\n",
      "train loss:0.42976866363510524\n",
      "train loss:0.41640756123796485\n",
      "train loss:0.47661529246466455\n",
      "train loss:0.3677328486111176\n",
      "train loss:0.5437225137069108\n",
      "train loss:0.44271294644345\n",
      "train loss:0.4315554995919889\n",
      "train loss:0.5419662110260148\n",
      "train loss:0.6255799947486421\n",
      "train loss:0.5013774020533149\n",
      "train loss:0.40963987870092955\n",
      "train loss:0.34273316501857315\n",
      "train loss:0.44170812955777145\n",
      "train loss:0.49738906129710636\n",
      "train loss:0.4114828784980978\n",
      "train loss:0.45716701295893203\n",
      "train loss:0.5274750292740343\n",
      "train loss:0.4193375409383007\n",
      "train loss:0.4200903925137339\n",
      "train loss:0.42097192915617876\n",
      "train loss:0.5315440543567422\n",
      "train loss:0.3608024516019511\n",
      "train loss:0.443564133704944\n",
      "train loss:0.4875186202419339\n",
      "train loss:0.36844257401993014\n",
      "train loss:0.40947589626138814\n",
      "train loss:0.5070607968020937\n",
      "train loss:0.46291757342846346\n",
      "train loss:0.4408272583670245\n",
      "train loss:0.3700886957242413\n",
      "train loss:0.3949110477601183\n",
      "train loss:0.3684471187599614\n",
      "train loss:0.3016815569716488\n",
      "train loss:0.47062707655002617\n",
      "train loss:0.3507368893495039\n",
      "train loss:0.5740335483252397\n",
      "train loss:0.49488383241481515\n",
      "train loss:0.46063930285387833\n",
      "train loss:0.4454479201501053\n",
      "train loss:0.5486139074413537\n",
      "train loss:0.35131974008746375\n",
      "train loss:0.4086618281738296\n",
      "train loss:0.5963675077191959\n",
      "train loss:0.4460166892295115\n",
      "train loss:0.5209775359163316\n",
      "train loss:0.4566076815665931\n",
      "train loss:0.4216798750991952\n",
      "train loss:0.33371672241793654\n",
      "train loss:0.364037297330432\n",
      "train loss:0.32617489457985693\n",
      "train loss:0.4561716259178626\n",
      "train loss:0.45781025630652955\n",
      "train loss:0.4888456862189164\n",
      "train loss:0.5187631490446509\n",
      "train loss:0.4536652251815804\n",
      "train loss:0.6884061720438183\n",
      "train loss:0.5126496259246274\n",
      "train loss:0.40214439899709187\n",
      "train loss:0.4766142220582995\n",
      "train loss:0.4165766413629002\n",
      "train loss:0.4355123392897626\n",
      "train loss:0.40096962422347376\n",
      "train loss:0.3776348744998785\n",
      "train loss:0.5286901571513527\n",
      "train loss:0.4516471626092587\n",
      "train loss:0.5106375124599812\n",
      "train loss:0.41683003041808064\n",
      "train loss:0.5324721281164907\n",
      "train loss:0.39363695079082406\n",
      "train loss:0.4762248242246969\n",
      "train loss:0.5381464597633758\n",
      "train loss:0.5165812347672574\n",
      "train loss:0.5880221589873197\n",
      "train loss:0.44725308224667804\n",
      "train loss:0.4152944198076432\n",
      "train loss:0.5144421222859731\n",
      "train loss:0.4697537472162333\n",
      "train loss:0.42827481476092677\n",
      "train loss:0.33562315895755773\n",
      "train loss:0.5815108027357415\n",
      "train loss:0.32150935327859465\n",
      "train loss:0.5089479809773332\n",
      "train loss:0.3871060939994695\n",
      "train loss:0.40522307206878644\n",
      "train loss:0.4630478954830076\n",
      "train loss:0.6166767905432438\n",
      "train loss:0.33377150644656445\n",
      "train loss:0.4131729310950258\n",
      "train loss:0.40739174461845595\n",
      "train loss:0.3688172952971233\n",
      "train loss:0.4006750253128718\n",
      "train loss:0.41255271659353804\n",
      "train loss:0.49132772721696727\n",
      "train loss:0.519834089178817\n",
      "train loss:0.5532252747367731\n",
      "train loss:0.45335551388734224\n",
      "train loss:0.3673698888416464\n",
      "train loss:0.476965886137922\n",
      "train loss:0.3726185476661295\n",
      "train loss:0.383515896112381\n",
      "train loss:0.473470463099822\n",
      "train loss:0.3317954055862666\n",
      "train loss:0.39617418881471456\n",
      "train loss:0.35377541802484375\n",
      "train loss:0.5313641408706506\n",
      "train loss:0.31509421765064005\n",
      "train loss:0.5096987369023399\n",
      "train loss:0.4926465992068231\n",
      "train loss:0.42526747420889793\n",
      "train loss:0.4737304469450717\n",
      "train loss:0.44805372062424126\n",
      "=== epoch:12, train acc:0.8, test acc:0.807 ===\n",
      "train loss:0.43819796323110016\n",
      "train loss:0.4747220763492562\n",
      "train loss:0.39504287127497373\n",
      "train loss:0.3541307349315238\n",
      "train loss:0.4980171076470284\n",
      "train loss:0.44570628491040226\n",
      "train loss:0.4743605573987983\n",
      "train loss:0.4030757688278612\n",
      "train loss:0.4051088690365199\n",
      "train loss:0.47837297434286863\n",
      "train loss:0.5677575003673909\n",
      "train loss:0.41124054852087805\n",
      "train loss:0.3052567031360861\n",
      "train loss:0.3812594093623224\n",
      "train loss:0.6130407831416944\n",
      "train loss:0.4629287228193349\n",
      "train loss:0.40210651810262166\n",
      "train loss:0.3737289829066414\n",
      "train loss:0.44728719447177734\n",
      "train loss:0.36834489368357076\n",
      "train loss:0.39910069447756974\n",
      "train loss:0.5150622493889511\n",
      "train loss:0.39878186531491605\n",
      "train loss:0.38973000813186526\n",
      "train loss:0.4450518130275789\n",
      "train loss:0.5666984287259573\n",
      "train loss:0.39006320179540593\n",
      "train loss:0.5327075452160791\n",
      "train loss:0.4075128461563582\n",
      "train loss:0.5255397252226173\n",
      "train loss:0.4293811111298199\n",
      "train loss:0.45365307296995183\n",
      "train loss:0.40239769902606576\n",
      "train loss:0.47311083709409274\n",
      "train loss:0.4739726235058964\n",
      "train loss:0.37983969814042273\n",
      "train loss:0.3455892085644087\n",
      "train loss:0.4541011062639416\n",
      "train loss:0.5185421945172717\n",
      "train loss:0.5621641321887207\n",
      "train loss:0.3879671417459079\n",
      "train loss:0.43107445258599947\n",
      "train loss:0.49440324486665005\n",
      "train loss:0.4393348545170759\n",
      "train loss:0.3963845218035831\n",
      "train loss:0.3983157566520543\n",
      "train loss:0.533692893964216\n",
      "train loss:0.5414099483561987\n",
      "train loss:0.3952575880912657\n",
      "train loss:0.5287703138887497\n",
      "train loss:0.3477770905581593\n",
      "train loss:0.47221437675473077\n",
      "train loss:0.3862108919823193\n",
      "train loss:0.4270908110934014\n",
      "train loss:0.5144849134574829\n",
      "train loss:0.5249670553478859\n",
      "train loss:0.36579184443882035\n",
      "train loss:0.3828976384247681\n",
      "train loss:0.4141594477333184\n",
      "train loss:0.3419795572952429\n",
      "train loss:0.4304904279714174\n",
      "train loss:0.4515834594809773\n",
      "train loss:0.35153212112553783\n",
      "train loss:0.523442301238747\n",
      "train loss:0.48609182103224086\n",
      "train loss:0.4863062364821473\n",
      "train loss:0.43383110178765505\n",
      "train loss:0.518917359608824\n",
      "train loss:0.5250893465159033\n",
      "train loss:0.4455956265527354\n",
      "train loss:0.4672881162262827\n",
      "train loss:0.471215644942822\n",
      "train loss:0.5750850667094674\n",
      "train loss:0.40600014503851944\n",
      "train loss:0.4924462547588105\n",
      "train loss:0.4109035615096805\n",
      "train loss:0.3843615319286511\n",
      "train loss:0.6295116865569524\n",
      "train loss:0.6204515945128193\n",
      "train loss:0.49984017567549804\n",
      "train loss:0.39643211364293074\n",
      "train loss:0.5823927206019299\n",
      "train loss:0.4904316994195454\n",
      "train loss:0.3638189388774342\n",
      "train loss:0.6499588161515514\n",
      "train loss:0.528341847990945\n",
      "train loss:0.5010641671916052\n",
      "train loss:0.39167195935844307\n",
      "train loss:0.35319270846874423\n",
      "train loss:0.48204665299170235\n",
      "train loss:0.42040625416263727\n",
      "train loss:0.4258469498924173\n",
      "train loss:0.31630629990909614\n",
      "train loss:0.452467246082998\n",
      "train loss:0.4915431986056857\n",
      "train loss:0.3839134848818505\n",
      "train loss:0.542913790323887\n",
      "train loss:0.5009495548827069\n",
      "train loss:0.4006252467136217\n",
      "train loss:0.3895949176030848\n",
      "train loss:0.2583015309286517\n",
      "train loss:0.378070861364555\n",
      "train loss:0.5713436917313552\n",
      "train loss:0.406089373050888\n",
      "train loss:0.45935202011187476\n",
      "train loss:0.36136117543731217\n",
      "train loss:0.3929270976137093\n",
      "train loss:0.5130469327152801\n",
      "train loss:0.47142362820794514\n",
      "train loss:0.4664166878228071\n",
      "train loss:0.3889541630439309\n",
      "train loss:0.48185160349973993\n",
      "train loss:0.5163283204619645\n",
      "train loss:0.423382800162465\n",
      "train loss:0.4387396929233001\n",
      "train loss:0.4263890944566922\n",
      "train loss:0.344014774414542\n",
      "train loss:0.43143288601418467\n",
      "train loss:0.4542207247911345\n",
      "train loss:0.45066564849314283\n",
      "train loss:0.43327005855809003\n",
      "train loss:0.5284299432309982\n",
      "train loss:0.4945811989855955\n",
      "train loss:0.5140797556803054\n",
      "train loss:0.4606165539115765\n",
      "train loss:0.4501442775170681\n",
      "train loss:0.47352172715640395\n",
      "train loss:0.4992651203783535\n",
      "train loss:0.39697109832957594\n",
      "train loss:0.38143923995524615\n",
      "train loss:0.4738206178520663\n",
      "train loss:0.46671691453748737\n",
      "train loss:0.44134061188895785\n",
      "train loss:0.4892673907304052\n",
      "train loss:0.3868024694730202\n",
      "train loss:0.5618588640834424\n",
      "train loss:0.5913495916878828\n",
      "train loss:0.5951787434970938\n",
      "train loss:0.47692969290180115\n",
      "train loss:0.5188162560624436\n",
      "train loss:0.42087514562392087\n",
      "train loss:0.39976736074951136\n",
      "train loss:0.37312186164152833\n",
      "train loss:0.4756779770859038\n",
      "train loss:0.4844952604549351\n",
      "train loss:0.39745336457390373\n",
      "train loss:0.4230383647493062\n",
      "train loss:0.3906239885551095\n",
      "train loss:0.3534023637035952\n",
      "train loss:0.5235946636842763\n",
      "train loss:0.5557408455420813\n",
      "train loss:0.4233043456738283\n",
      "train loss:0.44841097974918825\n",
      "train loss:0.29839156389285526\n",
      "train loss:0.5188909651321745\n",
      "train loss:0.4155793554599347\n",
      "train loss:0.48261217613248625\n",
      "train loss:0.6565224670716031\n",
      "train loss:0.30748052277982807\n",
      "train loss:0.42240177008137764\n",
      "train loss:0.39322663504960526\n",
      "train loss:0.413393915919063\n",
      "train loss:0.4458733773163989\n",
      "train loss:0.42483547157684515\n",
      "train loss:0.36493213348647024\n",
      "train loss:0.46276742400092546\n",
      "train loss:0.4003193251737619\n",
      "train loss:0.49381314412911403\n",
      "train loss:0.3707154061634758\n",
      "train loss:0.49160208409405975\n",
      "train loss:0.5714039886565819\n",
      "train loss:0.34945488228332616\n",
      "train loss:0.5856060452005019\n",
      "train loss:0.5279941814874227\n",
      "train loss:0.3628004123113962\n",
      "train loss:0.517650056288497\n",
      "train loss:0.46827936079990445\n",
      "train loss:0.29748960021677123\n",
      "train loss:0.2582601559808963\n",
      "train loss:0.480564169994944\n",
      "train loss:0.47949863457921815\n",
      "train loss:0.4594528565404379\n",
      "train loss:0.3674080452805799\n",
      "train loss:0.5167454354397856\n",
      "train loss:0.6387498402219807\n",
      "train loss:0.6117645818018219\n",
      "train loss:0.32196466138819474\n",
      "train loss:0.3947529104432654\n",
      "train loss:0.47985125978386056\n",
      "train loss:0.539412375842468\n",
      "train loss:0.3795919709945288\n",
      "train loss:0.4683590790893465\n",
      "train loss:0.46007458030001175\n",
      "train loss:0.36749700741913244\n",
      "train loss:0.47438561870986473\n",
      "train loss:0.5101621809047627\n",
      "train loss:0.3203187913132745\n",
      "train loss:0.31529922471177796\n",
      "train loss:0.40203517807873923\n",
      "train loss:0.4932020794703048\n",
      "train loss:0.4006933589181651\n",
      "train loss:0.2717635757788357\n",
      "train loss:0.4139711309686821\n",
      "train loss:0.40189069260468757\n",
      "train loss:0.39962007845783437\n",
      "train loss:0.5701368593374736\n",
      "train loss:0.5399928669266431\n",
      "train loss:0.4012693286653197\n",
      "train loss:0.3165879195543072\n",
      "train loss:0.39953072429033587\n",
      "train loss:0.43053559424698884\n",
      "train loss:0.45625753823142673\n",
      "train loss:0.3343385929337965\n",
      "train loss:0.41543768486940225\n",
      "train loss:0.5886804010196218\n",
      "train loss:0.45549502681517345\n",
      "train loss:0.3401626413390961\n",
      "train loss:0.433324738355624\n",
      "train loss:0.406679283331013\n",
      "train loss:0.4374644342061063\n",
      "train loss:0.40142958850680793\n",
      "train loss:0.47915860371769364\n",
      "train loss:0.4902527775008691\n",
      "train loss:0.5904543780503642\n",
      "train loss:0.5238138923616658\n",
      "train loss:0.35239489562184273\n",
      "train loss:0.7341890329542298\n",
      "train loss:0.49388548134506927\n",
      "train loss:0.41696440905645105\n",
      "train loss:0.4203369076039954\n",
      "train loss:0.3509234902442204\n",
      "train loss:0.45809105637118447\n",
      "train loss:0.4036664564439515\n",
      "train loss:0.3416347309806156\n",
      "train loss:0.39423080463862087\n",
      "train loss:0.4777672946802071\n",
      "train loss:0.42642136356020877\n",
      "train loss:0.44831229379083853\n",
      "train loss:0.4618682409354147\n",
      "train loss:0.5785572215282486\n",
      "train loss:0.345944508365222\n",
      "train loss:0.3952529724633762\n",
      "train loss:0.42528178603628375\n",
      "train loss:0.40060829741792475\n",
      "train loss:0.4896517172849532\n",
      "train loss:0.4114682307466723\n",
      "train loss:0.4752489237881627\n",
      "train loss:0.4956604154838087\n",
      "train loss:0.5229513627314826\n",
      "train loss:0.36815351918708056\n",
      "train loss:0.5772329652468161\n",
      "train loss:0.4684654377789424\n",
      "train loss:0.47529799166421755\n",
      "train loss:0.35538387417195877\n",
      "train loss:0.3730900102932248\n",
      "train loss:0.34818567517394894\n",
      "train loss:0.5982482679015536\n",
      "train loss:0.5792024367110112\n",
      "train loss:0.43453403295450654\n",
      "train loss:0.5412484642581568\n",
      "train loss:0.45623991572217343\n",
      "train loss:0.40370717551447305\n",
      "train loss:0.4979125629433076\n",
      "train loss:0.5557250320508895\n",
      "train loss:0.4219230817279949\n",
      "train loss:0.40778274478680415\n",
      "train loss:0.48473989019128255\n",
      "train loss:0.49635408132689596\n",
      "train loss:0.46238182018185264\n",
      "train loss:0.4608426127184827\n",
      "train loss:0.5062673592831901\n",
      "train loss:0.5349366604779979\n",
      "train loss:0.5149388618590128\n",
      "train loss:0.44624654772543304\n",
      "train loss:0.6529112931772727\n",
      "train loss:0.38695062999940655\n",
      "train loss:0.39827128228193637\n",
      "train loss:0.5221694281961473\n",
      "train loss:0.40218159491619865\n",
      "train loss:0.34382633751141456\n",
      "train loss:0.48480046837111224\n",
      "train loss:0.44604801356337087\n",
      "train loss:0.4169605546250168\n",
      "train loss:0.4095391676550989\n",
      "train loss:0.5555991972586575\n",
      "train loss:0.41430351532173476\n",
      "train loss:0.3863688266379876\n",
      "train loss:0.4041139952586995\n",
      "train loss:0.3830439483489353\n",
      "train loss:0.43526589666104276\n",
      "train loss:0.39442467751276067\n",
      "train loss:0.49291516525054435\n",
      "train loss:0.6141774095643051\n",
      "train loss:0.5127733327007404\n",
      "train loss:0.49057715730528473\n",
      "train loss:0.45398435066475673\n",
      "train loss:0.5068724424904199\n",
      "train loss:0.4974889334105869\n",
      "train loss:0.4280180634420851\n",
      "train loss:0.5257555667407998\n",
      "train loss:0.511699428794694\n",
      "train loss:0.5030969641631678\n",
      "train loss:0.3846104661702421\n",
      "train loss:0.4365663545858729\n",
      "train loss:0.679596851630895\n",
      "train loss:0.522306654118405\n",
      "train loss:0.4348620978480081\n",
      "train loss:0.3939365438065052\n",
      "train loss:0.40329765036768916\n",
      "train loss:0.40071253102762505\n",
      "train loss:0.37270414236168486\n",
      "train loss:0.4474859056922647\n",
      "train loss:0.4221363940974544\n",
      "train loss:0.48004149399393475\n",
      "train loss:0.5452466975725437\n",
      "train loss:0.45397469878536134\n",
      "train loss:0.44653851147626233\n",
      "train loss:0.3821837937515094\n",
      "train loss:0.5023892159045669\n",
      "train loss:0.35956365630833964\n",
      "train loss:0.46248570300713226\n",
      "train loss:0.44311981284729896\n",
      "train loss:0.4435052092247698\n",
      "train loss:0.38503478453621903\n",
      "train loss:0.3685281010585119\n",
      "train loss:0.5118993575382297\n",
      "train loss:0.44358434593839535\n",
      "train loss:0.4548034103783337\n",
      "train loss:0.43469204548480705\n",
      "train loss:0.424324654068073\n",
      "train loss:0.5571693528797691\n",
      "train loss:0.3562790336996112\n",
      "train loss:0.45902003545188336\n",
      "train loss:0.38125718485706345\n",
      "train loss:0.4682915690999285\n",
      "train loss:0.5753516103563089\n",
      "train loss:0.4545043032085415\n",
      "train loss:0.47964594145431677\n",
      "train loss:0.4267233788593661\n",
      "train loss:0.5009619752224187\n",
      "train loss:0.3821092628755526\n",
      "train loss:0.41639554095980275\n",
      "train loss:0.48706465128062787\n",
      "train loss:0.37488536054145855\n",
      "train loss:0.44002781754062953\n",
      "train loss:0.4966015871189565\n",
      "train loss:0.5003972390111869\n",
      "train loss:0.36307040931907686\n",
      "train loss:0.5120277931996442\n",
      "train loss:0.46369264693154233\n",
      "train loss:0.42086724570093814\n",
      "train loss:0.53069486969262\n",
      "train loss:0.41360264532068297\n",
      "train loss:0.4792817502158394\n",
      "train loss:0.39078589004704006\n",
      "train loss:0.416881847008787\n",
      "train loss:0.4306518908023208\n",
      "train loss:0.5238943379931223\n",
      "train loss:0.5938771731093165\n",
      "train loss:0.5081467660046972\n",
      "train loss:0.413687786882442\n",
      "train loss:0.5101646884991742\n",
      "train loss:0.4555891030719749\n",
      "train loss:0.44677586471045194\n",
      "train loss:0.5423824518569519\n",
      "train loss:0.4739538262623774\n",
      "train loss:0.5386686368647965\n",
      "train loss:0.48154811481849774\n",
      "train loss:0.4611244154576814\n",
      "train loss:0.4628409632886599\n",
      "train loss:0.47887852680513743\n",
      "train loss:0.436250947172873\n",
      "train loss:0.5699578196965199\n",
      "train loss:0.538889035618296\n",
      "train loss:0.4425599509269481\n",
      "train loss:0.4187699410871906\n",
      "train loss:0.45627379414944885\n",
      "train loss:0.5292508263423327\n",
      "train loss:0.42005277246024336\n",
      "train loss:0.3749796885801176\n",
      "train loss:0.35828335962745506\n",
      "train loss:0.40857714894888053\n",
      "train loss:0.4060326746437397\n",
      "train loss:0.3916367837350458\n",
      "train loss:0.4755135444111273\n",
      "train loss:0.3389547677615795\n",
      "train loss:0.35152132787504753\n",
      "train loss:0.4214105204968568\n",
      "train loss:0.36614997762789353\n",
      "train loss:0.36160621007152005\n",
      "train loss:0.4674506925670081\n",
      "train loss:0.41818891800110447\n",
      "train loss:0.41956588557681473\n",
      "train loss:0.3952361363344521\n",
      "train loss:0.46448220756038666\n",
      "train loss:0.4137430793832849\n",
      "train loss:0.43062118902955\n",
      "train loss:0.4413742748532702\n",
      "train loss:0.4680109184447718\n",
      "train loss:0.6783983667384547\n",
      "train loss:0.4913607588335111\n",
      "train loss:0.5215819716303007\n",
      "train loss:0.4685073447473707\n",
      "train loss:0.3618712398605467\n",
      "train loss:0.30929908988367266\n",
      "train loss:0.45711224478698353\n",
      "train loss:0.4690952854982312\n",
      "train loss:0.40534247745970303\n",
      "train loss:0.4668954763114981\n",
      "train loss:0.4381974391820709\n",
      "train loss:0.6147430990738194\n",
      "train loss:0.390237947927138\n",
      "train loss:0.43766705608922024\n",
      "train loss:0.42351561220539063\n",
      "train loss:0.4627140506097988\n",
      "train loss:0.38895699646292287\n",
      "train loss:0.5552150751025762\n",
      "train loss:0.5457659806882406\n",
      "train loss:0.40135040355307267\n",
      "train loss:0.47204006881937405\n",
      "train loss:0.43123279187594865\n",
      "train loss:0.3714065526730067\n",
      "train loss:0.4220886646259549\n",
      "train loss:0.4084394910111355\n",
      "train loss:0.48978626554666677\n",
      "train loss:0.5626831923573775\n",
      "train loss:0.4392182926560608\n",
      "train loss:0.40974917878685224\n",
      "train loss:0.4196892153093247\n",
      "train loss:0.5535071859239774\n",
      "train loss:0.5468263286305095\n",
      "train loss:0.6081058440964314\n",
      "train loss:0.6163402612469079\n",
      "train loss:0.3571267259481637\n",
      "train loss:0.45080537557752015\n",
      "train loss:0.4848307009054563\n",
      "train loss:0.45161173809771027\n",
      "train loss:0.367241146393501\n",
      "train loss:0.4541800725870827\n",
      "train loss:0.46399531135566974\n",
      "train loss:0.5343957903668417\n",
      "train loss:0.4317830237961935\n",
      "train loss:0.3406790412163651\n",
      "train loss:0.4478898903053758\n",
      "train loss:0.3369556775931464\n",
      "train loss:0.577055451013388\n",
      "train loss:0.4517358782349266\n",
      "train loss:0.3413733629602192\n",
      "train loss:0.5679134852698065\n",
      "train loss:0.5020975949217275\n",
      "train loss:0.38949895465762485\n",
      "train loss:0.49588648743105784\n",
      "train loss:0.41182573848637605\n",
      "train loss:0.42865104381023017\n",
      "train loss:0.49172984289148225\n",
      "train loss:0.4515131480547743\n",
      "train loss:0.40008343971916815\n",
      "train loss:0.2978151362734091\n",
      "train loss:0.4484004422608068\n",
      "train loss:0.3513495869814186\n",
      "train loss:0.5728397940073677\n",
      "train loss:0.2714704099641093\n",
      "train loss:0.2987106839978444\n",
      "train loss:0.5838989237751031\n",
      "train loss:0.35120834493097475\n",
      "train loss:0.33483129632214187\n",
      "train loss:0.5426908542523566\n",
      "train loss:0.4694334112683181\n",
      "train loss:0.4207466873940694\n",
      "train loss:0.5290719084861673\n",
      "train loss:0.4405706698805419\n",
      "train loss:0.5382317716026213\n",
      "train loss:0.41779126661286675\n",
      "train loss:0.5229047124101684\n",
      "train loss:0.3724373489833734\n",
      "train loss:0.42593400835212625\n",
      "train loss:0.4715913573101523\n",
      "train loss:0.41214824510925424\n",
      "train loss:0.38939381858902533\n",
      "train loss:0.3071533572590425\n",
      "train loss:0.32595784934936056\n",
      "train loss:0.38546184990993865\n",
      "train loss:0.3404589664976016\n",
      "train loss:0.3719015097364601\n",
      "train loss:0.4878958003839735\n",
      "train loss:0.3480050431612781\n",
      "train loss:0.3799569115998615\n",
      "train loss:0.49999074638206037\n",
      "train loss:0.4546770777701083\n",
      "train loss:0.3901963841359715\n",
      "train loss:0.326318165464767\n",
      "train loss:0.448326662927224\n",
      "train loss:0.29486504346767045\n",
      "train loss:0.4763359688124717\n",
      "train loss:0.45902471502987036\n",
      "train loss:0.45569606364126714\n",
      "train loss:0.40249993883296065\n",
      "train loss:0.35136537297273684\n",
      "train loss:0.5381591676948847\n",
      "train loss:0.3870416333149572\n",
      "train loss:0.4120111360533084\n",
      "train loss:0.4019560366088742\n",
      "train loss:0.5095517678724438\n",
      "train loss:0.35968206503405203\n",
      "train loss:0.3301266962445426\n",
      "train loss:0.4185279374590963\n",
      "train loss:0.38776183481909954\n",
      "train loss:0.31349556963239117\n",
      "train loss:0.40094239617421\n",
      "train loss:0.3921956304200208\n",
      "train loss:0.3851543008410752\n",
      "train loss:0.5283583063982489\n",
      "train loss:0.3014033799742029\n",
      "train loss:0.4186448520005229\n",
      "train loss:0.4144560484825393\n",
      "train loss:0.40090800566454876\n",
      "train loss:0.38369350255962104\n",
      "train loss:0.3743238403596574\n",
      "train loss:0.591233999753618\n",
      "train loss:0.49624578203304537\n",
      "train loss:0.5294386350595653\n",
      "train loss:0.3453550198479634\n",
      "train loss:0.5180014688628273\n",
      "train loss:0.42298571125825385\n",
      "train loss:0.3585311177972988\n",
      "train loss:0.5878607215714977\n",
      "train loss:0.5401686357109162\n",
      "train loss:0.4221702723705741\n",
      "train loss:0.31877767944201646\n",
      "train loss:0.5719031936199537\n",
      "train loss:0.3785875981138594\n",
      "train loss:0.40918159495135265\n",
      "train loss:0.4642639720803524\n",
      "train loss:0.4558494663244463\n",
      "train loss:0.5052932462899957\n",
      "train loss:0.3736678962757308\n",
      "train loss:0.40639683738755683\n",
      "train loss:0.39395145230442813\n",
      "train loss:0.5656490904774851\n",
      "train loss:0.33281849222209275\n",
      "train loss:0.4234001428368803\n",
      "train loss:0.39539033965460385\n",
      "train loss:0.36533415135338937\n",
      "train loss:0.3302549671603641\n",
      "train loss:0.4045256181638247\n",
      "train loss:0.3673106077801058\n",
      "train loss:0.3095019361025968\n",
      "train loss:0.4129409873011689\n",
      "train loss:0.3593784917017231\n",
      "train loss:0.5198082956745962\n",
      "train loss:0.44630608424737084\n",
      "train loss:0.40526109909819064\n",
      "train loss:0.3658590097452339\n",
      "train loss:0.40273343967284186\n",
      "train loss:0.5010312315839246\n",
      "train loss:0.3772735679710466\n",
      "train loss:0.6126013778105209\n",
      "train loss:0.3517126681095897\n",
      "train loss:0.3978333777544931\n",
      "train loss:0.39659171199210924\n",
      "train loss:0.3625293924510748\n",
      "train loss:0.3987207717559012\n",
      "train loss:0.3894661985187765\n",
      "train loss:0.36075344058979736\n",
      "train loss:0.6154502952105767\n",
      "train loss:0.5001445552518423\n",
      "train loss:0.42853353297408725\n",
      "train loss:0.41639293547138223\n",
      "train loss:0.42875553154671137\n",
      "train loss:0.3287772643688294\n",
      "train loss:0.384929587781802\n",
      "train loss:0.5157643276280469\n",
      "train loss:0.6304150916255034\n",
      "train loss:0.43325129315613675\n",
      "train loss:0.3951195311651292\n",
      "train loss:0.4304828486775134\n",
      "train loss:0.5996646687763427\n",
      "train loss:0.4192406572953072\n",
      "train loss:0.4043951432964053\n",
      "train loss:0.3190737981720797\n",
      "train loss:0.4354071219365608\n",
      "train loss:0.35456078996022306\n",
      "train loss:0.447718906264221\n",
      "train loss:0.4535701556442408\n",
      "train loss:0.32753534428053277\n",
      "train loss:0.5926349543578872\n",
      "train loss:0.4458730111402497\n",
      "train loss:0.5349655695268083\n",
      "train loss:0.4336323953475299\n",
      "train loss:0.4403461979256963\n",
      "train loss:0.36549188686067724\n",
      "train loss:0.47955978835862484\n",
      "train loss:0.4755467958294086\n",
      "train loss:0.47827365603475064\n",
      "train loss:0.4437701689528803\n",
      "train loss:0.6669010811321228\n",
      "train loss:0.4849116662422276\n",
      "train loss:0.41061462489199185\n",
      "train loss:0.5660125906592376\n",
      "train loss:0.4466317368236306\n",
      "=== epoch:13, train acc:0.811, test acc:0.812 ===\n",
      "train loss:0.410851677976047\n",
      "train loss:0.49278875997758226\n",
      "train loss:0.5222088085945662\n",
      "train loss:0.46568383463971513\n",
      "train loss:0.44381697786099544\n",
      "train loss:0.4249139561261719\n",
      "train loss:0.5006380215689454\n",
      "train loss:0.4380688945498498\n",
      "train loss:0.42963291511107554\n",
      "train loss:0.47785373867689557\n",
      "train loss:0.43086210222316834\n",
      "train loss:0.44053057532078554\n",
      "train loss:0.3978369962449696\n",
      "train loss:0.49132464881757576\n",
      "train loss:0.5339454710940088\n",
      "train loss:0.4244739232798934\n",
      "train loss:0.3694777524033984\n",
      "train loss:0.6084510959490569\n",
      "train loss:0.48578973975682355\n",
      "train loss:0.3677631923037016\n",
      "train loss:0.5310938082018993\n",
      "train loss:0.30056376823536624\n",
      "train loss:0.3886565645305656\n",
      "train loss:0.5751959298933058\n",
      "train loss:0.5237330052734521\n",
      "train loss:0.45216958139778574\n",
      "train loss:0.49266631563104274\n",
      "train loss:0.4186243251370524\n",
      "train loss:0.5245204190015208\n",
      "train loss:0.5835807954510088\n",
      "train loss:0.560158530303428\n",
      "train loss:0.6044425406567132\n",
      "train loss:0.38174485227307575\n",
      "train loss:0.45130988797064553\n",
      "train loss:0.4221142445477221\n",
      "train loss:0.358899460700072\n",
      "train loss:0.4534824655848457\n",
      "train loss:0.5023256159445935\n",
      "train loss:0.38252084755700544\n",
      "train loss:0.3510577610362916\n",
      "train loss:0.498366820646794\n",
      "train loss:0.4182261293313195\n",
      "train loss:0.39263990965968737\n",
      "train loss:0.43707130427524027\n",
      "train loss:0.41022367268151627\n",
      "train loss:0.3103927257985075\n",
      "train loss:0.4578432901407499\n",
      "train loss:0.5358214063100033\n",
      "train loss:0.3943482870478273\n",
      "train loss:0.3608380758805761\n",
      "train loss:0.4815944057683141\n",
      "train loss:0.49184114889454433\n",
      "train loss:0.3842705582332508\n",
      "train loss:0.5885057060230358\n",
      "train loss:0.39032314305794713\n",
      "train loss:0.4595701729906709\n",
      "train loss:0.43799275954067146\n",
      "train loss:0.47175780606197065\n",
      "train loss:0.37035785569345675\n",
      "train loss:0.2963657324230658\n",
      "train loss:0.36687227775667935\n",
      "train loss:0.5335986708349182\n",
      "train loss:0.4983382135231897\n",
      "train loss:0.45762515049874836\n",
      "train loss:0.36421696811639903\n",
      "train loss:0.49357007037716794\n",
      "train loss:0.3824051165847349\n",
      "train loss:0.3332342972250843\n",
      "train loss:0.40564863010878377\n",
      "train loss:0.3673122500642124\n",
      "train loss:0.3480056071447129\n",
      "train loss:0.3295441004408476\n",
      "train loss:0.46466970124198875\n",
      "train loss:0.5284851605920343\n",
      "train loss:0.46344734499154844\n",
      "train loss:0.3628394782891904\n",
      "train loss:0.4745450769164243\n",
      "train loss:0.4103120410875294\n",
      "train loss:0.4473505080383773\n",
      "train loss:0.2658192371430406\n",
      "train loss:0.35133294257053677\n",
      "train loss:0.3350447207326106\n",
      "train loss:0.40862797214102264\n",
      "train loss:0.6137923669992622\n",
      "train loss:0.2848315109626839\n",
      "train loss:0.4264806763617863\n",
      "train loss:0.3340366664377155\n",
      "train loss:0.3606022006033334\n",
      "train loss:0.40387016093163974\n",
      "train loss:0.3317659896847643\n",
      "train loss:0.5025742481238076\n",
      "train loss:0.3538596309422271\n",
      "train loss:0.5910683594397281\n",
      "train loss:0.42914837098717024\n",
      "train loss:0.35005959698181544\n",
      "train loss:0.6284305757733107\n",
      "train loss:0.5448468981619734\n",
      "train loss:0.43906594366479623\n",
      "train loss:0.32589776199523357\n",
      "train loss:0.39245021814717307\n",
      "train loss:0.39400305139115877\n",
      "train loss:0.395018196954697\n",
      "train loss:0.43823778785722584\n",
      "train loss:0.32579207499564494\n",
      "train loss:0.4660563967708551\n",
      "train loss:0.44939157775052346\n",
      "train loss:0.39846640987399573\n",
      "train loss:0.44293582071557674\n",
      "train loss:0.5210758529243512\n",
      "train loss:0.27371247012082367\n",
      "train loss:0.48503620561265853\n",
      "train loss:0.6967443801478542\n",
      "train loss:0.3693777498436707\n",
      "train loss:0.42235847678129024\n",
      "train loss:0.44187358170289803\n",
      "train loss:0.38558937084902195\n",
      "train loss:0.365104512317683\n",
      "train loss:0.3355697112657176\n",
      "train loss:0.36457575380674045\n",
      "train loss:0.40386528695115137\n",
      "train loss:0.39215509497373036\n",
      "train loss:0.4159141909842367\n",
      "train loss:0.4729875866705161\n",
      "train loss:0.41486384127271564\n",
      "train loss:0.40159674250220534\n",
      "train loss:0.4244975322919961\n",
      "train loss:0.3983569869961292\n",
      "train loss:0.35546759827812663\n",
      "train loss:0.4518084759506311\n",
      "train loss:0.48981801321539126\n",
      "train loss:0.4187314310069173\n",
      "train loss:0.35973803813153105\n",
      "train loss:0.31662398117142465\n",
      "train loss:0.5143308686610214\n",
      "train loss:0.4235121747291929\n",
      "train loss:0.38954052596075867\n",
      "train loss:0.3793143115073043\n",
      "train loss:0.3044447936674115\n",
      "train loss:0.3368385968373752\n",
      "train loss:0.48049020040990825\n",
      "train loss:0.37258783273549617\n",
      "train loss:0.41114306136939605\n",
      "train loss:0.540037286479561\n",
      "train loss:0.4147458057080747\n",
      "train loss:0.3622014317326204\n",
      "train loss:0.4911650196972085\n",
      "train loss:0.4386573465303269\n",
      "train loss:0.49848510405201407\n",
      "train loss:0.39037463559085234\n",
      "train loss:0.36681365303479974\n",
      "train loss:0.3427857809813147\n",
      "train loss:0.34892894870370705\n",
      "train loss:0.45390760194409085\n",
      "train loss:0.46780105473782885\n",
      "train loss:0.4075018056987519\n",
      "train loss:0.3133014682143296\n",
      "train loss:0.33813443532828463\n",
      "train loss:0.4046597473078964\n",
      "train loss:0.3993101649241717\n",
      "train loss:0.3944256682137533\n",
      "train loss:0.47034546338904726\n",
      "train loss:0.3532479246466679\n",
      "train loss:0.5195242610087054\n",
      "train loss:0.2954955445929469\n",
      "train loss:0.4072206151013693\n",
      "train loss:0.30667482844439176\n",
      "train loss:0.41789473250666986\n",
      "train loss:0.3557638110238227\n",
      "train loss:0.42697246529548294\n",
      "train loss:0.7264359338062601\n",
      "train loss:0.4851003070800507\n",
      "train loss:0.5675790684987063\n",
      "train loss:0.5119153842125117\n",
      "train loss:0.28102142093834426\n",
      "train loss:0.3652347101697419\n",
      "train loss:0.4735783340381727\n",
      "train loss:0.4369939941715045\n",
      "train loss:0.3362362487844895\n",
      "train loss:0.2775599119682664\n",
      "train loss:0.4861111345193577\n",
      "train loss:0.39211462610453607\n",
      "train loss:0.45922067182845266\n",
      "train loss:0.3970009249230204\n",
      "train loss:0.43547331652390237\n",
      "train loss:0.3842215131404558\n",
      "train loss:0.6216093243637814\n",
      "train loss:0.4566392015093832\n",
      "train loss:0.4840376699422763\n",
      "train loss:0.3493213071946728\n",
      "train loss:0.4308121987591301\n",
      "train loss:0.47669815981454616\n",
      "train loss:0.3469886919434488\n",
      "train loss:0.459405194615911\n",
      "train loss:0.5347707798443598\n",
      "train loss:0.5185085251679428\n",
      "train loss:0.38002039553016403\n",
      "train loss:0.3116402268128733\n",
      "train loss:0.5296028436040877\n",
      "train loss:0.2733478406159346\n",
      "train loss:0.5432893234632923\n",
      "train loss:0.46952966638426785\n",
      "train loss:0.41955854026000433\n",
      "train loss:0.5000830322142131\n",
      "train loss:0.39346999499576696\n",
      "train loss:0.4920058048901918\n",
      "train loss:0.371993899976839\n",
      "train loss:0.39889103026347306\n",
      "train loss:0.3250457458301528\n",
      "train loss:0.3854224948216764\n",
      "train loss:0.41192722955465366\n",
      "train loss:0.48746090663615044\n",
      "train loss:0.31807045643888104\n",
      "train loss:0.5194692909974249\n",
      "train loss:0.39880966395276923\n",
      "train loss:0.3978164556166632\n",
      "train loss:0.43952489478915596\n",
      "train loss:0.36463223944846973\n",
      "train loss:0.38897063786096175\n",
      "train loss:0.5450961762868809\n",
      "train loss:0.39096032346385234\n",
      "train loss:0.43281218786184517\n",
      "train loss:0.3613649993178118\n",
      "train loss:0.3434163990067079\n",
      "train loss:0.4051424863116189\n",
      "train loss:0.38825272003726047\n",
      "train loss:0.4849356264494821\n",
      "train loss:0.4910239667393818\n",
      "train loss:0.3795229068570958\n",
      "train loss:0.41487968912333495\n",
      "train loss:0.38448115827751045\n",
      "train loss:0.3673950472799956\n",
      "train loss:0.37773738161349546\n",
      "train loss:0.28343193711729225\n",
      "train loss:0.45198384306724365\n",
      "train loss:0.4530795451144676\n",
      "train loss:0.3580334478551939\n",
      "train loss:0.3393882325967589\n",
      "train loss:0.40284723052875576\n",
      "train loss:0.36421937661079135\n",
      "train loss:0.36352713285924915\n",
      "train loss:0.45117266133695716\n",
      "train loss:0.48323536896431923\n",
      "train loss:0.37026642310877717\n",
      "train loss:0.5248619070485988\n",
      "train loss:0.3200257387089057\n",
      "train loss:0.46131839127980356\n",
      "train loss:0.5153275087556474\n",
      "train loss:0.3172662042075441\n",
      "train loss:0.4723697576569744\n",
      "train loss:0.36378452270123346\n",
      "train loss:0.39150876037374893\n",
      "train loss:0.3869553668991926\n",
      "train loss:0.502552171364658\n",
      "train loss:0.394250754041514\n",
      "train loss:0.37867622580346\n",
      "train loss:0.43050631103286086\n",
      "train loss:0.34115403428376867\n",
      "train loss:0.35999778971558166\n",
      "train loss:0.3946761654123746\n",
      "train loss:0.3830199524343561\n",
      "train loss:0.4884136264212195\n",
      "train loss:0.45610488852842307\n",
      "train loss:0.5000734183473594\n",
      "train loss:0.39650804625447655\n",
      "train loss:0.3639827282215303\n",
      "train loss:0.41985282869438095\n",
      "train loss:0.45720880454305\n",
      "train loss:0.39404242852571225\n",
      "train loss:0.29491891892152233\n",
      "train loss:0.3833874720938512\n",
      "train loss:0.601482119304949\n",
      "train loss:0.5242284160147919\n",
      "train loss:0.5340080184166603\n",
      "train loss:0.38681437936206975\n",
      "train loss:0.37878874562798204\n",
      "train loss:0.32383105362729475\n",
      "train loss:0.4319082089942398\n",
      "train loss:0.44497535015531364\n",
      "train loss:0.4429382183898156\n",
      "train loss:0.5685304691671026\n",
      "train loss:0.38531063246685177\n",
      "train loss:0.28137388288304593\n",
      "train loss:0.38336811425421863\n",
      "train loss:0.5672346499256239\n",
      "train loss:0.4560729547795631\n",
      "train loss:0.5727523641249178\n",
      "train loss:0.4089315526684373\n",
      "train loss:0.5008852761609419\n",
      "train loss:0.49919415777632936\n",
      "train loss:0.39719915649348414\n",
      "train loss:0.260476616988235\n",
      "train loss:0.3590600636847137\n",
      "train loss:0.4104002635777787\n",
      "train loss:0.27911839485440637\n",
      "train loss:0.36748202329490104\n",
      "train loss:0.3630176821141466\n",
      "train loss:0.34701168263706395\n",
      "train loss:0.31320868922862266\n",
      "train loss:0.4367388001075432\n",
      "train loss:0.41997960434171383\n",
      "train loss:0.3893036351904244\n",
      "train loss:0.37109866277519105\n",
      "train loss:0.44796699992178657\n",
      "train loss:0.3334719065690998\n",
      "train loss:0.36382959154645755\n",
      "train loss:0.39726345686771786\n",
      "train loss:0.27712407840781383\n",
      "train loss:0.3887543571316728\n",
      "train loss:0.23569251228851398\n",
      "train loss:0.4165724069843997\n",
      "train loss:0.5884915944883689\n",
      "train loss:0.30270433483326337\n",
      "train loss:0.44275629191386146\n",
      "train loss:0.3441096732358374\n",
      "train loss:0.35703995742670047\n",
      "train loss:0.43734836925756454\n",
      "train loss:0.47332280810132354\n",
      "train loss:0.3480396980466718\n",
      "train loss:0.3134022984245203\n",
      "train loss:0.3538810011398034\n",
      "train loss:0.32150085657772237\n",
      "train loss:0.4893978956737844\n",
      "train loss:0.26457308096908283\n",
      "train loss:0.45496581224283494\n",
      "train loss:0.36095662005827983\n",
      "train loss:0.4962072799016851\n",
      "train loss:0.3650130875469767\n",
      "train loss:0.40232582988043075\n",
      "train loss:0.3724758193367256\n",
      "train loss:0.3621668208403683\n",
      "train loss:0.42147758304380517\n",
      "train loss:0.41170627980289565\n",
      "train loss:0.2971137031784571\n",
      "train loss:0.32332338630517776\n",
      "train loss:0.37113559050908557\n",
      "train loss:0.4321366363134011\n",
      "train loss:0.35996008637189125\n",
      "train loss:0.3162467929761646\n",
      "train loss:0.333224367143826\n",
      "train loss:0.37420493666472554\n",
      "train loss:0.36102350878933737\n",
      "train loss:0.5673189706745339\n",
      "train loss:0.38224493942071974\n",
      "train loss:0.3383186954511984\n",
      "train loss:0.301752999690043\n",
      "train loss:0.3490209199764353\n",
      "train loss:0.415272192812433\n",
      "train loss:0.3393795329759879\n",
      "train loss:0.32814342329487917\n",
      "train loss:0.7114529513616279\n",
      "train loss:0.32577134221490206\n",
      "train loss:0.39047900199452534\n",
      "train loss:0.4239954065235099\n",
      "train loss:0.2779074210984243\n",
      "train loss:0.2932403922150457\n",
      "train loss:0.44985547497542017\n",
      "train loss:0.49710509223943267\n",
      "train loss:0.34600178471382065\n",
      "train loss:0.3662291312227249\n",
      "train loss:0.550685988084804\n",
      "train loss:0.3271422430906146\n",
      "train loss:0.4385903798343196\n",
      "train loss:0.4316316401229484\n",
      "train loss:0.31326305759134\n",
      "train loss:0.43448297104333866\n",
      "train loss:0.267427287829081\n",
      "train loss:0.28965188312278706\n",
      "train loss:0.44250601707175297\n",
      "train loss:0.46229572585703393\n",
      "train loss:0.5011279698443921\n",
      "train loss:0.28127567159394423\n",
      "train loss:0.2954438384740517\n",
      "train loss:0.3580199205005071\n",
      "train loss:0.3333563979597217\n",
      "train loss:0.46497444877201843\n",
      "train loss:0.3890906296988133\n",
      "train loss:0.43708181469379154\n",
      "train loss:0.31709432541345356\n",
      "train loss:0.37441822253042184\n",
      "train loss:0.3556642639369921\n",
      "train loss:0.4617142021387338\n",
      "train loss:0.41549970499847994\n",
      "train loss:0.44168878920830834\n",
      "train loss:0.4345536236001405\n",
      "train loss:0.5234223748357826\n",
      "train loss:0.5529600067036865\n",
      "train loss:0.4076471507473303\n",
      "train loss:0.42871533548746543\n",
      "train loss:0.37383932650109963\n",
      "train loss:0.3689397318317377\n",
      "train loss:0.37078673330214096\n",
      "train loss:0.35209280234160156\n",
      "train loss:0.586871924034839\n",
      "train loss:0.38781067204743846\n",
      "train loss:0.30805948173628606\n",
      "train loss:0.39685442278077365\n",
      "train loss:0.4994021195628578\n",
      "train loss:0.40024073778851305\n",
      "train loss:0.48987234829335363\n",
      "train loss:0.5152303286525232\n",
      "train loss:0.3726829341104251\n",
      "train loss:0.3525006026783077\n",
      "train loss:0.3093730527592358\n",
      "train loss:0.38507848105356607\n",
      "train loss:0.5209627926204363\n",
      "train loss:0.32016381014526857\n",
      "train loss:0.32751721425714614\n",
      "train loss:0.41763578784197875\n",
      "train loss:0.44260835647935504\n",
      "train loss:0.4758970335085877\n",
      "train loss:0.3999837880246398\n",
      "train loss:0.3660823867837356\n",
      "train loss:0.3494755305402377\n",
      "train loss:0.47760899375089444\n",
      "train loss:0.3478170073903533\n",
      "train loss:0.3726038732588631\n",
      "train loss:0.36271895459296977\n",
      "train loss:0.3202971103617249\n",
      "train loss:0.379456543855041\n",
      "train loss:0.2865489903682183\n",
      "train loss:0.2789231042282724\n",
      "train loss:0.4546256687048579\n",
      "train loss:0.5321084849736415\n",
      "train loss:0.4067006716159012\n",
      "train loss:0.3121675629084969\n",
      "train loss:0.38263942484278923\n",
      "train loss:0.607300129883836\n",
      "train loss:0.37278817725223534\n",
      "train loss:0.41056273089241685\n",
      "train loss:0.32198312643446314\n",
      "train loss:0.3018822090072994\n",
      "train loss:0.2988477720898404\n",
      "train loss:0.3105275217494671\n",
      "train loss:0.3900729360130248\n",
      "train loss:0.4278518896898334\n",
      "train loss:0.46506349545137743\n",
      "train loss:0.46076807395775377\n",
      "train loss:0.4012680366065899\n",
      "train loss:0.30711211747899164\n",
      "train loss:0.34561604956462155\n",
      "train loss:0.39473955160540974\n",
      "train loss:0.35491482541110264\n",
      "train loss:0.40461363564204966\n",
      "train loss:0.38565652224274755\n",
      "train loss:0.46336043689301293\n",
      "train loss:0.3360026146494458\n",
      "train loss:0.2728522742776321\n",
      "train loss:0.38777519275984346\n",
      "train loss:0.3027026252327739\n",
      "train loss:0.32520961184049424\n",
      "train loss:0.3012668445557756\n",
      "train loss:0.32031808460478517\n",
      "train loss:0.40521148733131157\n",
      "train loss:0.3000081002172344\n",
      "train loss:0.43004453175039337\n",
      "train loss:0.2988219919164495\n",
      "train loss:0.36619897695110454\n",
      "train loss:0.4623660528966698\n",
      "train loss:0.336651940479897\n",
      "train loss:0.5296594422195031\n",
      "train loss:0.34405014329235073\n",
      "train loss:0.3312751598018199\n",
      "train loss:0.3679060100233318\n",
      "train loss:0.4203535398980724\n",
      "train loss:0.225472549039302\n",
      "train loss:0.506785713916662\n",
      "train loss:0.2965166740888821\n",
      "train loss:0.3282189807315341\n",
      "train loss:0.4024012126150664\n",
      "train loss:0.39626781930888455\n",
      "train loss:0.37803474427631273\n",
      "train loss:0.47932161907016374\n",
      "train loss:0.3330866706274346\n",
      "train loss:0.2569233229963723\n",
      "train loss:0.257629546240703\n",
      "train loss:0.5115535860925061\n",
      "train loss:0.35950278265955\n",
      "train loss:0.31585182448048793\n",
      "train loss:0.476262545582284\n",
      "train loss:0.3804632632479171\n",
      "train loss:0.27008401875739013\n",
      "train loss:0.4247715020087932\n",
      "train loss:0.4803148130092753\n",
      "train loss:0.30063010886903696\n",
      "train loss:0.3713293416830791\n",
      "train loss:0.3351248146455789\n",
      "train loss:0.34695163415524505\n",
      "train loss:0.44322207185700163\n",
      "train loss:0.3527590627388395\n",
      "train loss:0.3927042206825941\n",
      "train loss:0.3959611166433137\n",
      "train loss:0.4490098685586532\n",
      "train loss:0.2774060111463207\n",
      "train loss:0.43266614424760064\n",
      "train loss:0.2744361379795454\n",
      "train loss:0.4337123929809487\n",
      "train loss:0.28085748281232414\n",
      "train loss:0.295108671108636\n",
      "train loss:0.3090872449195841\n",
      "train loss:0.37646575719896747\n",
      "train loss:0.44254386214433744\n",
      "train loss:0.4033085445557603\n",
      "train loss:0.39094940475335904\n",
      "train loss:0.2885809192784953\n",
      "train loss:0.3942099547446379\n",
      "train loss:0.47813604306049584\n",
      "train loss:0.4159383411597809\n",
      "train loss:0.43661039039893773\n",
      "train loss:0.3505367196907575\n",
      "train loss:0.35770737298064664\n",
      "train loss:0.5351667524288605\n",
      "train loss:0.2776749338162893\n",
      "train loss:0.3972784277070018\n",
      "train loss:0.3014443192127859\n",
      "train loss:0.433678088494476\n",
      "train loss:0.43062004364532463\n",
      "train loss:0.35539214201744007\n",
      "train loss:0.40493833805684515\n",
      "train loss:0.39290542374065723\n",
      "train loss:0.4553337895634939\n",
      "train loss:0.36359670744558253\n",
      "train loss:0.40426898336540795\n",
      "train loss:0.4174529489212251\n",
      "train loss:0.3322669157270347\n",
      "train loss:0.4734190864935264\n",
      "train loss:0.41281092508359024\n",
      "train loss:0.2937560871500069\n",
      "train loss:0.421096274353909\n",
      "train loss:0.48562893781226385\n",
      "train loss:0.49804657751707077\n",
      "train loss:0.4730250324608061\n",
      "train loss:0.3827408141181114\n",
      "train loss:0.4177042425292181\n",
      "train loss:0.4533971234601342\n",
      "train loss:0.4786798432081929\n",
      "train loss:0.45808380665446213\n",
      "train loss:0.376584593987122\n",
      "train loss:0.47948310427265844\n",
      "train loss:0.349980002244589\n",
      "train loss:0.3565625088485232\n",
      "train loss:0.4338294458843329\n",
      "train loss:0.37650755956122844\n",
      "train loss:0.5039850925890306\n",
      "train loss:0.397940527208477\n",
      "train loss:0.49037842789725306\n",
      "train loss:0.48636447124886945\n",
      "train loss:0.31789921784279235\n",
      "train loss:0.31893564179617445\n",
      "train loss:0.2777318588289035\n",
      "train loss:0.3221543545531512\n",
      "train loss:0.33993748658462314\n",
      "train loss:0.4383258909446697\n",
      "train loss:0.40647889547807475\n",
      "train loss:0.5477585240297025\n",
      "train loss:0.24643851115789317\n",
      "train loss:0.4481816720971773\n",
      "train loss:0.4854701807643123\n",
      "train loss:0.30821844984561997\n",
      "train loss:0.3442050510236612\n",
      "train loss:0.3048433728458202\n",
      "train loss:0.31552640697173273\n",
      "train loss:0.3710920624686217\n",
      "train loss:0.4240661646252652\n",
      "train loss:0.2740358334395027\n",
      "train loss:0.32222222248477306\n",
      "train loss:0.2309817096358523\n",
      "train loss:0.38853236299818145\n",
      "train loss:0.26343183409875276\n",
      "train loss:0.38477795086375105\n",
      "train loss:0.3303261386828455\n",
      "train loss:0.35370584177689307\n",
      "train loss:0.45253545790776856\n",
      "train loss:0.3328202216742467\n",
      "train loss:0.39817679084456836\n",
      "train loss:0.3746915058890613\n",
      "train loss:0.3746497433606548\n",
      "train loss:0.3903796176119904\n",
      "train loss:0.5368263444011319\n",
      "train loss:0.28210601169981947\n",
      "train loss:0.3985897918609825\n",
      "train loss:0.36944090107762617\n",
      "train loss:0.41354917913479006\n",
      "train loss:0.39768389521381964\n",
      "train loss:0.41103976836129247\n",
      "train loss:0.2984205095308382\n",
      "train loss:0.33377740945388595\n",
      "train loss:0.40546554853221045\n",
      "train loss:0.31015136114016084\n",
      "train loss:0.35632452482976035\n",
      "train loss:0.2879049705927008\n",
      "train loss:0.5290150660105737\n",
      "train loss:0.29406137129500265\n",
      "train loss:0.4123612101711745\n",
      "train loss:0.3718233836991307\n",
      "train loss:0.47913817675777604\n",
      "train loss:0.45250621964993953\n",
      "train loss:0.27925881582352985\n",
      "train loss:0.3750402166210804\n",
      "train loss:0.3541602094136453\n",
      "train loss:0.3863091583081802\n",
      "=== epoch:14, train acc:0.825, test acc:0.843 ===\n",
      "train loss:0.4348785608680881\n",
      "train loss:0.39862782322336093\n",
      "train loss:0.45357648547076623\n",
      "train loss:0.403319748329245\n",
      "train loss:0.35393337654943813\n",
      "train loss:0.38195528179344057\n",
      "train loss:0.2783629756446509\n",
      "train loss:0.48564517132344165\n",
      "train loss:0.42517737063868777\n",
      "train loss:0.37574106631543047\n",
      "train loss:0.3056790717812282\n",
      "train loss:0.3080598231364166\n",
      "train loss:0.4793177408412659\n",
      "train loss:0.40799905698460215\n",
      "train loss:0.4318676267076782\n",
      "train loss:0.5221914330322261\n",
      "train loss:0.4874353376533838\n",
      "train loss:0.3699957542403671\n",
      "train loss:0.35197214575382596\n",
      "train loss:0.38250399825619935\n",
      "train loss:0.43379712490658384\n",
      "train loss:0.46485772690269234\n",
      "train loss:0.30280694473832265\n",
      "train loss:0.5582322518701638\n",
      "train loss:0.41929946638635207\n",
      "train loss:0.323816541103225\n",
      "train loss:0.3200490283413351\n",
      "train loss:0.29194449651029847\n",
      "train loss:0.36684400189991506\n",
      "train loss:0.4129060360276693\n",
      "train loss:0.41162860787526817\n",
      "train loss:0.4131886049133394\n",
      "train loss:0.2974978602909862\n",
      "train loss:0.2856000091945158\n",
      "train loss:0.3235449135586292\n",
      "train loss:0.3984770398913365\n",
      "train loss:0.37596867307390525\n",
      "train loss:0.4326133911222433\n",
      "train loss:0.3878589737117227\n",
      "train loss:0.3850083345631474\n",
      "train loss:0.5821670254098955\n",
      "train loss:0.3417251386990958\n",
      "train loss:0.4083424896060378\n",
      "train loss:0.3878428893676664\n",
      "train loss:0.43382512754581737\n",
      "train loss:0.3717945507040185\n",
      "train loss:0.33629555581187004\n",
      "train loss:0.35111399094574947\n",
      "train loss:0.37432045009983644\n",
      "train loss:0.49887492129656136\n",
      "train loss:0.4573502705238933\n",
      "train loss:0.42097623809544826\n",
      "train loss:0.44485651407081905\n",
      "train loss:0.412987902671131\n",
      "train loss:0.3671261783091691\n",
      "train loss:0.3159972170910966\n",
      "train loss:0.4115281435020431\n",
      "train loss:0.400104667517409\n",
      "train loss:0.30277715994120413\n",
      "train loss:0.2755101352528782\n",
      "train loss:0.2909801512103096\n",
      "train loss:0.37547075343336295\n",
      "train loss:0.46588462980143847\n",
      "train loss:0.3596833609942031\n",
      "train loss:0.2731530197804294\n",
      "train loss:0.36991540466636386\n",
      "train loss:0.3819678505908183\n",
      "train loss:0.48932215618948915\n",
      "train loss:0.2841934617977228\n",
      "train loss:0.4685706670354611\n",
      "train loss:0.270709768446052\n",
      "train loss:0.3046517129364177\n",
      "train loss:0.4046222421361778\n",
      "train loss:0.31452550112789357\n",
      "train loss:0.37628054294545643\n",
      "train loss:0.39400910764112645\n",
      "train loss:0.5013995519118731\n",
      "train loss:0.35585014006749544\n",
      "train loss:0.5211778739455338\n",
      "train loss:0.366897880666963\n",
      "train loss:0.4472266493961147\n",
      "train loss:0.4929359088440713\n",
      "train loss:0.3567095020979081\n",
      "train loss:0.435330322522582\n",
      "train loss:0.45979259950739\n",
      "train loss:0.36320707274138747\n",
      "train loss:0.4246861002857569\n",
      "train loss:0.5871425126568036\n",
      "train loss:0.3267983563757734\n",
      "train loss:0.45647955608887547\n",
      "train loss:0.3538259992461958\n",
      "train loss:0.412588258415372\n",
      "train loss:0.33798416049715563\n",
      "train loss:0.34563100265803903\n",
      "train loss:0.3225736650032435\n",
      "train loss:0.5212260604190129\n",
      "train loss:0.4095797875892385\n",
      "train loss:0.4484116330279359\n",
      "train loss:0.4150017533000427\n",
      "train loss:0.4674731951849617\n",
      "train loss:0.3754258299117751\n",
      "train loss:0.47708741317639664\n",
      "train loss:0.36289030751115253\n",
      "train loss:0.4041630065377184\n",
      "train loss:0.3619798726109078\n",
      "train loss:0.41513100150058674\n",
      "train loss:0.43341033278306573\n",
      "train loss:0.4712819624078264\n",
      "train loss:0.338726727165019\n",
      "train loss:0.4023793546307941\n",
      "train loss:0.5683076429383902\n",
      "train loss:0.44665935170636983\n",
      "train loss:0.41038813133713425\n",
      "train loss:0.5216496465550695\n",
      "train loss:0.337561110316489\n",
      "train loss:0.33821592882205737\n",
      "train loss:0.2499506698792157\n",
      "train loss:0.3476013020535151\n",
      "train loss:0.34609681248065643\n",
      "train loss:0.38349842713200016\n",
      "train loss:0.3097171992442438\n",
      "train loss:0.3298753254478519\n",
      "train loss:0.5009941843978662\n",
      "train loss:0.4286844120543586\n",
      "train loss:0.48128019346153345\n",
      "train loss:0.3501031893729135\n",
      "train loss:0.4238090263521933\n",
      "train loss:0.33222769022084725\n",
      "train loss:0.4442060886003852\n",
      "train loss:0.38751936386776814\n",
      "train loss:0.4666864585802778\n",
      "train loss:0.3824566265870201\n",
      "train loss:0.33451208988430037\n",
      "train loss:0.5167657935870114\n",
      "train loss:0.6532509925565789\n",
      "train loss:0.30601368791622063\n",
      "train loss:0.3440441575053523\n",
      "train loss:0.3901674267512705\n",
      "train loss:0.25580456270339014\n",
      "train loss:0.41241518426934626\n",
      "train loss:0.4662095047178099\n",
      "train loss:0.32662799644544677\n",
      "train loss:0.3943228545522654\n",
      "train loss:0.3787750939930662\n",
      "train loss:0.34372715421855005\n",
      "train loss:0.4480557154816349\n",
      "train loss:0.47339098967382376\n",
      "train loss:0.3114544818989639\n",
      "train loss:0.3332344103495889\n",
      "train loss:0.41214874930279577\n",
      "train loss:0.36081685825135695\n",
      "train loss:0.3493734184611534\n",
      "train loss:0.27519947342674267\n",
      "train loss:0.40483992810785246\n",
      "train loss:0.38348570080033156\n",
      "train loss:0.4800867514145974\n",
      "train loss:0.49801824344588236\n",
      "train loss:0.39535735446576226\n",
      "train loss:0.3756834779342423\n",
      "train loss:0.42245743568999344\n",
      "train loss:0.27317463845015916\n",
      "train loss:0.35316045001032803\n",
      "train loss:0.4386433516600951\n",
      "train loss:0.37373954815396154\n",
      "train loss:0.3601887422805002\n",
      "train loss:0.25874259145412404\n",
      "train loss:0.26114750985258284\n",
      "train loss:0.29400969339247\n",
      "train loss:0.27956935653839254\n",
      "train loss:0.23002084626290525\n",
      "train loss:0.35246921794096947\n",
      "train loss:0.2879723082016388\n",
      "train loss:0.329629767284516\n",
      "train loss:0.4102670810614885\n",
      "train loss:0.5145397374744286\n",
      "train loss:0.3032767497824749\n",
      "train loss:0.5599424332273122\n",
      "train loss:0.5133441638017859\n",
      "train loss:0.34119245144875193\n",
      "train loss:0.40926309954354956\n",
      "train loss:0.5007178031650653\n",
      "train loss:0.4999640316641378\n",
      "train loss:0.3059032762261469\n",
      "train loss:0.3870029209243775\n",
      "train loss:0.30777935266420964\n",
      "train loss:0.3725781911016454\n",
      "train loss:0.416370262876597\n",
      "train loss:0.43186163053833715\n",
      "train loss:0.3520188124504475\n",
      "train loss:0.5029828426521323\n",
      "train loss:0.2791057377573281\n",
      "train loss:0.33887115024436226\n",
      "train loss:0.4223790942776494\n",
      "train loss:0.4932731383492458\n",
      "train loss:0.4314157858030954\n",
      "train loss:0.3562455249713304\n",
      "train loss:0.4483099110087768\n",
      "train loss:0.29903531820361584\n",
      "train loss:0.34409044088798013\n",
      "train loss:0.3193689562744738\n",
      "train loss:0.43724997178028124\n",
      "train loss:0.4243081356519294\n",
      "train loss:0.3416328689181597\n",
      "train loss:0.3843653138508524\n",
      "train loss:0.291824186721281\n",
      "train loss:0.335687232393264\n",
      "train loss:0.44021465499175716\n",
      "train loss:0.26861900827940827\n",
      "train loss:0.5548015107273971\n",
      "train loss:0.40784272031282554\n",
      "train loss:0.5447648110269366\n",
      "train loss:0.31909654622965183\n",
      "train loss:0.40252464128730686\n",
      "train loss:0.3430793557136241\n",
      "train loss:0.35090172745181214\n",
      "train loss:0.389423579896146\n",
      "train loss:0.41205555096109914\n",
      "train loss:0.37483910990512653\n",
      "train loss:0.3151396132741913\n",
      "train loss:0.26993829941201314\n",
      "train loss:0.43202923913965674\n",
      "train loss:0.38578627991080794\n",
      "train loss:0.40202343244168054\n",
      "train loss:0.3206983610780162\n",
      "train loss:0.3359244518153154\n",
      "train loss:0.4108443654985065\n",
      "train loss:0.39946052664737375\n",
      "train loss:0.26679464900417177\n",
      "train loss:0.30008783072135553\n",
      "train loss:0.3629395665188971\n",
      "train loss:0.40927048441894115\n",
      "train loss:0.41369774964791006\n",
      "train loss:0.4811564313235981\n",
      "train loss:0.22844565097361605\n",
      "train loss:0.5170042515120401\n",
      "train loss:0.531949599308495\n",
      "train loss:0.3631911091843423\n",
      "train loss:0.31421910296714145\n",
      "train loss:0.5165312723020175\n",
      "train loss:0.3247462053368026\n",
      "train loss:0.3959130113091715\n",
      "train loss:0.32200119702574503\n",
      "train loss:0.3174394858975253\n",
      "train loss:0.4691570230602743\n",
      "train loss:0.418360274146379\n",
      "train loss:0.32740500829672164\n",
      "train loss:0.40481953790336533\n",
      "train loss:0.43157855327429145\n",
      "train loss:0.36724953278695854\n",
      "train loss:0.5790734242792387\n",
      "train loss:0.3447982224392664\n",
      "train loss:0.2845224844305366\n",
      "train loss:0.2984764265053433\n",
      "train loss:0.4458474374421547\n",
      "train loss:0.40697479754310656\n",
      "train loss:0.3355576860849981\n",
      "train loss:0.5075072055301391\n",
      "train loss:0.3841618356675611\n",
      "train loss:0.4118985170417377\n",
      "train loss:0.4379438514102151\n",
      "train loss:0.455424248441195\n",
      "train loss:0.3985236042509886\n",
      "train loss:0.4344997001424595\n",
      "train loss:0.3891504802803611\n",
      "train loss:0.3711331648023704\n",
      "train loss:0.46203628016115156\n",
      "train loss:0.35897922906065005\n",
      "train loss:0.3099109433660489\n",
      "train loss:0.30395309955427663\n",
      "train loss:0.31715548876190414\n",
      "train loss:0.4078854185383313\n",
      "train loss:0.2931553145507739\n",
      "train loss:0.3690826374887877\n",
      "train loss:0.2688707673486152\n",
      "train loss:0.4891273056884466\n",
      "train loss:0.31616253065175226\n",
      "train loss:0.38760132384904106\n",
      "train loss:0.2629121417760616\n",
      "train loss:0.4045939443560867\n",
      "train loss:0.459377022038553\n",
      "train loss:0.40453896836071496\n",
      "train loss:0.3817930984377113\n",
      "train loss:0.397283413604736\n",
      "train loss:0.3667795964925993\n",
      "train loss:0.21983853560254127\n",
      "train loss:0.34196327787378733\n",
      "train loss:0.2057817628833519\n",
      "train loss:0.34779078813962483\n",
      "train loss:0.3675600649286853\n",
      "train loss:0.37056532716063645\n",
      "train loss:0.39365277195005943\n",
      "train loss:0.49540034780106057\n",
      "train loss:0.2971147544765222\n",
      "train loss:0.4156381910853655\n",
      "train loss:0.34900733339522155\n",
      "train loss:0.4000926936014983\n",
      "train loss:0.3267786552789092\n",
      "train loss:0.4066363732171807\n",
      "train loss:0.4277013718408996\n",
      "train loss:0.3290827458086487\n",
      "train loss:0.2770337536819984\n",
      "train loss:0.4677724637294722\n",
      "train loss:0.4310714973077492\n",
      "train loss:0.25646192382611604\n",
      "train loss:0.29052850360183613\n",
      "train loss:0.36306000148250306\n",
      "train loss:0.2911510975550219\n",
      "train loss:0.33709477435615104\n",
      "train loss:0.3249085906990027\n",
      "train loss:0.38800360854996524\n",
      "train loss:0.40893201394569423\n",
      "train loss:0.36056095513184266\n",
      "train loss:0.26924896664702136\n",
      "train loss:0.3700838625191389\n",
      "train loss:0.37576189499925305\n",
      "train loss:0.2735213354989022\n",
      "train loss:0.5703453019909276\n",
      "train loss:0.324591108608304\n",
      "train loss:0.3649457406307146\n",
      "train loss:0.42299974197016127\n",
      "train loss:0.3505015752020537\n",
      "train loss:0.36290743665337083\n",
      "train loss:0.18126293188101614\n",
      "train loss:0.4023123304869378\n",
      "train loss:0.40883589892095373\n",
      "train loss:0.26385902256030713\n",
      "train loss:0.5852247878244704\n",
      "train loss:0.4734539887603587\n",
      "train loss:0.26211001067940903\n",
      "train loss:0.3376470723319342\n",
      "train loss:0.29736233017395597\n",
      "train loss:0.23520665634130047\n",
      "train loss:0.4562034488536469\n",
      "train loss:0.3607164991176881\n",
      "train loss:0.16425419321679122\n",
      "train loss:0.28872952340018515\n",
      "train loss:0.3175429659493068\n",
      "train loss:0.3436711043118175\n",
      "train loss:0.3556826380736448\n",
      "train loss:0.29133647821254854\n",
      "train loss:0.46039399711490225\n",
      "train loss:0.32203817559463643\n",
      "train loss:0.26722423074650037\n",
      "train loss:0.4138956342643463\n",
      "train loss:0.2273862710927627\n",
      "train loss:0.3537388251653762\n",
      "train loss:0.3849802121801411\n",
      "train loss:0.346328605644497\n",
      "train loss:0.24169766375805557\n",
      "train loss:0.41166344952421113\n",
      "train loss:0.40787632247423905\n",
      "train loss:0.39224360773810185\n",
      "train loss:0.34114828520979884\n",
      "train loss:0.3320933939768163\n",
      "train loss:0.44051489313626896\n",
      "train loss:0.41369489865494347\n",
      "train loss:0.502596545554458\n",
      "train loss:0.4268646775660509\n",
      "train loss:0.42603080007984595\n",
      "train loss:0.30353327779720446\n",
      "train loss:0.39928677865294915\n",
      "train loss:0.2977156886587208\n",
      "train loss:0.27245521877325274\n",
      "train loss:0.33553160499172763\n",
      "train loss:0.42441331098992985\n",
      "train loss:0.370771754205786\n",
      "train loss:0.2870331293248151\n",
      "train loss:0.42428531154134097\n",
      "train loss:0.39298669193172964\n",
      "train loss:0.33647170403034354\n",
      "train loss:0.41095946770494685\n",
      "train loss:0.4225392822879405\n",
      "train loss:0.32362800663523417\n",
      "train loss:0.3955538936525636\n",
      "train loss:0.3120933958819484\n",
      "train loss:0.5075289742181168\n",
      "train loss:0.46809613609528017\n",
      "train loss:0.19288460435781313\n",
      "train loss:0.45035198393378795\n",
      "train loss:0.3401488524907479\n",
      "train loss:0.6060122933812727\n",
      "train loss:0.4572167946588116\n",
      "train loss:0.33880670617234054\n",
      "train loss:0.2532587817166692\n",
      "train loss:0.4417703482964405\n",
      "train loss:0.3173440018527714\n",
      "train loss:0.28066634165821713\n",
      "train loss:0.38688285153666463\n",
      "train loss:0.3996599651926819\n",
      "train loss:0.42854356776647834\n",
      "train loss:0.5150673563866399\n",
      "train loss:0.4193227470640577\n",
      "train loss:0.5121291019376865\n",
      "train loss:0.3173465129376199\n",
      "train loss:0.40649224644736587\n",
      "train loss:0.3759908751428569\n",
      "train loss:0.36052050911567257\n",
      "train loss:0.3813491596435605\n",
      "train loss:0.26592058710542643\n",
      "train loss:0.42980234245181703\n",
      "train loss:0.33909528439957676\n",
      "train loss:0.39320295666191674\n",
      "train loss:0.3280636778813681\n",
      "train loss:0.3820640008063878\n",
      "train loss:0.3245526298553627\n",
      "train loss:0.33261151857969196\n",
      "train loss:0.2730588574608243\n",
      "train loss:0.3192242532854142\n",
      "train loss:0.33407455821627685\n",
      "train loss:0.34314620977899174\n",
      "train loss:0.3341928924551503\n",
      "train loss:0.3533539066907027\n",
      "train loss:0.3076508803102161\n",
      "train loss:0.29760374323601224\n",
      "train loss:0.35715878559450426\n",
      "train loss:0.2807089512361283\n",
      "train loss:0.4993506301453033\n",
      "train loss:0.3882451265845075\n",
      "train loss:0.35197238345918985\n",
      "train loss:0.4395032098274836\n",
      "train loss:0.3460274303855514\n",
      "train loss:0.4716994966019767\n",
      "train loss:0.28376964701616364\n",
      "train loss:0.37315344982040805\n",
      "train loss:0.2926710862225605\n",
      "train loss:0.3969060418690148\n",
      "train loss:0.342970595481688\n",
      "train loss:0.38576518467609533\n",
      "train loss:0.4863579198143925\n",
      "train loss:0.4414909503591302\n",
      "train loss:0.35104569644744515\n",
      "train loss:0.20778471734656123\n",
      "train loss:0.5019430130879075\n",
      "train loss:0.3408569013725664\n",
      "train loss:0.3476558668346798\n",
      "train loss:0.3193902680761751\n",
      "train loss:0.3839144540883015\n",
      "train loss:0.3688261924608205\n",
      "train loss:0.3714452412117483\n",
      "train loss:0.3753711247380396\n",
      "train loss:0.27284107721588424\n",
      "train loss:0.49274114514142364\n",
      "train loss:0.36778303866233\n",
      "train loss:0.3668768793419713\n",
      "train loss:0.32507879941129636\n",
      "train loss:0.5288800000990388\n",
      "train loss:0.4886605335412896\n",
      "train loss:0.3694246187497792\n",
      "train loss:0.4790412155759984\n",
      "train loss:0.3776229609781142\n",
      "train loss:0.3044086305026036\n",
      "train loss:0.4722823077694297\n",
      "train loss:0.31951268238574954\n",
      "train loss:0.2556697392205694\n",
      "train loss:0.36997006461399096\n",
      "train loss:0.3170428174525098\n",
      "train loss:0.396836534160461\n",
      "train loss:0.4432188658990323\n",
      "train loss:0.30684071827061304\n",
      "train loss:0.35345801563495316\n",
      "train loss:0.2921320329118045\n",
      "train loss:0.4166184362132158\n",
      "train loss:0.33460874526391765\n",
      "train loss:0.33625600968097463\n",
      "train loss:0.2789201379927626\n",
      "train loss:0.40953718353263413\n",
      "train loss:0.4181102616277441\n",
      "train loss:0.40807914912465826\n",
      "train loss:0.41781738934824864\n",
      "train loss:0.43085840456629904\n",
      "train loss:0.3293638962324584\n",
      "train loss:0.306147177690953\n",
      "train loss:0.29086088472802707\n",
      "train loss:0.3531407676668363\n",
      "train loss:0.3211629592635595\n",
      "train loss:0.3816873092271608\n",
      "train loss:0.4014632014247674\n",
      "train loss:0.3853759542898096\n",
      "train loss:0.26617135100588596\n",
      "train loss:0.27321526404294444\n",
      "train loss:0.31579952717596393\n",
      "train loss:0.312786290082809\n",
      "train loss:0.38179634374855603\n",
      "train loss:0.4289119718970571\n",
      "train loss:0.3149920990054551\n",
      "train loss:0.4577157730835477\n",
      "train loss:0.45685486928864755\n",
      "train loss:0.4472320328817752\n",
      "train loss:0.4009427010678991\n",
      "train loss:0.2833088908415024\n",
      "train loss:0.4508260469135562\n",
      "train loss:0.3458129899545937\n",
      "train loss:0.3194190145302869\n",
      "train loss:0.46193593766209423\n",
      "train loss:0.46854211179388744\n",
      "train loss:0.2852099008684155\n",
      "train loss:0.36278144991901895\n",
      "train loss:0.4582924217909603\n",
      "train loss:0.306536064141442\n",
      "train loss:0.40841851464305085\n",
      "train loss:0.364392798528559\n",
      "train loss:0.32986238320824074\n",
      "train loss:0.2971174028228468\n",
      "train loss:0.4602882042190947\n",
      "train loss:0.3423076792625232\n",
      "train loss:0.2633796481352062\n",
      "train loss:0.31745518069973067\n",
      "train loss:0.3892391239991385\n",
      "train loss:0.4295180509022064\n",
      "train loss:0.5042667985302259\n",
      "train loss:0.4136139435747244\n",
      "train loss:0.4044632657991808\n",
      "train loss:0.30362620896406123\n",
      "train loss:0.4426849141966778\n",
      "train loss:0.27334717311545953\n",
      "train loss:0.22935790884078358\n",
      "train loss:0.5021113065155663\n",
      "train loss:0.3903441062617858\n",
      "train loss:0.34701397943932283\n",
      "train loss:0.41666540750224795\n",
      "train loss:0.4501535735998112\n",
      "train loss:0.34260758126017243\n",
      "train loss:0.3306909791839626\n",
      "train loss:0.26888375924798047\n",
      "train loss:0.35369186326868357\n",
      "train loss:0.43467054879152217\n",
      "train loss:0.34259476615764994\n",
      "train loss:0.33143384071725046\n",
      "train loss:0.46548517672655876\n",
      "train loss:0.37677299900108097\n",
      "train loss:0.3194291936165731\n",
      "train loss:0.34040659303548165\n",
      "train loss:0.33930163597880414\n",
      "train loss:0.3876908595435273\n",
      "train loss:0.37895468811233635\n",
      "train loss:0.564194840581914\n",
      "train loss:0.26494866629273583\n",
      "train loss:0.3824834356992184\n",
      "train loss:0.3994113745553412\n",
      "train loss:0.5176955040129073\n",
      "train loss:0.25325485213924664\n",
      "train loss:0.2853287909352031\n",
      "train loss:0.37847647516164984\n",
      "train loss:0.28810976662385307\n",
      "train loss:0.21757519831316646\n",
      "train loss:0.38873770907375665\n",
      "train loss:0.3952476945410153\n",
      "train loss:0.40129714191157206\n",
      "train loss:0.27036886948909133\n",
      "train loss:0.5055986747921224\n",
      "train loss:0.3395379818452461\n",
      "train loss:0.35112847823187665\n",
      "train loss:0.4195802549494027\n",
      "train loss:0.44311156848160765\n",
      "train loss:0.4292043342446272\n",
      "train loss:0.42758132322086767\n",
      "train loss:0.28329106400117166\n",
      "train loss:0.22057634852265784\n",
      "train loss:0.3503317782033857\n",
      "train loss:0.4374379166547153\n",
      "train loss:0.38151023540450846\n",
      "train loss:0.33581962529919873\n",
      "train loss:0.3907324209052998\n",
      "train loss:0.26759965478457415\n",
      "train loss:0.42422996070947067\n",
      "train loss:0.4157741447889475\n",
      "train loss:0.40647693414319513\n",
      "train loss:0.22832794265657305\n",
      "train loss:0.3224193543198325\n",
      "train loss:0.39555738202092505\n",
      "train loss:0.33906108138565755\n",
      "train loss:0.3385049265652158\n",
      "train loss:0.3384431594596177\n",
      "train loss:0.389895581136103\n",
      "train loss:0.3058400438714382\n",
      "train loss:0.26363543513825105\n",
      "train loss:0.48110844482305937\n",
      "train loss:0.33010086349030615\n",
      "train loss:0.3872555928598006\n",
      "train loss:0.36733755258890916\n",
      "train loss:0.3316079273014019\n",
      "train loss:0.44935334113065273\n",
      "train loss:0.26843838927140234\n",
      "train loss:0.3923399460946246\n",
      "train loss:0.3116917620597838\n",
      "train loss:0.6107979367382942\n",
      "train loss:0.3444578360448707\n",
      "train loss:0.1671749290136362\n",
      "train loss:0.34713700482484205\n",
      "train loss:0.30304953536542567\n",
      "train loss:0.27524471709431786\n",
      "train loss:0.4638376545529325\n",
      "train loss:0.353882743690998\n",
      "train loss:0.5070554433394652\n",
      "train loss:0.35645093502041014\n",
      "train loss:0.4638577150458086\n",
      "train loss:0.4505752910675988\n",
      "train loss:0.33070042971422464\n",
      "train loss:0.377607392999437\n",
      "train loss:0.35748324693941824\n",
      "=== epoch:15, train acc:0.833, test acc:0.848 ===\n",
      "train loss:0.30952089417332423\n",
      "train loss:0.29943244520396867\n",
      "train loss:0.3749336217119867\n",
      "train loss:0.30541162362265695\n",
      "train loss:0.42737832631125855\n",
      "train loss:0.3749123931720995\n",
      "train loss:0.37259344360723745\n",
      "train loss:0.2977682731532352\n",
      "train loss:0.1569619365885022\n",
      "train loss:0.44829689401820283\n",
      "train loss:0.3370591089232749\n",
      "train loss:0.3918254184761021\n",
      "train loss:0.34367270723248167\n",
      "train loss:0.4097169543690714\n",
      "train loss:0.4115138446332629\n",
      "train loss:0.3603941513178699\n",
      "train loss:0.28071010379125577\n",
      "train loss:0.2506529951799092\n",
      "train loss:0.34547182999025383\n",
      "train loss:0.4022296127999597\n",
      "train loss:0.43293690337778223\n",
      "train loss:0.45406105975984606\n",
      "train loss:0.3595862831217391\n",
      "train loss:0.27894745179939795\n",
      "train loss:0.35658580011593477\n",
      "train loss:0.32595927883831544\n",
      "train loss:0.41899852117581554\n",
      "train loss:0.42219007011561216\n",
      "train loss:0.31908393211415403\n",
      "train loss:0.4067980027096008\n",
      "train loss:0.26694799045145046\n",
      "train loss:0.2811628177079742\n",
      "train loss:0.35469109778799174\n",
      "train loss:0.4483717819479213\n",
      "train loss:0.3879916757483119\n",
      "train loss:0.4409046561367487\n",
      "train loss:0.3249056806649138\n",
      "train loss:0.48482500108581517\n",
      "train loss:0.4469215222172551\n",
      "train loss:0.3829904490336209\n",
      "train loss:0.2644986590893724\n",
      "train loss:0.3079838029119415\n",
      "train loss:0.3321413212701011\n",
      "train loss:0.3990991586363141\n",
      "train loss:0.35527727376432766\n",
      "train loss:0.33620340262144055\n",
      "train loss:0.2778091158592148\n",
      "train loss:0.34044780250669215\n",
      "train loss:0.23042042088748244\n",
      "train loss:0.30833920350186966\n",
      "train loss:0.3150213934209807\n",
      "train loss:0.45660526346918073\n",
      "train loss:0.3440688476129201\n",
      "train loss:0.2591781154128378\n",
      "train loss:0.28600971623608307\n",
      "train loss:0.34553942830675605\n",
      "train loss:0.3043457252107372\n",
      "train loss:0.5638719170427758\n",
      "train loss:0.33022769718384787\n",
      "train loss:0.2774173840663135\n",
      "train loss:0.33716515010492626\n",
      "train loss:0.2991883960718796\n",
      "train loss:0.36385854713996124\n",
      "train loss:0.34430160080007466\n",
      "train loss:0.4296141569369747\n",
      "train loss:0.346564013108956\n",
      "train loss:0.20941605660312568\n",
      "train loss:0.373912651283434\n",
      "train loss:0.36466198217607615\n",
      "train loss:0.31171714759995983\n",
      "train loss:0.37893043553600336\n",
      "train loss:0.3682925247864057\n",
      "train loss:0.392445320085865\n",
      "train loss:0.4481750293695591\n",
      "train loss:0.3310205884592669\n",
      "train loss:0.3689515567927708\n",
      "train loss:0.40989333676331896\n",
      "train loss:0.24509920808155386\n",
      "train loss:0.3575393717529649\n",
      "train loss:0.2380009741799537\n",
      "train loss:0.2802749825771536\n",
      "train loss:0.28149880429691737\n",
      "train loss:0.3472774488743454\n",
      "train loss:0.32799360044577425\n",
      "train loss:0.34371584370636776\n",
      "train loss:0.29761260377070964\n",
      "train loss:0.3333235310958031\n",
      "train loss:0.38981873869532613\n",
      "train loss:0.38954422907401215\n",
      "train loss:0.2400831452589839\n",
      "train loss:0.4301108545957242\n",
      "train loss:0.46058525916666787\n",
      "train loss:0.34724725307252896\n",
      "train loss:0.3703230321626239\n",
      "train loss:0.37629050055361873\n",
      "train loss:0.35080766298243604\n",
      "train loss:0.26786133085212194\n",
      "train loss:0.3950162986922588\n",
      "train loss:0.4123210738878825\n",
      "train loss:0.35729052436557096\n",
      "train loss:0.3699799131238819\n",
      "train loss:0.32635293199176596\n",
      "train loss:0.36412818958644144\n",
      "train loss:0.3402550945648278\n",
      "train loss:0.36242007377903124\n",
      "train loss:0.4254734539498342\n",
      "train loss:0.3341871118241817\n",
      "train loss:0.28081891224687505\n",
      "train loss:0.38160978942250146\n",
      "train loss:0.34495733268488205\n",
      "train loss:0.34367883008300815\n",
      "train loss:0.2990889521174214\n",
      "train loss:0.3702850381228588\n",
      "train loss:0.43237847990211903\n",
      "train loss:0.21981185557519844\n",
      "train loss:0.23817751380920935\n",
      "train loss:0.4040018032467493\n",
      "train loss:0.3954635564528811\n",
      "train loss:0.3174265015825451\n",
      "train loss:0.3869851438938203\n",
      "train loss:0.36374761266060596\n",
      "train loss:0.3743010477540098\n",
      "train loss:0.22157587865178358\n",
      "train loss:0.3326083399135122\n",
      "train loss:0.3478339837605924\n",
      "train loss:0.31363181019364017\n",
      "train loss:0.4390036821130311\n",
      "train loss:0.3896880982820579\n",
      "train loss:0.3562513709472671\n",
      "train loss:0.39054145979541\n",
      "train loss:0.39345964034627284\n",
      "train loss:0.233197702893704\n",
      "train loss:0.4059706776149752\n",
      "train loss:0.3899134852866704\n",
      "train loss:0.232830883755059\n",
      "train loss:0.3680729786418814\n",
      "train loss:0.2727570141207736\n",
      "train loss:0.4780898671337359\n",
      "train loss:0.29577531624871556\n",
      "train loss:0.37976794919891793\n",
      "train loss:0.3395584706290484\n",
      "train loss:0.4595387688154936\n",
      "train loss:0.3360784681804618\n",
      "train loss:0.415701575290871\n",
      "train loss:0.31061480628874677\n",
      "train loss:0.41064024416706874\n",
      "train loss:0.36863467977571196\n",
      "train loss:0.3523607746583822\n",
      "train loss:0.3458034113087764\n",
      "train loss:0.23391659708904744\n",
      "train loss:0.36714734724844383\n",
      "train loss:0.4440959805301168\n",
      "train loss:0.4060150141750169\n",
      "train loss:0.38347289872517765\n",
      "train loss:0.37441856780309885\n",
      "train loss:0.32993909463455656\n",
      "train loss:0.2622952777276836\n",
      "train loss:0.37314359750923126\n",
      "train loss:0.4862389132919418\n",
      "train loss:0.3966424838527762\n",
      "train loss:0.3490445984782109\n",
      "train loss:0.4181542958395522\n",
      "train loss:0.3986014099530079\n",
      "train loss:0.37711124054747147\n",
      "train loss:0.4489580784807824\n",
      "train loss:0.3020267050934768\n",
      "train loss:0.30836642149879473\n",
      "train loss:0.24858391850227368\n",
      "train loss:0.27924540947281856\n",
      "train loss:0.4848347761539287\n",
      "train loss:0.3691294154584619\n",
      "train loss:0.37115598486372664\n",
      "train loss:0.33204100347548393\n",
      "train loss:0.26173649203227956\n",
      "train loss:0.21131687857859086\n",
      "train loss:0.34133625837975246\n",
      "train loss:0.4024697110797363\n",
      "train loss:0.4112899616333096\n",
      "train loss:0.29744035549130887\n",
      "train loss:0.3154946049637147\n",
      "train loss:0.2799695783852983\n",
      "train loss:0.33797848037830946\n",
      "train loss:0.3843959838852765\n",
      "train loss:0.4743069021382816\n",
      "train loss:0.295355775111682\n",
      "train loss:0.5021449914303231\n",
      "train loss:0.24806379016024163\n",
      "train loss:0.4018094846246777\n",
      "train loss:0.39823501586558896\n",
      "train loss:0.25349977773238636\n",
      "train loss:0.30461031813017575\n",
      "train loss:0.30192450086860734\n",
      "train loss:0.5090424112530976\n",
      "train loss:0.2729568789802145\n",
      "train loss:0.5308974812067493\n",
      "train loss:0.2738516716947662\n",
      "train loss:0.35620298878700213\n",
      "train loss:0.5128764064984417\n",
      "train loss:0.31561358810328427\n",
      "train loss:0.4916329082820104\n",
      "train loss:0.3403343687954801\n",
      "train loss:0.34906755383794563\n",
      "train loss:0.3559049286490694\n",
      "train loss:0.5397448679810201\n",
      "train loss:0.3053101255995902\n",
      "train loss:0.3735066553880078\n",
      "train loss:0.33944503669705606\n",
      "train loss:0.5106546734082282\n",
      "train loss:0.4567304803041956\n",
      "train loss:0.4724679846678025\n",
      "train loss:0.3924857690026354\n",
      "train loss:0.3669845618918771\n",
      "train loss:0.30397961085154385\n",
      "train loss:0.32407337633264777\n",
      "train loss:0.35621923903189134\n",
      "train loss:0.44733387737368235\n",
      "train loss:0.3333709312921302\n",
      "train loss:0.38891976267565376\n",
      "train loss:0.30327937380257747\n",
      "train loss:0.2870619818550826\n",
      "train loss:0.33384103177279334\n",
      "train loss:0.29188783647163075\n",
      "train loss:0.35528786886157976\n",
      "train loss:0.34348202842300346\n",
      "train loss:0.36969141714786163\n",
      "train loss:0.3861402040723489\n",
      "train loss:0.29427097346443887\n",
      "train loss:0.22333628319748533\n",
      "train loss:0.2496980213484704\n",
      "train loss:0.29918537103145343\n",
      "train loss:0.34567183630623516\n",
      "train loss:0.33391448107923494\n",
      "train loss:0.3806422767145376\n",
      "train loss:0.2885804159504996\n",
      "train loss:0.2899803260745667\n",
      "train loss:0.3000825117504051\n",
      "train loss:0.39525448357477266\n",
      "train loss:0.5109795482459717\n",
      "train loss:0.3333387627188014\n",
      "train loss:0.23357221030211522\n",
      "train loss:0.21286299347933535\n",
      "train loss:0.27857884472085603\n",
      "train loss:0.339279025820907\n",
      "train loss:0.252249191095835\n",
      "train loss:0.26890581683968573\n",
      "train loss:0.42526926009651034\n",
      "train loss:0.3324245967040929\n",
      "train loss:0.40833464141855985\n",
      "train loss:0.3004400349983549\n",
      "train loss:0.40535496712071356\n",
      "train loss:0.41562872049625005\n",
      "train loss:0.5078301742899046\n",
      "train loss:0.22137548607748317\n",
      "train loss:0.5487008805350728\n",
      "train loss:0.3421884916688401\n",
      "train loss:0.3110074607709063\n",
      "train loss:0.45380377015789014\n",
      "train loss:0.2412244139625753\n",
      "train loss:0.5559800452165683\n",
      "train loss:0.2523293161213237\n",
      "train loss:0.4146193695810264\n",
      "train loss:0.3341856803214563\n",
      "train loss:0.4068582987903659\n",
      "train loss:0.3100512119068253\n",
      "train loss:0.3632488251174408\n",
      "train loss:0.4547045401242187\n",
      "train loss:0.5313621324627729\n",
      "train loss:0.3419409230839346\n",
      "train loss:0.3681903294660348\n",
      "train loss:0.3350864742811317\n",
      "train loss:0.30302768048321205\n",
      "train loss:0.3472014056239616\n",
      "train loss:0.2751213310155339\n",
      "train loss:0.30574297828726865\n",
      "train loss:0.33989643392075897\n",
      "train loss:0.26025626176293015\n",
      "train loss:0.4036767714100739\n",
      "train loss:0.4311341623658628\n",
      "train loss:0.3617748447577456\n",
      "train loss:0.2350435235674179\n",
      "train loss:0.3911014604410549\n",
      "train loss:0.3477105547049632\n",
      "train loss:0.35788859222129865\n",
      "train loss:0.26043856984582336\n",
      "train loss:0.34316299611786766\n",
      "train loss:0.20026969998313457\n",
      "train loss:0.3568835889680657\n",
      "train loss:0.36157237786043417\n",
      "train loss:0.39713057432102106\n",
      "train loss:0.4183570083275226\n",
      "train loss:0.37577339163263085\n",
      "train loss:0.3942910575761342\n",
      "train loss:0.4639323966276639\n",
      "train loss:0.38706860328424303\n",
      "train loss:0.3488326770790344\n",
      "train loss:0.3945191032790043\n",
      "train loss:0.4683724557617947\n",
      "train loss:0.2807232603183937\n",
      "train loss:0.36075210677517916\n",
      "train loss:0.40969941672987803\n",
      "train loss:0.27373747335497334\n",
      "train loss:0.4403224131311452\n",
      "train loss:0.4341922747355051\n",
      "train loss:0.266990758823578\n",
      "train loss:0.31396765188667985\n",
      "train loss:0.44502258951931656\n",
      "train loss:0.44300617819202787\n",
      "train loss:0.42106769167335095\n",
      "train loss:0.41570351462787997\n",
      "train loss:0.3624333486567454\n",
      "train loss:0.2291202695073548\n",
      "train loss:0.39478808846669866\n",
      "train loss:0.2903830401839629\n",
      "train loss:0.552579636876862\n",
      "train loss:0.4221572307905769\n",
      "train loss:0.36226866223848037\n",
      "train loss:0.38082097550643534\n",
      "train loss:0.45599492239052053\n",
      "train loss:0.2429240296420409\n",
      "train loss:0.41053880761709893\n",
      "train loss:0.37273853194599627\n",
      "train loss:0.2689169729921373\n",
      "train loss:0.33336800050324233\n",
      "train loss:0.2842121717064756\n",
      "train loss:0.28496895403797906\n",
      "train loss:0.3183520982885658\n",
      "train loss:0.27610730215660095\n",
      "train loss:0.2811349330483457\n",
      "train loss:0.25403435917385925\n",
      "train loss:0.42997236270180417\n",
      "train loss:0.42097201514748744\n",
      "train loss:0.24179273144984903\n",
      "train loss:0.37270434661765756\n",
      "train loss:0.3320723236405753\n",
      "train loss:0.349863010463268\n",
      "train loss:0.3164772463931242\n",
      "train loss:0.28729059710068844\n",
      "train loss:0.3992734875752501\n",
      "train loss:0.39113537963150835\n",
      "train loss:0.3258076007688489\n",
      "train loss:0.3113931850458646\n",
      "train loss:0.2662334157241258\n",
      "train loss:0.5971251535195348\n",
      "train loss:0.3630327325776914\n",
      "train loss:0.4121822239075986\n",
      "train loss:0.46727607252666814\n",
      "train loss:0.28655819953877437\n",
      "train loss:0.2645415486737819\n",
      "train loss:0.4334632659061851\n",
      "train loss:0.3624040968887282\n",
      "train loss:0.3342871981161237\n",
      "train loss:0.3786217994548253\n",
      "train loss:0.5593996693426889\n",
      "train loss:0.4850687462436509\n",
      "train loss:0.43071812116592717\n",
      "train loss:0.31281249403955025\n",
      "train loss:0.3591106972888827\n",
      "train loss:0.44761200821446195\n",
      "train loss:0.4971713751273104\n",
      "train loss:0.3870800553934072\n",
      "train loss:0.36614333526641885\n",
      "train loss:0.3005819355898442\n",
      "train loss:0.35374141372054796\n",
      "train loss:0.2761687936002065\n",
      "train loss:0.30398589561058403\n",
      "train loss:0.250037809485335\n",
      "train loss:0.42849169338397525\n",
      "train loss:0.30936064525023727\n",
      "train loss:0.4571588711002055\n",
      "train loss:0.45307221075302273\n",
      "train loss:0.28061681146559736\n",
      "train loss:0.33955174071900596\n",
      "train loss:0.2945597975971095\n",
      "train loss:0.4218490875727335\n",
      "train loss:0.28166985981409004\n",
      "train loss:0.4294717042940923\n",
      "train loss:0.35205674915062457\n",
      "train loss:0.4138358663356363\n",
      "train loss:0.3380526475217938\n",
      "train loss:0.3307819362336737\n",
      "train loss:0.45932083248152966\n",
      "train loss:0.37486941945020386\n",
      "train loss:0.40846841348990504\n",
      "train loss:0.47123093790694165\n",
      "train loss:0.5940216597861417\n",
      "train loss:0.384458331253655\n",
      "train loss:0.38581214067827246\n",
      "train loss:0.3324228847529135\n",
      "train loss:0.29822355791173444\n",
      "train loss:0.28517207613327683\n",
      "train loss:0.4348538121904674\n",
      "train loss:0.31052766670128484\n",
      "train loss:0.4356625006700887\n",
      "train loss:0.3271507600919212\n",
      "train loss:0.37068435260373256\n",
      "train loss:0.36815104331610776\n",
      "train loss:0.2920956311838328\n",
      "train loss:0.47105864222557065\n",
      "train loss:0.32376591484904954\n",
      "train loss:0.3916756793456148\n",
      "train loss:0.2671036609269726\n",
      "train loss:0.3518359608123828\n",
      "train loss:0.2923057832886029\n",
      "train loss:0.3027523315754244\n",
      "train loss:0.3252429065077997\n",
      "train loss:0.36152223764262\n",
      "train loss:0.3612566258804323\n",
      "train loss:0.31926796084991155\n",
      "train loss:0.30891742311771614\n",
      "train loss:0.2555584955086293\n",
      "train loss:0.34258094801978195\n",
      "train loss:0.36684185296972577\n",
      "train loss:0.25389510469438703\n",
      "train loss:0.5046741526529503\n",
      "train loss:0.3821769021373477\n",
      "train loss:0.361738929784008\n",
      "train loss:0.3151971138713084\n",
      "train loss:0.35378130850881534\n",
      "train loss:0.2649155066196443\n",
      "train loss:0.24824540501282638\n",
      "train loss:0.277034538119354\n",
      "train loss:0.33356534274437233\n",
      "train loss:0.38337411281091627\n",
      "train loss:0.3712227400604389\n",
      "train loss:0.5833442251581599\n",
      "train loss:0.3620004432482301\n",
      "train loss:0.47828406305074994\n",
      "train loss:0.3848891390207993\n",
      "train loss:0.27291227992181954\n",
      "train loss:0.30419656373751613\n",
      "train loss:0.47090695999850846\n",
      "train loss:0.3840262912291284\n",
      "train loss:0.3998294392612782\n",
      "train loss:0.3033351984706254\n",
      "train loss:0.5719111169199738\n",
      "train loss:0.3453877492884432\n",
      "train loss:0.391903460175936\n",
      "train loss:0.46717814095148463\n",
      "train loss:0.30418158040327403\n",
      "train loss:0.370404593202435\n",
      "train loss:0.38019670227734603\n",
      "train loss:0.35574736017281544\n",
      "train loss:0.2783857246360225\n",
      "train loss:0.35851192998662257\n",
      "train loss:0.22413031083481708\n",
      "train loss:0.408202179451223\n",
      "train loss:0.2209802346822828\n",
      "train loss:0.3247457999151425\n",
      "train loss:0.3444982005007208\n",
      "train loss:0.4596964366831296\n",
      "train loss:0.4020146704550293\n",
      "train loss:0.5394990760500692\n",
      "train loss:0.3996201754043891\n",
      "train loss:0.3450430000407504\n",
      "train loss:0.4134026231328766\n",
      "train loss:0.21111064636746882\n",
      "train loss:0.26114351176967515\n",
      "train loss:0.3992175953819909\n",
      "train loss:0.31081797750814266\n",
      "train loss:0.49991412660342355\n",
      "train loss:0.24933780071448974\n",
      "train loss:0.4025996986070354\n",
      "train loss:0.36351144915300887\n",
      "train loss:0.5261603720449609\n",
      "train loss:0.2835101668756842\n",
      "train loss:0.31771996435614297\n",
      "train loss:0.3871245804111629\n",
      "train loss:0.40944398075559685\n",
      "train loss:0.4062572207300967\n",
      "train loss:0.25908141082786085\n",
      "train loss:0.4983956618080842\n",
      "train loss:0.30675977685372974\n",
      "train loss:0.3288165704096125\n",
      "train loss:0.28657248906278315\n",
      "train loss:0.5584259120747574\n",
      "train loss:0.3786871358581003\n",
      "train loss:0.2779186809730435\n",
      "train loss:0.24183769696951984\n",
      "train loss:0.3314684222606836\n",
      "train loss:0.3454586433009474\n",
      "train loss:0.42988034979243567\n",
      "train loss:0.3611243596673777\n",
      "train loss:0.31747289048050126\n",
      "train loss:0.36638722434575505\n",
      "train loss:0.3515098977257183\n",
      "train loss:0.28467406113476995\n",
      "train loss:0.22267562733558105\n",
      "train loss:0.29464372723851767\n",
      "train loss:0.35533386450439686\n",
      "train loss:0.2905111729064553\n",
      "train loss:0.2923875408838246\n",
      "train loss:0.28520623330403483\n",
      "train loss:0.3538666577117512\n",
      "train loss:0.5003539752205954\n",
      "train loss:0.22160883340054988\n",
      "train loss:0.515437936941881\n",
      "train loss:0.39659535944175106\n",
      "train loss:0.37439159213614326\n",
      "train loss:0.34401204348150294\n",
      "train loss:0.5034083951949542\n",
      "train loss:0.2767797811722926\n",
      "train loss:0.47424868883916027\n",
      "train loss:0.5059414253861657\n",
      "train loss:0.30544269048764844\n",
      "train loss:0.3892487668432532\n",
      "train loss:0.28937289814157774\n",
      "train loss:0.44872841677611675\n",
      "train loss:0.3529421059722435\n",
      "train loss:0.44693230318359467\n",
      "train loss:0.2934045035678394\n",
      "train loss:0.2950662199267565\n",
      "train loss:0.31533478911483553\n",
      "train loss:0.34513888400659387\n",
      "train loss:0.35694264094089445\n",
      "train loss:0.3082847926851945\n",
      "train loss:0.3709615470088125\n",
      "train loss:0.48761833500331386\n",
      "train loss:0.389443001075744\n",
      "train loss:0.3542293411465481\n",
      "train loss:0.46668159624610395\n",
      "train loss:0.36559099686631263\n",
      "train loss:0.3037245986093211\n",
      "train loss:0.2105261437119249\n",
      "train loss:0.2865715804382618\n",
      "train loss:0.4598805809626771\n",
      "train loss:0.4022715879989123\n",
      "train loss:0.37846566798591796\n",
      "train loss:0.29534243633662305\n",
      "train loss:0.3423854211563557\n",
      "train loss:0.24631794491773273\n",
      "train loss:0.2503401216516191\n",
      "train loss:0.27518879181944483\n",
      "train loss:0.42645378625478664\n",
      "train loss:0.45522272860726576\n",
      "train loss:0.26076000674525784\n",
      "train loss:0.32528295090827547\n",
      "train loss:0.26915311918776047\n",
      "train loss:0.42122906839780727\n",
      "train loss:0.3582085273952243\n",
      "train loss:0.5887786726329058\n",
      "train loss:0.35690388446528487\n",
      "train loss:0.23036185362245007\n",
      "train loss:0.31535206917709757\n",
      "train loss:0.2432599248270964\n",
      "train loss:0.3849022882544261\n",
      "train loss:0.5184202006512696\n",
      "train loss:0.46353667762763834\n",
      "train loss:0.4988209715286355\n",
      "train loss:0.3807292695311801\n",
      "train loss:0.32488595817907945\n",
      "train loss:0.43671127799310233\n",
      "train loss:0.44153343249617477\n",
      "train loss:0.288405770677268\n",
      "train loss:0.408966012283939\n",
      "train loss:0.3647638509644628\n",
      "train loss:0.2695631200287671\n",
      "train loss:0.41595736379242676\n",
      "train loss:0.29507934067196123\n",
      "train loss:0.34284565364962377\n",
      "train loss:0.2284139223654577\n",
      "train loss:0.30675250891784483\n",
      "train loss:0.36227359868252196\n",
      "train loss:0.3932333765039593\n",
      "train loss:0.43291732644112413\n",
      "train loss:0.2891214570950823\n",
      "train loss:0.35047467400234333\n",
      "train loss:0.3565114881844354\n",
      "train loss:0.3999925263536025\n",
      "train loss:0.3603753003580878\n",
      "train loss:0.40331158690619984\n",
      "train loss:0.391932601022653\n",
      "train loss:0.3482517683667778\n",
      "train loss:0.5004369481111107\n",
      "train loss:0.2731191241318291\n",
      "train loss:0.268252823863342\n",
      "train loss:0.26446174125503413\n",
      "train loss:0.21416368124827778\n",
      "train loss:0.47147868098693574\n",
      "train loss:0.3176697221624554\n",
      "train loss:0.29688032115287655\n",
      "train loss:0.33112621021721905\n",
      "train loss:0.32484490376510955\n",
      "train loss:0.2729619085308563\n",
      "train loss:0.31114507266843905\n",
      "train loss:0.3527838536233105\n",
      "train loss:0.329067106726926\n",
      "train loss:0.3892093424907218\n",
      "train loss:0.32997812536909926\n",
      "train loss:0.3405570928455767\n",
      "train loss:0.36252344944926407\n",
      "train loss:0.3763066087111025\n",
      "train loss:0.23670721558584656\n",
      "train loss:0.3451833292963334\n",
      "train loss:0.3019155459516649\n",
      "train loss:0.2906099527467261\n",
      "train loss:0.47181564695152983\n",
      "train loss:0.2947748578237253\n",
      "train loss:0.26835128564545413\n",
      "train loss:0.3574256170015941\n",
      "train loss:0.2545263824413421\n",
      "=== epoch:16, train acc:0.845, test acc:0.853 ===\n",
      "train loss:0.41791889827085216\n",
      "train loss:0.2631631525169193\n",
      "train loss:0.3823685788113525\n",
      "train loss:0.4842319031828754\n",
      "train loss:0.35507732590572433\n",
      "train loss:0.49480303455349256\n",
      "train loss:0.3294541866800178\n",
      "train loss:0.3093604010734829\n",
      "train loss:0.3352648240523729\n",
      "train loss:0.383666346562299\n",
      "train loss:0.25939668129645593\n",
      "train loss:0.30226056303369836\n",
      "train loss:0.33205912732822845\n",
      "train loss:0.4829228671129583\n",
      "train loss:0.337629001046692\n",
      "train loss:0.21796488787007323\n",
      "train loss:0.35413198230981857\n",
      "train loss:0.37096061764203975\n",
      "train loss:0.29083940217831067\n",
      "train loss:0.31619596882201917\n",
      "train loss:0.2647816265819024\n",
      "train loss:0.384326475649456\n",
      "train loss:0.3332293203250088\n",
      "train loss:0.33170875588844956\n",
      "train loss:0.5413205871338415\n",
      "train loss:0.24784816942743115\n",
      "train loss:0.39645751725958533\n",
      "train loss:0.26589986612479727\n",
      "train loss:0.42026499813891405\n",
      "train loss:0.400919839505863\n",
      "train loss:0.32716984233391444\n",
      "train loss:0.3736934732758946\n",
      "train loss:0.3690489326792408\n",
      "train loss:0.3302259055914337\n",
      "train loss:0.37125835724030687\n",
      "train loss:0.3010542989580549\n",
      "train loss:0.39749673467716007\n",
      "train loss:0.3287152720681302\n",
      "train loss:0.670698605608749\n",
      "train loss:0.45462467602063383\n",
      "train loss:0.3746958200356476\n",
      "train loss:0.24890979114272022\n",
      "train loss:0.22862907273084157\n",
      "train loss:0.37901558699215143\n",
      "train loss:0.324395019543912\n",
      "train loss:0.3798801292117722\n",
      "train loss:0.2848174211706491\n",
      "train loss:0.27664470471166225\n",
      "train loss:0.27843443475349855\n",
      "train loss:0.4541033464520538\n",
      "train loss:0.3780347836634612\n",
      "train loss:0.3633953423457512\n",
      "train loss:0.27012201292708743\n",
      "train loss:0.37990178699345406\n",
      "train loss:0.4063121290322411\n",
      "train loss:0.33326483949620966\n",
      "train loss:0.3090268398722385\n",
      "train loss:0.3371874267265688\n",
      "train loss:0.28664825949547607\n",
      "train loss:0.28576159188080036\n",
      "train loss:0.28429772313157536\n",
      "train loss:0.3177227348221011\n",
      "train loss:0.4340441648367724\n",
      "train loss:0.34512213654504975\n",
      "train loss:0.2938760294830606\n",
      "train loss:0.2721021962447885\n",
      "train loss:0.42327819181042287\n",
      "train loss:0.32770261310217075\n",
      "train loss:0.30340509911184704\n",
      "train loss:0.22220001322712193\n",
      "train loss:0.24363239236717799\n",
      "train loss:0.35395325963874774\n",
      "train loss:0.5072346601461941\n",
      "train loss:0.3866599796854132\n",
      "train loss:0.3231987282346631\n",
      "train loss:0.33340308127224155\n",
      "train loss:0.32704545925473794\n",
      "train loss:0.439622132237055\n",
      "train loss:0.2410738466150211\n",
      "train loss:0.3237204644953017\n",
      "train loss:0.35315540648171656\n",
      "train loss:0.3806069346080628\n",
      "train loss:0.274505399779741\n",
      "train loss:0.3302732597532354\n",
      "train loss:0.2739774109435611\n",
      "train loss:0.3145223116811682\n",
      "train loss:0.3708958703340284\n",
      "train loss:0.3389365910185667\n",
      "train loss:0.3563889414355559\n",
      "train loss:0.3605715183701818\n",
      "train loss:0.33893117815216306\n",
      "train loss:0.4263762189350189\n",
      "train loss:0.25418255910247106\n",
      "train loss:0.3254656584962718\n",
      "train loss:0.3618283832615727\n",
      "train loss:0.2927901425900435\n",
      "train loss:0.24914698535563154\n",
      "train loss:0.2841641999044995\n",
      "train loss:0.24106325966534217\n",
      "train loss:0.25750711018587086\n",
      "train loss:0.3477025451938177\n",
      "train loss:0.2136648502421556\n",
      "train loss:0.2666316158748331\n",
      "train loss:0.4813899547868128\n",
      "train loss:0.3275938241933974\n",
      "train loss:0.4763815329586393\n",
      "train loss:0.5036487478848816\n",
      "train loss:0.4835512524532751\n",
      "train loss:0.22792941338721068\n",
      "train loss:0.22957962127366102\n",
      "train loss:0.4203723528435512\n",
      "train loss:0.2683562658002934\n",
      "train loss:0.22648547409573674\n",
      "train loss:0.3137487552134006\n",
      "train loss:0.33775130311809526\n",
      "train loss:0.4538331057682259\n",
      "train loss:0.34814622550228264\n",
      "train loss:0.26318287448598116\n",
      "train loss:0.24636468890836516\n",
      "train loss:0.242610481614103\n",
      "train loss:0.32437864513466463\n",
      "train loss:0.27858244271873234\n",
      "train loss:0.36180542295560836\n",
      "train loss:0.41254265860775874\n",
      "train loss:0.26120192605453735\n",
      "train loss:0.33587335383589106\n",
      "train loss:0.40395176795675736\n",
      "train loss:0.2966315611385191\n",
      "train loss:0.25238726643185855\n",
      "train loss:0.3240569638228431\n",
      "train loss:0.5566986296034124\n",
      "train loss:0.23177900607198995\n",
      "train loss:0.41263779645629944\n",
      "train loss:0.32246131769852027\n",
      "train loss:0.22896666416912606\n",
      "train loss:0.24377020038362937\n",
      "train loss:0.3555453982549801\n",
      "train loss:0.3055389025608876\n",
      "train loss:0.43203127765702787\n",
      "train loss:0.23793343994374572\n",
      "train loss:0.25542934410615453\n",
      "train loss:0.32580051570180474\n",
      "train loss:0.2516544200360195\n",
      "train loss:0.41621275175165307\n",
      "train loss:0.45430238496305025\n",
      "train loss:0.3636365171497811\n",
      "train loss:0.3388228732434765\n",
      "train loss:0.22659613872890108\n",
      "train loss:0.41764127400446865\n",
      "train loss:0.27107579792626824\n",
      "train loss:0.2214407455727727\n",
      "train loss:0.4534496709561892\n",
      "train loss:0.29442926990434914\n",
      "train loss:0.4283741375475494\n",
      "train loss:0.37866242244531373\n",
      "train loss:0.3020069684416457\n",
      "train loss:0.4398091804183768\n",
      "train loss:0.5584978916164078\n",
      "train loss:0.3972991507169455\n",
      "train loss:0.261476682645781\n",
      "train loss:0.3480304633218199\n",
      "train loss:0.34005961870070694\n",
      "train loss:0.34281419741163544\n",
      "train loss:0.4610552354272627\n",
      "train loss:0.311425390727439\n",
      "train loss:0.5086878334571374\n",
      "train loss:0.4218353283521368\n",
      "train loss:0.35647992232754355\n",
      "train loss:0.28820160263826755\n",
      "train loss:0.38366113280322645\n",
      "train loss:0.26817617668852856\n",
      "train loss:0.23824270823891547\n",
      "train loss:0.2686202296239378\n",
      "train loss:0.31530491360007573\n",
      "train loss:0.25607123761835654\n",
      "train loss:0.39672542965167573\n",
      "train loss:0.32110431074643947\n",
      "train loss:0.25638044187645814\n",
      "train loss:0.36956720336006266\n",
      "train loss:0.28117401461441166\n",
      "train loss:0.318504879823043\n",
      "train loss:0.27327473541580594\n",
      "train loss:0.2779937489459466\n",
      "train loss:0.3237802279338704\n",
      "train loss:0.41334416122917383\n",
      "train loss:0.3195774375585252\n",
      "train loss:0.3454369887088201\n",
      "train loss:0.4071364069477885\n",
      "train loss:0.1931268967415158\n",
      "train loss:0.328912727675784\n",
      "train loss:0.4006791896736175\n",
      "train loss:0.5276193106217992\n",
      "train loss:0.27958233945381833\n",
      "train loss:0.4063426760638022\n",
      "train loss:0.36332274325313435\n",
      "train loss:0.3552161787500681\n",
      "train loss:0.3848956727721245\n",
      "train loss:0.3681141117770256\n",
      "train loss:0.37597268794222205\n",
      "train loss:0.3550397203199824\n",
      "train loss:0.3441621648628873\n",
      "train loss:0.41254200716291156\n",
      "train loss:0.32133148066446277\n",
      "train loss:0.28977932670452394\n",
      "train loss:0.40533223997249657\n",
      "train loss:0.44322516957401265\n",
      "train loss:0.3490252461187248\n",
      "train loss:0.36127993074841525\n",
      "train loss:0.3878757301925688\n",
      "train loss:0.4303090006080719\n",
      "train loss:0.6972005645201566\n",
      "train loss:0.31490118249344207\n",
      "train loss:0.2778414644351202\n",
      "train loss:0.3561136866032894\n",
      "train loss:0.45836467438304396\n",
      "train loss:0.3049461917650704\n",
      "train loss:0.41835540634867435\n",
      "train loss:0.33725634953063677\n",
      "train loss:0.2981952736206121\n",
      "train loss:0.3007203603752041\n",
      "train loss:0.4403706430215744\n",
      "train loss:0.20999353722430034\n",
      "train loss:0.6184465034780754\n",
      "train loss:0.23609984193679381\n",
      "train loss:0.46028705769258693\n",
      "train loss:0.33758327710593294\n",
      "train loss:0.3310241017868896\n",
      "train loss:0.4692334718461349\n",
      "train loss:0.2699667220105539\n",
      "train loss:0.29395555514109317\n",
      "train loss:0.27939668006559104\n",
      "train loss:0.3903044108795619\n",
      "train loss:0.23370069604038293\n",
      "train loss:0.40709628316897\n",
      "train loss:0.41228379572371343\n",
      "train loss:0.38211167872454105\n",
      "train loss:0.37699973830352157\n",
      "train loss:0.2590801160471465\n",
      "train loss:0.3989230310413674\n",
      "train loss:0.25510367803601003\n",
      "train loss:0.31739514378500666\n",
      "train loss:0.2582041915080911\n",
      "train loss:0.26708779282418915\n",
      "train loss:0.3336036619971117\n",
      "train loss:0.4189803366020067\n",
      "train loss:0.2077513889970964\n",
      "train loss:0.35165191490729397\n",
      "train loss:0.40043235099833185\n",
      "train loss:0.31132627667478546\n",
      "train loss:0.32678676265334644\n",
      "train loss:0.39107752299696186\n",
      "train loss:0.3838554926926498\n",
      "train loss:0.525772404964533\n",
      "train loss:0.41691176837478117\n",
      "train loss:0.3036555772676789\n",
      "train loss:0.2909305210640939\n",
      "train loss:0.3645165303834258\n",
      "train loss:0.34096173634664917\n",
      "train loss:0.3474282437852855\n",
      "train loss:0.3622739020008426\n",
      "train loss:0.3290283308334363\n",
      "train loss:0.3712331615326023\n",
      "train loss:0.429700199614986\n",
      "train loss:0.37629197245003715\n",
      "train loss:0.2784309155419574\n",
      "train loss:0.45079468737655576\n",
      "train loss:0.28544107822164017\n",
      "train loss:0.3981043356302896\n",
      "train loss:0.32328861352908866\n",
      "train loss:0.3463277925896254\n",
      "train loss:0.32123305429211174\n",
      "train loss:0.28640813684978256\n",
      "train loss:0.2816563391668458\n",
      "train loss:0.2211129146708263\n",
      "train loss:0.3400461236026179\n",
      "train loss:0.3156254316427544\n",
      "train loss:0.40075094524979016\n",
      "train loss:0.350664611586588\n",
      "train loss:0.30710423532319475\n",
      "train loss:0.5333708414843116\n",
      "train loss:0.24020896376166398\n",
      "train loss:0.2774479794212079\n",
      "train loss:0.5070594452542887\n",
      "train loss:0.46558945985299316\n",
      "train loss:0.28720162691485834\n",
      "train loss:0.33451785796691413\n",
      "train loss:0.43170179007890425\n",
      "train loss:0.40550826241509424\n",
      "train loss:0.3445644802143957\n",
      "train loss:0.2192245135599081\n",
      "train loss:0.3434413348744473\n",
      "train loss:0.2841888800733628\n",
      "train loss:0.4938069292197419\n",
      "train loss:0.30242927678668885\n",
      "train loss:0.21293085035410658\n",
      "train loss:0.4894339739844202\n",
      "train loss:0.2020412664000016\n",
      "train loss:0.26768623589060564\n",
      "train loss:0.3557165569236181\n",
      "train loss:0.38104734847132365\n",
      "train loss:0.4043661536305456\n",
      "train loss:0.43009856450265427\n",
      "train loss:0.3474930253538681\n",
      "train loss:0.37597624400642815\n",
      "train loss:0.269218773607353\n",
      "train loss:0.2649314052597765\n",
      "train loss:0.3793162698488314\n",
      "train loss:0.29047552177100766\n",
      "train loss:0.5769545151507343\n",
      "train loss:0.5309987037381907\n",
      "train loss:0.2907192091428091\n",
      "train loss:0.3217668211684963\n",
      "train loss:0.4050697635047824\n",
      "train loss:0.41691597741235575\n",
      "train loss:0.3052509388187615\n",
      "train loss:0.30505391256909653\n",
      "train loss:0.41182536714386375\n",
      "train loss:0.3507763522981995\n",
      "train loss:0.3561180333794617\n",
      "train loss:0.4405821645212217\n",
      "train loss:0.21417705170045628\n",
      "train loss:0.25184222667704037\n",
      "train loss:0.3890974523801991\n",
      "train loss:0.24265516210496313\n",
      "train loss:0.39260653814137597\n",
      "train loss:0.1961765891425028\n",
      "train loss:0.2669320029323957\n",
      "train loss:0.3768200161608917\n",
      "train loss:0.530146684343427\n",
      "train loss:0.30209841006709853\n",
      "train loss:0.20347609554264814\n",
      "train loss:0.37276638714146015\n",
      "train loss:0.32980651809407546\n",
      "train loss:0.3566719176417358\n",
      "train loss:0.3664855207499261\n",
      "train loss:0.31511184713648754\n",
      "train loss:0.3995773101945256\n",
      "train loss:0.44085321816388245\n",
      "train loss:0.3569153219396371\n",
      "train loss:0.2946610505257097\n",
      "train loss:0.26556280672787463\n",
      "train loss:0.3648629568349482\n",
      "train loss:0.2734765689603542\n",
      "train loss:0.36513880679569793\n",
      "train loss:0.37630088232805536\n",
      "train loss:0.23330330697638452\n",
      "train loss:0.3671447240387702\n",
      "train loss:0.335094365289267\n",
      "train loss:0.3481677726796459\n",
      "train loss:0.4158335557502821\n",
      "train loss:0.31641573459644623\n",
      "train loss:0.32214810133314115\n",
      "train loss:0.22816765721032883\n",
      "train loss:0.36284299416888366\n",
      "train loss:0.3166941737631151\n",
      "train loss:0.29788631653648423\n",
      "train loss:0.453838899244085\n",
      "train loss:0.35494395143433627\n",
      "train loss:0.3122979360037458\n",
      "train loss:0.2473262386673791\n",
      "train loss:0.3484805637320169\n",
      "train loss:0.30534532037725304\n",
      "train loss:0.4121236381696506\n",
      "train loss:0.36829546281306164\n",
      "train loss:0.3232258959386507\n",
      "train loss:0.44107190453348444\n",
      "train loss:0.4002430519688625\n",
      "train loss:0.3547592997025735\n",
      "train loss:0.2817270084028126\n",
      "train loss:0.2529228458504383\n",
      "train loss:0.3484478998793692\n",
      "train loss:0.3064827175299365\n",
      "train loss:0.2284457726646384\n",
      "train loss:0.47862712333996343\n",
      "train loss:0.2414879247726281\n",
      "train loss:0.3452267862438548\n",
      "train loss:0.29571881031444947\n",
      "train loss:0.47390861941543927\n",
      "train loss:0.5583325165690065\n",
      "train loss:0.2819017935587067\n",
      "train loss:0.37904614108920937\n",
      "train loss:0.29812247312622825\n",
      "train loss:0.47766814411604697\n",
      "train loss:0.3800972868858915\n",
      "train loss:0.4687506914694948\n",
      "train loss:0.3752234051926229\n",
      "train loss:0.3559794096421451\n",
      "train loss:0.5086709110470775\n",
      "train loss:0.25033907310427606\n",
      "train loss:0.4681470307462078\n",
      "train loss:0.4222163106858571\n",
      "train loss:0.2601666876220512\n",
      "train loss:0.3440469830035643\n",
      "train loss:0.3367494788564986\n",
      "train loss:0.3294277488052087\n",
      "train loss:0.45361383506005354\n",
      "train loss:0.32191321715136034\n",
      "train loss:0.39336952877610853\n",
      "train loss:0.2988673155355291\n",
      "train loss:0.5038380124405815\n",
      "train loss:0.39077909784141995\n",
      "train loss:0.3334367409980697\n",
      "train loss:0.34465574694004447\n",
      "train loss:0.3212614222479397\n",
      "train loss:0.42500672908957304\n",
      "train loss:0.2579272328862181\n",
      "train loss:0.46141133400921236\n",
      "train loss:0.2823289929232952\n",
      "train loss:0.4981640989980552\n",
      "train loss:0.3713054042513969\n",
      "train loss:0.35058197494280596\n",
      "train loss:0.21384187430618973\n",
      "train loss:0.3627211810220625\n",
      "train loss:0.4446003457119631\n",
      "train loss:0.3526622008524572\n",
      "train loss:0.2496209574254228\n",
      "train loss:0.3679218761971399\n",
      "train loss:0.28061695409339504\n",
      "train loss:0.38789806558367834\n",
      "train loss:0.3408604788512233\n",
      "train loss:0.286713901047669\n",
      "train loss:0.34658299898749556\n",
      "train loss:0.4124326071566122\n",
      "train loss:0.25374877315247985\n",
      "train loss:0.3544851076962429\n",
      "train loss:0.3196892369885643\n",
      "train loss:0.32680434749472326\n",
      "train loss:0.30932459163339093\n",
      "train loss:0.35805651385810977\n",
      "train loss:0.4085628288877836\n",
      "train loss:0.29817261243573806\n",
      "train loss:0.24646361624840055\n",
      "train loss:0.39641709962800625\n",
      "train loss:0.37854272723729854\n",
      "train loss:0.3028002477564427\n",
      "train loss:0.3221464104871245\n",
      "train loss:0.3347264102540266\n",
      "train loss:0.2591840139373936\n",
      "train loss:0.2716817802113492\n",
      "train loss:0.30333299373839706\n",
      "train loss:0.39085107234118643\n",
      "train loss:0.40317661329084714\n",
      "train loss:0.25822998470407066\n",
      "train loss:0.3537428314655017\n",
      "train loss:0.24901661570341146\n",
      "train loss:0.26213406768441094\n",
      "train loss:0.3448306629061604\n",
      "train loss:0.24907874541720246\n",
      "train loss:0.30077396670789525\n",
      "train loss:0.3081832163432497\n",
      "train loss:0.3529680853656983\n",
      "train loss:0.430338407463635\n",
      "train loss:0.2755839771164439\n",
      "train loss:0.2904526124592156\n",
      "train loss:0.32308226570253323\n",
      "train loss:0.351866295782675\n",
      "train loss:0.389289792316881\n",
      "train loss:0.3372179553687741\n",
      "train loss:0.2484823500141113\n",
      "train loss:0.23812697026630208\n",
      "train loss:0.2362523831002636\n",
      "train loss:0.2740121033381067\n",
      "train loss:0.41717457578479417\n",
      "train loss:0.44220037037599724\n",
      "train loss:0.41437146655601564\n",
      "train loss:0.43195700846932317\n",
      "train loss:0.3245793249803836\n",
      "train loss:0.3068514307601458\n",
      "train loss:0.4593748144920612\n",
      "train loss:0.42593279560166075\n",
      "train loss:0.23680871514964413\n",
      "train loss:0.3065023824288932\n",
      "train loss:0.27093095310625737\n",
      "train loss:0.30678140603179416\n",
      "train loss:0.3455599936315348\n",
      "train loss:0.38537085007677696\n",
      "train loss:0.32269578465860005\n",
      "train loss:0.29100054898120076\n",
      "train loss:0.31153796478604084\n",
      "train loss:0.23146660670214186\n",
      "train loss:0.4597795605120851\n",
      "train loss:0.4175743651044977\n",
      "train loss:0.31934457320095544\n",
      "train loss:0.3936932812572385\n",
      "train loss:0.2788258884138234\n",
      "train loss:0.24781675445524398\n",
      "train loss:0.33394239287306854\n",
      "train loss:0.45194207608583753\n",
      "train loss:0.45899839816992133\n",
      "train loss:0.38970948872080624\n",
      "train loss:0.3000656681087976\n",
      "train loss:0.2601711916889029\n",
      "train loss:0.33680369733704046\n",
      "train loss:0.22022790695792266\n",
      "train loss:0.3013329034771971\n",
      "train loss:0.2924908864187346\n",
      "train loss:0.28318682792096206\n",
      "train loss:0.46847943422668764\n",
      "train loss:0.3680872068981023\n",
      "train loss:0.39818227300153664\n",
      "train loss:0.35294081761093027\n",
      "train loss:0.39916367463701424\n",
      "train loss:0.3585878058422092\n",
      "train loss:0.3536949176438117\n",
      "train loss:0.41225294269109314\n",
      "train loss:0.38230832869739456\n",
      "train loss:0.39058188981717484\n",
      "train loss:0.31434842463003926\n",
      "train loss:0.30563534224927563\n",
      "train loss:0.43383214474143855\n",
      "train loss:0.34051175313820875\n",
      "train loss:0.31024483828396937\n",
      "train loss:0.4563511815811938\n",
      "train loss:0.34808283929212147\n",
      "train loss:0.4107220078503996\n",
      "train loss:0.27622165227231343\n",
      "train loss:0.399001052541961\n",
      "train loss:0.3185025154898129\n",
      "train loss:0.2828025695014365\n",
      "train loss:0.38850344372384027\n",
      "train loss:0.27682182912891623\n",
      "train loss:0.3490498783967935\n",
      "train loss:0.20214272815820614\n",
      "train loss:0.40460640942460663\n",
      "train loss:0.21683706016611576\n",
      "train loss:0.42974971077747154\n",
      "train loss:0.3344800319024534\n",
      "train loss:0.3176424540923357\n",
      "train loss:0.5027170888560973\n",
      "train loss:0.3655291730987603\n",
      "train loss:0.3196627688400404\n",
      "train loss:0.3281428154593283\n",
      "train loss:0.3705500509219217\n",
      "train loss:0.3651572449537768\n",
      "train loss:0.3113740770418427\n",
      "train loss:0.2621831953259556\n",
      "train loss:0.40216926768019234\n",
      "train loss:0.4842811689201968\n",
      "train loss:0.22670449655922145\n",
      "train loss:0.16646782029228582\n",
      "train loss:0.4287788742532922\n",
      "train loss:0.3831592902223799\n",
      "train loss:0.3017454504207463\n",
      "train loss:0.3288403707232474\n",
      "train loss:0.28637775079731126\n",
      "train loss:0.2710048192427503\n",
      "train loss:0.38473382650685845\n",
      "train loss:0.22159873578222666\n",
      "train loss:0.37293509743024594\n",
      "train loss:0.30608846387703303\n",
      "train loss:0.2952760552354633\n",
      "train loss:0.35944529921698737\n",
      "train loss:0.36044095479618876\n",
      "train loss:0.48796799772171423\n",
      "train loss:0.3810188203134067\n",
      "train loss:0.5731426604701703\n",
      "train loss:0.2796686232569203\n",
      "train loss:0.402458186995443\n",
      "train loss:0.20792068586092388\n",
      "train loss:0.4098150807222619\n",
      "train loss:0.35144132148220614\n",
      "train loss:0.532704797162048\n",
      "train loss:0.4253041941058154\n",
      "train loss:0.39105076427780594\n",
      "train loss:0.36347762235288295\n",
      "train loss:0.34967809050845383\n",
      "train loss:0.2423127872916703\n",
      "train loss:0.37198527504679857\n",
      "train loss:0.21664338169014602\n",
      "train loss:0.51707100883714\n",
      "train loss:0.3015722495006043\n",
      "train loss:0.4108324779820371\n",
      "train loss:0.32097680405866014\n",
      "train loss:0.3424807090510063\n",
      "train loss:0.4235561943788562\n",
      "train loss:0.3093485309721101\n",
      "train loss:0.3629836543680199\n",
      "train loss:0.2589898567763096\n",
      "train loss:0.3837489742862714\n",
      "train loss:0.348534289428772\n",
      "train loss:0.27524815964692745\n",
      "train loss:0.23088916070662951\n",
      "train loss:0.2800694927592562\n",
      "train loss:0.3836000463209514\n",
      "train loss:0.45376349992237386\n",
      "train loss:0.24609864176415286\n",
      "train loss:0.3438763548569305\n",
      "train loss:0.39104508335679017\n",
      "train loss:0.24947910453368136\n",
      "train loss:0.4948230068050236\n",
      "train loss:0.23807324514180933\n",
      "train loss:0.46013175614570606\n",
      "train loss:0.33862319340501096\n",
      "train loss:0.2720923475003042\n",
      "train loss:0.279677686154414\n",
      "train loss:0.39762180881226045\n",
      "train loss:0.30139791127597065\n",
      "train loss:0.26074293829894235\n",
      "train loss:0.5434142761721312\n",
      "train loss:0.40236206821543485\n",
      "=== epoch:17, train acc:0.877, test acc:0.863 ===\n",
      "train loss:0.2833036096741555\n",
      "train loss:0.26577024691658147\n",
      "train loss:0.4454938197085535\n",
      "train loss:0.32409031266387067\n",
      "train loss:0.27067010172238787\n",
      "train loss:0.2968687040396895\n",
      "train loss:0.3859230868869387\n",
      "train loss:0.29171701193483257\n",
      "train loss:0.27695170577036543\n",
      "train loss:0.3887172123471651\n",
      "train loss:0.3056194710370485\n",
      "train loss:0.3381053754606767\n",
      "train loss:0.4247886012460325\n",
      "train loss:0.306554525231361\n",
      "train loss:0.34533060663546694\n",
      "train loss:0.39616831331632796\n",
      "train loss:0.29586104304364597\n",
      "train loss:0.39570645314640884\n",
      "train loss:0.264325469493366\n",
      "train loss:0.3944114404385573\n",
      "train loss:0.21985683491640767\n",
      "train loss:0.32170718085155026\n",
      "train loss:0.28889836128327007\n",
      "train loss:0.269234569324341\n",
      "train loss:0.43315233222891764\n",
      "train loss:0.2708300301948798\n",
      "train loss:0.4169794808450555\n",
      "train loss:0.3394306044032821\n",
      "train loss:0.37811575224336685\n",
      "train loss:0.29299501469529354\n",
      "train loss:0.27411814236400706\n",
      "train loss:0.3309377497924503\n",
      "train loss:0.5090418767826181\n",
      "train loss:0.4676735771242312\n",
      "train loss:0.4440613943872247\n",
      "train loss:0.3761935563229875\n",
      "train loss:0.4671422669159586\n",
      "train loss:0.26009078376011874\n",
      "train loss:0.35107102498206444\n",
      "train loss:0.273704038171485\n",
      "train loss:0.43249033907837414\n",
      "train loss:0.2699802289347909\n",
      "train loss:0.307535993919218\n",
      "train loss:0.3952377107535834\n",
      "train loss:0.3070735119785495\n",
      "train loss:0.326178872268688\n",
      "train loss:0.41157133310837124\n",
      "train loss:0.31367145201694635\n",
      "train loss:0.3490161115669706\n",
      "train loss:0.284185074841619\n",
      "train loss:0.3051060762798879\n",
      "train loss:0.3078690111067139\n",
      "train loss:0.275379352639405\n",
      "train loss:0.40767377893999573\n",
      "train loss:0.2866568784239169\n",
      "train loss:0.3472485805645287\n",
      "train loss:0.28379186425699765\n",
      "train loss:0.3160587824691057\n",
      "train loss:0.439809461647817\n",
      "train loss:0.2976739891909032\n",
      "train loss:0.26689021399798923\n",
      "train loss:0.32960019210694663\n",
      "train loss:0.36280832628839677\n",
      "train loss:0.34372042754743115\n",
      "train loss:0.288990409710999\n",
      "train loss:0.26616138866472494\n",
      "train loss:0.5123431163644624\n",
      "train loss:0.26735581530992525\n",
      "train loss:0.33869385266840796\n",
      "train loss:0.3766335400221667\n",
      "train loss:0.2587532619083183\n",
      "train loss:0.4012703609276946\n",
      "train loss:0.4782368309099653\n",
      "train loss:0.40479797991484767\n",
      "train loss:0.28143680399061505\n",
      "train loss:0.2850935853836372\n",
      "train loss:0.23893694101183274\n",
      "train loss:0.35439228976547804\n",
      "train loss:0.36231618644032465\n",
      "train loss:0.2519806602527363\n",
      "train loss:0.42283070804011325\n",
      "train loss:0.3060198246925514\n",
      "train loss:0.4249543736457448\n",
      "train loss:0.4607111071192626\n",
      "train loss:0.3138082972362378\n",
      "train loss:0.28920958798592267\n",
      "train loss:0.3924350748055152\n",
      "train loss:0.4207552089094975\n",
      "train loss:0.28895446086768456\n",
      "train loss:0.397444202901173\n",
      "train loss:0.34900706895593636\n",
      "train loss:0.29762381923384224\n",
      "train loss:0.45134248582770503\n",
      "train loss:0.3272931902340048\n",
      "train loss:0.32453456498052013\n",
      "train loss:0.1601212515820472\n",
      "train loss:0.463996910770613\n",
      "train loss:0.2507150579845144\n",
      "train loss:0.316404477856224\n",
      "train loss:0.4759816579527586\n",
      "train loss:0.5147654592240481\n",
      "train loss:0.24289792164446136\n",
      "train loss:0.3233223282121302\n",
      "train loss:0.3688134271560447\n",
      "train loss:0.37180477959158886\n",
      "train loss:0.4614226190256373\n",
      "train loss:0.29951893044619965\n",
      "train loss:0.40816550171782906\n",
      "train loss:0.2542697089909885\n",
      "train loss:0.2755977435281493\n",
      "train loss:0.29323573578913925\n",
      "train loss:0.3022433020774468\n",
      "train loss:0.2885175029339651\n",
      "train loss:0.31701973048667137\n",
      "train loss:0.34847547405842627\n",
      "train loss:0.3874640194498922\n",
      "train loss:0.3030521251318658\n",
      "train loss:0.22473509358848087\n",
      "train loss:0.5656088606470129\n",
      "train loss:0.3825755709638816\n",
      "train loss:0.3086971328208556\n",
      "train loss:0.22914171111317264\n",
      "train loss:0.42422622159670764\n",
      "train loss:0.4043077114078036\n",
      "train loss:0.4464391651786024\n",
      "train loss:0.34708296827413343\n",
      "train loss:0.3345192859948293\n",
      "train loss:0.30448121358416047\n",
      "train loss:0.4161286110329058\n",
      "train loss:0.41656579041859515\n",
      "train loss:0.2772189327092427\n",
      "train loss:0.3634861193256145\n",
      "train loss:0.48418967975946464\n",
      "train loss:0.22691669292020555\n",
      "train loss:0.2816265990054865\n",
      "train loss:0.41162628248097766\n",
      "train loss:0.3249317010010123\n",
      "train loss:0.39270808496334075\n",
      "train loss:0.341662022384342\n",
      "train loss:0.3224371776552652\n",
      "train loss:0.4288119942863125\n",
      "train loss:0.4513466485741653\n",
      "train loss:0.22530994337940563\n",
      "train loss:0.3424666591114762\n",
      "train loss:0.33322271596650394\n",
      "train loss:0.312537159111591\n",
      "train loss:0.2837905982139482\n",
      "train loss:0.579645782725487\n",
      "train loss:0.42228056630970784\n",
      "train loss:0.3451562295401851\n",
      "train loss:0.35235724894893017\n",
      "train loss:0.3710660537097038\n",
      "train loss:0.3656716249638645\n",
      "train loss:0.26047364335469714\n",
      "train loss:0.3250530235584474\n",
      "train loss:0.3869849918547238\n",
      "train loss:0.36139230076236645\n",
      "train loss:0.3845231184622571\n",
      "train loss:0.4268055068107084\n",
      "train loss:0.2541961299369186\n",
      "train loss:0.32221627068252295\n",
      "train loss:0.37044787766915055\n",
      "train loss:0.32811835341975315\n",
      "train loss:0.29836113136381986\n",
      "train loss:0.378129472495903\n",
      "train loss:0.21941655519380443\n",
      "train loss:0.3831992747728739\n",
      "train loss:0.41409599570765243\n",
      "train loss:0.34521192322861005\n",
      "train loss:0.3858857055309153\n",
      "train loss:0.3545315216803801\n",
      "train loss:0.429644517377315\n",
      "train loss:0.27780950382871034\n",
      "train loss:0.2678109386851296\n",
      "train loss:0.3675901745149689\n",
      "train loss:0.4067046942523272\n",
      "train loss:0.2690960076753607\n",
      "train loss:0.26875344024901887\n",
      "train loss:0.31920400078372724\n",
      "train loss:0.34788981589581913\n",
      "train loss:0.3102625748835544\n",
      "train loss:0.33774064707172174\n",
      "train loss:0.3144175499905642\n",
      "train loss:0.42315112160173485\n",
      "train loss:0.3322853023614198\n",
      "train loss:0.3768459522153275\n",
      "train loss:0.32071142034158256\n",
      "train loss:0.2684017579614803\n",
      "train loss:0.5113555580470376\n",
      "train loss:0.42067121496659793\n",
      "train loss:0.3247193372497618\n",
      "train loss:0.2954978301045821\n",
      "train loss:0.4259587453107199\n",
      "train loss:0.220442746334669\n",
      "train loss:0.30727728740086385\n",
      "train loss:0.4475007605512389\n",
      "train loss:0.44073419926899277\n",
      "train loss:0.4399244566102156\n",
      "train loss:0.39609599396587236\n",
      "train loss:0.2882141526213342\n",
      "train loss:0.3093339426047205\n",
      "train loss:0.4415895458695218\n",
      "train loss:0.3464264703957015\n",
      "train loss:0.33769058142648284\n",
      "train loss:0.2779393511770891\n",
      "train loss:0.2669688277306653\n",
      "train loss:0.41186489097544543\n",
      "train loss:0.3729241766965526\n",
      "train loss:0.24857508260419137\n",
      "train loss:0.35361542055301404\n",
      "train loss:0.2843846739657264\n",
      "train loss:0.3227776621278133\n",
      "train loss:0.3033574100826175\n",
      "train loss:0.2811477630739296\n",
      "train loss:0.2466561549070896\n",
      "train loss:0.34809138779100857\n",
      "train loss:0.2209705561345626\n",
      "train loss:0.3252838057357579\n",
      "train loss:0.36072041064340543\n",
      "train loss:0.3462685674187471\n",
      "train loss:0.40129752155304826\n",
      "train loss:0.2924629904320769\n",
      "train loss:0.3108946763616908\n",
      "train loss:0.41676421089135984\n",
      "train loss:0.4041646052327322\n",
      "train loss:0.2778004152472934\n",
      "train loss:0.25976076934545117\n",
      "train loss:0.33177354852265856\n",
      "train loss:0.4463361365359846\n",
      "train loss:0.36021480619174506\n",
      "train loss:0.3473660102880668\n",
      "train loss:0.3219486287214805\n",
      "train loss:0.4250350773115624\n",
      "train loss:0.40352578127919786\n",
      "train loss:0.3237948045604268\n",
      "train loss:0.294227852211976\n",
      "train loss:0.446977405088229\n",
      "train loss:0.504223796482592\n",
      "train loss:0.34696329155183697\n",
      "train loss:0.3129853345425372\n",
      "train loss:0.33251274548050114\n",
      "train loss:0.2903326597544043\n",
      "train loss:0.3276229757540739\n",
      "train loss:0.24633379007451192\n",
      "train loss:0.31157099802777904\n",
      "train loss:0.34906545084115836\n",
      "train loss:0.3673024285388207\n",
      "train loss:0.32251731668455547\n",
      "train loss:0.3852640089060512\n",
      "train loss:0.3596502623523463\n",
      "train loss:0.5461784709623579\n",
      "train loss:0.3019859919076769\n",
      "train loss:0.3284382573307887\n",
      "train loss:0.2747737403783511\n",
      "train loss:0.45830279164344695\n",
      "train loss:0.44607164052759124\n",
      "train loss:0.21356715445192928\n",
      "train loss:0.3941524456332735\n",
      "train loss:0.4209770515008342\n",
      "train loss:0.42859868771589105\n",
      "train loss:0.41735331438676626\n",
      "train loss:0.3333030173235458\n",
      "train loss:0.2610868868119048\n",
      "train loss:0.3157029143354249\n",
      "train loss:0.23614300400097435\n",
      "train loss:0.3479008546001216\n",
      "train loss:0.3282614327254268\n",
      "train loss:0.27269906842921604\n",
      "train loss:0.33850978208790594\n",
      "train loss:0.2727661235890652\n",
      "train loss:0.1783484776808778\n",
      "train loss:0.25436428070765915\n",
      "train loss:0.30756299637625456\n",
      "train loss:0.46768173985609257\n",
      "train loss:0.28862300898328236\n",
      "train loss:0.39158763557052034\n",
      "train loss:0.3927554221574082\n",
      "train loss:0.3045488132491982\n",
      "train loss:0.3857184330831437\n",
      "train loss:0.45541296889720984\n",
      "train loss:0.28188512052323134\n",
      "train loss:0.333140411598043\n",
      "train loss:0.31066331175998846\n",
      "train loss:0.4207963864959543\n",
      "train loss:0.2972077766135072\n",
      "train loss:0.42299620281349254\n",
      "train loss:0.3611128733441197\n",
      "train loss:0.35827911688887915\n",
      "train loss:0.48300699238944816\n",
      "train loss:0.35009783063397315\n",
      "train loss:0.4366275548256164\n",
      "train loss:0.2774719909999687\n",
      "train loss:0.3065957074474149\n",
      "train loss:0.29185441321119365\n",
      "train loss:0.38082534971416915\n",
      "train loss:0.3392237952731618\n",
      "train loss:0.37995055440578795\n",
      "train loss:0.22869606144304405\n",
      "train loss:0.3758329874576992\n",
      "train loss:0.28266668415654883\n",
      "train loss:0.31440760425203224\n",
      "train loss:0.3835340767991323\n",
      "train loss:0.2717657186563719\n",
      "train loss:0.305871495079386\n",
      "train loss:0.39011973917140336\n",
      "train loss:0.4160854039401121\n",
      "train loss:0.31066104709433423\n",
      "train loss:0.2621250728967171\n",
      "train loss:0.34038729105981425\n",
      "train loss:0.2568886596185667\n",
      "train loss:0.30460418793240934\n",
      "train loss:0.3990339565377119\n",
      "train loss:0.3266385118516845\n",
      "train loss:0.34450261667980336\n",
      "train loss:0.2184049027743318\n",
      "train loss:0.3255012682028483\n",
      "train loss:0.2941389995896002\n",
      "train loss:0.32461753665448134\n",
      "train loss:0.3767721940884902\n",
      "train loss:0.39798975077037846\n",
      "train loss:0.3330484729725699\n",
      "train loss:0.344733830468345\n",
      "train loss:0.3015000666355969\n",
      "train loss:0.37542691348330637\n",
      "train loss:0.32116798878789515\n",
      "train loss:0.3838321852405634\n",
      "train loss:0.3294192131609182\n",
      "train loss:0.19417072289181644\n",
      "train loss:0.3773333563582216\n",
      "train loss:0.3475743231332746\n",
      "train loss:0.2577581351038838\n",
      "train loss:0.3325461526557079\n",
      "train loss:0.2524909879066152\n",
      "train loss:0.38131668051708845\n",
      "train loss:0.3278318285669347\n",
      "train loss:0.2519186003505143\n",
      "train loss:0.3490020903703386\n",
      "train loss:0.45147300276700525\n",
      "train loss:0.38808677956850807\n",
      "train loss:0.28396747735703587\n",
      "train loss:0.37485651398533226\n",
      "train loss:0.39922314273896636\n",
      "train loss:0.3170879292139113\n",
      "train loss:0.44333649486828314\n",
      "train loss:0.2186038657627466\n",
      "train loss:0.4248135204538193\n",
      "train loss:0.2803501678279824\n",
      "train loss:0.28319111642776046\n",
      "train loss:0.3055481814954064\n",
      "train loss:0.37896566865853415\n",
      "train loss:0.27521821690345355\n",
      "train loss:0.2809139672429153\n",
      "train loss:0.2613494537712008\n",
      "train loss:0.2738209332950118\n",
      "train loss:0.4411711338447924\n",
      "train loss:0.2036317508181762\n",
      "train loss:0.2593831579141251\n",
      "train loss:0.4306130748583958\n",
      "train loss:0.37465919756945565\n",
      "train loss:0.3814404899908607\n",
      "train loss:0.3798913040481818\n",
      "train loss:0.2640034390138801\n",
      "train loss:0.29684649358636783\n",
      "train loss:0.31718200497232973\n",
      "train loss:0.3986351240053397\n",
      "train loss:0.28998335274800574\n",
      "train loss:0.3211800027258559\n",
      "train loss:0.2972092806646969\n",
      "train loss:0.29486758914415073\n",
      "train loss:0.30821590338536536\n",
      "train loss:0.41617923238273863\n",
      "train loss:0.258750495908592\n",
      "train loss:0.3202196574018433\n",
      "train loss:0.3510611251444945\n",
      "train loss:0.3220397170807855\n",
      "train loss:0.3864496248086639\n",
      "train loss:0.2740414202651858\n",
      "train loss:0.39062057025326646\n",
      "train loss:0.349973944570734\n",
      "train loss:0.33942774679165877\n",
      "train loss:0.3201526385585142\n",
      "train loss:0.2959284691397845\n",
      "train loss:0.33777207514748986\n",
      "train loss:0.4165915209676673\n",
      "train loss:0.44776981406877525\n",
      "train loss:0.3244308560290007\n",
      "train loss:0.2814162603533976\n",
      "train loss:0.23518509839899748\n",
      "train loss:0.28804442082728177\n",
      "train loss:0.32715443407636774\n",
      "train loss:0.36822249049086314\n",
      "train loss:0.23237935497451176\n",
      "train loss:0.3487106165761739\n",
      "train loss:0.30190029121579526\n",
      "train loss:0.43377047644613503\n",
      "train loss:0.42243867753647885\n",
      "train loss:0.3593218303790253\n",
      "train loss:0.35219710970684076\n",
      "train loss:0.29132521578281756\n",
      "train loss:0.34862770591780234\n",
      "train loss:0.3343774747672886\n",
      "train loss:0.4250034106500924\n",
      "train loss:0.3547312660745833\n",
      "train loss:0.22816074914747464\n",
      "train loss:0.18957612014388375\n",
      "train loss:0.3708761272465751\n",
      "train loss:0.26007631627237315\n",
      "train loss:0.3201067686037319\n",
      "train loss:0.3400068932145385\n",
      "train loss:0.25690970406287145\n",
      "train loss:0.31936524691866364\n",
      "train loss:0.2545293687421011\n",
      "train loss:0.2969958417168673\n",
      "train loss:0.29610399178398267\n",
      "train loss:0.2836671282281633\n",
      "train loss:0.31425623159410565\n",
      "train loss:0.4066964781711041\n",
      "train loss:0.2602621965389538\n",
      "train loss:0.2870195026608258\n",
      "train loss:0.316148513484372\n",
      "train loss:0.31059516000455206\n",
      "train loss:0.34407608203903\n",
      "train loss:0.31218448630688234\n",
      "train loss:0.4900238432891072\n",
      "train loss:0.258188207281036\n",
      "train loss:0.3430878151561213\n",
      "train loss:0.25594861420618237\n",
      "train loss:0.2062882518094803\n",
      "train loss:0.30548530081753056\n",
      "train loss:0.3935670108346542\n",
      "train loss:0.2238434470942762\n",
      "train loss:0.3222484835194439\n",
      "train loss:0.2874003976124922\n",
      "train loss:0.3310647634104837\n",
      "train loss:0.282610608722151\n",
      "train loss:0.41592372112759873\n",
      "train loss:0.23300937253245582\n",
      "train loss:0.2945922873532597\n",
      "train loss:0.4210812035886544\n",
      "train loss:0.2151328546388188\n",
      "train loss:0.2745102587665289\n",
      "train loss:0.26764037920974804\n",
      "train loss:0.24200463826702912\n",
      "train loss:0.2559353744237155\n",
      "train loss:0.4040198797700692\n",
      "train loss:0.3650098303914648\n",
      "train loss:0.38216424053017917\n",
      "train loss:0.2576751914667993\n",
      "train loss:0.33019902394115547\n",
      "train loss:0.4390224822334523\n",
      "train loss:0.4299987594949581\n",
      "train loss:0.2410166118731939\n",
      "train loss:0.31381856317116935\n",
      "train loss:0.24602853559768953\n",
      "train loss:0.4367395326761864\n",
      "train loss:0.3086643218836815\n",
      "train loss:0.3749300542911134\n",
      "train loss:0.19419626987920516\n",
      "train loss:0.30694539319089176\n",
      "train loss:0.3892330593636256\n",
      "train loss:0.35617771830286765\n",
      "train loss:0.2661490106067516\n",
      "train loss:0.3609478344046409\n",
      "train loss:0.23310238891873422\n",
      "train loss:0.2787047982524278\n",
      "train loss:0.21553877563065402\n",
      "train loss:0.35137771779101706\n",
      "train loss:0.3611924222468143\n",
      "train loss:0.3187621391390835\n",
      "train loss:0.2555415827252293\n",
      "train loss:0.29926244894048926\n",
      "train loss:0.2724519204908705\n",
      "train loss:0.3446308659302737\n",
      "train loss:0.19242285405144305\n",
      "train loss:0.32669794085213577\n",
      "train loss:0.4848441047095149\n",
      "train loss:0.2637437311834436\n",
      "train loss:0.280262783346872\n",
      "train loss:0.37959813830166256\n",
      "train loss:0.3712851672785758\n",
      "train loss:0.3927202053018724\n",
      "train loss:0.24532045327124005\n",
      "train loss:0.3103888830172836\n",
      "train loss:0.36117840749346264\n",
      "train loss:0.22570405213961503\n",
      "train loss:0.5028132924114117\n",
      "train loss:0.27160908693140123\n",
      "train loss:0.27743429250157875\n",
      "train loss:0.32543448463033664\n",
      "train loss:0.3594894685083628\n",
      "train loss:0.34078705012200816\n",
      "train loss:0.4246493931029836\n",
      "train loss:0.3281154337170058\n",
      "train loss:0.30312081838825167\n",
      "train loss:0.4215885881112622\n",
      "train loss:0.3140334691494071\n",
      "train loss:0.27065214979930685\n",
      "train loss:0.3052995200288007\n",
      "train loss:0.24348620905420557\n",
      "train loss:0.3024316860644553\n",
      "train loss:0.5937244410837759\n",
      "train loss:0.21977059016757217\n",
      "train loss:0.3993783407280038\n",
      "train loss:0.36055420021021656\n",
      "train loss:0.3218655436284417\n",
      "train loss:0.33284544682405803\n",
      "train loss:0.2993900365648473\n",
      "train loss:0.3319379335600071\n",
      "train loss:0.32682093557410286\n",
      "train loss:0.41406014487987436\n",
      "train loss:0.23364915430327163\n",
      "train loss:0.26803914522554406\n",
      "train loss:0.21976378864962653\n",
      "train loss:0.3635300142676536\n",
      "train loss:0.27319606428822235\n",
      "train loss:0.24520184905669012\n",
      "train loss:0.30581765247474857\n",
      "train loss:0.3726689270986998\n",
      "train loss:0.1888891636755861\n",
      "train loss:0.41631948559846793\n",
      "train loss:0.6200936487364292\n",
      "train loss:0.27943905352287024\n",
      "train loss:0.2758244365836638\n",
      "train loss:0.3458671235099047\n",
      "train loss:0.26554753992620744\n",
      "train loss:0.28399066251146726\n",
      "train loss:0.29250329199920727\n",
      "train loss:0.491608846260108\n",
      "train loss:0.28984261602019373\n",
      "train loss:0.29341118797996424\n",
      "train loss:0.3733840078127506\n",
      "train loss:0.4017177433329415\n",
      "train loss:0.5551830174706543\n",
      "train loss:0.35065947161793437\n",
      "train loss:0.2421837354116759\n",
      "train loss:0.38842095922106135\n",
      "train loss:0.2679626736940388\n",
      "train loss:0.43884837635763707\n",
      "train loss:0.24565885937052442\n",
      "train loss:0.3730425678077908\n",
      "train loss:0.4669731049432159\n",
      "train loss:0.3237817758532017\n",
      "train loss:0.2888916428739215\n",
      "train loss:0.4100953235511345\n",
      "train loss:0.33575565473461033\n",
      "train loss:0.23117284621153739\n",
      "train loss:0.34215366931896113\n",
      "train loss:0.24512102839689462\n",
      "train loss:0.2908256101703024\n",
      "train loss:0.39485723241816717\n",
      "train loss:0.27652138792125674\n",
      "train loss:0.35768873862160044\n",
      "train loss:0.24390789771176175\n",
      "train loss:0.3953796112315859\n",
      "train loss:0.44346692906025675\n",
      "train loss:0.25391253667792896\n",
      "train loss:0.2902955552606206\n",
      "train loss:0.29749196663648525\n",
      "train loss:0.2768833963852173\n",
      "train loss:0.3295418750328007\n",
      "train loss:0.43040614754960493\n",
      "train loss:0.3128862640918168\n",
      "train loss:0.3029078619492681\n",
      "train loss:0.28749523069425936\n",
      "train loss:0.4513462972402103\n",
      "train loss:0.4491439160035945\n",
      "train loss:0.2526415574968285\n",
      "train loss:0.339398648231236\n",
      "train loss:0.2851289563306192\n",
      "train loss:0.19820790439390937\n",
      "train loss:0.3351403953486482\n",
      "train loss:0.3687617779937737\n",
      "train loss:0.5263669205342095\n",
      "train loss:0.32535897172302564\n",
      "train loss:0.23165509004512266\n",
      "train loss:0.24727111452697328\n",
      "train loss:0.2279831345553216\n",
      "train loss:0.5404569845037925\n",
      "train loss:0.42349056897958703\n",
      "train loss:0.349235867390491\n",
      "train loss:0.29951949486487656\n",
      "train loss:0.2730150264673441\n",
      "train loss:0.37821514363490655\n",
      "train loss:0.30720627852759774\n",
      "train loss:0.27389130610795204\n",
      "train loss:0.3499629269577032\n",
      "train loss:0.4414763901143761\n",
      "train loss:0.3191071595574985\n",
      "train loss:0.33165536309925886\n",
      "train loss:0.20524390653680422\n",
      "train loss:0.38224968567085055\n",
      "train loss:0.3068119466770398\n",
      "train loss:0.21707976873109136\n",
      "train loss:0.39205973184006054\n",
      "train loss:0.2360420294427495\n",
      "train loss:0.3197436129293073\n",
      "train loss:0.2779349400896465\n",
      "train loss:0.41182941167688425\n",
      "train loss:0.3551044332050369\n",
      "train loss:0.324911363209203\n",
      "=== epoch:18, train acc:0.853, test acc:0.853 ===\n",
      "train loss:0.30962346701481025\n",
      "train loss:0.5054334389258769\n",
      "train loss:0.2852989151217004\n",
      "train loss:0.21626457388465437\n",
      "train loss:0.24768799348054032\n",
      "train loss:0.20670889072245494\n",
      "train loss:0.20269051756433737\n",
      "train loss:0.26710449219490745\n",
      "train loss:0.32205829665398594\n",
      "train loss:0.4363231678698668\n",
      "train loss:0.22355593330483908\n",
      "train loss:0.27342690528005387\n",
      "train loss:0.42800587748321156\n",
      "train loss:0.359490286579357\n",
      "train loss:0.2824699988968096\n",
      "train loss:0.30994337951195106\n",
      "train loss:0.2939865080694245\n",
      "train loss:0.2053940787381071\n",
      "train loss:0.27523883610901817\n",
      "train loss:0.46728916137397614\n",
      "train loss:0.38053068736739426\n",
      "train loss:0.28167130442035054\n",
      "train loss:0.3743196876519871\n",
      "train loss:0.39072432001352736\n",
      "train loss:0.2686505796924571\n",
      "train loss:0.34919144434469\n",
      "train loss:0.19377248687597529\n",
      "train loss:0.26941263712435265\n",
      "train loss:0.4063636288539837\n",
      "train loss:0.24135607111200807\n",
      "train loss:0.2325446151925786\n",
      "train loss:0.3123638791707773\n",
      "train loss:0.3962075497669453\n",
      "train loss:0.2474757788910974\n",
      "train loss:0.4046909593105423\n",
      "train loss:0.3382303165469056\n",
      "train loss:0.26138234339922567\n",
      "train loss:0.2845458567003112\n",
      "train loss:0.17899495942055754\n",
      "train loss:0.3091043006832071\n",
      "train loss:0.4888872565463905\n",
      "train loss:0.30515288493414494\n",
      "train loss:0.33131228420910325\n",
      "train loss:0.2596406491334833\n",
      "train loss:0.39302146542443156\n",
      "train loss:0.42182574342447743\n",
      "train loss:0.28458776733545077\n",
      "train loss:0.25421982700171136\n",
      "train loss:0.29578090613897245\n",
      "train loss:0.37531697378633133\n",
      "train loss:0.2548929785216098\n",
      "train loss:0.3664815988898603\n",
      "train loss:0.31694632375392623\n",
      "train loss:0.21759097901297098\n",
      "train loss:0.44089827907097784\n",
      "train loss:0.3127370254722973\n",
      "train loss:0.27670154129429025\n",
      "train loss:0.35639453687673417\n",
      "train loss:0.3969212380302227\n",
      "train loss:0.3709018618539854\n",
      "train loss:0.31663676594000684\n",
      "train loss:0.3007415128043665\n",
      "train loss:0.5061609768811164\n",
      "train loss:0.38986193027786414\n",
      "train loss:0.35092664716437033\n",
      "train loss:0.4068798292413053\n",
      "train loss:0.37851706771287347\n",
      "train loss:0.25951776708358776\n",
      "train loss:0.32643892718043593\n",
      "train loss:0.3787726653413035\n",
      "train loss:0.32826673216213237\n",
      "train loss:0.40168635638647104\n",
      "train loss:0.2960421304010023\n",
      "train loss:0.34222435048346367\n",
      "train loss:0.42550852918903365\n",
      "train loss:0.2536908265322472\n",
      "train loss:0.32786731471161623\n",
      "train loss:0.3687883950448663\n",
      "train loss:0.17211025630281993\n",
      "train loss:0.385148676609475\n",
      "train loss:0.28355677908645033\n",
      "train loss:0.31729590240418104\n",
      "train loss:0.34506712134943307\n",
      "train loss:0.3336806460992597\n",
      "train loss:0.31255271900923476\n",
      "train loss:0.2962761185796807\n",
      "train loss:0.3651710665033203\n",
      "train loss:0.33564765347573877\n",
      "train loss:0.3295265398713791\n",
      "train loss:0.3062042651001009\n",
      "train loss:0.34423043208499615\n",
      "train loss:0.32737900627606636\n",
      "train loss:0.35025128770940256\n",
      "train loss:0.2744698363394885\n",
      "train loss:0.27264125332997025\n",
      "train loss:0.38240502999892373\n",
      "train loss:0.31848871038624843\n",
      "train loss:0.5074669120197476\n",
      "train loss:0.2530273581116698\n",
      "train loss:0.3064753765155846\n",
      "train loss:0.27479879190769446\n",
      "train loss:0.3635776304311523\n",
      "train loss:0.3949376948787955\n",
      "train loss:0.23608684426851326\n",
      "train loss:0.3812301641170738\n",
      "train loss:0.4647502783018798\n",
      "train loss:0.3580661386744871\n",
      "train loss:0.2593415636841126\n",
      "train loss:0.281899817077295\n",
      "train loss:0.431540900271122\n",
      "train loss:0.40955339537698643\n",
      "train loss:0.47308798228688054\n",
      "train loss:0.2800418138254262\n",
      "train loss:0.2635525170798564\n",
      "train loss:0.4241841908172708\n",
      "train loss:0.1773432582321239\n",
      "train loss:0.19778088131883295\n",
      "train loss:0.20477350972419295\n",
      "train loss:0.3381593329384174\n",
      "train loss:0.22511203918922645\n",
      "train loss:0.24884642780503186\n",
      "train loss:0.46853495656947375\n",
      "train loss:0.4450100778148254\n",
      "train loss:0.26717935889055733\n",
      "train loss:0.3818035945540621\n",
      "train loss:0.44345238421329275\n",
      "train loss:0.3674920781863257\n",
      "train loss:0.34876728363280884\n",
      "train loss:0.24125891863942037\n",
      "train loss:0.29389881461200607\n",
      "train loss:0.36475934196387805\n",
      "train loss:0.40555332066685496\n",
      "train loss:0.5598593676310282\n",
      "train loss:0.2713793499285952\n",
      "train loss:0.4195033119583379\n",
      "train loss:0.2616432223118326\n",
      "train loss:0.2089044652911711\n",
      "train loss:0.43617095093633174\n",
      "train loss:0.307344650841011\n",
      "train loss:0.3583895099873943\n",
      "train loss:0.26702444697862915\n",
      "train loss:0.4181024161477191\n",
      "train loss:0.4197721295935055\n",
      "train loss:0.4340357958860356\n",
      "train loss:0.4128861823996741\n",
      "train loss:0.38573438616955613\n",
      "train loss:0.41105000189468577\n",
      "train loss:0.26919632502407065\n",
      "train loss:0.31351517371783405\n",
      "train loss:0.4010570568192607\n",
      "train loss:0.31695588567013405\n",
      "train loss:0.35428037708029675\n",
      "train loss:0.4743455802478252\n",
      "train loss:0.2891031634086037\n",
      "train loss:0.37643361213138143\n",
      "train loss:0.23202974636003212\n",
      "train loss:0.293186867429882\n",
      "train loss:0.3660131949227941\n",
      "train loss:0.2558023959469436\n",
      "train loss:0.3923232019063337\n",
      "train loss:0.3731804662523461\n",
      "train loss:0.31128848601921794\n",
      "train loss:0.25966531734772624\n",
      "train loss:0.28212684595639925\n",
      "train loss:0.5468057152941601\n",
      "train loss:0.2502755193158496\n",
      "train loss:0.3132821909419436\n",
      "train loss:0.2639944398781255\n",
      "train loss:0.4504797109747918\n",
      "train loss:0.2941292073043627\n",
      "train loss:0.2913138895776775\n",
      "train loss:0.39207923427218583\n",
      "train loss:0.38681350862994246\n",
      "train loss:0.28095070631816144\n",
      "train loss:0.40424604700855293\n",
      "train loss:0.28518042575758895\n",
      "train loss:0.37382570561312967\n",
      "train loss:0.3439500949679537\n",
      "train loss:0.25343123471192003\n",
      "train loss:0.49757403049332494\n",
      "train loss:0.3375543006251847\n",
      "train loss:0.27903654449952536\n",
      "train loss:0.25593430271250184\n",
      "train loss:0.23657483112281444\n",
      "train loss:0.29249445813901925\n",
      "train loss:0.23263125312897037\n",
      "train loss:0.3425506636345495\n",
      "train loss:0.31935978118180275\n",
      "train loss:0.2626858445974695\n",
      "train loss:0.4608339525144502\n",
      "train loss:0.34632009822410287\n",
      "train loss:0.24439092019391342\n",
      "train loss:0.5156011064527091\n",
      "train loss:0.36066157818995803\n",
      "train loss:0.4484443270202341\n",
      "train loss:0.3581728474883307\n",
      "train loss:0.24402809702718137\n",
      "train loss:0.30317280883843134\n",
      "train loss:0.4627285330873564\n",
      "train loss:0.3229437049543185\n",
      "train loss:0.28070003422232787\n",
      "train loss:0.4950405205978177\n",
      "train loss:0.44330452575341106\n",
      "train loss:0.27075306729238696\n",
      "train loss:0.2126062522772102\n",
      "train loss:0.3949163743029554\n",
      "train loss:0.2785550423764117\n",
      "train loss:0.2570959395614104\n",
      "train loss:0.3410498964635743\n",
      "train loss:0.33623838095707803\n",
      "train loss:0.251245323100189\n",
      "train loss:0.2780440118632209\n",
      "train loss:0.32110144988138467\n",
      "train loss:0.34365205704852253\n",
      "train loss:0.3633023670578596\n",
      "train loss:0.3640458205929781\n",
      "train loss:0.3705925533703373\n",
      "train loss:0.2972652999510813\n",
      "train loss:0.2899907022967996\n",
      "train loss:0.3135494895603763\n",
      "train loss:0.4719846423758256\n",
      "train loss:0.4718226564163243\n",
      "train loss:0.48144349196969793\n",
      "train loss:0.4848291164557741\n",
      "train loss:0.24980347776061085\n",
      "train loss:0.30886579791549074\n",
      "train loss:0.27090256306903177\n",
      "train loss:0.3503996633295018\n",
      "train loss:0.2729314598647526\n",
      "train loss:0.2620256213636954\n",
      "train loss:0.36806126301577907\n",
      "train loss:0.3369050251956542\n",
      "train loss:0.4224449451755135\n",
      "train loss:0.26317945785220687\n",
      "train loss:0.4359142445478416\n",
      "train loss:0.3164925375936431\n",
      "train loss:0.2957579508360252\n",
      "train loss:0.3635806705973185\n",
      "train loss:0.2532717357562331\n",
      "train loss:0.3888015289584129\n",
      "train loss:0.2696824894847429\n",
      "train loss:0.4326518548624471\n",
      "train loss:0.2550971912399659\n",
      "train loss:0.3440811786165197\n",
      "train loss:0.37786823958081556\n",
      "train loss:0.3292196725487662\n",
      "train loss:0.23809541944814966\n",
      "train loss:0.28365478838408426\n",
      "train loss:0.3090608241798739\n",
      "train loss:0.2932106181711735\n",
      "train loss:0.29387147168658057\n",
      "train loss:0.2711063109060201\n",
      "train loss:0.3887130218180376\n",
      "train loss:0.33016149606204515\n",
      "train loss:0.2123878905514343\n",
      "train loss:0.22119278046932553\n",
      "train loss:0.36843846634044064\n",
      "train loss:0.32943474612243245\n",
      "train loss:0.40270009953830427\n",
      "train loss:0.25072975883347093\n",
      "train loss:0.335036039730424\n",
      "train loss:0.33762208837975116\n",
      "train loss:0.38136260799696653\n",
      "train loss:0.4099267080985257\n",
      "train loss:0.31745410964836596\n",
      "train loss:0.3885340424106659\n",
      "train loss:0.2930486259641367\n",
      "train loss:0.24820368509527824\n",
      "train loss:0.21735812044765632\n",
      "train loss:0.36524358297305526\n",
      "train loss:0.4137327960760673\n",
      "train loss:0.25079204938372235\n",
      "train loss:0.1508083391237601\n",
      "train loss:0.3200447654911004\n",
      "train loss:0.32301815568369785\n",
      "train loss:0.26051564131113847\n",
      "train loss:0.26364805490899984\n",
      "train loss:0.23229704354698005\n",
      "train loss:0.3526578421884885\n",
      "train loss:0.32607439819728484\n",
      "train loss:0.31002003525340155\n",
      "train loss:0.408199703634397\n",
      "train loss:0.2837684316336015\n",
      "train loss:0.21521344627132347\n",
      "train loss:0.27221785461928627\n",
      "train loss:0.3382389198291267\n",
      "train loss:0.4044931414322667\n",
      "train loss:0.21061326370032554\n",
      "train loss:0.38339112718862794\n",
      "train loss:0.3082388284903064\n",
      "train loss:0.47215601581523553\n",
      "train loss:0.30046700327235576\n",
      "train loss:0.40442575771045386\n",
      "train loss:0.35939455772894163\n",
      "train loss:0.303003758823208\n",
      "train loss:0.2916323622982886\n",
      "train loss:0.28238823455165585\n",
      "train loss:0.3260555714720453\n",
      "train loss:0.4210383632070264\n",
      "train loss:0.2645519564713339\n",
      "train loss:0.3758957734553853\n",
      "train loss:0.34888152713057186\n",
      "train loss:0.3928402536251451\n",
      "train loss:0.3237438043730585\n",
      "train loss:0.18900396818275972\n",
      "train loss:0.2614466802550836\n",
      "train loss:0.3572717553377121\n",
      "train loss:0.31355737458545335\n",
      "train loss:0.28576216679243055\n",
      "train loss:0.48978051936051387\n",
      "train loss:0.28098783168396524\n",
      "train loss:0.4250187946084957\n",
      "train loss:0.31152380926913403\n",
      "train loss:0.37702980390037505\n",
      "train loss:0.2563800382523079\n",
      "train loss:0.2423429850833769\n",
      "train loss:0.24140699504252208\n",
      "train loss:0.2706897053931908\n",
      "train loss:0.2604161562773895\n",
      "train loss:0.34365238520188107\n",
      "train loss:0.2758102918403813\n",
      "train loss:0.2806899209940598\n",
      "train loss:0.3811693585892156\n",
      "train loss:0.29597993861563476\n",
      "train loss:0.4069435345988747\n",
      "train loss:0.3657831061373774\n",
      "train loss:0.2868639742158747\n",
      "train loss:0.3556647540101983\n",
      "train loss:0.36377850592228794\n",
      "train loss:0.19907546824795108\n",
      "train loss:0.25215360313210444\n",
      "train loss:0.3523666262757245\n",
      "train loss:0.22406249000603393\n",
      "train loss:0.3688762690017017\n",
      "train loss:0.42216048987464816\n",
      "train loss:0.3321617347512631\n",
      "train loss:0.20641136611812683\n",
      "train loss:0.33565715138418967\n",
      "train loss:0.3984016806899168\n",
      "train loss:0.3011062174658183\n",
      "train loss:0.42079920026938233\n",
      "train loss:0.29325902900025985\n",
      "train loss:0.1800692074213696\n",
      "train loss:0.2643331598043513\n",
      "train loss:0.33897005041168526\n",
      "train loss:0.30607197539787123\n",
      "train loss:0.2671890085888922\n",
      "train loss:0.5525207205921591\n",
      "train loss:0.1928748331769211\n",
      "train loss:0.35891719781866416\n",
      "train loss:0.3874115187658537\n",
      "train loss:0.40156481245501746\n",
      "train loss:0.3980557955937\n",
      "train loss:0.2807016648894499\n",
      "train loss:0.3714039287462982\n",
      "train loss:0.334263759473098\n",
      "train loss:0.3836283732096467\n",
      "train loss:0.4706075258133303\n",
      "train loss:0.35818800440813986\n",
      "train loss:0.43293279444427346\n",
      "train loss:0.21828934390493637\n",
      "train loss:0.24918281523079222\n",
      "train loss:0.3653896789711494\n",
      "train loss:0.2819262671190961\n",
      "train loss:0.48405638663534745\n",
      "train loss:0.2200948535166582\n",
      "train loss:0.29187892659475256\n",
      "train loss:0.3554982531496131\n",
      "train loss:0.20508341477782863\n",
      "train loss:0.3700562035125286\n",
      "train loss:0.6295488088915013\n",
      "train loss:0.28700206116345767\n",
      "train loss:0.19804055964179315\n",
      "train loss:0.3973416395826119\n",
      "train loss:0.321225411689716\n",
      "train loss:0.43002043052810096\n",
      "train loss:0.29079239487168024\n",
      "train loss:0.5465499436630834\n",
      "train loss:0.39190090673432176\n",
      "train loss:0.21345412174194564\n",
      "train loss:0.37581814255979723\n",
      "train loss:0.33514257191754154\n",
      "train loss:0.32576639466436974\n",
      "train loss:0.25228098265256205\n",
      "train loss:0.3438558095635896\n",
      "train loss:0.40084970913554363\n",
      "train loss:0.28463346915246523\n",
      "train loss:0.3640853644637886\n",
      "train loss:0.3631756083565142\n",
      "train loss:0.29243592694352616\n",
      "train loss:0.2918288152075536\n",
      "train loss:0.40807843129601684\n",
      "train loss:0.3224047604185289\n",
      "train loss:0.25618170168268917\n",
      "train loss:0.372620671868603\n",
      "train loss:0.34186083246513727\n",
      "train loss:0.2801099929444439\n",
      "train loss:0.2360956914589739\n",
      "train loss:0.4564221131206181\n",
      "train loss:0.4735048027688144\n",
      "train loss:0.30565671775495973\n",
      "train loss:0.3193711334984709\n",
      "train loss:0.42448844245596523\n",
      "train loss:0.2772367509469779\n",
      "train loss:0.21273751035524224\n",
      "train loss:0.23404799885088823\n",
      "train loss:0.25110241877955913\n",
      "train loss:0.4651604696512149\n",
      "train loss:0.6410179055085854\n",
      "train loss:0.2858703657959996\n",
      "train loss:0.23896114428476095\n",
      "train loss:0.28475906507455223\n",
      "train loss:0.30441305849623623\n",
      "train loss:0.3310318935462089\n",
      "train loss:0.5025706416514064\n",
      "train loss:0.35341457072809634\n",
      "train loss:0.3333823915767352\n",
      "train loss:0.3635416168677875\n",
      "train loss:0.2975347398444734\n",
      "train loss:0.256928266842267\n",
      "train loss:0.28653926992270784\n",
      "train loss:0.3623753392556972\n",
      "train loss:0.3499398853601171\n",
      "train loss:0.20859438254332965\n",
      "train loss:0.39887057581540747\n",
      "train loss:0.35556136954812445\n",
      "train loss:0.391679783379568\n",
      "train loss:0.3386893812021489\n",
      "train loss:0.37127351646608564\n",
      "train loss:0.36115392848575534\n",
      "train loss:0.34289851495202617\n",
      "train loss:0.39111149292567843\n",
      "train loss:0.3589072177502057\n",
      "train loss:0.3254909330477522\n",
      "train loss:0.48305692425904034\n",
      "train loss:0.36867870013853554\n",
      "train loss:0.3281549447435107\n",
      "train loss:0.4935599031484889\n",
      "train loss:0.3165860260483636\n",
      "train loss:0.3947764867003501\n",
      "train loss:0.5985860878908459\n",
      "train loss:0.2492050177441591\n",
      "train loss:0.38324618635529356\n",
      "train loss:0.4937381889052911\n",
      "train loss:0.2698821802063797\n",
      "train loss:0.2685340376437789\n",
      "train loss:0.41843532867206945\n",
      "train loss:0.45785504042645153\n",
      "train loss:0.3178215527586035\n",
      "train loss:0.46247607370584504\n",
      "train loss:0.2980796887709707\n",
      "train loss:0.44968953956945973\n",
      "train loss:0.2809277872263575\n",
      "train loss:0.2514297351507537\n",
      "train loss:0.30534332411735754\n",
      "train loss:0.32271497065849175\n",
      "train loss:0.4147540532479694\n",
      "train loss:0.3889450833687657\n",
      "train loss:0.3076605352136267\n",
      "train loss:0.3131991732027769\n",
      "train loss:0.30233485555536327\n",
      "train loss:0.3742266179323596\n",
      "train loss:0.3248573880468924\n",
      "train loss:0.25420521714494004\n",
      "train loss:0.42996098852101744\n",
      "train loss:0.3014879392467139\n",
      "train loss:0.3191869924204237\n",
      "train loss:0.3158162392785563\n",
      "train loss:0.3042744881501978\n",
      "train loss:0.35196583058253905\n",
      "train loss:0.4087073110492524\n",
      "train loss:0.2980947066699751\n",
      "train loss:0.3162257244625304\n",
      "train loss:0.43215448289558545\n",
      "train loss:0.34377081850137636\n",
      "train loss:0.40002288706594413\n",
      "train loss:0.28177180395484025\n",
      "train loss:0.28376007323071234\n",
      "train loss:0.3403892706245759\n",
      "train loss:0.31278022533228494\n",
      "train loss:0.35048843172050176\n",
      "train loss:0.23341709720906942\n",
      "train loss:0.3553019452859055\n",
      "train loss:0.2312338443089579\n",
      "train loss:0.2635419911047336\n",
      "train loss:0.3093707442424531\n",
      "train loss:0.4126320959442171\n",
      "train loss:0.36344931877197334\n",
      "train loss:0.4098100073369712\n",
      "train loss:0.3676866803977012\n",
      "train loss:0.3331421174005467\n",
      "train loss:0.38324541653402583\n",
      "train loss:0.22406464196843756\n",
      "train loss:0.22399330944397758\n",
      "train loss:0.4226921824695811\n",
      "train loss:0.3315714629261077\n",
      "train loss:0.23404302091298468\n",
      "train loss:0.23274386012649212\n",
      "train loss:0.33757731140239433\n",
      "train loss:0.35915088396018985\n",
      "train loss:0.23292402365708603\n",
      "train loss:0.33768048323103955\n",
      "train loss:0.30207232856964755\n",
      "train loss:0.23478287152805813\n",
      "train loss:0.318592884911534\n",
      "train loss:0.44435458833123925\n",
      "train loss:0.26621805111047636\n",
      "train loss:0.23062205199168417\n",
      "train loss:0.3655598374761276\n",
      "train loss:0.2732640354214197\n",
      "train loss:0.24656569550125113\n",
      "train loss:0.25566961803293836\n",
      "train loss:0.3559678073435085\n",
      "train loss:0.23707724034915945\n",
      "train loss:0.4325495715479499\n",
      "train loss:0.244487723569973\n",
      "train loss:0.328262597726458\n",
      "train loss:0.3688424004627952\n",
      "train loss:0.22978615746210262\n",
      "train loss:0.30704480206583595\n",
      "train loss:0.2172382994880125\n",
      "train loss:0.26274027204795525\n",
      "train loss:0.2777704972219995\n",
      "train loss:0.2538883837680943\n",
      "train loss:0.24802283091213684\n",
      "train loss:0.41496379163371316\n",
      "train loss:0.4121451652204047\n",
      "train loss:0.3080567702112598\n",
      "train loss:0.25811161724942155\n",
      "train loss:0.3080158006817287\n",
      "train loss:0.5172668391458428\n",
      "train loss:0.17285986452204127\n",
      "train loss:0.24405972523856811\n",
      "train loss:0.3218850924497591\n",
      "train loss:0.278285789769147\n",
      "train loss:0.4117284084771191\n",
      "train loss:0.23377520499277288\n",
      "train loss:0.4442014607573612\n",
      "train loss:0.432956913259643\n",
      "train loss:0.3304002553388464\n",
      "train loss:0.29379952746650073\n",
      "train loss:0.3630365280344142\n",
      "train loss:0.38215827645816236\n",
      "train loss:0.4001736663205156\n",
      "train loss:0.3391447997282217\n",
      "train loss:0.2920748672639762\n",
      "train loss:0.3020228945174776\n",
      "train loss:0.23946908582760185\n",
      "train loss:0.31556809168414834\n",
      "train loss:0.31236746993599185\n",
      "train loss:0.4665262547651991\n",
      "train loss:0.4643601251297503\n",
      "train loss:0.4030692787652033\n",
      "train loss:0.4119563206552027\n",
      "train loss:0.26900978709631984\n",
      "train loss:0.28694112495889107\n",
      "train loss:0.4090317053286949\n",
      "train loss:0.41497032036471615\n",
      "train loss:0.39158405107719296\n",
      "train loss:0.34427807694744295\n",
      "train loss:0.5045781906323763\n",
      "train loss:0.4670354430870548\n",
      "train loss:0.4074491446903839\n",
      "train loss:0.34664859494201855\n",
      "train loss:0.34721888854317773\n",
      "train loss:0.24372887899081777\n",
      "train loss:0.31698101651187066\n",
      "train loss:0.3268407827444635\n",
      "train loss:0.26722551540931716\n",
      "train loss:0.39024115440016194\n",
      "train loss:0.21614096028709945\n",
      "train loss:0.3374844166504865\n",
      "train loss:0.32896011583208645\n",
      "train loss:0.27594243868532276\n",
      "train loss:0.34730929708910824\n",
      "train loss:0.32135439488962414\n",
      "train loss:0.3731184688049327\n",
      "train loss:0.3044044739324023\n",
      "train loss:0.229133339386063\n",
      "train loss:0.2975702582966243\n",
      "train loss:0.30026355355928547\n",
      "train loss:0.17602347584310912\n",
      "train loss:0.40941641718701954\n",
      "train loss:0.2694971485604025\n",
      "train loss:0.2781670917590369\n",
      "train loss:0.26620465762415985\n",
      "train loss:0.28096711083875087\n",
      "train loss:0.3412628550608642\n",
      "train loss:0.37625735471275584\n",
      "train loss:0.5679992213991204\n",
      "train loss:0.2785342045962554\n",
      "train loss:0.30143324704154045\n",
      "train loss:0.2959965180430302\n",
      "train loss:0.24422041810764517\n",
      "train loss:0.41974246605142135\n",
      "train loss:0.2132815830159449\n",
      "train loss:0.19834707129688442\n",
      "train loss:0.37856013608597405\n",
      "train loss:0.39666377757417115\n",
      "train loss:0.3647081430896754\n",
      "=== epoch:19, train acc:0.853, test acc:0.86 ===\n",
      "train loss:0.38829565699854135\n",
      "train loss:0.45927126993166\n",
      "train loss:0.34561849448609366\n",
      "train loss:0.20308102199696734\n",
      "train loss:0.27928709401409607\n",
      "train loss:0.2633379960346672\n",
      "train loss:0.25047458627213154\n",
      "train loss:0.48619944804504756\n",
      "train loss:0.269379061169195\n",
      "train loss:0.2984701406670272\n",
      "train loss:0.2751444809520288\n",
      "train loss:0.19313289782481188\n",
      "train loss:0.3523252948197412\n",
      "train loss:0.36195213711761914\n",
      "train loss:0.37244839153114806\n",
      "train loss:0.31251343146771304\n",
      "train loss:0.23418852956336184\n",
      "train loss:0.249496643791434\n",
      "train loss:0.28354559543982627\n",
      "train loss:0.21903501860967275\n",
      "train loss:0.22574723624980197\n",
      "train loss:0.28508020559735864\n",
      "train loss:0.37188772935776204\n",
      "train loss:0.38432203479736493\n",
      "train loss:0.3257301027060779\n",
      "train loss:0.3149469390671664\n",
      "train loss:0.34002761432490913\n",
      "train loss:0.2431690940548109\n",
      "train loss:0.23574789367594245\n",
      "train loss:0.4213941834639683\n",
      "train loss:0.20491172313918948\n",
      "train loss:0.28621347846472195\n",
      "train loss:0.38599889443235297\n",
      "train loss:0.21384504792468398\n",
      "train loss:0.2847916377801235\n",
      "train loss:0.331145834384643\n",
      "train loss:0.4023086056621538\n",
      "train loss:0.29103238840005885\n",
      "train loss:0.48096311254623925\n",
      "train loss:0.3549498871713385\n",
      "train loss:0.33327316641747506\n",
      "train loss:0.2677357331356318\n",
      "train loss:0.41902473829795317\n",
      "train loss:0.42268509862875975\n",
      "train loss:0.3346325840408935\n",
      "train loss:0.39209002560992856\n",
      "train loss:0.3889787194993406\n",
      "train loss:0.27095525726981995\n",
      "train loss:0.3615896438689596\n",
      "train loss:0.39003536867059063\n",
      "train loss:0.48447871754848826\n",
      "train loss:0.44688416851015417\n",
      "train loss:0.2569415980116982\n",
      "train loss:0.34819613988756243\n",
      "train loss:0.32995914049465347\n",
      "train loss:0.30292104202307707\n",
      "train loss:0.2798997720490625\n",
      "train loss:0.4065676010966346\n",
      "train loss:0.33386951825858324\n",
      "train loss:0.3012010999909435\n",
      "train loss:0.3045905566936897\n",
      "train loss:0.1885854079267196\n",
      "train loss:0.23367622031664217\n",
      "train loss:0.28846781799010307\n",
      "train loss:0.3213149925555222\n",
      "train loss:0.24806671776102507\n",
      "train loss:0.27677861483829785\n",
      "train loss:0.4052698405582763\n",
      "train loss:0.37316885912109576\n",
      "train loss:0.2184779690273999\n",
      "train loss:0.43815240710305564\n",
      "train loss:0.28932048800687793\n",
      "train loss:0.38487602946699623\n",
      "train loss:0.5476621607178079\n",
      "train loss:0.36124810010983466\n",
      "train loss:0.3297085913797806\n",
      "train loss:0.3511270501287775\n",
      "train loss:0.26634814007802143\n",
      "train loss:0.5182409624212627\n",
      "train loss:0.393560959858968\n",
      "train loss:0.2710140095547691\n",
      "train loss:0.4586282316197593\n",
      "train loss:0.3325749921639408\n",
      "train loss:0.38799196472921893\n",
      "train loss:0.46085047091499687\n",
      "train loss:0.22623321780278388\n",
      "train loss:0.3627157448162411\n",
      "train loss:0.3681797529115727\n",
      "train loss:0.30945541547828476\n",
      "train loss:0.2691687803099873\n",
      "train loss:0.38104180824450606\n",
      "train loss:0.23776100694654148\n",
      "train loss:0.35382073889561416\n",
      "train loss:0.24992893919637363\n",
      "train loss:0.37818481661530634\n",
      "train loss:0.3618231155456678\n",
      "train loss:0.2794467933476878\n",
      "train loss:0.3189754708021089\n",
      "train loss:0.32393025823211263\n",
      "train loss:0.2889774209940097\n",
      "train loss:0.34692601374946863\n",
      "train loss:0.31278566693127774\n",
      "train loss:0.35823934978131416\n",
      "train loss:0.2457023267520472\n",
      "train loss:0.23851728924918483\n",
      "train loss:0.21966045894907452\n",
      "train loss:0.27914937690218095\n",
      "train loss:0.29458501891520605\n",
      "train loss:0.3451201606002479\n",
      "train loss:0.3290539004396703\n",
      "train loss:0.36029084283306984\n",
      "train loss:0.5005359872888382\n",
      "train loss:0.4371152119894743\n",
      "train loss:0.29384626283717596\n",
      "train loss:0.5688721935411106\n",
      "train loss:0.33612960150326837\n",
      "train loss:0.20911354616105066\n",
      "train loss:0.28909418554086747\n",
      "train loss:0.21347350254989259\n",
      "train loss:0.28616171557680714\n",
      "train loss:0.20164796897214085\n",
      "train loss:0.2578550804722379\n",
      "train loss:0.40324305209953415\n",
      "train loss:0.2579843735978289\n",
      "train loss:0.20366435772042532\n",
      "train loss:0.3021746028324102\n",
      "train loss:0.34528087401873964\n",
      "train loss:0.37709172620806014\n",
      "train loss:0.4200950597682458\n",
      "train loss:0.3780969989033781\n",
      "train loss:0.4350647125626956\n",
      "train loss:0.22638665855983967\n",
      "train loss:0.4685054908477791\n",
      "train loss:0.3006648903682852\n",
      "train loss:0.30427594940398484\n",
      "train loss:0.31206647193083104\n",
      "train loss:0.34179142101053955\n",
      "train loss:0.23735063777132967\n",
      "train loss:0.44374674973504624\n",
      "train loss:0.22360384682831966\n",
      "train loss:0.38545032884533503\n",
      "train loss:0.2145152270756595\n",
      "train loss:0.23350069266500778\n",
      "train loss:0.3017258689725946\n",
      "train loss:0.47342558320303646\n",
      "train loss:0.339336416337291\n",
      "train loss:0.38966706971062787\n",
      "train loss:0.2526492018505745\n",
      "train loss:0.2880908942110348\n",
      "train loss:0.3567708162748151\n",
      "train loss:0.2778011951271762\n",
      "train loss:0.2710262097091991\n",
      "train loss:0.36668091797442537\n",
      "train loss:0.349228972584056\n",
      "train loss:0.32869867549588916\n",
      "train loss:0.27474747107366054\n",
      "train loss:0.2771906507835162\n",
      "train loss:0.34512028570983694\n",
      "train loss:0.3590019788640976\n",
      "train loss:0.5087533650336142\n",
      "train loss:0.32038112163928917\n",
      "train loss:0.3590514574413193\n",
      "train loss:0.2876358850226055\n",
      "train loss:0.3784782553936648\n",
      "train loss:0.26905817692074413\n",
      "train loss:0.25131989680536926\n",
      "train loss:0.3164172815190921\n",
      "train loss:0.44155808857078854\n",
      "train loss:0.3141574933472931\n",
      "train loss:0.39631828482332165\n",
      "train loss:0.260899860516498\n",
      "train loss:0.21551983197439054\n",
      "train loss:0.3030821661678995\n",
      "train loss:0.29344432211393384\n",
      "train loss:0.20597800452034498\n",
      "train loss:0.27643316943454965\n",
      "train loss:0.24564500561873237\n",
      "train loss:0.32299664807297795\n",
      "train loss:0.3394803884493401\n",
      "train loss:0.36123247015718307\n",
      "train loss:0.24919301499708463\n",
      "train loss:0.27632306873700097\n",
      "train loss:0.15508145448700467\n",
      "train loss:0.34093065989272503\n",
      "train loss:0.26318919051402045\n",
      "train loss:0.3984427399174599\n",
      "train loss:0.2166469738825162\n",
      "train loss:0.3615959503396603\n",
      "train loss:0.23618180575053174\n",
      "train loss:0.39345736919961277\n",
      "train loss:0.2738327445580271\n",
      "train loss:0.23533583469471625\n",
      "train loss:0.3383529326333436\n",
      "train loss:0.1958790951674735\n",
      "train loss:0.3277883694829078\n",
      "train loss:0.3352692700350981\n",
      "train loss:0.3133782907659892\n",
      "train loss:0.3015947975132308\n",
      "train loss:0.24138183572126592\n",
      "train loss:0.41816998670235256\n",
      "train loss:0.2821116238728709\n",
      "train loss:0.3785397090200709\n",
      "train loss:0.38793848900965927\n",
      "train loss:0.32654906041327\n",
      "train loss:0.2731642178669604\n",
      "train loss:0.4145102947498037\n",
      "train loss:0.20595462956370827\n",
      "train loss:0.29550447532235574\n",
      "train loss:0.4259253212294819\n",
      "train loss:0.24788789794225063\n",
      "train loss:0.40552088204023357\n",
      "train loss:0.2234441656533789\n",
      "train loss:0.39169061707063973\n",
      "train loss:0.25793499100059636\n",
      "train loss:0.31321994587801316\n",
      "train loss:0.322083031318933\n",
      "train loss:0.40148892887927395\n",
      "train loss:0.25733691170386536\n",
      "train loss:0.45110860855029933\n",
      "train loss:0.2833202986127265\n",
      "train loss:0.46162383985914185\n",
      "train loss:0.17840275039946285\n",
      "train loss:0.25161310263181574\n",
      "train loss:0.2738725766278482\n",
      "train loss:0.22313632254875407\n",
      "train loss:0.27982126234527643\n",
      "train loss:0.30508122127782084\n",
      "train loss:0.33075191143497773\n",
      "train loss:0.2504226711881632\n",
      "train loss:0.42470852825149313\n",
      "train loss:0.4021424209997413\n",
      "train loss:0.3465246901659449\n",
      "train loss:0.3923366820059513\n",
      "train loss:0.3319602531928858\n",
      "train loss:0.34682024259218347\n",
      "train loss:0.3699911010561378\n",
      "train loss:0.22753540591786625\n",
      "train loss:0.3435727595979871\n",
      "train loss:0.30515320034819376\n",
      "train loss:0.1896692084904289\n",
      "train loss:0.30300850415946223\n",
      "train loss:0.30814834772893407\n",
      "train loss:0.5268541021421365\n",
      "train loss:0.2880194587971434\n",
      "train loss:0.313781721895142\n",
      "train loss:0.3212736938331714\n",
      "train loss:0.5527589218776102\n",
      "train loss:0.2963879823264905\n",
      "train loss:0.2932988217079232\n",
      "train loss:0.33237964513001556\n",
      "train loss:0.3214041036337089\n",
      "train loss:0.28175873444474087\n",
      "train loss:0.28390989223414975\n",
      "train loss:0.2971370084988961\n",
      "train loss:0.2308029720127949\n",
      "train loss:0.18304833233704088\n",
      "train loss:0.3051502099012181\n",
      "train loss:0.25125555203303135\n",
      "train loss:0.31504226343664476\n",
      "train loss:0.24979196037299922\n",
      "train loss:0.4268076966525932\n",
      "train loss:0.36193691525817256\n",
      "train loss:0.40438543079001266\n",
      "train loss:0.35315615973521475\n",
      "train loss:0.4166928473774331\n",
      "train loss:0.2843481390151691\n",
      "train loss:0.3141664766829411\n",
      "train loss:0.26925613061260917\n",
      "train loss:0.308498289387139\n",
      "train loss:0.3631309320878709\n",
      "train loss:0.323409907590247\n",
      "train loss:0.3234030107514046\n",
      "train loss:0.47263508211289745\n",
      "train loss:0.279804453134277\n",
      "train loss:0.3621223759007986\n",
      "train loss:0.21584560646097625\n",
      "train loss:0.2223338128592023\n",
      "train loss:0.3810618920481464\n",
      "train loss:0.3393623943222457\n",
      "train loss:0.3474107603175963\n",
      "train loss:0.3335107856787618\n",
      "train loss:0.29457961655783665\n",
      "train loss:0.1852866973905259\n",
      "train loss:0.3200554802490651\n",
      "train loss:0.3125313839771606\n",
      "train loss:0.2534862333822032\n",
      "train loss:0.21605899678139462\n",
      "train loss:0.4532363111271399\n",
      "train loss:0.23867102829471815\n",
      "train loss:0.36749570320988345\n",
      "train loss:0.3175151156908079\n",
      "train loss:0.3263921043503797\n",
      "train loss:0.31857573215320795\n",
      "train loss:0.3504298550875635\n",
      "train loss:0.45763077555749754\n",
      "train loss:0.321011685515834\n",
      "train loss:0.39313418993829524\n",
      "train loss:0.3989048127241474\n",
      "train loss:0.1943758511384709\n",
      "train loss:0.3783329228519753\n",
      "train loss:0.2978461019838026\n",
      "train loss:0.37293551415079534\n",
      "train loss:0.3202400618469579\n",
      "train loss:0.24254726114294176\n",
      "train loss:0.3924614316313844\n",
      "train loss:0.5984948579043015\n",
      "train loss:0.3584323884717538\n",
      "train loss:0.269142979123873\n",
      "train loss:0.2702938599394109\n",
      "train loss:0.42061324103153347\n",
      "train loss:0.30707398945770104\n",
      "train loss:0.29044551040579897\n",
      "train loss:0.2950262201475966\n",
      "train loss:0.24235458446121363\n",
      "train loss:0.3833847824050436\n",
      "train loss:0.4292175948453131\n",
      "train loss:0.3500909765384685\n",
      "train loss:0.2595689016884922\n",
      "train loss:0.4051434010201455\n",
      "train loss:0.4335581343732395\n",
      "train loss:0.4385140009276739\n",
      "train loss:0.2661507174371386\n",
      "train loss:0.3878032195661076\n",
      "train loss:0.31534985734630455\n",
      "train loss:0.20694045765047422\n",
      "train loss:0.28261729220890075\n",
      "train loss:0.30072452261047716\n",
      "train loss:0.22981871949734153\n",
      "train loss:0.3394801143277568\n",
      "train loss:0.366983491440561\n",
      "train loss:0.4519125768915584\n",
      "train loss:0.33581387686001885\n",
      "train loss:0.3073662357414084\n",
      "train loss:0.37931963261370655\n",
      "train loss:0.3039952191163931\n",
      "train loss:0.305428685916093\n",
      "train loss:0.3231658942638975\n",
      "train loss:0.3271452020631316\n",
      "train loss:0.3027427658820891\n",
      "train loss:0.17844596281261393\n",
      "train loss:0.24842069480177162\n",
      "train loss:0.26597781602871995\n",
      "train loss:0.27769954469931885\n",
      "train loss:0.3077817674621316\n",
      "train loss:0.3680932035027887\n",
      "train loss:0.31238800584593285\n",
      "train loss:0.34741911224056865\n",
      "train loss:0.2992303239803337\n",
      "train loss:0.3871403732581078\n",
      "train loss:0.2231490421625114\n",
      "train loss:0.39681231015521784\n",
      "train loss:0.2598371621846239\n",
      "train loss:0.3027594370382633\n",
      "train loss:0.36182722497793374\n",
      "train loss:0.3418498114063885\n",
      "train loss:0.26981079113299905\n",
      "train loss:0.39663757279480516\n",
      "train loss:0.36276551184005185\n",
      "train loss:0.28105686453414136\n",
      "train loss:0.43908351518596944\n",
      "train loss:0.36705044233313955\n",
      "train loss:0.3099045791999504\n",
      "train loss:0.25212882646682383\n",
      "train loss:0.23700505201573546\n",
      "train loss:0.36806450205635777\n",
      "train loss:0.38859469899782867\n",
      "train loss:0.3040051273776758\n",
      "train loss:0.305987057872507\n",
      "train loss:0.39424730405755315\n",
      "train loss:0.35311467290340026\n",
      "train loss:0.2975964474925406\n",
      "train loss:0.37345492619229587\n",
      "train loss:0.7192141901499269\n",
      "train loss:0.2349670035019774\n",
      "train loss:0.35340999867158424\n",
      "train loss:0.263812225825214\n",
      "train loss:0.5116543252575876\n",
      "train loss:0.2862621626208776\n",
      "train loss:0.3455527792607283\n",
      "train loss:0.35567069321679873\n",
      "train loss:0.31113066047133947\n",
      "train loss:0.22488018081048966\n",
      "train loss:0.30086786411765504\n",
      "train loss:0.3683413055376056\n",
      "train loss:0.3931839965292719\n",
      "train loss:0.29749824017497833\n",
      "train loss:0.2812690843795665\n",
      "train loss:0.20615738710064535\n",
      "train loss:0.321120888350388\n",
      "train loss:0.3074744433010068\n",
      "train loss:0.3560360345125927\n",
      "train loss:0.36283399903941865\n",
      "train loss:0.13364498022869584\n",
      "train loss:0.37352925087956423\n",
      "train loss:0.2886171136919944\n",
      "train loss:0.2729754024827003\n",
      "train loss:0.38901470238283425\n",
      "train loss:0.3341654301534448\n",
      "train loss:0.297361931769631\n",
      "train loss:0.3072543516323314\n",
      "train loss:0.19638411849560725\n",
      "train loss:0.257916563106239\n",
      "train loss:0.25401601969901166\n",
      "train loss:0.35061228277179574\n",
      "train loss:0.46767393514522715\n",
      "train loss:0.3452009831912139\n",
      "train loss:0.23093589485394708\n",
      "train loss:0.22366237510612055\n",
      "train loss:0.3872290366423961\n",
      "train loss:0.4085517956051785\n",
      "train loss:0.375117218862047\n",
      "train loss:0.2907648803863999\n",
      "train loss:0.16898023504378457\n",
      "train loss:0.46698856726690624\n",
      "train loss:0.31428986735693454\n",
      "train loss:0.3108215238188812\n",
      "train loss:0.3390272646777786\n",
      "train loss:0.3126377079288291\n",
      "train loss:0.31466566626545744\n",
      "train loss:0.25566780432859143\n",
      "train loss:0.2906350031593385\n",
      "train loss:0.24504743656373734\n",
      "train loss:0.30339199336689693\n",
      "train loss:0.3218872386140674\n",
      "train loss:0.25075728358103555\n",
      "train loss:0.2941134322764348\n",
      "train loss:0.31953085409246923\n",
      "train loss:0.3846176186928888\n",
      "train loss:0.32474051205440935\n",
      "train loss:0.3108419734521596\n",
      "train loss:0.3063764835629988\n",
      "train loss:0.2401808324485143\n",
      "train loss:0.3815530054978513\n",
      "train loss:0.29512953270106435\n",
      "train loss:0.43870081222332075\n",
      "train loss:0.36068649962161786\n",
      "train loss:0.358325886039834\n",
      "train loss:0.3762667214008799\n",
      "train loss:0.25915915929859784\n",
      "train loss:0.33841211338430155\n",
      "train loss:0.3777321591732138\n",
      "train loss:0.28635974819363985\n",
      "train loss:0.3359088671238888\n",
      "train loss:0.3088517846774511\n",
      "train loss:0.3215180420970281\n",
      "train loss:0.25983130699485085\n",
      "train loss:0.30595601121463945\n",
      "train loss:0.29172571239664713\n",
      "train loss:0.2839244594553779\n",
      "train loss:0.4242129904543216\n",
      "train loss:0.33740240157593576\n",
      "train loss:0.19942750203129825\n",
      "train loss:0.30001552357844913\n",
      "train loss:0.36355444530604075\n",
      "train loss:0.3191521747015969\n",
      "train loss:0.2750519039377106\n",
      "train loss:0.39652503179331183\n",
      "train loss:0.3530181833153996\n",
      "train loss:0.3361481650701055\n",
      "train loss:0.42839381913491886\n",
      "train loss:0.300406903716889\n",
      "train loss:0.3388115772738133\n",
      "train loss:0.24790190923371733\n",
      "train loss:0.3227319227098872\n",
      "train loss:0.26202144016269663\n",
      "train loss:0.498605580060117\n",
      "train loss:0.18945635374309902\n",
      "train loss:0.28977651917192215\n",
      "train loss:0.5467851087517933\n",
      "train loss:0.3756911626057813\n",
      "train loss:0.3911427547954556\n",
      "train loss:0.3152868629918118\n",
      "train loss:0.2911753782067823\n",
      "train loss:0.3274868172753965\n",
      "train loss:0.16122556048360018\n",
      "train loss:0.29517523988317174\n",
      "train loss:0.1736886957009053\n",
      "train loss:0.3017367340396508\n",
      "train loss:0.19553371629641755\n",
      "train loss:0.2713795632267289\n",
      "train loss:0.24904639245575821\n",
      "train loss:0.37046029989332674\n",
      "train loss:0.24228399192436778\n",
      "train loss:0.38847657038744166\n",
      "train loss:0.2348008003893872\n",
      "train loss:0.3492809378712965\n",
      "train loss:0.2706016980290974\n",
      "train loss:0.23372780651093691\n",
      "train loss:0.2896646358844855\n",
      "train loss:0.5425763424897657\n",
      "train loss:0.4377366223899305\n",
      "train loss:0.2115728602455245\n",
      "train loss:0.29338350727112456\n",
      "train loss:0.35094074813179654\n",
      "train loss:0.23285858696350162\n",
      "train loss:0.27236933852880596\n",
      "train loss:0.2687660658922785\n",
      "train loss:0.30342364888045614\n",
      "train loss:0.25955637412849575\n",
      "train loss:0.3074223585506773\n",
      "train loss:0.3402842428549231\n",
      "train loss:0.31223079016999916\n",
      "train loss:0.2697541130139261\n",
      "train loss:0.3303113802445116\n",
      "train loss:0.35562170787793435\n",
      "train loss:0.29660215307451876\n",
      "train loss:0.32199775915259343\n",
      "train loss:0.32837049009621694\n",
      "train loss:0.27668307150613936\n",
      "train loss:0.33330252043485126\n",
      "train loss:0.36006628419436765\n",
      "train loss:0.28593859546975187\n",
      "train loss:0.34522910125174555\n",
      "train loss:0.38425737324049664\n",
      "train loss:0.3917586414882579\n",
      "train loss:0.2508479761214088\n",
      "train loss:0.3169324042328766\n",
      "train loss:0.23476643121705806\n",
      "train loss:0.3409800403277314\n",
      "train loss:0.3788107140445513\n",
      "train loss:0.34650502699734226\n",
      "train loss:0.33962111558049385\n",
      "train loss:0.2632233404577979\n",
      "train loss:0.4283958182395255\n",
      "train loss:0.2182133214089668\n",
      "train loss:0.26071556344742264\n",
      "train loss:0.28245421668244963\n",
      "train loss:0.29832437819376506\n",
      "train loss:0.3026897982418005\n",
      "train loss:0.38304698422585093\n",
      "train loss:0.3789207052971165\n",
      "train loss:0.18496668513129344\n",
      "train loss:0.309631688031668\n",
      "train loss:0.32512498437995624\n",
      "train loss:0.3969331970549899\n",
      "train loss:0.294815641118974\n",
      "train loss:0.38479793818675256\n",
      "train loss:0.32993674804678236\n",
      "train loss:0.34099866378482474\n",
      "train loss:0.3515769624532238\n",
      "train loss:0.3378046779464242\n",
      "train loss:0.2594581136146119\n",
      "train loss:0.3702242420836354\n",
      "train loss:0.25872795674728877\n",
      "train loss:0.43129175027532407\n",
      "train loss:0.30217188786212823\n",
      "train loss:0.31851808307189694\n",
      "train loss:0.40473193692104487\n",
      "train loss:0.2639074376517911\n",
      "train loss:0.277666124343923\n",
      "train loss:0.30582525317254433\n",
      "train loss:0.3620518977622336\n",
      "train loss:0.40280997759086673\n",
      "train loss:0.31602080438546337\n",
      "train loss:0.32846323869519545\n",
      "train loss:0.40106805126595707\n",
      "train loss:0.4271782318984348\n",
      "train loss:0.32821251824272407\n",
      "train loss:0.38879803434293814\n",
      "train loss:0.19081532984572608\n",
      "train loss:0.3423953398615083\n",
      "train loss:0.4674455410088101\n",
      "train loss:0.34121825801881217\n",
      "train loss:0.1869987842618569\n",
      "train loss:0.3713430848576772\n",
      "train loss:0.27160707869738354\n",
      "train loss:0.36276887517369816\n",
      "train loss:0.40277136320681195\n",
      "train loss:0.30446400776950266\n",
      "train loss:0.2884736625801715\n",
      "train loss:0.18460640949257928\n",
      "train loss:0.29011696374186846\n",
      "train loss:0.21739735060330911\n",
      "train loss:0.4078414419821334\n",
      "train loss:0.5314678891135608\n",
      "train loss:0.3402161957309964\n",
      "train loss:0.2641941079412744\n",
      "train loss:0.4017755538660768\n",
      "train loss:0.22360068467627134\n",
      "train loss:0.2970538245522161\n",
      "train loss:0.4901712405843392\n",
      "train loss:0.19033282239854465\n",
      "train loss:0.1564197939800837\n",
      "train loss:0.35187715521395796\n",
      "train loss:0.4893526949125181\n",
      "train loss:0.3735375105572143\n",
      "train loss:0.23036693738142922\n",
      "train loss:0.25289161668034044\n",
      "train loss:0.22440172668227157\n",
      "train loss:0.27934694204958893\n",
      "train loss:0.310607774410797\n",
      "train loss:0.32544633658638217\n",
      "train loss:0.3455470071840049\n",
      "train loss:0.505253306245417\n",
      "train loss:0.41001372238915523\n",
      "train loss:0.3704079688594982\n",
      "train loss:0.3160326112703702\n",
      "train loss:0.3986489419316359\n",
      "train loss:0.2336394783604897\n",
      "train loss:0.3044604324054551\n",
      "=== epoch:20, train acc:0.867, test acc:0.865 ===\n",
      "train loss:0.3396109510510974\n",
      "train loss:0.21961352108402538\n",
      "train loss:0.49994362217100385\n",
      "train loss:0.3326818778652126\n",
      "train loss:0.3338114759379097\n",
      "train loss:0.29323101955198355\n",
      "train loss:0.3084717469627266\n",
      "train loss:0.22649146605317938\n",
      "train loss:0.2347466323538325\n",
      "train loss:0.3385677106555892\n",
      "train loss:0.3699090546690105\n",
      "train loss:0.36153973744310014\n",
      "train loss:0.2974017717135393\n",
      "train loss:0.2666770462674256\n",
      "train loss:0.24458531586171922\n",
      "train loss:0.18612980274623073\n",
      "train loss:0.2140233567342618\n",
      "train loss:0.25691625402319695\n",
      "train loss:0.32536019103008873\n",
      "train loss:0.2592999922589441\n",
      "train loss:0.23669477734555855\n",
      "train loss:0.22659040484953688\n",
      "train loss:0.2209857000839697\n",
      "train loss:0.31335043768096094\n",
      "train loss:0.38222196436467726\n",
      "train loss:0.379043936587027\n",
      "train loss:0.20470794109271417\n",
      "train loss:0.34061397504746177\n",
      "train loss:0.2755575962829386\n",
      "train loss:0.20402571945633405\n",
      "train loss:0.24699070832013018\n",
      "train loss:0.24931672273529312\n",
      "train loss:0.341856140911005\n",
      "train loss:0.365049940971156\n",
      "train loss:0.32668449942688943\n",
      "train loss:0.29182093669984954\n",
      "train loss:0.2841935590689347\n",
      "train loss:0.3552959665199058\n",
      "train loss:0.24602675766349635\n",
      "train loss:0.26274826629956516\n",
      "train loss:0.28283804315399563\n",
      "train loss:0.27331459321685914\n",
      "train loss:0.32942550164298695\n",
      "train loss:0.2805759908782582\n",
      "train loss:0.2988796232689937\n",
      "train loss:0.2779639509347714\n",
      "train loss:0.3181305650500852\n",
      "train loss:0.40128156817818633\n",
      "train loss:0.2768478115866636\n",
      "train loss:0.29879469498534816\n",
      "train loss:0.27495829697594903\n",
      "train loss:0.2719504913871943\n",
      "train loss:0.28938862447418734\n",
      "train loss:0.2737014337046235\n",
      "train loss:0.35395556337923767\n",
      "train loss:0.41474272742408297\n",
      "train loss:0.37223737677605767\n",
      "train loss:0.40662811370385266\n",
      "train loss:0.31598499835097477\n",
      "train loss:0.42454765792806975\n",
      "train loss:0.3428475598698444\n",
      "train loss:0.4133497593567425\n",
      "train loss:0.4008349512748397\n",
      "train loss:0.20657372429156784\n",
      "train loss:0.2952696313223705\n",
      "train loss:0.3055069203695003\n",
      "train loss:0.3334827538793246\n",
      "train loss:0.3228358545119754\n",
      "train loss:0.32436612390014885\n",
      "train loss:0.24032118608807515\n",
      "train loss:0.4257864848075997\n",
      "train loss:0.27316620250268925\n",
      "train loss:0.2663324969010343\n",
      "train loss:0.2830502740456907\n",
      "train loss:0.27394835292438985\n",
      "train loss:0.19520058726636808\n",
      "train loss:0.20743302597542862\n",
      "train loss:0.29674499086582434\n",
      "train loss:0.3587323327922159\n",
      "train loss:0.34444469143494866\n",
      "train loss:0.29748685036047656\n",
      "train loss:0.2764189267901738\n",
      "train loss:0.3026465734984849\n",
      "train loss:0.28209723221027977\n",
      "train loss:0.3593549547547498\n",
      "train loss:0.3930658933044541\n",
      "train loss:0.25071147666733906\n",
      "train loss:0.36776337850503865\n",
      "train loss:0.22721636807381482\n",
      "train loss:0.2960456070872774\n",
      "train loss:0.3168517708966261\n",
      "train loss:0.2923665147483618\n",
      "train loss:0.20345664885111625\n",
      "train loss:0.3424790920977341\n",
      "train loss:0.30042766925537906\n",
      "train loss:0.39254156247857785\n",
      "train loss:0.2728341279782924\n",
      "train loss:0.19816107590226742\n",
      "train loss:0.2561743893921473\n",
      "train loss:0.39682292000286634\n",
      "train loss:0.4086963741206124\n",
      "train loss:0.21716547043978637\n",
      "train loss:0.31842020327679144\n",
      "train loss:0.2475096774059622\n",
      "train loss:0.3847949829247515\n",
      "train loss:0.18982411185042478\n",
      "train loss:0.2828057004325582\n",
      "train loss:0.32307078723899124\n",
      "train loss:0.337027425226385\n",
      "train loss:0.386676703006663\n",
      "train loss:0.30290558578780374\n",
      "train loss:0.23363790945353174\n",
      "train loss:0.2602067803013972\n",
      "train loss:0.28642401158650893\n",
      "train loss:0.2177491733255196\n",
      "train loss:0.20771987536412365\n",
      "train loss:0.3101959545918489\n",
      "train loss:0.25373502967984557\n",
      "train loss:0.2385376830912954\n",
      "train loss:0.34307377695585445\n",
      "train loss:0.293697541248907\n",
      "train loss:0.38658129401543495\n",
      "train loss:0.4571505149794312\n",
      "train loss:0.2412905774484055\n",
      "train loss:0.33230555998229017\n",
      "train loss:0.4888611754892166\n",
      "train loss:0.4033985228674735\n",
      "train loss:0.31816133684253706\n",
      "train loss:0.2741881241486001\n",
      "train loss:0.31972385902953454\n",
      "train loss:0.16132779256225177\n",
      "train loss:0.36739064426103674\n",
      "train loss:0.28877500294396563\n",
      "train loss:0.39172508333585754\n",
      "train loss:0.28445318327461805\n",
      "train loss:0.35417006688403174\n",
      "train loss:0.3843878498972975\n",
      "train loss:0.2865771831385438\n",
      "train loss:0.3380086117408077\n",
      "train loss:0.28565324656858326\n",
      "train loss:0.329027701062198\n",
      "train loss:0.31775367703636526\n",
      "train loss:0.22857773676833976\n",
      "train loss:0.43577288749022997\n",
      "train loss:0.263401988455463\n",
      "train loss:0.19918668614463184\n",
      "train loss:0.27717874752060206\n",
      "train loss:0.27965116689673253\n",
      "train loss:0.28898548526589257\n",
      "train loss:0.3444378277876872\n",
      "train loss:0.25862562425613433\n",
      "train loss:0.27581729094317525\n",
      "train loss:0.37136021033228234\n",
      "train loss:0.2795741311514784\n",
      "train loss:0.28093362585674847\n",
      "train loss:0.4490664564476038\n",
      "train loss:0.3133986900275858\n",
      "train loss:0.2271137420292473\n",
      "train loss:0.21755742354330265\n",
      "train loss:0.34738541253430333\n",
      "train loss:0.2822934902096297\n",
      "train loss:0.2144544960046342\n",
      "train loss:0.2638117737660113\n",
      "train loss:0.24953389467007747\n",
      "train loss:0.5736755955436814\n",
      "train loss:0.385327712463619\n",
      "train loss:0.3089462787715058\n",
      "train loss:0.31893486637475277\n",
      "train loss:0.3043931832492859\n",
      "train loss:0.3745108905575705\n",
      "train loss:0.3942889819608208\n",
      "train loss:0.24391372279598747\n",
      "train loss:0.39568577945537425\n",
      "train loss:0.39661980293081345\n",
      "train loss:0.4261329329651155\n",
      "train loss:0.4371587525778051\n",
      "train loss:0.2774916251797791\n",
      "train loss:0.30207585164388395\n",
      "train loss:0.27602746321376004\n",
      "train loss:0.42765868795403683\n",
      "train loss:0.24509891823430377\n",
      "train loss:0.3034240598482934\n",
      "train loss:0.2951662595672025\n",
      "train loss:0.408613326130658\n",
      "train loss:0.2857678912859463\n",
      "train loss:0.28107905499871844\n",
      "train loss:0.23069441238509492\n",
      "train loss:0.27362907713765267\n",
      "train loss:0.23796747974977367\n",
      "train loss:0.27650077666973016\n",
      "train loss:0.2218954871329324\n",
      "train loss:0.23603959262422294\n",
      "train loss:0.33168130495649784\n",
      "train loss:0.2414313500353565\n",
      "train loss:0.339160783656492\n",
      "train loss:0.3927549240737897\n",
      "train loss:0.319570938714451\n",
      "train loss:0.40182351569440933\n",
      "train loss:0.40474007928556427\n",
      "train loss:0.3960009148155406\n",
      "train loss:0.2719028236022461\n",
      "train loss:0.35024279373433026\n",
      "train loss:0.26947358611045813\n",
      "train loss:0.3222495187315937\n",
      "train loss:0.31771950529796483\n",
      "train loss:0.24200122843242777\n",
      "train loss:0.2351636738709712\n",
      "train loss:0.3411707684694174\n",
      "train loss:0.28065084650173866\n",
      "train loss:0.2813155517161195\n",
      "train loss:0.24043773273479807\n",
      "train loss:0.32422545895650445\n",
      "train loss:0.2454797256951177\n",
      "train loss:0.2959143130268201\n",
      "train loss:0.3065858929478605\n",
      "train loss:0.45642147102509595\n",
      "train loss:0.3229289149331798\n",
      "train loss:0.21050687035024068\n",
      "train loss:0.3003422817789008\n",
      "train loss:0.2591415681567474\n",
      "train loss:0.3304987262156164\n",
      "train loss:0.25468542258951027\n",
      "train loss:0.2801838186825133\n",
      "train loss:0.3152873768584418\n",
      "train loss:0.34174987800272577\n",
      "train loss:0.40405598210561605\n",
      "train loss:0.34717598184538767\n",
      "train loss:0.3333248529025403\n",
      "train loss:0.29902950845760257\n",
      "train loss:0.2176254014374711\n",
      "train loss:0.4255050045100292\n",
      "train loss:0.2999228946947113\n",
      "train loss:0.31159749475208454\n",
      "train loss:0.17044422991199812\n",
      "train loss:0.21265445750565917\n",
      "train loss:0.3618688102947904\n",
      "train loss:0.25909924935687967\n",
      "train loss:0.27445985724570787\n",
      "train loss:0.28120658662904957\n",
      "train loss:0.27594749151236364\n",
      "train loss:0.23548797852982403\n",
      "train loss:0.27125490035426786\n",
      "train loss:0.3562491021645272\n",
      "train loss:0.222088536883828\n",
      "train loss:0.2210293419812162\n",
      "train loss:0.2631972494267121\n",
      "train loss:0.28290117647380375\n",
      "train loss:0.266786640978\n",
      "train loss:0.22849431966937406\n",
      "train loss:0.48100377332386307\n",
      "train loss:0.20771348256412037\n",
      "train loss:0.3223318570604107\n",
      "train loss:0.3590906858216257\n",
      "train loss:0.4173350748774653\n",
      "train loss:0.3086095546654985\n",
      "train loss:0.263033987301425\n",
      "train loss:0.29794593633413624\n",
      "train loss:0.31530692700996005\n",
      "train loss:0.2880595502652714\n",
      "train loss:0.4028716822018907\n",
      "train loss:0.2736749377374458\n",
      "train loss:0.1577638585138594\n",
      "train loss:0.2737664997460049\n",
      "train loss:0.29218018135852863\n",
      "train loss:0.24824977499433787\n",
      "train loss:0.3832004768497555\n",
      "train loss:0.3537356306717481\n",
      "train loss:0.28772318256116686\n",
      "train loss:0.21872462130710443\n",
      "train loss:0.3281202197335016\n",
      "train loss:0.2705621201326504\n",
      "train loss:0.20399666205706635\n",
      "train loss:0.4013405360886918\n",
      "train loss:0.4837720708883657\n",
      "train loss:0.37844459977266326\n",
      "train loss:0.26597769929576154\n",
      "train loss:0.27050016205549055\n",
      "train loss:0.3112466328740572\n",
      "train loss:0.2757577000146134\n",
      "train loss:0.4108076770265903\n",
      "train loss:0.2997331354311861\n",
      "train loss:0.2311680682956741\n",
      "train loss:0.44756024027306773\n",
      "train loss:0.2532949091757737\n",
      "train loss:0.3694109044597323\n",
      "train loss:0.27699324608883047\n",
      "train loss:0.2089913020264742\n",
      "train loss:0.373817858549589\n",
      "train loss:0.24209563643231707\n",
      "train loss:0.3453887431239078\n",
      "train loss:0.30576920988015655\n",
      "train loss:0.2967838141909344\n",
      "train loss:0.3108866709676328\n",
      "train loss:0.2717115001622652\n",
      "train loss:0.29629514357631825\n",
      "train loss:0.23743733288950541\n",
      "train loss:0.2026472523000879\n",
      "train loss:0.27164639219872216\n",
      "train loss:0.4031832995004827\n",
      "train loss:0.25153721160816983\n",
      "train loss:0.42584914745771907\n",
      "train loss:0.2527193909328137\n",
      "train loss:0.36709400516404406\n",
      "train loss:0.43452788074891247\n",
      "train loss:0.32994203248404735\n",
      "train loss:0.27708776201925817\n",
      "train loss:0.40900258536087347\n",
      "train loss:0.18320955904981798\n",
      "train loss:0.3269572263905877\n",
      "train loss:0.2245837274343885\n",
      "train loss:0.21340247210566482\n",
      "train loss:0.2845755895356841\n",
      "train loss:0.45289829645221735\n",
      "train loss:0.38057121803758465\n",
      "train loss:0.2794959045125045\n",
      "train loss:0.39473764552702184\n",
      "train loss:0.2400192000703874\n",
      "train loss:0.32400453911356764\n",
      "train loss:0.41463248362933575\n",
      "train loss:0.20941254350601043\n",
      "train loss:0.308864156004484\n",
      "train loss:0.28144587898289763\n",
      "train loss:0.311060868557227\n",
      "train loss:0.2445741411697825\n",
      "train loss:0.19416155308661193\n",
      "train loss:0.2659105090382383\n",
      "train loss:0.18658852163264986\n",
      "train loss:0.3771742966941462\n",
      "train loss:0.40414490343151094\n",
      "train loss:0.26184770700219495\n",
      "train loss:0.3675343932335889\n",
      "train loss:0.2771964452112892\n",
      "train loss:0.2132849853312687\n",
      "train loss:0.29141308705352803\n",
      "train loss:0.21537441481754813\n",
      "train loss:0.34699316605210145\n",
      "train loss:0.29355530708727234\n",
      "train loss:0.2978498144324359\n",
      "train loss:0.21320136781429305\n",
      "train loss:0.3397143397187253\n",
      "train loss:0.32796131812768453\n",
      "train loss:0.32161254971579617\n",
      "train loss:0.28549666497328824\n",
      "train loss:0.3497096092154387\n",
      "train loss:0.33673935536382893\n",
      "train loss:0.22462253825923115\n",
      "train loss:0.2495256526892031\n",
      "train loss:0.2564568056211711\n",
      "train loss:0.2931449383952167\n",
      "train loss:0.30926040871317445\n",
      "train loss:0.32187747198508254\n",
      "train loss:0.39526759356643504\n",
      "train loss:0.24496459686538585\n",
      "train loss:0.20832421730366008\n",
      "train loss:0.2616810124042624\n",
      "train loss:0.4232536936028892\n",
      "train loss:0.333652628133668\n",
      "train loss:0.2594144601401938\n",
      "train loss:0.308771754738742\n",
      "train loss:0.4737476059415578\n",
      "train loss:0.27631288062022213\n",
      "train loss:0.2882213625832163\n",
      "train loss:0.36208780372871274\n",
      "train loss:0.37986873117467385\n",
      "train loss:0.28029276907773526\n",
      "train loss:0.4284699362479104\n",
      "train loss:0.33919487765242096\n",
      "train loss:0.185516814023926\n",
      "train loss:0.4020280135623102\n",
      "train loss:0.24827493237986908\n",
      "train loss:0.41327416803901396\n",
      "train loss:0.24894364263291305\n",
      "train loss:0.2583919471288479\n",
      "train loss:0.29915859999231276\n",
      "train loss:0.27745966005155986\n",
      "train loss:0.3578886001382254\n",
      "train loss:0.30427938618186323\n",
      "train loss:0.2662160303763547\n",
      "train loss:0.3316772453647442\n",
      "train loss:0.348104251888254\n",
      "train loss:0.2781112158596566\n",
      "train loss:0.20043907167331942\n",
      "train loss:0.3544833226617376\n",
      "train loss:0.24618184873882976\n",
      "train loss:0.3374288289881957\n",
      "train loss:0.3216444022459006\n",
      "train loss:0.6371797234113858\n",
      "train loss:0.29769475023408226\n",
      "train loss:0.30758368923451057\n",
      "train loss:0.27068185450927773\n",
      "train loss:0.2590033301001629\n",
      "train loss:0.3428204629697562\n",
      "train loss:0.37272817537500147\n",
      "train loss:0.2816032867560695\n",
      "train loss:0.2789114465218583\n",
      "train loss:0.3470968177683823\n",
      "train loss:0.28054545659760205\n",
      "train loss:0.17571032284623483\n",
      "train loss:0.38354481527589485\n",
      "train loss:0.2819920531531154\n",
      "train loss:0.24027367990439527\n",
      "train loss:0.20239077418219506\n",
      "train loss:0.28042766938846797\n",
      "train loss:0.2527314417292158\n",
      "train loss:0.23759911233562633\n",
      "train loss:0.23030288680962027\n",
      "train loss:0.31631801076245447\n",
      "train loss:0.3996200564809863\n",
      "train loss:0.31505190407652894\n",
      "train loss:0.239856925518128\n",
      "train loss:0.2655915395111093\n",
      "train loss:0.3136148715533122\n",
      "train loss:0.24229506490442446\n",
      "train loss:0.368406824257347\n",
      "train loss:0.2102239040227558\n",
      "train loss:0.3646049244480676\n",
      "train loss:0.379768295824422\n",
      "train loss:0.23069179872089382\n",
      "train loss:0.3796793149587656\n",
      "train loss:0.3393725271192697\n",
      "train loss:0.25617610453221074\n",
      "train loss:0.47063192500972095\n",
      "train loss:0.43186839636805874\n",
      "train loss:0.20160323201081945\n",
      "train loss:0.23787652711974183\n",
      "train loss:0.2786962581820211\n",
      "train loss:0.42423227656237894\n",
      "train loss:0.237898673139734\n",
      "train loss:0.2812944324879166\n",
      "train loss:0.2837533220598412\n",
      "train loss:0.24281770626035595\n",
      "train loss:0.3436085080106115\n",
      "train loss:0.17708531266380603\n",
      "train loss:0.20299445010608014\n",
      "train loss:0.36647600852592527\n",
      "train loss:0.26750973241013654\n",
      "train loss:0.2929676614599914\n",
      "train loss:0.35251510302875533\n",
      "train loss:0.39021235064495197\n",
      "train loss:0.204907788021288\n",
      "train loss:0.3256089617315163\n",
      "train loss:0.24300001472649735\n",
      "train loss:0.18387958111122188\n",
      "train loss:0.38291626286729685\n",
      "train loss:0.25009173094712556\n",
      "train loss:0.540723701247876\n",
      "train loss:0.27119079939821195\n",
      "train loss:0.16701017775555588\n",
      "train loss:0.3727444815337908\n",
      "train loss:0.24496516424197942\n",
      "train loss:0.2515769433925943\n",
      "train loss:0.3780639673284415\n",
      "train loss:0.2572836643011186\n",
      "train loss:0.3070032208709449\n",
      "train loss:0.35427230632201345\n",
      "train loss:0.3613599283016239\n",
      "train loss:0.31399766024637876\n",
      "train loss:0.27157962040121353\n",
      "train loss:0.2817757177102495\n",
      "train loss:0.38542622163531787\n",
      "train loss:0.3207758510334675\n",
      "train loss:0.32054220101265907\n",
      "train loss:0.26124771321635604\n",
      "train loss:0.22041300317094753\n",
      "train loss:0.3272111497048358\n",
      "train loss:0.28445966553267293\n",
      "train loss:0.25380472436359414\n",
      "train loss:0.21276444200207348\n",
      "train loss:0.32100436509528324\n",
      "train loss:0.2746128281709287\n",
      "train loss:0.2884015263822092\n",
      "train loss:0.31825401878299947\n",
      "train loss:0.3108805386701356\n",
      "train loss:0.3025301939241862\n",
      "train loss:0.19007018764251551\n",
      "train loss:0.2662956014835749\n",
      "train loss:0.30072995936022695\n",
      "train loss:0.276822235947946\n",
      "train loss:0.23536238024942108\n",
      "train loss:0.5053966356714925\n",
      "train loss:0.2740063842494908\n",
      "train loss:0.24060421092403173\n",
      "train loss:0.3208861006727346\n",
      "train loss:0.30629588369777355\n",
      "train loss:0.3462856241381096\n",
      "train loss:0.1939332059263266\n",
      "train loss:0.2781347731502709\n",
      "train loss:0.18609528059173336\n",
      "train loss:0.5360327205847646\n",
      "train loss:0.19791144759098064\n",
      "train loss:0.16048848571909777\n",
      "train loss:0.34588877658248923\n",
      "train loss:0.24813820093804892\n",
      "train loss:0.3401419134597752\n",
      "train loss:0.2832789700675012\n",
      "train loss:0.377035997783121\n",
      "train loss:0.3752953525629198\n",
      "train loss:0.14920251630755177\n",
      "train loss:0.2303203355954459\n",
      "train loss:0.22999608583275977\n",
      "train loss:0.4735762298050528\n",
      "train loss:0.23433448315077995\n",
      "train loss:0.26437544060068396\n",
      "train loss:0.21658014366521564\n",
      "train loss:0.18309718180780743\n",
      "train loss:0.2683061099394466\n",
      "train loss:0.2141343773798008\n",
      "train loss:0.4149499360377004\n",
      "train loss:0.3210919494107349\n",
      "train loss:0.3803858408938822\n",
      "train loss:0.3353394668126898\n",
      "train loss:0.31245977974870764\n",
      "train loss:0.2377919641298149\n",
      "train loss:0.26797011175669144\n",
      "train loss:0.26281339969117296\n",
      "train loss:0.2243491867030214\n",
      "train loss:0.34477895428736766\n",
      "train loss:0.15830887210213473\n",
      "train loss:0.23469803650099982\n",
      "train loss:0.3567194918202563\n",
      "train loss:0.2497575136220177\n",
      "train loss:0.27904810187876594\n",
      "train loss:0.22173624370149686\n",
      "train loss:0.239077460055451\n",
      "train loss:0.20888996357257408\n",
      "train loss:0.19563426810093124\n",
      "train loss:0.2809770717787519\n",
      "train loss:0.23007578168614354\n",
      "train loss:0.3232269708068467\n",
      "train loss:0.25988586387087514\n",
      "train loss:0.30734162975885726\n",
      "train loss:0.21021160817014775\n",
      "train loss:0.24946742574750377\n",
      "train loss:0.27254819227579963\n",
      "train loss:0.2519293678129962\n",
      "train loss:0.22969867345191333\n",
      "train loss:0.17630798906678385\n",
      "train loss:0.264637428544436\n",
      "train loss:0.22449591287372736\n",
      "train loss:0.22501058908050253\n",
      "train loss:0.35110354956567447\n",
      "train loss:0.36818380049762134\n",
      "train loss:0.2526579299505053\n",
      "train loss:0.39695143618821954\n",
      "train loss:0.3273530844433809\n",
      "train loss:0.1553877116109213\n",
      "train loss:0.33648158069851847\n",
      "train loss:0.3367649185312535\n",
      "train loss:0.24612354014440818\n",
      "train loss:0.24570112412694492\n",
      "train loss:0.33024485617258476\n",
      "train loss:0.23463381735921396\n",
      "train loss:0.28407941502805706\n",
      "train loss:0.20581701604986175\n",
      "train loss:0.22188918785953227\n",
      "train loss:0.26250864315700556\n",
      "train loss:0.28428909037817596\n",
      "train loss:0.37053645723966816\n",
      "train loss:0.28309529027194547\n",
      "train loss:0.21725975878803375\n",
      "train loss:0.23144391245307333\n",
      "train loss:0.36815775436915815\n",
      "train loss:0.31346386878241544\n",
      "train loss:0.2666113631483211\n",
      "train loss:0.25453220055207165\n",
      "train loss:0.2901952419574531\n",
      "train loss:0.40325038384747386\n",
      "train loss:0.3030679500955031\n",
      "train loss:0.24839184543540896\n",
      "train loss:0.34872043572667116\n",
      "train loss:0.30011578599146993\n",
      "train loss:0.21673490489440944\n",
      "train loss:0.35331038157999584\n",
      "train loss:0.3224155710946653\n",
      "train loss:0.33952769320346027\n",
      "train loss:0.30422201882136385\n",
      "train loss:0.251967076802782\n",
      "train loss:0.18733976948976783\n",
      "train loss:0.12121617910080683\n",
      "train loss:0.26150310031722745\n",
      "train loss:0.2096778739717365\n",
      "train loss:0.15784570312194113\n",
      "train loss:0.22048695286696623\n",
      "train loss:0.2595814024578625\n",
      "train loss:0.33167758647400364\n",
      "train loss:0.2105309143129721\n",
      "train loss:0.26794972742493117\n",
      "train loss:0.40334474900215794\n",
      "train loss:0.18099277180621826\n",
      "train loss:0.28716151281673696\n",
      "train loss:0.24251623319405433\n",
      "train loss:0.21886324105962413\n",
      "train loss:0.23894982718954158\n",
      "train loss:0.24455759034005692\n",
      "train loss:0.2997913632989271\n",
      "train loss:0.2634170136380283\n",
      "train loss:0.25929010700321553\n",
      "train loss:0.3274030272144674\n",
      "train loss:0.15923464916265598\n",
      "걸린 시간 : 1808.1587181091309\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.9262\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 60000개 data\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100, 100], output_size=10,\n",
    "                             activation='relu', weight_init_std='relu', use_dropout=True, dropout_ration = 0.5)  \n",
    "# affine-relu-dropout -affine-relu-dropout -affine-relu-dropout -affine-relu-dropout -affine-relu-dropout \n",
    "# -affine-relu-dropout -affine-relu-dropout -affine-softmax\n",
    "\n",
    "trainer_2 = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=20, mini_batch_size=100, # 60000개 데이터 // 100 = 600개 * 20번\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000) # 총 12000번!!\n",
    "\n",
    "trainer_2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "466d1f00-f1a0-4041-8324-91e3c97fbbd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.867"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_2.train_acc_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "269c551a-eb8a-4512-a3d3-5d3034cb000f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15923464916265598\n",
      "0.797746090363644\n"
     ]
    }
   ],
   "source": [
    "print(trainer_2.train_loss_list[-1])\n",
    "print(trainer.train_loss_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2209e0-bc73-4102-8d57-e0748ba55883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from deep_convnet import DeepConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = DeepConvNet() \n",
    "# conv-relu-conv-relu-pool -conv-relu-conv-relu-pool -conv-relu\n",
    "# -conv-relu-pool -affine-relu-dropout -affine-dropout-softmax\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=20, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보관\n",
    "network.save_params(\"deep_convnet_params.pkl\")\n",
    "print(\"Saved Network Parameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7bd48a1-71d1-4b52-9b31-cd2d2575f470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train_acc_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "93fb745a-0c53-4ef7-b7bb-fad0ef13ace5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600.0 12000\n"
     ]
    }
   ],
   "source": [
    "iter_per_epoch = max(60000 / 100, 1)\n",
    "max_iter = int(20 * iter_per_epoch)\n",
    "print(iter_per_epoch, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31d923cb-9b72-426c-8730-cf5e71639273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 비교\n",
    "x = np.arange(trainer_2.epochs)\n",
    "plt.plot(x, trainer_2.train_acc_list, marker='o', label='multilayernet')\n",
    "plt.plot(x, trainer.train_acc_list, marker='s', label='deepconvnet')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.xticks(np.arange(0, trainer_2.epochs, 5))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72ee4f74-63b4-48c2-8001-6314e487b669",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.299285690332038\n",
      "=== epoch:1, train acc:0.133, test acc:0.132 ===\n",
      "train loss:2.2933920788870714\n",
      "train loss:2.28933525644492\n",
      "train loss:2.283067904376638\n",
      "train loss:2.2812689226816545\n",
      "train loss:2.2529173027038274\n",
      "train loss:2.2366377450650448\n",
      "train loss:2.229158572445792\n",
      "train loss:2.244634955777349\n",
      "train loss:2.201370311795443\n",
      "train loss:2.163568804766224\n",
      "train loss:2.113296558797133\n",
      "train loss:2.104122826983899\n",
      "train loss:2.0788443853050413\n",
      "train loss:2.0285435026402463\n",
      "train loss:1.9368165558069261\n",
      "train loss:1.8829865078216743\n",
      "train loss:1.7841518457548258\n",
      "train loss:1.759290652691451\n",
      "train loss:1.741940823078839\n",
      "train loss:1.6344926112225406\n",
      "train loss:1.6844649571606767\n",
      "train loss:1.481083264372806\n",
      "train loss:1.43135383991817\n",
      "train loss:1.257742758831012\n",
      "train loss:1.2362828881385772\n",
      "train loss:1.1894019906936801\n",
      "train loss:1.043204194926063\n",
      "train loss:1.1242415968263413\n",
      "train loss:1.1239321024852027\n",
      "train loss:0.9599443799482696\n",
      "train loss:0.8809503914761687\n",
      "train loss:0.9163548079931247\n",
      "train loss:0.834036172503029\n",
      "train loss:0.7827437283648235\n",
      "train loss:0.6565064440402427\n",
      "train loss:0.7320792013922454\n",
      "train loss:0.7025958994956705\n",
      "train loss:0.5272225125751356\n",
      "train loss:0.7140486614938935\n",
      "train loss:0.7562188845288835\n",
      "train loss:0.7382845883638648\n",
      "train loss:0.6805530864999045\n",
      "train loss:0.5944152078071836\n",
      "train loss:0.7325791404495816\n",
      "train loss:0.6725923513409445\n",
      "train loss:0.4710903806676063\n",
      "train loss:0.5065839347026199\n",
      "train loss:0.513877680203179\n",
      "train loss:0.5100832506663162\n",
      "train loss:0.6630261454972937\n",
      "train loss:0.6087404258306903\n",
      "train loss:0.4862420735180528\n",
      "train loss:0.564470301791782\n",
      "train loss:0.43060715758450413\n",
      "train loss:0.48458137196829204\n",
      "train loss:0.4677434656968228\n",
      "train loss:0.5802006967208518\n",
      "train loss:0.3777162962898331\n",
      "train loss:0.5716006370082254\n",
      "train loss:0.4745960814910701\n",
      "train loss:0.5120677617963748\n",
      "train loss:0.37168190263983275\n",
      "train loss:0.5004329642514544\n",
      "train loss:0.39680893257875044\n",
      "train loss:0.3945572615670527\n",
      "train loss:0.570432985183736\n",
      "train loss:0.4038096115087674\n",
      "train loss:0.4469308427513557\n",
      "train loss:0.41028447520450256\n",
      "train loss:0.340872142185711\n",
      "train loss:0.45780185034761245\n",
      "train loss:0.4786431872309543\n",
      "train loss:0.6465095692954338\n",
      "train loss:0.46292776467771235\n",
      "train loss:0.5494733725723914\n",
      "train loss:0.5655622521840984\n",
      "train loss:0.42550957362365927\n",
      "train loss:0.4596980173676044\n",
      "train loss:0.5132438624479393\n",
      "train loss:0.4264294661980413\n",
      "train loss:0.41150767300647745\n",
      "train loss:0.46067085348639\n",
      "train loss:0.38009949230852386\n",
      "train loss:0.4772216051957098\n",
      "train loss:0.28202126956643214\n",
      "train loss:0.4005182441068773\n",
      "train loss:0.47145444238183737\n",
      "train loss:0.4220545775348337\n",
      "train loss:0.42854805948301583\n",
      "train loss:0.6726926594374397\n",
      "train loss:0.4749802638200223\n",
      "train loss:0.4578961613222662\n",
      "train loss:0.411535257490646\n",
      "train loss:0.498605085208348\n",
      "train loss:0.5328591160218111\n",
      "train loss:0.5413688583458767\n",
      "train loss:0.37330593374356114\n",
      "train loss:0.37316449561224085\n",
      "train loss:0.5379779279099828\n",
      "train loss:0.30177095658100883\n",
      "train loss:0.3320008823753948\n",
      "train loss:0.6641737080645559\n",
      "train loss:0.5597213475570453\n",
      "train loss:0.37907132225037493\n",
      "train loss:0.39687062039691995\n",
      "train loss:0.4158323905015442\n",
      "train loss:0.5283832181084954\n",
      "train loss:0.44975416715309124\n",
      "train loss:0.31795480167260776\n",
      "train loss:0.41586369212300495\n",
      "train loss:0.48584633700405705\n",
      "train loss:0.4393072287209228\n",
      "train loss:0.3612526764389219\n",
      "train loss:0.327366448097261\n",
      "train loss:0.39073660818411854\n",
      "train loss:0.3832191404598141\n",
      "train loss:0.49378496642500286\n",
      "train loss:0.38759074009923866\n",
      "train loss:0.40852440181364275\n",
      "train loss:0.36793113314348114\n",
      "train loss:0.4481843865043721\n",
      "train loss:0.5271332453819915\n",
      "train loss:0.42050692484118607\n",
      "train loss:0.3523364671305357\n",
      "train loss:0.3928585654427309\n",
      "train loss:0.34856942953347997\n",
      "train loss:0.3411857674243455\n",
      "train loss:0.2862889585697956\n",
      "train loss:0.3807544289351953\n",
      "train loss:0.29750308398724146\n",
      "train loss:0.28976562310209575\n",
      "train loss:0.566578191540973\n",
      "train loss:0.24479704571738106\n",
      "train loss:0.5232219352546802\n",
      "train loss:0.18227819709666016\n",
      "train loss:0.3988100436017236\n",
      "train loss:0.3293646673144608\n",
      "train loss:0.24060366981284048\n",
      "train loss:0.35738653269557885\n",
      "train loss:0.4859949386064171\n",
      "train loss:0.2651150746965246\n",
      "train loss:0.4100436338969144\n",
      "train loss:0.45076778031124426\n",
      "train loss:0.30622564143180686\n",
      "train loss:0.3884448394331635\n",
      "train loss:0.40897706706445996\n",
      "train loss:0.3323155217832619\n",
      "train loss:0.21381475442790093\n",
      "train loss:0.5378437569923153\n",
      "train loss:0.25377841386706085\n",
      "train loss:0.2648433187632809\n",
      "train loss:0.23001350137310642\n",
      "train loss:0.34036415095002404\n",
      "train loss:0.42313879141904365\n",
      "train loss:0.23771842944896185\n",
      "train loss:0.22360404868234723\n",
      "train loss:0.23840004316767907\n",
      "train loss:0.5808977575963074\n",
      "train loss:0.27106051562650796\n",
      "train loss:0.2911961096014297\n",
      "train loss:0.2446300154887537\n",
      "train loss:0.2136257594770526\n",
      "train loss:0.36909453066252085\n",
      "train loss:0.4309980769746266\n",
      "train loss:0.19864710145356823\n",
      "train loss:0.3596037909672986\n",
      "train loss:0.2978088657726219\n",
      "train loss:0.29845640535695084\n",
      "train loss:0.1842717996655017\n",
      "train loss:0.33258651753635954\n",
      "train loss:0.23560061842369634\n",
      "train loss:0.4240156629808734\n",
      "train loss:0.46073329247953737\n",
      "train loss:0.3684248182249013\n",
      "train loss:0.25439835928630017\n",
      "train loss:0.31696987535891996\n",
      "train loss:0.4859684456828947\n",
      "train loss:0.41098941235665193\n",
      "train loss:0.20971757301198188\n",
      "train loss:0.3253218795056108\n",
      "train loss:0.35160609968147155\n",
      "train loss:0.2745642167696657\n",
      "train loss:0.4332070878768011\n",
      "train loss:0.25299631128097105\n",
      "train loss:0.22519549167955163\n",
      "train loss:0.2922122329485462\n",
      "train loss:0.3569061875271304\n",
      "train loss:0.2968417186420996\n",
      "train loss:0.21661306352054605\n",
      "train loss:0.3538345110979592\n",
      "train loss:0.32908421625691875\n",
      "train loss:0.3085874112031015\n",
      "train loss:0.24309223806293137\n",
      "train loss:0.24088318697402247\n",
      "train loss:0.224782924990691\n",
      "train loss:0.22518914878042007\n",
      "train loss:0.3959171170903474\n",
      "train loss:0.30539248522877954\n",
      "train loss:0.2338184519695029\n",
      "train loss:0.4097446253895572\n",
      "train loss:0.40812134473364475\n",
      "train loss:0.21279360438400854\n",
      "train loss:0.3071808652869821\n",
      "train loss:0.2748199550825429\n",
      "train loss:0.2491109581130535\n",
      "train loss:0.30148773119683453\n",
      "train loss:0.2795824549961861\n",
      "train loss:0.2848185842860378\n",
      "train loss:0.19968868292656913\n",
      "train loss:0.41460528887792397\n",
      "train loss:0.2978954165293759\n",
      "train loss:0.47487209242412426\n",
      "train loss:0.20985410458158937\n",
      "train loss:0.2701784011272511\n",
      "train loss:0.2459790592954246\n",
      "train loss:0.19054975102533636\n",
      "train loss:0.30227468067374746\n",
      "train loss:0.29309335003094655\n",
      "train loss:0.39652119288846327\n",
      "train loss:0.3024875822914572\n",
      "train loss:0.445225844432238\n",
      "train loss:0.38983488398056226\n",
      "train loss:0.3039120921401586\n",
      "train loss:0.20727122656179414\n",
      "train loss:0.28760503455887976\n",
      "train loss:0.19735085244636102\n",
      "train loss:0.1636554260406632\n",
      "train loss:0.2406958245146093\n",
      "train loss:0.31222612401550603\n",
      "train loss:0.30221929607382714\n",
      "train loss:0.439012358824285\n",
      "train loss:0.1643049416019717\n",
      "train loss:0.24573582786042994\n",
      "train loss:0.22455557920566274\n",
      "train loss:0.307651614061507\n",
      "train loss:0.1978006838879841\n",
      "train loss:0.23477514213929965\n",
      "train loss:0.369117925202241\n",
      "train loss:0.36680877849045657\n",
      "train loss:0.22694825924359965\n",
      "train loss:0.2478594977340736\n",
      "train loss:0.2699008142769014\n",
      "train loss:0.2718833880426853\n",
      "train loss:0.34005831611911785\n",
      "train loss:0.2679199618279955\n",
      "train loss:0.21589078217377544\n",
      "train loss:0.34349357810653314\n",
      "train loss:0.2852410421114654\n",
      "train loss:0.19000939516267912\n",
      "train loss:0.26431220454305854\n",
      "train loss:0.38639258195910275\n",
      "train loss:0.18070724306246805\n",
      "train loss:0.2658288226955579\n",
      "train loss:0.36166979447953346\n",
      "train loss:0.2257416049497163\n",
      "train loss:0.21953918907833536\n",
      "train loss:0.2911542178733383\n",
      "train loss:0.17085676229306831\n",
      "train loss:0.1549137957877605\n",
      "train loss:0.29605550402018616\n",
      "train loss:0.23602515764332427\n",
      "train loss:0.32555516813920127\n",
      "train loss:0.2773821295147425\n",
      "train loss:0.2354537770109395\n",
      "train loss:0.2824787665465139\n",
      "train loss:0.15884118902876215\n",
      "train loss:0.4478093035001612\n",
      "train loss:0.16427611901166347\n",
      "train loss:0.15744402268401284\n",
      "train loss:0.30828132621568005\n",
      "train loss:0.1651233276390338\n",
      "train loss:0.36311035983988793\n",
      "train loss:0.29436166414515663\n",
      "train loss:0.1233950483719303\n",
      "train loss:0.25965703749736674\n",
      "train loss:0.23798858673156015\n",
      "train loss:0.196760913086518\n",
      "train loss:0.1898659364009912\n",
      "train loss:0.22284852229169114\n",
      "train loss:0.211537313474917\n",
      "train loss:0.20260256450343903\n",
      "train loss:0.19432886107379815\n",
      "train loss:0.2689993788595134\n",
      "train loss:0.18047420148527726\n",
      "train loss:0.2883141087809412\n",
      "train loss:0.28007504339026434\n",
      "train loss:0.20981299632210182\n",
      "train loss:0.20099469017261726\n",
      "train loss:0.13408286003269124\n",
      "train loss:0.15703413361662416\n",
      "train loss:0.19517901690453546\n",
      "train loss:0.19221248175387504\n",
      "train loss:0.22805391282457993\n",
      "train loss:0.17009168440992437\n",
      "train loss:0.16707185806437114\n",
      "train loss:0.2743642683006353\n",
      "train loss:0.22007719950511595\n",
      "train loss:0.36105276460327834\n",
      "train loss:0.14928750268200777\n",
      "train loss:0.16332100447584585\n",
      "train loss:0.18082000608551563\n",
      "train loss:0.17803357387954363\n",
      "train loss:0.33735677117048923\n",
      "train loss:0.15010501248626698\n",
      "train loss:0.2034466869548385\n",
      "train loss:0.20587979312149252\n",
      "train loss:0.1357375717763882\n",
      "train loss:0.22014952394887952\n",
      "train loss:0.28283424317285516\n",
      "train loss:0.24506236637467274\n",
      "train loss:0.18606048719553347\n",
      "train loss:0.31275920835500526\n",
      "train loss:0.19889015101761712\n",
      "train loss:0.14066507882956397\n",
      "train loss:0.2606018356177745\n",
      "train loss:0.13428139620802265\n",
      "train loss:0.15372629824210482\n",
      "train loss:0.18788971525951964\n",
      "train loss:0.19233354974662784\n",
      "train loss:0.22924005765862976\n",
      "train loss:0.22773704578579132\n",
      "train loss:0.08852628043174197\n",
      "train loss:0.12722631122398606\n",
      "train loss:0.23577061418278722\n",
      "train loss:0.17372514057688832\n",
      "train loss:0.18453257371262477\n",
      "train loss:0.21926175148258392\n",
      "train loss:0.17961326621885154\n",
      "train loss:0.3008100184629789\n",
      "train loss:0.21897695123353608\n",
      "train loss:0.23128818836558096\n",
      "train loss:0.264571768681485\n",
      "train loss:0.19537427417432254\n",
      "train loss:0.2747295555283171\n",
      "train loss:0.21054069768665662\n",
      "train loss:0.24544400606407443\n",
      "train loss:0.3204505018826669\n",
      "train loss:0.2779056908931512\n",
      "train loss:0.1708526441125946\n",
      "train loss:0.23926593869602214\n",
      "train loss:0.25679103697248534\n",
      "train loss:0.1949405275886802\n",
      "train loss:0.21282509422839727\n",
      "train loss:0.20582656829298304\n",
      "train loss:0.14464242295362642\n",
      "train loss:0.1631202638574889\n",
      "train loss:0.26595497459038914\n",
      "train loss:0.22635032453360707\n",
      "train loss:0.1913811142165267\n",
      "train loss:0.28646832853158566\n",
      "train loss:0.17038420640547058\n",
      "train loss:0.15131861501742644\n",
      "train loss:0.10369105167705951\n",
      "train loss:0.11950192572956347\n",
      "train loss:0.20886423022671882\n",
      "train loss:0.35952595245460794\n",
      "train loss:0.1806593625176812\n",
      "train loss:0.3457043659204702\n",
      "train loss:0.06530912269264111\n",
      "train loss:0.28986593236542446\n",
      "train loss:0.23325294368361377\n",
      "train loss:0.25785390542088327\n",
      "train loss:0.21276410406768506\n",
      "train loss:0.26959637513247686\n",
      "train loss:0.1930568822672777\n",
      "train loss:0.15516795585969564\n",
      "train loss:0.33279971413180376\n",
      "train loss:0.11135008039163524\n",
      "train loss:0.1432400697848361\n",
      "train loss:0.20370768936104888\n",
      "train loss:0.17644334119971547\n",
      "train loss:0.23326649859119267\n",
      "train loss:0.18359822557961075\n",
      "train loss:0.1628914249927234\n",
      "train loss:0.13956993331079756\n",
      "train loss:0.1253307922736558\n",
      "train loss:0.18257047158367912\n",
      "train loss:0.19795904898019678\n",
      "train loss:0.0893608693760239\n",
      "train loss:0.17890365595704374\n",
      "train loss:0.11388902774707219\n",
      "train loss:0.25424116613951364\n",
      "train loss:0.2870479182907052\n",
      "train loss:0.27987276533212335\n",
      "train loss:0.1756513211122598\n",
      "train loss:0.15025624580042177\n",
      "train loss:0.12375351480327672\n",
      "train loss:0.23444669002473906\n",
      "train loss:0.20525075431357923\n",
      "train loss:0.1826276826170627\n",
      "train loss:0.25356950320867555\n",
      "train loss:0.24464660502773722\n",
      "train loss:0.15431302738126124\n",
      "train loss:0.1366535627842164\n",
      "train loss:0.31611277330528714\n",
      "train loss:0.14503363303836866\n",
      "train loss:0.2374960428105498\n",
      "train loss:0.11903750449627129\n",
      "train loss:0.1833790698902352\n",
      "train loss:0.17466063463315337\n",
      "train loss:0.17701923178263393\n",
      "train loss:0.13749431243593863\n",
      "train loss:0.2611509571849424\n",
      "train loss:0.14340562013785785\n",
      "train loss:0.18181421298093883\n",
      "train loss:0.17447105650921896\n",
      "train loss:0.2876177941291255\n",
      "train loss:0.20506754565887522\n",
      "train loss:0.15280016507788727\n",
      "train loss:0.15074065427609692\n",
      "train loss:0.1725085003288764\n",
      "train loss:0.22382809059706016\n",
      "train loss:0.1591571891666296\n",
      "train loss:0.2794704237461435\n",
      "train loss:0.16520740773652512\n",
      "train loss:0.14569253286381734\n",
      "train loss:0.13490825369865656\n",
      "train loss:0.09384925541243899\n",
      "train loss:0.32870790773803177\n",
      "train loss:0.19613473256566105\n",
      "train loss:0.17465049335049168\n",
      "train loss:0.1817957136547876\n",
      "train loss:0.1798648365958657\n",
      "train loss:0.3295599846662733\n",
      "train loss:0.19532518298756985\n",
      "train loss:0.10141426081893343\n",
      "train loss:0.10045321383479164\n",
      "train loss:0.1502616143472252\n",
      "train loss:0.10820425108847745\n",
      "train loss:0.25632148855794795\n",
      "train loss:0.13303305342387245\n",
      "train loss:0.1880048644172633\n",
      "train loss:0.15415328987365215\n",
      "train loss:0.16677964681235835\n",
      "train loss:0.24964637182415253\n",
      "train loss:0.15491410921726942\n",
      "train loss:0.15470417357550473\n",
      "train loss:0.11062494227255978\n",
      "train loss:0.1249994774525614\n",
      "train loss:0.08044405964353811\n",
      "train loss:0.12078192511183246\n",
      "train loss:0.2574263583403196\n",
      "train loss:0.18370396121190716\n",
      "train loss:0.19232521375354925\n",
      "train loss:0.1442569308561609\n",
      "train loss:0.18068745433847608\n",
      "train loss:0.09123482618708899\n",
      "train loss:0.23382353544441056\n",
      "train loss:0.2778447435054067\n",
      "train loss:0.1518714009788957\n",
      "train loss:0.15980803312639655\n",
      "train loss:0.24722616458881144\n",
      "train loss:0.17539691305451627\n",
      "train loss:0.12666772189816902\n",
      "train loss:0.1904495898445539\n",
      "train loss:0.24127376295884484\n",
      "train loss:0.08363796519449995\n",
      "train loss:0.08716101532286002\n",
      "train loss:0.11373574281350234\n",
      "train loss:0.1559755991446706\n",
      "train loss:0.22461018211417585\n",
      "train loss:0.10579154596056534\n",
      "train loss:0.1787753346058677\n",
      "train loss:0.20555926351589549\n",
      "train loss:0.06857121045010077\n",
      "train loss:0.17894740551826072\n",
      "train loss:0.1964736730161799\n",
      "train loss:0.2262554840357634\n",
      "train loss:0.0913346489200856\n",
      "train loss:0.0938582423310052\n",
      "train loss:0.2464455571287005\n",
      "train loss:0.1800693135484282\n",
      "train loss:0.10366405988156074\n",
      "train loss:0.06028400755952629\n",
      "train loss:0.2493237336334089\n",
      "train loss:0.0906614412547433\n",
      "train loss:0.09343257172881574\n",
      "train loss:0.09308899731057693\n",
      "train loss:0.10942251154893289\n",
      "train loss:0.15774799903936684\n",
      "train loss:0.14819552993587748\n",
      "train loss:0.08849437840950382\n",
      "train loss:0.19625860485514146\n",
      "train loss:0.17609572286497913\n",
      "train loss:0.23068746917469962\n",
      "train loss:0.1116274099341399\n",
      "train loss:0.19882796897893293\n",
      "train loss:0.1787318381991733\n",
      "train loss:0.07271844154765546\n",
      "train loss:0.21457360197251482\n",
      "train loss:0.2180119503925853\n",
      "train loss:0.14834782112140565\n",
      "train loss:0.15544250767207846\n",
      "train loss:0.13158060879462696\n",
      "train loss:0.23849123086776722\n",
      "train loss:0.10860791092965587\n",
      "train loss:0.22505484023986017\n",
      "train loss:0.135697001716242\n",
      "train loss:0.18010191264746336\n",
      "train loss:0.23084670068185248\n",
      "train loss:0.12691689773524753\n",
      "train loss:0.060327050360065346\n",
      "train loss:0.14544005372800453\n",
      "train loss:0.11700084135957085\n",
      "train loss:0.16436592500059224\n",
      "train loss:0.21001644635759883\n",
      "train loss:0.19840907724388074\n",
      "train loss:0.23328085600527668\n",
      "train loss:0.12720177482411868\n",
      "train loss:0.16016882522662082\n",
      "train loss:0.1335651989887179\n",
      "train loss:0.12131897827335296\n",
      "train loss:0.10027550093800397\n",
      "train loss:0.1507064387848704\n",
      "train loss:0.21254812254445757\n",
      "train loss:0.12415029645603953\n",
      "train loss:0.11925219036734283\n",
      "train loss:0.2313797945321383\n",
      "train loss:0.1177738392375668\n",
      "train loss:0.07766465053334218\n",
      "train loss:0.1293855387404458\n",
      "train loss:0.1856562001308679\n",
      "train loss:0.12043788096212588\n",
      "train loss:0.07841056395172466\n",
      "train loss:0.21098514020592923\n",
      "train loss:0.06001280404642729\n",
      "train loss:0.174805472399169\n",
      "train loss:0.13592422475034147\n",
      "train loss:0.12207134727593587\n",
      "train loss:0.12235412716028443\n",
      "train loss:0.15814436676342322\n",
      "train loss:0.14952571294964329\n",
      "train loss:0.08465153304235715\n",
      "train loss:0.2520893537180588\n",
      "train loss:0.1945902780040576\n",
      "train loss:0.10068856686023185\n",
      "train loss:0.11047251386156168\n",
      "train loss:0.12236665274697484\n",
      "train loss:0.24423356208554092\n",
      "train loss:0.12272982447255602\n",
      "train loss:0.12360618315446043\n",
      "train loss:0.1008744276352289\n",
      "train loss:0.08074779278085074\n",
      "train loss:0.1849911047298008\n",
      "train loss:0.23140651592558434\n",
      "train loss:0.07516722509276548\n",
      "train loss:0.1578930525616649\n",
      "train loss:0.20024125658446434\n",
      "train loss:0.16308048637897263\n",
      "train loss:0.13789228847176957\n",
      "train loss:0.15734111422297495\n",
      "train loss:0.11982743936690463\n",
      "train loss:0.13711130461792323\n",
      "train loss:0.10982887441285274\n",
      "train loss:0.10372186751571037\n",
      "train loss:0.15122134822490468\n",
      "train loss:0.1866631439836268\n",
      "train loss:0.10639064626641138\n",
      "train loss:0.09621697685675101\n",
      "train loss:0.22085314996923383\n",
      "train loss:0.09410095466979579\n",
      "train loss:0.1415390110030928\n",
      "train loss:0.16139543601040532\n",
      "train loss:0.09936790626401447\n",
      "train loss:0.05264595624092142\n",
      "train loss:0.16512305801222527\n",
      "train loss:0.07116526699877052\n",
      "train loss:0.2302259340867109\n",
      "train loss:0.10438033446891723\n",
      "train loss:0.07693022006041852\n",
      "train loss:0.12098567873734219\n",
      "train loss:0.094446441760019\n",
      "train loss:0.14332066608385458\n",
      "train loss:0.1823806787752661\n",
      "train loss:0.14894405493920626\n",
      "train loss:0.20666128481485366\n",
      "train loss:0.08723095367035244\n",
      "train loss:0.10066629424985236\n",
      "train loss:0.05882790711053799\n",
      "train loss:0.09004443007160189\n",
      "train loss:0.13069816201147386\n",
      "train loss:0.12603879888667024\n",
      "train loss:0.07510287716465375\n",
      "train loss:0.20204063695046948\n",
      "train loss:0.12949434084610126\n",
      "train loss:0.11494168246825932\n",
      "train loss:0.055555065191488714\n",
      "train loss:0.14827437738194377\n",
      "train loss:0.12572755513580963\n",
      "train loss:0.10831111931091403\n",
      "train loss:0.15150843411952367\n",
      "train loss:0.055628252729932035\n",
      "train loss:0.05110107626587694\n",
      "train loss:0.13188416705835246\n",
      "train loss:0.09367933364194567\n",
      "train loss:0.10457199486225485\n",
      "train loss:0.2805129315388904\n",
      "train loss:0.24057375936994774\n",
      "train loss:0.07732700202625112\n",
      "train loss:0.14471254490800484\n",
      "=== epoch:2, train acc:0.947, test acc:0.947 ===\n",
      "train loss:0.1901427664100902\n",
      "train loss:0.20662303251240816\n",
      "train loss:0.14086414362170166\n",
      "train loss:0.1414484799220298\n",
      "train loss:0.15801921819418466\n",
      "train loss:0.17594721826726745\n",
      "train loss:0.25389431557742737\n",
      "train loss:0.17628015227981358\n",
      "train loss:0.17573637858685864\n",
      "train loss:0.1287203413106418\n",
      "train loss:0.20282084156598135\n",
      "train loss:0.1389561315068298\n",
      "train loss:0.10207208111299831\n",
      "train loss:0.12864956407291359\n",
      "train loss:0.14873405975323573\n",
      "train loss:0.14432670790116225\n",
      "train loss:0.13517118303677433\n",
      "train loss:0.13602437015892055\n",
      "train loss:0.08405946549828322\n",
      "train loss:0.05216726357046271\n",
      "train loss:0.24757266340810516\n",
      "train loss:0.11477433394084846\n",
      "train loss:0.053913465633369545\n",
      "train loss:0.09191895506993397\n",
      "train loss:0.26460037864473157\n",
      "train loss:0.1192029125707961\n",
      "train loss:0.09222940726711672\n",
      "train loss:0.10503067880248614\n",
      "train loss:0.08003088203974584\n",
      "train loss:0.07910098343467045\n",
      "train loss:0.09808036824679363\n",
      "train loss:0.06140123180954659\n",
      "train loss:0.0676058438577318\n",
      "train loss:0.13418710932680264\n",
      "train loss:0.18489690900249245\n",
      "train loss:0.08044547324500069\n",
      "train loss:0.09775475316624173\n",
      "train loss:0.23874492955010013\n",
      "train loss:0.09237341303885714\n",
      "train loss:0.19318245175274293\n",
      "train loss:0.08768947365280509\n",
      "train loss:0.1680022961837733\n",
      "train loss:0.13442467999374408\n",
      "train loss:0.18371807558472145\n",
      "train loss:0.03493496405766852\n",
      "train loss:0.059383721362615674\n",
      "train loss:0.0935588965178822\n",
      "train loss:0.07551467347859264\n",
      "train loss:0.12971190579448677\n",
      "train loss:0.07397902477433628\n",
      "train loss:0.12079472855784909\n",
      "train loss:0.1033449509357246\n",
      "train loss:0.08472868834215327\n",
      "train loss:0.11255862536997326\n",
      "train loss:0.07500036210028728\n",
      "train loss:0.16253881972692583\n",
      "train loss:0.10039247988114376\n",
      "train loss:0.12898776768714768\n",
      "train loss:0.11066804962875704\n",
      "train loss:0.08355746692618321\n",
      "train loss:0.08167358618433768\n",
      "train loss:0.11906163444353186\n",
      "train loss:0.10896092439727559\n",
      "train loss:0.12042011300535461\n",
      "train loss:0.11022686259385249\n",
      "train loss:0.16324656134575669\n",
      "train loss:0.07609443885881287\n",
      "train loss:0.2071537019739741\n",
      "train loss:0.09691616519366336\n",
      "train loss:0.10181971447624974\n",
      "train loss:0.14347620254695923\n",
      "train loss:0.12024196142655938\n",
      "train loss:0.08443704747960931\n",
      "train loss:0.059187966258840846\n",
      "train loss:0.06136483552522445\n",
      "train loss:0.15107604963620014\n",
      "train loss:0.14427179213418273\n",
      "train loss:0.14744122936715492\n",
      "train loss:0.1818589308939725\n",
      "train loss:0.07516169421984156\n",
      "train loss:0.046040684181121276\n",
      "train loss:0.07625173842579634\n",
      "train loss:0.11638188821012234\n",
      "train loss:0.11079665444872534\n",
      "train loss:0.0949022104768988\n",
      "train loss:0.07476446064003286\n",
      "train loss:0.13906807736086976\n",
      "train loss:0.0572263187716866\n",
      "train loss:0.0775935102998857\n",
      "train loss:0.1921168348460478\n",
      "train loss:0.05818777676124902\n",
      "train loss:0.11902786193765456\n",
      "train loss:0.11437261900572192\n",
      "train loss:0.10169390893271504\n",
      "train loss:0.12986825319630968\n",
      "train loss:0.20073557196445482\n",
      "train loss:0.1511283774234714\n",
      "train loss:0.048654760399617365\n",
      "train loss:0.14021200706632034\n",
      "train loss:0.079581705329982\n",
      "train loss:0.09188930337334522\n",
      "train loss:0.026116164854970952\n",
      "train loss:0.13372788536395144\n",
      "train loss:0.05531920092232536\n",
      "train loss:0.14602761964668215\n",
      "train loss:0.07279138310188714\n",
      "train loss:0.14762629524083626\n",
      "train loss:0.12772121734480257\n",
      "train loss:0.12367050298153494\n",
      "train loss:0.11104849376242969\n",
      "train loss:0.08591985035852588\n",
      "train loss:0.08711324802977437\n",
      "train loss:0.07655872892112633\n",
      "train loss:0.12944066497111878\n",
      "train loss:0.19628133686797508\n",
      "train loss:0.07480285065142128\n",
      "train loss:0.08701239028191145\n",
      "train loss:0.26261309582460224\n",
      "train loss:0.09543840862221797\n",
      "train loss:0.14111884571747313\n",
      "train loss:0.01812534117879455\n",
      "train loss:0.05605627532317872\n",
      "train loss:0.1201694612208502\n",
      "train loss:0.051033873241049\n",
      "train loss:0.07560526399743556\n",
      "train loss:0.13511098949044079\n",
      "train loss:0.10650974702588484\n",
      "train loss:0.18068850160924954\n",
      "train loss:0.06310421939355275\n",
      "train loss:0.06887220050288244\n",
      "train loss:0.06242153443339722\n",
      "train loss:0.0889112484231917\n",
      "train loss:0.06338443021440804\n",
      "train loss:0.07159203747837303\n",
      "train loss:0.08760294262676009\n",
      "train loss:0.061378753285897626\n",
      "train loss:0.16340116898467028\n",
      "train loss:0.05054399642298473\n",
      "train loss:0.125021316935489\n",
      "train loss:0.21422070284956388\n",
      "train loss:0.23183917350835223\n",
      "train loss:0.09233628317446\n",
      "train loss:0.0830719312265474\n",
      "train loss:0.048047813896614826\n",
      "train loss:0.1322189766166535\n",
      "train loss:0.04614414323614636\n",
      "train loss:0.1240542790016304\n",
      "train loss:0.10402143363981027\n",
      "train loss:0.06004897585926603\n",
      "train loss:0.12899539448646716\n",
      "train loss:0.10123260176521015\n",
      "train loss:0.0663334444097051\n",
      "train loss:0.10176189620170044\n",
      "train loss:0.11052833214090672\n",
      "train loss:0.06266778355799223\n",
      "train loss:0.09370498783356988\n",
      "train loss:0.05791353004617861\n",
      "train loss:0.028746071616322477\n",
      "train loss:0.10288807088717376\n",
      "train loss:0.07057599100134829\n",
      "train loss:0.08400589064573419\n",
      "train loss:0.0757975797933162\n",
      "train loss:0.07100604670223688\n",
      "train loss:0.09115393470276924\n",
      "train loss:0.09675475617397117\n",
      "train loss:0.03853596901260419\n",
      "train loss:0.13177761822630715\n",
      "train loss:0.1177135130395349\n",
      "train loss:0.04020114227760927\n",
      "train loss:0.10219694553408433\n",
      "train loss:0.1163688875715816\n",
      "train loss:0.15210485329254814\n",
      "train loss:0.0721270877610918\n",
      "train loss:0.14631125065649642\n",
      "train loss:0.12661445182163444\n",
      "train loss:0.12731496034196638\n",
      "train loss:0.0746873653802679\n",
      "train loss:0.06858840243987546\n",
      "train loss:0.15490167228458326\n",
      "train loss:0.05511679309648293\n",
      "train loss:0.10194392833008013\n",
      "train loss:0.1255504383967157\n",
      "train loss:0.09596823379626784\n",
      "train loss:0.08628200083147534\n",
      "train loss:0.05010075755539976\n",
      "train loss:0.1294520145598883\n",
      "train loss:0.13873763660353539\n",
      "train loss:0.1413021073993618\n",
      "train loss:0.14087514119684952\n",
      "train loss:0.05770340528151729\n",
      "train loss:0.08926370440586659\n",
      "train loss:0.11158180501117884\n",
      "train loss:0.040801996686860915\n",
      "train loss:0.08491453417958518\n",
      "train loss:0.07549216982045431\n",
      "train loss:0.06742117160428253\n",
      "train loss:0.2056576332984337\n",
      "train loss:0.165512122062209\n",
      "train loss:0.08908639042466988\n",
      "train loss:0.029528346168991547\n",
      "train loss:0.054921054393620175\n",
      "train loss:0.13615911727015256\n",
      "train loss:0.10321410379549677\n",
      "train loss:0.05902090605084404\n",
      "train loss:0.08661909630008881\n",
      "train loss:0.12901390509839655\n",
      "train loss:0.029600122168338044\n",
      "train loss:0.10973999310336703\n",
      "train loss:0.03795198042589846\n",
      "train loss:0.08972997628720376\n",
      "train loss:0.050595887226985464\n",
      "train loss:0.08584298174475644\n",
      "train loss:0.14524600327710468\n",
      "train loss:0.03756337721654057\n",
      "train loss:0.05254090981129461\n",
      "train loss:0.10895245699330877\n",
      "train loss:0.04874522859055745\n",
      "train loss:0.0836036450195578\n",
      "train loss:0.1094755576683833\n",
      "train loss:0.09734600800456061\n",
      "train loss:0.10545544683710491\n",
      "train loss:0.048024383691567946\n",
      "train loss:0.2806537616462472\n",
      "train loss:0.10121268684950493\n",
      "train loss:0.1032779726404767\n",
      "train loss:0.11046684174565921\n",
      "train loss:0.058174318021633156\n",
      "train loss:0.12028874310264777\n",
      "train loss:0.1015065954491098\n",
      "train loss:0.11796096359577238\n",
      "train loss:0.07455698312216341\n",
      "train loss:0.08035994727647763\n",
      "train loss:0.19502470266648145\n",
      "train loss:0.07144281563061082\n",
      "train loss:0.06934271965610003\n",
      "train loss:0.20491843260935996\n",
      "train loss:0.16620963885578757\n",
      "train loss:0.18114922469797531\n",
      "train loss:0.09270207081924052\n",
      "train loss:0.1291962097136865\n",
      "train loss:0.07906128185621662\n",
      "train loss:0.12166046417509435\n",
      "train loss:0.06851646394737224\n",
      "train loss:0.07935050934156715\n",
      "train loss:0.04567528561159784\n",
      "train loss:0.14094191197213127\n",
      "train loss:0.1668226271955663\n",
      "train loss:0.0656199231288667\n",
      "train loss:0.11582180591140158\n",
      "train loss:0.1273622155417239\n",
      "train loss:0.09823585259460478\n",
      "train loss:0.1486968357884205\n",
      "train loss:0.057055505821774884\n",
      "train loss:0.07677049458362899\n",
      "train loss:0.1561739973152979\n",
      "train loss:0.07035441606597231\n",
      "train loss:0.05034902291803875\n",
      "train loss:0.07894459120993885\n",
      "train loss:0.06747945884247074\n",
      "train loss:0.15698882428261218\n",
      "train loss:0.06455279322445145\n",
      "train loss:0.1384432515289765\n",
      "train loss:0.10443670072907443\n",
      "train loss:0.24582078516920616\n",
      "train loss:0.07270409188390116\n",
      "train loss:0.10351745837080387\n",
      "train loss:0.08773059438348614\n",
      "train loss:0.06219648673140324\n",
      "train loss:0.27709110788915486\n",
      "train loss:0.08382167353195837\n",
      "train loss:0.021836659835815227\n",
      "train loss:0.06548806289342447\n",
      "train loss:0.07787018160591219\n",
      "train loss:0.14803964187564586\n",
      "train loss:0.10094882089873272\n",
      "train loss:0.16881528227330975\n",
      "train loss:0.09681976911615957\n",
      "train loss:0.0981745219164465\n",
      "train loss:0.06627874132099308\n",
      "train loss:0.06913909868188793\n",
      "train loss:0.14762967165919788\n",
      "train loss:0.10334545820076332\n",
      "train loss:0.17644844136947063\n",
      "train loss:0.11474101975377911\n",
      "train loss:0.10447585212775562\n",
      "train loss:0.06506098817668486\n",
      "train loss:0.04144950554155006\n",
      "train loss:0.1552497697476894\n",
      "train loss:0.10128921989800407\n",
      "train loss:0.06063303309249375\n",
      "train loss:0.06034247408019271\n",
      "train loss:0.08257302151431477\n",
      "train loss:0.0646463150489023\n",
      "train loss:0.10561307611576405\n",
      "train loss:0.09727740786617468\n",
      "train loss:0.04816609052469035\n",
      "train loss:0.056655284619984786\n",
      "train loss:0.10018865009331492\n",
      "train loss:0.05053582316942295\n",
      "train loss:0.05972414549012536\n",
      "train loss:0.1740825303505838\n",
      "train loss:0.06324081282658688\n",
      "train loss:0.04810657123138923\n",
      "train loss:0.04275291165295131\n",
      "train loss:0.0746358499005568\n",
      "train loss:0.10684736904322127\n",
      "train loss:0.09425538571224408\n",
      "train loss:0.06663261551380711\n",
      "train loss:0.07056962329868219\n",
      "train loss:0.061389421209236954\n",
      "train loss:0.07364931387171005\n",
      "train loss:0.060290118704784394\n",
      "train loss:0.06115937573190364\n",
      "train loss:0.07624187425112652\n",
      "train loss:0.049628604322272965\n",
      "train loss:0.07083176791709316\n",
      "train loss:0.035867625075844706\n",
      "train loss:0.14866647560122487\n",
      "train loss:0.13221518532440465\n",
      "train loss:0.15448381405346434\n",
      "train loss:0.05017967197174981\n",
      "train loss:0.22216595877128853\n",
      "train loss:0.04248976183327879\n",
      "train loss:0.0633354589924012\n",
      "train loss:0.05973733953389284\n",
      "train loss:0.1777445943704307\n",
      "train loss:0.05156295830913769\n",
      "train loss:0.05389515882787524\n",
      "train loss:0.09217766472490403\n",
      "train loss:0.10293513256913442\n",
      "train loss:0.09800246083459452\n",
      "train loss:0.14565987164418626\n",
      "train loss:0.08410557001360106\n",
      "train loss:0.09259501470914583\n",
      "train loss:0.06280655523722918\n",
      "train loss:0.047118265772335584\n",
      "train loss:0.15841195125330768\n",
      "train loss:0.06819296059370925\n",
      "train loss:0.08962595604651387\n",
      "train loss:0.07368632336439902\n",
      "train loss:0.10874664717791456\n",
      "train loss:0.17210527587006255\n",
      "train loss:0.10964672151702053\n",
      "train loss:0.040762247585420124\n",
      "train loss:0.047589775434786145\n",
      "train loss:0.04855686702776973\n",
      "train loss:0.05513948471504165\n",
      "train loss:0.08970760652268325\n",
      "train loss:0.08115003699850522\n",
      "train loss:0.06423648243977481\n",
      "train loss:0.07312088398882956\n",
      "train loss:0.06274184859857045\n",
      "train loss:0.07996717491297929\n",
      "train loss:0.12404933693844296\n",
      "train loss:0.07989106131803102\n",
      "train loss:0.06636653645387793\n",
      "train loss:0.0970470529084839\n",
      "train loss:0.12107171039474615\n",
      "train loss:0.06483888880091988\n",
      "train loss:0.06362171579789411\n",
      "train loss:0.07600813124063537\n",
      "train loss:0.07007598766849572\n",
      "train loss:0.05761439341111667\n",
      "train loss:0.04515838000973714\n",
      "train loss:0.03743166507593564\n",
      "train loss:0.05791431384860256\n",
      "train loss:0.06123022452013675\n",
      "train loss:0.06690372792072558\n",
      "train loss:0.11019661334620469\n",
      "train loss:0.11003375594176809\n",
      "train loss:0.028476443580638527\n",
      "train loss:0.06562447731781135\n",
      "train loss:0.20676062350448518\n",
      "train loss:0.05265681731465041\n",
      "train loss:0.05276579562566331\n",
      "train loss:0.06420917695113441\n",
      "train loss:0.06700141207027781\n",
      "train loss:0.08970530450844333\n",
      "train loss:0.13830452370333912\n",
      "train loss:0.08689636031476444\n",
      "train loss:0.05133343551596461\n",
      "train loss:0.057587862193672124\n",
      "train loss:0.154967937640649\n",
      "train loss:0.02544507374355852\n",
      "train loss:0.1032956523195917\n",
      "train loss:0.09259502682408721\n",
      "train loss:0.07344638040790945\n",
      "train loss:0.05974301427222429\n",
      "train loss:0.07690003559202807\n",
      "train loss:0.03564019989777118\n",
      "train loss:0.08882744089332024\n",
      "train loss:0.06770741646699623\n",
      "train loss:0.027668506068195254\n",
      "train loss:0.0796303157546119\n",
      "train loss:0.08338887965640865\n",
      "train loss:0.06991068639241466\n",
      "train loss:0.08102385264796905\n",
      "train loss:0.08464038713190668\n",
      "train loss:0.11917722761737087\n",
      "train loss:0.08818096498970479\n",
      "train loss:0.03242142379551807\n",
      "train loss:0.04625788813643532\n",
      "train loss:0.16091274011664164\n",
      "train loss:0.15352218286881059\n",
      "train loss:0.1100596511592405\n",
      "train loss:0.06654327915654559\n",
      "train loss:0.07074195222954702\n",
      "train loss:0.08042874139525447\n",
      "train loss:0.051613261760327316\n",
      "train loss:0.038919059071751284\n",
      "train loss:0.06249301736265767\n",
      "train loss:0.07582259445698289\n",
      "train loss:0.09127798095915261\n",
      "train loss:0.10813592539686903\n",
      "train loss:0.08887115799366997\n",
      "train loss:0.07767745381626806\n",
      "train loss:0.1354216671067337\n",
      "train loss:0.06061708929723807\n",
      "train loss:0.052429505888561614\n",
      "train loss:0.04016041306062916\n",
      "train loss:0.10573111428181896\n",
      "train loss:0.035097586768122725\n",
      "train loss:0.046254092090093665\n",
      "train loss:0.09319166159465743\n",
      "train loss:0.0568371951373596\n",
      "train loss:0.04248738461028969\n",
      "train loss:0.10003951213371598\n",
      "train loss:0.03626755408682641\n",
      "train loss:0.08111230422089147\n",
      "train loss:0.09768721093459533\n",
      "train loss:0.0911292738746305\n",
      "train loss:0.02705257112908637\n",
      "train loss:0.08446252431849352\n",
      "train loss:0.06923292699445288\n",
      "train loss:0.034292447925252476\n",
      "train loss:0.03218062654033794\n",
      "train loss:0.054026527075536006\n",
      "train loss:0.06979098816704556\n",
      "train loss:0.06326503323270445\n",
      "train loss:0.18533884672664858\n",
      "train loss:0.03802020633625252\n",
      "train loss:0.12805462548013596\n",
      "train loss:0.054307949605128976\n",
      "train loss:0.03388268938824654\n",
      "train loss:0.09806645333508512\n",
      "train loss:0.05180665357300933\n",
      "train loss:0.05928449512975717\n",
      "train loss:0.06641015526193483\n",
      "train loss:0.0609520844863331\n",
      "train loss:0.10363059715583763\n",
      "train loss:0.06043013190542133\n",
      "train loss:0.08040478606817036\n",
      "train loss:0.06714235201109094\n",
      "train loss:0.06144348606016244\n",
      "train loss:0.09446342861416414\n",
      "train loss:0.07290372911655145\n",
      "train loss:0.050638689402365884\n",
      "train loss:0.057563913244041254\n",
      "train loss:0.11202909665814031\n",
      "train loss:0.12818938853792233\n",
      "train loss:0.0856502877185871\n",
      "train loss:0.06842303246467481\n",
      "train loss:0.0555040511129283\n",
      "train loss:0.1344845056771892\n",
      "train loss:0.014481066183162533\n",
      "train loss:0.11926354255266557\n",
      "train loss:0.051029289815171276\n",
      "train loss:0.18604069740833218\n",
      "train loss:0.057649718844594015\n",
      "train loss:0.05124394099722739\n",
      "train loss:0.043222556363067974\n",
      "train loss:0.15537967156642796\n",
      "train loss:0.029099562713712595\n",
      "train loss:0.0759915368482615\n",
      "train loss:0.03615615297420045\n",
      "train loss:0.07602056574570994\n",
      "train loss:0.15561818629457538\n",
      "train loss:0.034739785298665775\n",
      "train loss:0.04132293014298861\n",
      "train loss:0.04571107505630084\n",
      "train loss:0.048400387975341186\n",
      "train loss:0.10725230882894193\n",
      "train loss:0.023505436242348653\n",
      "train loss:0.12527019599335432\n",
      "train loss:0.05665836711178293\n",
      "train loss:0.03307184954625554\n",
      "train loss:0.07382042339247531\n",
      "train loss:0.034039414525192664\n",
      "train loss:0.06660649714950488\n",
      "train loss:0.09047608570912764\n",
      "train loss:0.03197242619696669\n",
      "train loss:0.06904968178458237\n",
      "train loss:0.053149739415014\n",
      "train loss:0.09694472567076964\n",
      "train loss:0.037016902407014045\n",
      "train loss:0.08400203214240996\n",
      "train loss:0.07910517954321304\n",
      "train loss:0.040743387740425366\n",
      "train loss:0.04430524468630653\n",
      "train loss:0.12058969096883089\n",
      "train loss:0.0640738662217093\n",
      "train loss:0.0504604546170759\n",
      "train loss:0.019858563247498198\n",
      "train loss:0.06388934798744532\n",
      "train loss:0.05452566304058507\n",
      "train loss:0.059229087475566475\n",
      "train loss:0.12394708786624901\n",
      "train loss:0.09710860261231949\n",
      "train loss:0.060904467952219876\n",
      "train loss:0.0405731597351192\n",
      "train loss:0.1205225784124187\n",
      "train loss:0.08143159331701087\n",
      "train loss:0.07185939368168723\n",
      "train loss:0.05653025099815659\n",
      "train loss:0.08839753356832596\n",
      "train loss:0.08911034538460721\n",
      "train loss:0.1336334959279431\n",
      "train loss:0.0927005551182701\n",
      "train loss:0.04452709602428093\n",
      "train loss:0.10509872430927455\n",
      "train loss:0.06699491337112665\n",
      "train loss:0.03638995780563577\n",
      "train loss:0.051996198353023076\n",
      "train loss:0.08381441949294598\n",
      "train loss:0.06055605369572139\n",
      "train loss:0.08803873613609085\n",
      "train loss:0.08614564181965471\n",
      "train loss:0.07037094048218621\n",
      "train loss:0.08435672762513935\n",
      "train loss:0.11619757521054193\n",
      "train loss:0.05312217573307223\n",
      "train loss:0.03974152299228858\n",
      "train loss:0.12207016502382623\n",
      "train loss:0.11191027605384746\n",
      "train loss:0.21127480601555992\n",
      "train loss:0.05366116912498324\n",
      "train loss:0.11885942632199085\n",
      "train loss:0.06833365350045338\n",
      "train loss:0.026351652594581044\n",
      "train loss:0.07465854028876116\n",
      "train loss:0.13157656775958518\n",
      "train loss:0.2060150837365487\n",
      "train loss:0.1770354131643314\n",
      "train loss:0.09203354572682214\n",
      "train loss:0.06672371338738574\n",
      "train loss:0.16946490699138356\n",
      "train loss:0.05118155806023537\n",
      "train loss:0.12041306915994257\n",
      "train loss:0.07636543894906922\n",
      "train loss:0.10202615914609056\n",
      "train loss:0.10433750316962591\n",
      "train loss:0.04561761544035559\n",
      "train loss:0.0509662544836069\n",
      "train loss:0.06072770945682496\n",
      "train loss:0.1664368572390809\n",
      "train loss:0.08906007771323778\n",
      "train loss:0.12152269498667553\n",
      "train loss:0.046621182117407865\n",
      "train loss:0.05746876268506934\n",
      "train loss:0.08435031695903016\n",
      "train loss:0.10850822816010368\n",
      "train loss:0.07052141363912272\n",
      "train loss:0.051690153294677084\n",
      "train loss:0.05660471024320654\n",
      "train loss:0.11188834246349083\n",
      "train loss:0.07514632624817008\n",
      "train loss:0.13767910094818933\n",
      "train loss:0.09316643927228385\n",
      "train loss:0.09617069688263692\n",
      "train loss:0.07375487763417013\n",
      "train loss:0.055215443943078556\n",
      "train loss:0.04457974675411708\n",
      "train loss:0.0908997180335355\n",
      "train loss:0.07279122570117984\n",
      "train loss:0.10774793206934588\n",
      "train loss:0.039428302052081275\n",
      "train loss:0.05174714274610568\n",
      "train loss:0.05379128507401662\n",
      "train loss:0.15311612883688336\n",
      "train loss:0.036486877477244925\n",
      "train loss:0.07001312574807662\n",
      "train loss:0.0662862180105437\n",
      "train loss:0.017774351794238082\n",
      "train loss:0.03816274088263989\n",
      "train loss:0.08767822849927212\n",
      "train loss:0.03915066339442877\n",
      "train loss:0.07037522463514413\n",
      "train loss:0.09165717496475143\n",
      "train loss:0.06869989309644987\n",
      "train loss:0.058505630115126904\n",
      "train loss:0.087165723478766\n",
      "train loss:0.07736731584395719\n",
      "train loss:0.09647993192293025\n",
      "train loss:0.11014801864010443\n",
      "train loss:0.048748076709725384\n",
      "train loss:0.09776463608988765\n",
      "train loss:0.11526401862705846\n",
      "train loss:0.04742691310817781\n",
      "train loss:0.08756889359323736\n",
      "train loss:0.08560322563274914\n",
      "=== epoch:3, train acc:0.974, test acc:0.973 ===\n",
      "train loss:0.05078399050903801\n",
      "train loss:0.030629492684138674\n",
      "train loss:0.06273162264033293\n",
      "train loss:0.04000723373249614\n",
      "train loss:0.12755223625619727\n",
      "train loss:0.05711030127484774\n",
      "train loss:0.08855934703666091\n",
      "train loss:0.056031633010274466\n",
      "train loss:0.03858149080925368\n",
      "train loss:0.0413352675482425\n",
      "train loss:0.07875863731723506\n",
      "train loss:0.1448847188757857\n",
      "train loss:0.02762748683194965\n",
      "train loss:0.07597757779690945\n",
      "train loss:0.13800973165418776\n",
      "train loss:0.022763769640990743\n",
      "train loss:0.08207119306905357\n",
      "train loss:0.06647675670222722\n",
      "train loss:0.12599721923014476\n",
      "train loss:0.05351506079000162\n",
      "train loss:0.025375404447244496\n",
      "train loss:0.1162305408333075\n",
      "train loss:0.04071208489894281\n",
      "train loss:0.10610847768816394\n",
      "train loss:0.06577606680736076\n",
      "train loss:0.19927064426998367\n",
      "train loss:0.04609579929212659\n",
      "train loss:0.06063831813807957\n",
      "train loss:0.10353755251581782\n",
      "train loss:0.06316209826773661\n",
      "train loss:0.10017780040491138\n",
      "train loss:0.059468762327707314\n",
      "train loss:0.11400247676598152\n",
      "train loss:0.07488773279057437\n",
      "train loss:0.1751146435542592\n",
      "train loss:0.07766037485668895\n",
      "train loss:0.11590101677850098\n",
      "train loss:0.13405531632882117\n",
      "train loss:0.06183552851276297\n",
      "train loss:0.08791946626278623\n",
      "train loss:0.04819281199101716\n",
      "train loss:0.055850646762916095\n",
      "train loss:0.03922065359046586\n",
      "train loss:0.0660616419065563\n",
      "train loss:0.09511619288204126\n",
      "train loss:0.0440318760239739\n",
      "train loss:0.025479867910823798\n",
      "train loss:0.08409603752244689\n",
      "train loss:0.0613383108007347\n",
      "train loss:0.02221814757152597\n",
      "train loss:0.13806507713489627\n",
      "train loss:0.047371571517287415\n",
      "train loss:0.1158883536283189\n",
      "train loss:0.05617017193367436\n",
      "train loss:0.08057237589695435\n",
      "train loss:0.11652763797589595\n",
      "train loss:0.023562709139889178\n",
      "train loss:0.028596191424608265\n",
      "train loss:0.09465310364489651\n",
      "train loss:0.13338314991706385\n",
      "train loss:0.05165732213705842\n",
      "train loss:0.09583880377331308\n",
      "train loss:0.07328102468569093\n",
      "train loss:0.045805425326224614\n",
      "train loss:0.03701458363438903\n",
      "train loss:0.019254007525036122\n",
      "train loss:0.04350819394056266\n",
      "train loss:0.13988308348900927\n",
      "train loss:0.050029574781756354\n",
      "train loss:0.058441312393395534\n",
      "train loss:0.1876492621703823\n",
      "train loss:0.12647501319753604\n",
      "train loss:0.03407969452907357\n",
      "train loss:0.044134330021816354\n",
      "train loss:0.06118478124317264\n",
      "train loss:0.07945147857640734\n",
      "train loss:0.035771926162036796\n",
      "train loss:0.06090276593843594\n",
      "train loss:0.09538585413673902\n",
      "train loss:0.13250042054275524\n",
      "train loss:0.0563730055114781\n",
      "train loss:0.05516700531860204\n",
      "train loss:0.08070208673448546\n",
      "train loss:0.04640995468377814\n",
      "train loss:0.13478376978905088\n",
      "train loss:0.021395116312978676\n",
      "train loss:0.048130548216599525\n",
      "train loss:0.040960450359439654\n",
      "train loss:0.034336518014247856\n",
      "train loss:0.06245585234142689\n",
      "train loss:0.07896689489189121\n",
      "train loss:0.14254781920115944\n",
      "train loss:0.03943624096473851\n",
      "train loss:0.018101135355007444\n",
      "train loss:0.11084003638786619\n",
      "train loss:0.033134338092407144\n",
      "train loss:0.05660444980630225\n",
      "train loss:0.07862391671197322\n",
      "train loss:0.058258936276125606\n",
      "train loss:0.08077659110068318\n",
      "train loss:0.047987767679848276\n",
      "train loss:0.01837194468418522\n",
      "train loss:0.0207472169833727\n",
      "train loss:0.07979834531467463\n",
      "train loss:0.0633842454799212\n",
      "train loss:0.040914375545194774\n",
      "train loss:0.04505387165026587\n",
      "train loss:0.0841747364515849\n",
      "train loss:0.06255591075269282\n",
      "train loss:0.09786982220228543\n",
      "train loss:0.05770937454448934\n",
      "train loss:0.07832534607851623\n",
      "train loss:0.014094490434241087\n",
      "train loss:0.04236398336846169\n",
      "train loss:0.31074517940441365\n",
      "train loss:0.08304994066447575\n",
      "train loss:0.038409423496619695\n",
      "train loss:0.08131001714476829\n",
      "train loss:0.0648267296784685\n",
      "train loss:0.06516782590300446\n",
      "train loss:0.019473502488257573\n",
      "train loss:0.03955447205282632\n",
      "train loss:0.019216404365875333\n",
      "train loss:0.06744408798006363\n",
      "train loss:0.08679228481151084\n",
      "train loss:0.05161773278888633\n",
      "train loss:0.12005065651075329\n",
      "train loss:0.04394715261195846\n",
      "train loss:0.03241111075173578\n",
      "train loss:0.05040617062037284\n",
      "train loss:0.02799386870139378\n",
      "train loss:0.021106066888757987\n",
      "train loss:0.047027102077023146\n",
      "train loss:0.07302332041665713\n",
      "train loss:0.041182484700649516\n",
      "train loss:0.06461923779838835\n",
      "train loss:0.09943150453694823\n",
      "train loss:0.06919757936548508\n",
      "train loss:0.08730139794069515\n",
      "train loss:0.04020676071779639\n",
      "train loss:0.05975417050037249\n",
      "train loss:0.09038083815105959\n",
      "train loss:0.03035727338774901\n",
      "train loss:0.09569809394727175\n",
      "train loss:0.06922420440095602\n",
      "train loss:0.03126135312318096\n",
      "train loss:0.048843746036466865\n",
      "train loss:0.06922560521042062\n",
      "train loss:0.08021189259372359\n",
      "train loss:0.05084665364305157\n",
      "train loss:0.10323005079297103\n",
      "train loss:0.11889081306008549\n",
      "train loss:0.11527991416320628\n",
      "train loss:0.057870129751471044\n",
      "train loss:0.16992978866747308\n",
      "train loss:0.0653053104163365\n",
      "train loss:0.05565915754150379\n",
      "train loss:0.12210461942890857\n",
      "train loss:0.17159490045854006\n",
      "train loss:0.11391560883488633\n",
      "train loss:0.05075075085646968\n",
      "train loss:0.10011684242716992\n",
      "train loss:0.06701204596631101\n",
      "train loss:0.06627940937665805\n",
      "train loss:0.06706160945837457\n",
      "train loss:0.05079108878878863\n",
      "train loss:0.0191540132224306\n",
      "train loss:0.08324950294610982\n",
      "train loss:0.11727905620943281\n",
      "train loss:0.038863171427439405\n",
      "train loss:0.10810412767739858\n",
      "train loss:0.08101831305017233\n",
      "train loss:0.07472873228137318\n",
      "train loss:0.0686526647409871\n",
      "train loss:0.10567086694290136\n",
      "train loss:0.060346426920538446\n",
      "train loss:0.07164049522970839\n",
      "train loss:0.0814609216966428\n",
      "train loss:0.178273756009536\n",
      "train loss:0.05295930650612126\n",
      "train loss:0.1526514264729623\n",
      "train loss:0.019163617927774882\n",
      "train loss:0.028883514034036096\n",
      "train loss:0.12570857437266553\n",
      "train loss:0.0836818708306692\n",
      "train loss:0.05270301653888906\n",
      "train loss:0.045825794110657896\n",
      "train loss:0.06569249340941455\n",
      "train loss:0.050268901938417924\n",
      "train loss:0.035330285159462835\n",
      "train loss:0.0277047974355029\n",
      "train loss:0.043584496763749155\n",
      "train loss:0.028440731480800827\n",
      "train loss:0.040781812351012456\n",
      "train loss:0.02477838550507764\n",
      "train loss:0.03993864923099972\n",
      "train loss:0.0731240414706592\n",
      "train loss:0.1002366130318901\n",
      "train loss:0.01628610353867147\n",
      "train loss:0.0671909439894845\n",
      "train loss:0.03314692279434135\n",
      "train loss:0.07933200283372083\n",
      "train loss:0.040437139680809746\n",
      "train loss:0.12839988600237975\n",
      "train loss:0.029127442737892983\n",
      "train loss:0.09394465204723859\n",
      "train loss:0.052653548984887735\n",
      "train loss:0.1345293424774826\n",
      "train loss:0.07523966069201041\n",
      "train loss:0.05369435404508316\n",
      "train loss:0.07086286261327873\n",
      "train loss:0.012720554450458131\n",
      "train loss:0.01938785053156788\n",
      "train loss:0.06557042378356451\n",
      "train loss:0.047025367760294666\n",
      "train loss:0.07018331495815561\n",
      "train loss:0.06992177037214513\n",
      "train loss:0.027858065139894216\n",
      "train loss:0.09593361540660127\n",
      "train loss:0.05223732886255921\n",
      "train loss:0.07943591429907836\n",
      "train loss:0.02476249326817324\n",
      "train loss:0.0450099315711805\n",
      "train loss:0.08339347563485962\n",
      "train loss:0.09654376280053867\n",
      "train loss:0.026966319212061794\n",
      "train loss:0.029175876990319618\n",
      "train loss:0.04654027769338954\n",
      "train loss:0.14572431462767077\n",
      "train loss:0.03155380135211584\n",
      "train loss:0.09366853486059\n",
      "train loss:0.08442649558057388\n",
      "train loss:0.04866071043292766\n",
      "train loss:0.06672389097592059\n",
      "train loss:0.056585580449300156\n",
      "train loss:0.059762589091743194\n",
      "train loss:0.09368706025814405\n",
      "train loss:0.024727638370542194\n",
      "train loss:0.07013303714645713\n",
      "train loss:0.1001235300447983\n",
      "train loss:0.07981396670525577\n",
      "train loss:0.08981465208197673\n",
      "train loss:0.037823078665487946\n",
      "train loss:0.03300866066227972\n",
      "train loss:0.022241595885771803\n",
      "train loss:0.0419977316012365\n",
      "train loss:0.058398210467227755\n",
      "train loss:0.0382625045004557\n",
      "train loss:0.06638961582297265\n",
      "train loss:0.11567678957641064\n",
      "train loss:0.0291786595165531\n",
      "train loss:0.028544493763111724\n",
      "train loss:0.1744740638243912\n",
      "train loss:0.060590138958188204\n",
      "train loss:0.07054099961094316\n",
      "train loss:0.07028821360116132\n",
      "train loss:0.0492226307963221\n",
      "train loss:0.043846399386556316\n",
      "train loss:0.05119460684408238\n",
      "train loss:0.05679816129292926\n",
      "train loss:0.07906977945532495\n",
      "train loss:0.03884361041318235\n",
      "train loss:0.03472744768870137\n",
      "train loss:0.025712930094326486\n",
      "train loss:0.04698927713284446\n",
      "train loss:0.04750359219073864\n",
      "train loss:0.05300777147864015\n",
      "train loss:0.04441934842423047\n",
      "train loss:0.08698126493377016\n",
      "train loss:0.032009875187299\n",
      "train loss:0.018060780167402906\n",
      "train loss:0.04664009937279328\n",
      "train loss:0.042577978195752035\n",
      "train loss:0.06091969791645055\n",
      "train loss:0.015068523567949403\n",
      "train loss:0.0642110653523305\n",
      "train loss:0.015354460822338233\n",
      "train loss:0.07468281625960418\n",
      "train loss:0.04138967959516464\n",
      "train loss:0.019891195630481293\n",
      "train loss:0.029218982229341172\n",
      "train loss:0.06960083343464812\n",
      "train loss:0.08738819249874608\n",
      "train loss:0.013442018552310473\n",
      "train loss:0.09543275752301533\n",
      "train loss:0.028740202051289812\n",
      "train loss:0.025385476489454894\n",
      "train loss:0.06150971231173545\n",
      "train loss:0.0418726832908118\n",
      "train loss:0.0673605654160471\n",
      "train loss:0.04252807913745823\n",
      "train loss:0.11951541688706017\n",
      "train loss:0.021669010418008377\n",
      "train loss:0.06605197784052814\n",
      "train loss:0.04487765350283742\n",
      "train loss:0.10561621839463484\n",
      "train loss:0.043639661797672584\n",
      "train loss:0.03411578225736712\n",
      "train loss:0.008568199396265568\n",
      "train loss:0.02801730476003414\n",
      "train loss:0.027983640830205426\n",
      "train loss:0.06122465492712016\n",
      "train loss:0.025054221296516414\n",
      "train loss:0.016450777558677587\n",
      "train loss:0.11553793782292782\n",
      "train loss:0.039556539920740194\n",
      "train loss:0.0628450282740311\n",
      "train loss:0.03285088504103809\n",
      "train loss:0.06865745406331547\n",
      "train loss:0.06079135770288699\n",
      "train loss:0.06414562489493254\n",
      "train loss:0.02361329261034872\n",
      "train loss:0.05241357595346569\n",
      "train loss:0.05006649208619658\n",
      "train loss:0.029024633930240976\n",
      "train loss:0.06078245146577401\n",
      "train loss:0.05673314989201099\n",
      "train loss:0.04577233659740362\n",
      "train loss:0.019452162900556066\n",
      "train loss:0.03550772705321806\n",
      "train loss:0.09468440010964221\n",
      "train loss:0.023192708015521965\n",
      "train loss:0.09197992738369909\n",
      "train loss:0.03797555985100694\n",
      "train loss:0.0559277364396793\n",
      "train loss:0.04352667311481539\n",
      "train loss:0.08179563348829705\n",
      "train loss:0.048369914892781744\n",
      "train loss:0.05949020719977075\n",
      "train loss:0.15551466376256418\n",
      "train loss:0.05886358992480775\n",
      "train loss:0.1460390992584161\n",
      "train loss:0.04236190149166866\n",
      "train loss:0.07585677338789448\n",
      "train loss:0.07796880634593967\n",
      "train loss:0.09434046690589583\n",
      "train loss:0.04721442840224328\n",
      "train loss:0.10065282215850584\n",
      "train loss:0.09551316029379837\n",
      "train loss:0.044106013597584114\n",
      "train loss:0.06037591407692698\n",
      "train loss:0.04982805108899718\n",
      "train loss:0.08863979559858008\n",
      "train loss:0.03269984615078442\n",
      "train loss:0.052132964532074874\n",
      "train loss:0.011640190216721625\n",
      "train loss:0.030233301206793274\n",
      "train loss:0.025877031623654827\n",
      "train loss:0.024019023170661295\n",
      "train loss:0.03146960017098309\n",
      "train loss:0.08335995952088596\n",
      "train loss:0.07302569773070536\n",
      "train loss:0.05001486698824576\n",
      "train loss:0.02177423045138375\n",
      "train loss:0.022635764270248097\n",
      "train loss:0.15121839250389305\n",
      "train loss:0.009747597871446759\n",
      "train loss:0.024842730643070566\n",
      "train loss:0.020189416526725935\n",
      "train loss:0.10087278647870022\n",
      "train loss:0.06801603303709497\n",
      "train loss:0.0741840826919969\n",
      "train loss:0.045590349698532436\n",
      "train loss:0.03082322379060582\n",
      "train loss:0.08549642400345675\n",
      "train loss:0.07900353970128723\n",
      "train loss:0.041388630464073276\n",
      "train loss:0.02028214877931269\n",
      "train loss:0.03188337899512659\n",
      "train loss:0.15282434681122403\n",
      "train loss:0.11123225696764485\n",
      "train loss:0.060700347537290054\n",
      "train loss:0.04550449557494941\n",
      "train loss:0.06542315547441471\n",
      "train loss:0.02914531604364509\n",
      "train loss:0.09696167931467295\n",
      "train loss:0.07263378930534653\n",
      "train loss:0.061141574239798005\n",
      "train loss:0.04814767044123155\n",
      "train loss:0.026060723059033886\n",
      "train loss:0.03469709039241021\n",
      "train loss:0.06519742380985878\n",
      "train loss:0.05542344288574078\n",
      "train loss:0.051092165178773\n",
      "train loss:0.047617549053425766\n",
      "train loss:0.0965133010047175\n",
      "train loss:0.06467043790523642\n",
      "train loss:0.01737977180141269\n",
      "train loss:0.01092957394136505\n",
      "train loss:0.06271218031209415\n",
      "train loss:0.10505181439771176\n",
      "train loss:0.03102252142113338\n",
      "train loss:0.11836293323222975\n",
      "train loss:0.11520574136737192\n",
      "train loss:0.0218430303908539\n",
      "train loss:0.03954582272800309\n",
      "train loss:0.08406171486949038\n",
      "train loss:0.06405289064674412\n",
      "train loss:0.05247659448767589\n",
      "train loss:0.09281522432989417\n",
      "train loss:0.06700166049364288\n",
      "train loss:0.07874442734682127\n",
      "train loss:0.09005881305509078\n",
      "train loss:0.016441726496018164\n",
      "train loss:0.014990744665930766\n",
      "train loss:0.03464736521776696\n",
      "train loss:0.09795510379716538\n",
      "train loss:0.008933209893992487\n",
      "train loss:0.03540856602036357\n",
      "train loss:0.05965634789062225\n",
      "train loss:0.08163732461684049\n",
      "train loss:0.026237773475688737\n",
      "train loss:0.0122959235172177\n",
      "train loss:0.06123748646118198\n",
      "train loss:0.09157972992676472\n",
      "train loss:0.0815855881083689\n",
      "train loss:0.06938802980606826\n",
      "train loss:0.021717415768563106\n",
      "train loss:0.08896149300204881\n",
      "train loss:0.08019942263959413\n",
      "train loss:0.04068115027098153\n",
      "train loss:0.04827891658828782\n",
      "train loss:0.04211396475853567\n",
      "train loss:0.027381715422965476\n",
      "train loss:0.030626761342166228\n",
      "train loss:0.0667430464446082\n",
      "train loss:0.0433115183231281\n",
      "train loss:0.05911531188609395\n",
      "train loss:0.037350224565456024\n",
      "train loss:0.062317717343006694\n",
      "train loss:0.0339344320856599\n",
      "train loss:0.017094771790037523\n",
      "train loss:0.05299476868448438\n",
      "train loss:0.06894601351508\n",
      "train loss:0.08774130154833464\n",
      "train loss:0.058696523649442396\n",
      "train loss:0.02163608774030132\n",
      "train loss:0.04971770504822941\n",
      "train loss:0.10273908989043712\n",
      "train loss:0.10504272513352195\n",
      "train loss:0.02653172606565907\n",
      "train loss:0.14042679586297402\n",
      "train loss:0.07924535466331091\n",
      "train loss:0.020784794882409762\n",
      "train loss:0.06323944785579447\n",
      "train loss:0.019546238682960116\n",
      "train loss:0.02997490274075638\n",
      "train loss:0.1829497384601884\n",
      "train loss:0.1073428522340156\n",
      "train loss:0.027457990193215195\n",
      "train loss:0.0644721470414867\n",
      "train loss:0.08346842476817656\n",
      "train loss:0.029150505591319216\n",
      "train loss:0.06797246339397581\n",
      "train loss:0.025439261313554992\n",
      "train loss:0.02284653830250884\n",
      "train loss:0.034919593789145276\n",
      "train loss:0.05060101171840206\n",
      "train loss:0.09400171364500964\n",
      "train loss:0.050324351244265556\n",
      "train loss:0.054676807923305176\n",
      "train loss:0.00891436634540608\n",
      "train loss:0.09461311391499928\n",
      "train loss:0.09105846554472864\n",
      "train loss:0.12762167189849521\n",
      "train loss:0.04906388330624875\n",
      "train loss:0.03167043338067674\n",
      "train loss:0.04798500326840256\n",
      "train loss:0.06829277454186919\n",
      "train loss:0.03457302384692723\n",
      "train loss:0.024900342849090443\n",
      "train loss:0.07298840801499305\n",
      "train loss:0.028422919095815232\n",
      "train loss:0.04139928677660138\n",
      "train loss:0.06232239342376171\n",
      "train loss:0.026904088777501074\n",
      "train loss:0.016750449256121274\n",
      "train loss:0.05101071547586737\n",
      "train loss:0.04791069150607174\n",
      "train loss:0.030640367911729405\n",
      "train loss:0.1219512356795227\n",
      "train loss:0.14727810035504832\n",
      "train loss:0.06023192515995804\n",
      "train loss:0.03776734439691037\n",
      "train loss:0.030251210789137187\n",
      "train loss:0.06030521335031262\n",
      "train loss:0.08640165885553476\n",
      "train loss:0.06712079800711987\n",
      "train loss:0.03311439719348619\n",
      "train loss:0.026927111115443277\n",
      "train loss:0.0074710883671855535\n",
      "train loss:0.06658850051790542\n",
      "train loss:0.018868880299078804\n",
      "train loss:0.04096792324011009\n",
      "train loss:0.04940394083949472\n",
      "train loss:0.03929662357039552\n",
      "train loss:0.03702997940441946\n",
      "train loss:0.02240679756834164\n",
      "train loss:0.0833145949115454\n",
      "train loss:0.022821171902262263\n",
      "train loss:0.057023436446897724\n",
      "train loss:0.14222232823878111\n",
      "train loss:0.07001343200968836\n",
      "train loss:0.07116427944487533\n",
      "train loss:0.03429484224089925\n",
      "train loss:0.08941712182532012\n",
      "train loss:0.03228131332761754\n",
      "train loss:0.013176450431299159\n",
      "train loss:0.03146035809835992\n",
      "train loss:0.1154250219829019\n",
      "train loss:0.06797801801432801\n",
      "train loss:0.014288636234896242\n",
      "train loss:0.025554180668580413\n",
      "train loss:0.030807762694772464\n",
      "train loss:0.0391076969281071\n",
      "train loss:0.01649758541373948\n",
      "train loss:0.00693477724008745\n",
      "train loss:0.04227122806572516\n",
      "train loss:0.05412510754824301\n",
      "train loss:0.03670659277741459\n",
      "train loss:0.09157825091687136\n",
      "train loss:0.01955551834284054\n",
      "train loss:0.020526004145209643\n",
      "train loss:0.02630520806382898\n",
      "train loss:0.04572253517821005\n",
      "train loss:0.01867996076852831\n",
      "train loss:0.025871775054322664\n",
      "train loss:0.030774835384933783\n",
      "train loss:0.07234262417168832\n",
      "train loss:0.09487493769828237\n",
      "train loss:0.05892488181782567\n",
      "train loss:0.05131490995797752\n",
      "train loss:0.010961451422309174\n",
      "train loss:0.015033053034981751\n",
      "train loss:0.04032985216988709\n",
      "train loss:0.03050257638634215\n",
      "train loss:0.020541560879285842\n",
      "train loss:0.015792655686670112\n",
      "train loss:0.008187841421297156\n",
      "train loss:0.1244081462619181\n",
      "train loss:0.06491933112992289\n",
      "train loss:0.12072995414276484\n",
      "train loss:0.03363359211514112\n",
      "train loss:0.02339488107802859\n",
      "train loss:0.06588458743172912\n",
      "train loss:0.01776955017759489\n",
      "train loss:0.06026164833938897\n",
      "train loss:0.040193371880992125\n",
      "train loss:0.07558020362409427\n",
      "train loss:0.12191564495090962\n",
      "train loss:0.028514589575638963\n",
      "train loss:0.08778741135289504\n",
      "train loss:0.06148706258241808\n",
      "train loss:0.08595873427881584\n",
      "train loss:0.04063497431216457\n",
      "train loss:0.09052351939092143\n",
      "train loss:0.08416001635855594\n",
      "train loss:0.01906207243614031\n",
      "train loss:0.0650381986972518\n",
      "train loss:0.0939931252811274\n",
      "train loss:0.05681593372690444\n",
      "train loss:0.039158210762018066\n",
      "train loss:0.022813545384065298\n",
      "train loss:0.05761035863097835\n",
      "train loss:0.05194939396655433\n",
      "train loss:0.01946807926431495\n",
      "train loss:0.01600398811915605\n",
      "train loss:0.09150189370963822\n",
      "train loss:0.05026303242549372\n",
      "train loss:0.03477919910025171\n",
      "train loss:0.036016901952054925\n",
      "train loss:0.05407371927966891\n",
      "train loss:0.12092492063999088\n",
      "train loss:0.01611809809707161\n",
      "train loss:0.03163190857772771\n",
      "train loss:0.13607888524312994\n",
      "train loss:0.08716035181949099\n",
      "train loss:0.025401642960149306\n",
      "train loss:0.04608001314276687\n",
      "train loss:0.0215501937539485\n",
      "train loss:0.0885223446483207\n",
      "train loss:0.05864020476285301\n",
      "train loss:0.03019355397293955\n",
      "train loss:0.012269882522545038\n",
      "train loss:0.04762064581891979\n",
      "train loss:0.03735758860086821\n",
      "train loss:0.09340900472100748\n",
      "train loss:0.04500787825499269\n",
      "train loss:0.03337588236179974\n",
      "train loss:0.07027352174848589\n",
      "train loss:0.03167998550499607\n",
      "train loss:0.031171949251561817\n",
      "train loss:0.033382545317025225\n",
      "train loss:0.042828660420437314\n",
      "train loss:0.03791681584932207\n",
      "train loss:0.042338516583556945\n",
      "train loss:0.03838664240570615\n",
      "train loss:0.046417737297178824\n",
      "train loss:0.02023327050161696\n",
      "train loss:0.03276125231517691\n",
      "=== epoch:4, train acc:0.986, test acc:0.98 ===\n",
      "train loss:0.09790377319085772\n",
      "train loss:0.02487187274025052\n",
      "train loss:0.10031034526291657\n",
      "train loss:0.06667994380624682\n",
      "train loss:0.06546637667783056\n",
      "train loss:0.031205678900253032\n",
      "train loss:0.11754767982035708\n",
      "train loss:0.022208446247975532\n",
      "train loss:0.013776473233712198\n",
      "train loss:0.016242029501457665\n",
      "train loss:0.041925237512103024\n",
      "train loss:0.02375367874400322\n",
      "train loss:0.05806732172764661\n",
      "train loss:0.06872939014265624\n",
      "train loss:0.02950487052783248\n",
      "train loss:0.06558050706398458\n",
      "train loss:0.013890929018504122\n",
      "train loss:0.06284810439965269\n",
      "train loss:0.0365743141889036\n",
      "train loss:0.03035913341195557\n",
      "train loss:0.05349930473379198\n",
      "train loss:0.05867675581770364\n",
      "train loss:0.029086326179365473\n",
      "train loss:0.0680964020861835\n",
      "train loss:0.02684074019119715\n",
      "train loss:0.02603835398584582\n",
      "train loss:0.08924426925943912\n",
      "train loss:0.13020141880249658\n",
      "train loss:0.09114515707699575\n",
      "train loss:0.03135944482756302\n",
      "train loss:0.013129384601562589\n",
      "train loss:0.04350762052002492\n",
      "train loss:0.020057795696657964\n",
      "train loss:0.03649490945594053\n",
      "train loss:0.06066042021774971\n",
      "train loss:0.026536194611640612\n",
      "train loss:0.029100307041240638\n",
      "train loss:0.021013935049574953\n",
      "train loss:0.0258834824543172\n",
      "train loss:0.09245930954024473\n",
      "train loss:0.042910515995678813\n",
      "train loss:0.043011339777468585\n",
      "train loss:0.02718167159056138\n",
      "train loss:0.035331800957772636\n",
      "train loss:0.009541908865082173\n",
      "train loss:0.11603245891036604\n",
      "train loss:0.034002548867546535\n",
      "train loss:0.0225811242289788\n",
      "train loss:0.05797380753441522\n",
      "train loss:0.043378963759145774\n",
      "train loss:0.05085933515105756\n",
      "train loss:0.05198829708202531\n",
      "train loss:0.05431417073796324\n",
      "train loss:0.05277352830304223\n",
      "train loss:0.03302821296126588\n",
      "train loss:0.016521169703496846\n",
      "train loss:0.017933717224433095\n",
      "train loss:0.04619331735495763\n",
      "train loss:0.015994085521653816\n",
      "train loss:0.01846514256211282\n",
      "train loss:0.038685799221402875\n",
      "train loss:0.050018704178385685\n",
      "train loss:0.0558732618131645\n",
      "train loss:0.040938380187204866\n",
      "train loss:0.0397530556008176\n",
      "train loss:0.04150367680867578\n",
      "train loss:0.029909981702310882\n",
      "train loss:0.014685583734281957\n",
      "train loss:0.013704975681737414\n",
      "train loss:0.019152836820868716\n",
      "train loss:0.053641300018793324\n",
      "train loss:0.04227897143246527\n",
      "train loss:0.018945815752813108\n",
      "train loss:0.10238493687468006\n",
      "train loss:0.060597087976519214\n",
      "train loss:0.024063252000346525\n",
      "train loss:0.03445677693869549\n",
      "train loss:0.08687382240531123\n",
      "train loss:0.025959367370684\n",
      "train loss:0.07087345353111205\n",
      "train loss:0.09563065334122633\n",
      "train loss:0.0594987829361531\n",
      "train loss:0.036142256677946356\n",
      "train loss:0.047798695908201516\n",
      "train loss:0.07649418059292715\n",
      "train loss:0.06561828163553377\n",
      "train loss:0.019579972889264413\n",
      "train loss:0.028570248437134436\n",
      "train loss:0.037414020691894596\n",
      "train loss:0.04926211861953351\n",
      "train loss:0.03866783319509684\n",
      "train loss:0.11062087259858666\n",
      "train loss:0.011026514154545999\n",
      "train loss:0.03072638700818412\n",
      "train loss:0.012557112837005644\n",
      "train loss:0.06909027140304656\n",
      "train loss:0.014946515512848783\n",
      "train loss:0.05092228046178204\n",
      "train loss:0.014606150951864308\n",
      "train loss:0.1282650082952942\n",
      "train loss:0.013636550844798393\n",
      "train loss:0.022216313176430903\n",
      "train loss:0.025963819464178573\n",
      "train loss:0.03607704671742863\n",
      "train loss:0.011574016888360496\n",
      "train loss:0.01700454054649043\n",
      "train loss:0.05906798437599848\n",
      "train loss:0.01716275019972479\n",
      "train loss:0.017260630210289646\n",
      "train loss:0.05231512213731713\n",
      "train loss:0.07985513478421764\n",
      "train loss:0.050346826989049874\n",
      "train loss:0.02885691636340257\n",
      "train loss:0.04809018657103003\n",
      "train loss:0.00881847533040584\n",
      "train loss:0.03536706161357997\n",
      "train loss:0.03608187396566226\n",
      "train loss:0.03438743015924986\n",
      "train loss:0.05264819515461702\n",
      "train loss:0.012866584193199531\n",
      "train loss:0.03482254328702194\n",
      "train loss:0.1829456322134426\n",
      "train loss:0.04467332260772878\n",
      "train loss:0.015909174770463032\n",
      "train loss:0.06702541895433617\n",
      "train loss:0.17190643913358833\n",
      "train loss:0.033615874475432024\n",
      "train loss:0.07046200426463999\n",
      "train loss:0.00958218419177502\n",
      "train loss:0.046190800647280435\n",
      "train loss:0.016181683124493723\n",
      "train loss:0.04078089230210893\n",
      "train loss:0.08073521431216428\n",
      "train loss:0.012567215460521362\n",
      "train loss:0.011831506006271171\n",
      "train loss:0.014005752811589015\n",
      "train loss:0.0994545788695365\n",
      "train loss:0.02417924434800276\n",
      "train loss:0.02944550221775477\n",
      "train loss:0.01592877068990242\n",
      "train loss:0.03882341825280607\n",
      "train loss:0.07950236286031295\n",
      "train loss:0.037395668781330944\n",
      "train loss:0.03165223214730884\n",
      "train loss:0.01773952760690231\n",
      "train loss:0.030862071712255013\n",
      "train loss:0.04591614536528228\n",
      "train loss:0.018267220780590662\n",
      "train loss:0.16927629198411406\n",
      "train loss:0.01877420950980855\n",
      "train loss:0.06208456105010471\n",
      "train loss:0.04554528272580766\n",
      "train loss:0.0846239014932507\n",
      "train loss:0.058913277779426274\n",
      "train loss:0.016248575623120015\n",
      "train loss:0.02592422239365763\n",
      "train loss:0.08653391082053644\n",
      "train loss:0.0993272397129484\n",
      "train loss:0.03296081385965911\n",
      "train loss:0.050559129071437024\n",
      "train loss:0.016379030368392482\n",
      "train loss:0.033484860618316285\n",
      "train loss:0.013503800230884478\n",
      "train loss:0.02561949904213173\n",
      "train loss:0.02292753823850311\n",
      "train loss:0.021546375764605896\n",
      "train loss:0.09108332873643026\n",
      "train loss:0.08234019090263436\n",
      "train loss:0.01694114955193446\n",
      "train loss:0.048627098685754565\n",
      "train loss:0.02235184001703123\n",
      "train loss:0.03061039315928628\n",
      "train loss:0.019174908613367344\n",
      "train loss:0.05330141397509907\n",
      "train loss:0.009071110340220583\n",
      "train loss:0.02379405019001027\n",
      "train loss:0.13713249318059517\n",
      "train loss:0.024311588802261173\n",
      "train loss:0.033767398401618284\n",
      "train loss:0.011721265012946293\n",
      "train loss:0.018992989389218302\n",
      "train loss:0.042839718098409814\n",
      "train loss:0.04895981469904263\n",
      "train loss:0.086664658726923\n",
      "train loss:0.02834023688337163\n",
      "train loss:0.019801893600041084\n",
      "train loss:0.14772825287603208\n",
      "train loss:0.04459360202067639\n",
      "train loss:0.029084711769627028\n",
      "train loss:0.04738751399597223\n",
      "train loss:0.04119732666859062\n",
      "train loss:0.03253525768065808\n",
      "train loss:0.008837281669662385\n",
      "train loss:0.01890352336295382\n",
      "train loss:0.026796837725313084\n",
      "train loss:0.01201088240631665\n",
      "train loss:0.1276774303663117\n",
      "train loss:0.035464784078001195\n",
      "train loss:0.03168303450248957\n",
      "train loss:0.07905181348257959\n",
      "train loss:0.05340325707398137\n",
      "train loss:0.005212921119224846\n",
      "train loss:0.05516941582960268\n",
      "train loss:0.026918630916619876\n",
      "train loss:0.05834310210946879\n",
      "train loss:0.02533240872317019\n",
      "train loss:0.029587637198622733\n",
      "train loss:0.04848075111696246\n",
      "train loss:0.0457865240465849\n",
      "train loss:0.045136915918782626\n",
      "train loss:0.018253213318837024\n",
      "train loss:0.02481326665068269\n",
      "train loss:0.03220726952703989\n",
      "train loss:0.049742603369936196\n",
      "train loss:0.02702783984848597\n",
      "train loss:0.033245929060329825\n",
      "train loss:0.019462419126159463\n",
      "train loss:0.04275674469609131\n",
      "train loss:0.021327879934986475\n",
      "train loss:0.031321467232825535\n",
      "train loss:0.04324753153236163\n",
      "train loss:0.06179322385763711\n",
      "train loss:0.01167021078090844\n",
      "train loss:0.012614620864260583\n",
      "train loss:0.07701758278879653\n",
      "train loss:0.04205754086897604\n",
      "train loss:0.046834538451734416\n",
      "train loss:0.01104613234788068\n",
      "train loss:0.06810398111277134\n",
      "train loss:0.03484730198745746\n",
      "train loss:0.008427731088380514\n",
      "train loss:0.01728113835473353\n",
      "train loss:0.050270174918179276\n",
      "train loss:0.041460528340093644\n",
      "train loss:0.04431992624584323\n",
      "train loss:0.015631942048092543\n",
      "train loss:0.017209225665280683\n",
      "train loss:0.03495096660229505\n",
      "train loss:0.040410735759810794\n",
      "train loss:0.02695942255870282\n",
      "train loss:0.025842179000143628\n",
      "train loss:0.0618566285515486\n",
      "train loss:0.03782578900279379\n",
      "train loss:0.020703757230330896\n",
      "train loss:0.01792769949703029\n",
      "train loss:0.03824125663217922\n",
      "train loss:0.062347710062531585\n",
      "train loss:0.011697301796014565\n",
      "train loss:0.08405230206770119\n",
      "train loss:0.02883187036793969\n",
      "train loss:0.06407434685459014\n",
      "train loss:0.13526780066753635\n",
      "train loss:0.03652858094530372\n",
      "train loss:0.04087930577436846\n",
      "train loss:0.08678446978307561\n",
      "train loss:0.0338312009477354\n",
      "train loss:0.01592623532802794\n",
      "train loss:0.12938146807004486\n",
      "train loss:0.033371459299980824\n",
      "train loss:0.04629011245341091\n",
      "train loss:0.022389584828898624\n",
      "train loss:0.024867446631970783\n",
      "train loss:0.011791017020083972\n",
      "train loss:0.0655485235426101\n",
      "train loss:0.07161302101945485\n",
      "train loss:0.045447404638850095\n",
      "train loss:0.0179863188945101\n",
      "train loss:0.025027323757474625\n",
      "train loss:0.04938454837571442\n",
      "train loss:0.031003075006330958\n",
      "train loss:0.024807618145139113\n",
      "train loss:0.13656048884651406\n",
      "train loss:0.046803239981220116\n",
      "train loss:0.04601419609439153\n",
      "train loss:0.035919538328634405\n",
      "train loss:0.02089158411859249\n",
      "train loss:0.07600117662268029\n",
      "train loss:0.008191766499740459\n",
      "train loss:0.025153135815013692\n",
      "train loss:0.03702489023184457\n",
      "train loss:0.04239165875630719\n",
      "train loss:0.05839532894189507\n",
      "train loss:0.046852071077328465\n",
      "train loss:0.03690116977247936\n",
      "train loss:0.022571096179390994\n",
      "train loss:0.012994356834894048\n",
      "train loss:0.09576259834267895\n",
      "train loss:0.04701725538173837\n",
      "train loss:0.033127935152477465\n",
      "train loss:0.0797624466399622\n",
      "train loss:0.03620959548147181\n",
      "train loss:0.07562246513598493\n",
      "train loss:0.03835750810746825\n",
      "train loss:0.03872558293343122\n",
      "train loss:0.005966585998535889\n",
      "train loss:0.027085677693341553\n",
      "train loss:0.010896437449052799\n",
      "train loss:0.05288703897804936\n",
      "train loss:0.024147738264602175\n",
      "train loss:0.04301304822981615\n",
      "train loss:0.032375696671938174\n",
      "train loss:0.010759448125951863\n",
      "train loss:0.035257938962602944\n",
      "train loss:0.03753449825725242\n",
      "train loss:0.04590331440214435\n",
      "train loss:0.03912297631765494\n",
      "train loss:0.00591581768048094\n",
      "train loss:0.11476426710493866\n",
      "train loss:0.035357179844130185\n",
      "train loss:0.059470255051899484\n",
      "train loss:0.0760672147185628\n",
      "train loss:0.024046091131521615\n",
      "train loss:0.09292330473544366\n",
      "train loss:0.01464851688556865\n",
      "train loss:0.040505813085868786\n",
      "train loss:0.03463492542415315\n",
      "train loss:0.022682744637187176\n",
      "train loss:0.05209902290399635\n",
      "train loss:0.04088903025549774\n",
      "train loss:0.03070094617336935\n",
      "train loss:0.027884015650234125\n",
      "train loss:0.04545021003207492\n",
      "train loss:0.04652115962227226\n",
      "train loss:0.02713292444944376\n",
      "train loss:0.04091923087698881\n",
      "train loss:0.034766913375646906\n",
      "train loss:0.15354686504151768\n",
      "train loss:0.06461441102087562\n",
      "train loss:0.03476359535546544\n",
      "train loss:0.03307221056737517\n",
      "train loss:0.035741429792403384\n",
      "train loss:0.014259567722184209\n",
      "train loss:0.09716327431380714\n",
      "train loss:0.018343361919741744\n",
      "train loss:0.03616640920647552\n",
      "train loss:0.011845079556338407\n",
      "train loss:0.02187972697811451\n",
      "train loss:0.04035737354237212\n",
      "train loss:0.09467793343697074\n",
      "train loss:0.03880504535117687\n",
      "train loss:0.021781230049085703\n",
      "train loss:0.021075073178197696\n",
      "train loss:0.022002495561709905\n",
      "train loss:0.01317926356061088\n",
      "train loss:0.022733439575415643\n",
      "train loss:0.04439363451829826\n",
      "train loss:0.07087731589145176\n",
      "train loss:0.02598289222149039\n",
      "train loss:0.14290099045619972\n",
      "train loss:0.005589192119688642\n",
      "train loss:0.05099162673760897\n",
      "train loss:0.02314512078466997\n",
      "train loss:0.06038857023410964\n",
      "train loss:0.08087647383329927\n",
      "train loss:0.04796463897651992\n",
      "train loss:0.06562733125652839\n",
      "train loss:0.014941416979874245\n",
      "train loss:0.009519835142884028\n",
      "train loss:0.08892925490693697\n",
      "train loss:0.009252653591731993\n",
      "train loss:0.04318139492501514\n",
      "train loss:0.04514495608974316\n",
      "train loss:0.03144050575806838\n",
      "train loss:0.04939988422710577\n",
      "train loss:0.06440004388780911\n",
      "train loss:0.025872874066947427\n",
      "train loss:0.06320647174000323\n",
      "train loss:0.030720410860457358\n",
      "train loss:0.0534418877321203\n",
      "train loss:0.032326429934940795\n",
      "train loss:0.02893049837581495\n",
      "train loss:0.06504611059029602\n",
      "train loss:0.05993779916527834\n",
      "train loss:0.0216564630857409\n",
      "train loss:0.07021348902569069\n",
      "train loss:0.026932388655852294\n",
      "train loss:0.026020843870125442\n",
      "train loss:0.05995348197772349\n",
      "train loss:0.09675923162432225\n",
      "train loss:0.008581994756829532\n",
      "train loss:0.1061778572857238\n",
      "train loss:0.028503729719590476\n",
      "train loss:0.019849328793345493\n",
      "train loss:0.022192818915570935\n",
      "train loss:0.01740637538854461\n",
      "train loss:0.06310317388689493\n",
      "train loss:0.040155785709633435\n",
      "train loss:0.04858854480839474\n",
      "train loss:0.012188248455992754\n",
      "train loss:0.008547527682632535\n",
      "train loss:0.01192128056205257\n",
      "train loss:0.020226018415152643\n",
      "train loss:0.011943064730402686\n",
      "train loss:0.09084451989971443\n",
      "train loss:0.03274599593614111\n",
      "train loss:0.03554173079798042\n",
      "train loss:0.03346946232308485\n",
      "train loss:0.035472114824684514\n",
      "train loss:0.04670780676731764\n",
      "train loss:0.03657134707084431\n",
      "train loss:0.06193237780008496\n",
      "train loss:0.06999218402015112\n",
      "train loss:0.1314550194787774\n",
      "train loss:0.02137319469033707\n",
      "train loss:0.02157062938210776\n",
      "train loss:0.04972533167618992\n",
      "train loss:0.06685322464633336\n",
      "train loss:0.013862791096919623\n",
      "train loss:0.05847981677990214\n",
      "train loss:0.06269530117947496\n",
      "train loss:0.03593761326946763\n",
      "train loss:0.057675555864659354\n",
      "train loss:0.04121647793613414\n",
      "train loss:0.03832150792757017\n",
      "train loss:0.05426120432464804\n",
      "train loss:0.12224681563064754\n",
      "train loss:0.10837212782964922\n",
      "train loss:0.07529843551751239\n",
      "train loss:0.012098972785592996\n",
      "train loss:0.03373282370068097\n",
      "train loss:0.015632609624267817\n",
      "train loss:0.06815283663165832\n",
      "train loss:0.011628682868517965\n",
      "train loss:0.035832362292273025\n",
      "train loss:0.06999371038646285\n",
      "train loss:0.07096347850214997\n",
      "train loss:0.023657117050903217\n",
      "train loss:0.030338798670506043\n",
      "train loss:0.08428011330480338\n",
      "train loss:0.011885673013780938\n",
      "train loss:0.03732207247736594\n",
      "train loss:0.02884923382569847\n",
      "train loss:0.05613541915008915\n",
      "train loss:0.019876869754906778\n",
      "train loss:0.024924873283199034\n",
      "train loss:0.06178510310027899\n",
      "train loss:0.07108593894614948\n",
      "train loss:0.06615288239649592\n",
      "train loss:0.021460883000865842\n",
      "train loss:0.04901323024020885\n",
      "train loss:0.0210504015737522\n",
      "train loss:0.01749496236446722\n",
      "train loss:0.012096593113809555\n",
      "train loss:0.021323291051950476\n",
      "train loss:0.018149876117371785\n",
      "train loss:0.025718227508668912\n",
      "train loss:0.018887849333678795\n",
      "train loss:0.02541176715491388\n",
      "train loss:0.05437992783736993\n",
      "train loss:0.023889089490703896\n",
      "train loss:0.04936995137947881\n",
      "train loss:0.027385690813806422\n",
      "train loss:0.038736081303076646\n",
      "train loss:0.043887520043068874\n",
      "train loss:0.03978733038370381\n",
      "train loss:0.03086144575785776\n",
      "train loss:0.00828263764567736\n",
      "train loss:0.021429438775452066\n",
      "train loss:0.009423640692991205\n",
      "train loss:0.044532104268391375\n",
      "train loss:0.013570668655087697\n",
      "train loss:0.022487627727997027\n",
      "train loss:0.07935173854949935\n",
      "train loss:0.07710944098847519\n",
      "train loss:0.041777467477089214\n",
      "train loss:0.03897206119358762\n",
      "train loss:0.06461523145365651\n",
      "train loss:0.025396246637160134\n",
      "train loss:0.01694218764524066\n",
      "train loss:0.023584331862101474\n",
      "train loss:0.07425158586043742\n",
      "train loss:0.022648084532366105\n",
      "train loss:0.007557395082066659\n",
      "train loss:0.030170675010386797\n",
      "train loss:0.03564113170436425\n",
      "train loss:0.08036950740067535\n",
      "train loss:0.05161156632202301\n",
      "train loss:0.025989522529474948\n",
      "train loss:0.030707198328837032\n",
      "train loss:0.05288783974596766\n",
      "train loss:0.03611430028679027\n",
      "train loss:0.01589247691604586\n",
      "train loss:0.0295149138269085\n",
      "train loss:0.025409640450198813\n",
      "train loss:0.04543826896138447\n",
      "train loss:0.01220449137529324\n",
      "train loss:0.011773366659424484\n",
      "train loss:0.014516258763458509\n",
      "train loss:0.05094102036081579\n",
      "train loss:0.007686769782045313\n",
      "train loss:0.012959154894765519\n",
      "train loss:0.023786886647916398\n",
      "train loss:0.08732948642757711\n",
      "train loss:0.03521558875300406\n",
      "train loss:0.010502806189242275\n",
      "train loss:0.022469277375636164\n",
      "train loss:0.022075404415793656\n",
      "train loss:0.03884364342027094\n",
      "train loss:0.06530587971530037\n",
      "train loss:0.05558287160009805\n",
      "train loss:0.06827532265265711\n",
      "train loss:0.014558724247534578\n",
      "train loss:0.040406666554984344\n",
      "train loss:0.014631762619719258\n",
      "train loss:0.01501784164283056\n",
      "train loss:0.04528171761972226\n",
      "train loss:0.011909810845413347\n",
      "train loss:0.019294522558409827\n",
      "train loss:0.020522712121684862\n",
      "train loss:0.027112372954854903\n",
      "train loss:0.01246640001390163\n",
      "train loss:0.021182339418780095\n",
      "train loss:0.032253731294057036\n",
      "train loss:0.03861142243398176\n",
      "train loss:0.022471933891512284\n",
      "train loss:0.05144782936975346\n",
      "train loss:0.052299093284265394\n",
      "train loss:0.010900803958868941\n",
      "train loss:0.015584152865434766\n",
      "train loss:0.0367634221164647\n",
      "train loss:0.03899441893776779\n",
      "train loss:0.043065228862641476\n",
      "train loss:0.013809256168387405\n",
      "train loss:0.014738229245617612\n",
      "train loss:0.02358050701397515\n",
      "train loss:0.025033301793551347\n",
      "train loss:0.040157119625826694\n",
      "train loss:0.09344119070867205\n",
      "train loss:0.03101686617770871\n",
      "train loss:0.02556481025561834\n",
      "train loss:0.052767560911461286\n",
      "train loss:0.04786759231187079\n",
      "train loss:0.029662798932507176\n",
      "train loss:0.03247364849732966\n",
      "train loss:0.030869630065879113\n",
      "train loss:0.023546911282002026\n",
      "train loss:0.02997566250081615\n",
      "train loss:0.019945985084775884\n",
      "train loss:0.029315570666925604\n",
      "train loss:0.020054701423992044\n",
      "train loss:0.03535766283059544\n",
      "train loss:0.016494287050294747\n",
      "train loss:0.025051443856849744\n",
      "train loss:0.011783171597825402\n",
      "train loss:0.04163615099928717\n",
      "train loss:0.03717729613499845\n",
      "train loss:0.05121540468041295\n",
      "train loss:0.07293955392780709\n",
      "train loss:0.03376252518352842\n",
      "train loss:0.05178416299889523\n",
      "train loss:0.019559560636447627\n",
      "train loss:0.0129451867747965\n",
      "train loss:0.03789033303492203\n",
      "train loss:0.07882265295278329\n",
      "train loss:0.015177218991133456\n",
      "train loss:0.04484138182984609\n",
      "train loss:0.04444976156830927\n",
      "train loss:0.06993623037755722\n",
      "train loss:0.031066043564462745\n",
      "train loss:0.014046330296100617\n",
      "train loss:0.03933053578116115\n",
      "train loss:0.013602038964634945\n",
      "train loss:0.06702139774837884\n",
      "train loss:0.05040818127410333\n",
      "train loss:0.010112526647569557\n",
      "train loss:0.022022472758669573\n",
      "train loss:0.022936942672917005\n",
      "train loss:0.04754903272645836\n",
      "train loss:0.06051405597741719\n",
      "train loss:0.017999309057040027\n",
      "train loss:0.04020187405996811\n",
      "train loss:0.012332295719415904\n",
      "train loss:0.0525140291443054\n",
      "train loss:0.03601241595006397\n",
      "train loss:0.095724691647898\n",
      "train loss:0.006224502807333055\n",
      "train loss:0.011482506258122393\n",
      "train loss:0.022274070131702493\n",
      "train loss:0.015232177206952994\n",
      "train loss:0.05683181128108185\n",
      "train loss:0.00424905454603206\n",
      "train loss:0.01938115187502346\n",
      "train loss:0.009329319077488558\n",
      "train loss:0.016828002721498897\n",
      "train loss:0.061565354563953685\n",
      "train loss:0.05083470562176128\n",
      "train loss:0.0201032010331398\n",
      "train loss:0.055717518279979415\n",
      "train loss:0.009451605758655467\n",
      "train loss:0.027546776319970965\n",
      "train loss:0.03990091461377179\n",
      "train loss:0.005422362248692044\n",
      "train loss:0.012138619724064771\n",
      "train loss:0.025662870631623324\n",
      "train loss:0.03263105705202687\n",
      "train loss:0.010601236798988112\n",
      "train loss:0.007919893744191909\n",
      "train loss:0.093492031485677\n",
      "train loss:0.01784295455977215\n",
      "train loss:0.09982421158024232\n",
      "=== epoch:5, train acc:0.986, test acc:0.98 ===\n",
      "train loss:0.03293566402333428\n",
      "train loss:0.01859333022095085\n",
      "train loss:0.011410838463285843\n",
      "train loss:0.029527513048843684\n",
      "train loss:0.04003635534650687\n",
      "train loss:0.01877615527749379\n",
      "train loss:0.052535079918200014\n",
      "train loss:0.07315244774175704\n",
      "train loss:0.030824256187168286\n",
      "train loss:0.012734279061325949\n",
      "train loss:0.011013475766716643\n",
      "train loss:0.02266321574573592\n",
      "train loss:0.019811612626854158\n",
      "train loss:0.090067379692794\n",
      "train loss:0.01568701880625387\n",
      "train loss:0.01456032981197942\n",
      "train loss:0.07647864936484239\n",
      "train loss:0.018645126503130008\n",
      "train loss:0.07141396934402955\n",
      "train loss:0.02048215370988787\n",
      "train loss:0.0327738262863855\n",
      "train loss:0.11832097363189463\n",
      "train loss:0.04652510064868583\n",
      "train loss:0.011716422561632675\n",
      "train loss:0.02514987576940329\n",
      "train loss:0.017387694804328634\n",
      "train loss:0.016735806936828114\n",
      "train loss:0.07181419462515334\n",
      "train loss:0.005789978151122549\n",
      "train loss:0.016869367115096644\n",
      "train loss:0.0728030235005695\n",
      "train loss:0.031490631508620893\n",
      "train loss:0.05253619706898453\n",
      "train loss:0.02615955786543588\n",
      "train loss:0.09370075602446523\n",
      "train loss:0.033534448876300534\n",
      "train loss:0.011862349662363382\n",
      "train loss:0.023134869204526665\n",
      "train loss:0.03689054428034705\n",
      "train loss:0.05533518949223534\n",
      "train loss:0.0314521775217393\n",
      "train loss:0.036742898191028134\n",
      "train loss:0.09831972578834294\n",
      "train loss:0.01674601181393461\n",
      "train loss:0.028336974586417703\n",
      "train loss:0.02744324656963229\n",
      "train loss:0.009493679294164298\n",
      "train loss:0.032407786612661804\n",
      "train loss:0.06052647730932329\n",
      "train loss:0.036057523527917495\n",
      "train loss:0.0915067950835902\n",
      "train loss:0.04047591489985566\n",
      "train loss:0.029249216716086\n",
      "train loss:0.024616724762928123\n",
      "train loss:0.011033392383428193\n",
      "train loss:0.03261012051021058\n",
      "train loss:0.016559635697863503\n",
      "train loss:0.01632665473735533\n",
      "train loss:0.030904724564926704\n",
      "train loss:0.08529007499132618\n",
      "train loss:0.02792711773610498\n",
      "train loss:0.024533587411046453\n",
      "train loss:0.04141672844105344\n",
      "train loss:0.029455610149733705\n",
      "train loss:0.027324241891433586\n",
      "train loss:0.008033747520840104\n",
      "train loss:0.020827077117811395\n",
      "train loss:0.05669521030491956\n",
      "train loss:0.03511548103104442\n",
      "train loss:0.007122933669919595\n",
      "train loss:0.023428291760200115\n",
      "train loss:0.014832562751608758\n",
      "train loss:0.03912072278534344\n",
      "train loss:0.017353671366481693\n",
      "train loss:0.020919100438564105\n",
      "train loss:0.05691619940460713\n",
      "train loss:0.060208859069748995\n",
      "train loss:0.01857779117499964\n",
      "train loss:0.018358705725742778\n",
      "train loss:0.01648160442178508\n",
      "train loss:0.023667193818888852\n",
      "train loss:0.04079211526179148\n",
      "train loss:0.02380574350863857\n",
      "train loss:0.03344048491115093\n",
      "train loss:0.03109203687688229\n",
      "train loss:0.025943720239601765\n",
      "train loss:0.02917605502180942\n",
      "train loss:0.006937850684702664\n",
      "train loss:0.011920493764040554\n",
      "train loss:0.06653107846307343\n",
      "train loss:0.011785694400592777\n",
      "train loss:0.053156886914627395\n",
      "train loss:0.021091207466039905\n",
      "train loss:0.053520109696756336\n",
      "train loss:0.05102752167401951\n",
      "train loss:0.29303209495126953\n",
      "train loss:0.04348332676373315\n",
      "train loss:0.022761433257437465\n",
      "train loss:0.03689993167711285\n",
      "train loss:0.0136500847582154\n",
      "train loss:0.021743113879844244\n",
      "train loss:0.043123092137957265\n",
      "train loss:0.02052734976442474\n",
      "train loss:0.04555108837414121\n",
      "train loss:0.022998466506790338\n",
      "train loss:0.0550218108803507\n",
      "train loss:0.008181906923662783\n",
      "train loss:0.008949032826522943\n",
      "train loss:0.04959184996289646\n",
      "train loss:0.06298952727511\n",
      "train loss:0.033908943153805576\n",
      "train loss:0.011644818120226157\n",
      "train loss:0.00954773561188553\n",
      "train loss:0.11730057799075588\n",
      "train loss:0.04661660560312574\n",
      "train loss:0.062252827636947804\n",
      "train loss:0.04261049664830062\n",
      "train loss:0.027670854807923376\n",
      "train loss:0.02364494171840727\n",
      "train loss:0.01637464297874885\n",
      "train loss:0.09627838984165804\n",
      "train loss:0.018000350689033898\n",
      "train loss:0.004209514807214505\n",
      "train loss:0.009565742206170036\n",
      "train loss:0.03726007140820413\n",
      "train loss:0.010797246141511521\n",
      "train loss:0.03333759790356867\n",
      "train loss:0.015708299571439696\n",
      "train loss:0.014930408318326838\n",
      "train loss:0.03056880549077975\n",
      "train loss:0.019102802606833117\n",
      "train loss:0.08598905621054558\n",
      "train loss:0.025504239994802124\n",
      "train loss:0.01983905141176085\n",
      "train loss:0.020545979309051082\n",
      "train loss:0.012320043496260664\n",
      "train loss:0.026269097997293097\n",
      "train loss:0.011160686809580454\n",
      "train loss:0.07465004548482859\n",
      "train loss:0.014557257884226966\n",
      "train loss:0.014589241509286296\n",
      "train loss:0.013361194851907674\n",
      "train loss:0.009971313455178467\n",
      "train loss:0.045779111129480035\n",
      "train loss:0.015189924694789277\n",
      "train loss:0.03587972378532207\n",
      "train loss:0.02577475413836907\n",
      "train loss:0.00557927054324951\n",
      "train loss:0.07116674056855599\n",
      "train loss:0.02405729884259923\n",
      "train loss:0.0200956558887155\n",
      "train loss:0.001994476764921383\n",
      "train loss:0.011010602994458113\n",
      "train loss:0.06118969340914002\n",
      "train loss:0.005228664658069393\n",
      "train loss:0.022701562103010123\n",
      "train loss:0.03183825931753506\n",
      "train loss:0.006485817905529759\n",
      "train loss:0.012073222551317623\n",
      "train loss:0.024501058930969703\n",
      "train loss:0.03759712560273626\n",
      "train loss:0.02068976786819319\n",
      "train loss:0.022784981874310953\n",
      "train loss:0.013326877684767124\n",
      "train loss:0.03289494371040214\n",
      "train loss:0.015830752818890396\n",
      "train loss:0.11691312074968754\n",
      "train loss:0.04216080310055781\n",
      "train loss:0.05474274224927903\n",
      "train loss:0.06260921212510702\n",
      "train loss:0.012329010203318454\n",
      "train loss:0.025082986599484772\n",
      "train loss:0.019890171379286576\n",
      "train loss:0.05363187765191386\n",
      "train loss:0.04850435092258226\n",
      "train loss:0.03339828864095936\n",
      "train loss:0.009357744618468397\n",
      "train loss:0.009948155582968396\n",
      "train loss:0.0600901605832071\n",
      "train loss:0.02509815172213406\n",
      "train loss:0.028767523242795665\n",
      "train loss:0.025667472672633248\n",
      "train loss:0.03168562904838059\n",
      "train loss:0.06455403311806987\n",
      "train loss:0.042041149070496726\n",
      "train loss:0.024138200279707287\n",
      "train loss:0.020624852496173726\n",
      "train loss:0.02217492967556836\n",
      "train loss:0.045513288597101374\n",
      "train loss:0.04268334508082176\n",
      "train loss:0.019558076596456645\n",
      "train loss:0.012958547423390468\n",
      "train loss:0.04308979608115166\n",
      "train loss:0.017102867106178605\n",
      "train loss:0.01907196280855931\n",
      "train loss:0.03447031394604534\n",
      "train loss:0.020936943012285394\n",
      "train loss:0.058250007200190315\n",
      "train loss:0.03175319262272675\n",
      "train loss:0.11315841629768096\n",
      "train loss:0.054942417082423416\n",
      "train loss:0.016254798213976178\n",
      "train loss:0.01939934394562512\n",
      "train loss:0.025641027769358334\n",
      "train loss:0.08311975830192138\n",
      "train loss:0.0332216639843096\n",
      "train loss:0.021790806177971418\n",
      "train loss:0.0108364809330308\n",
      "train loss:0.018926376667023296\n",
      "train loss:0.02534420437740487\n",
      "train loss:0.00785367496164906\n",
      "train loss:0.012398660709133001\n",
      "train loss:0.025273724550603477\n",
      "train loss:0.04469189521794423\n",
      "train loss:0.04289952898278306\n",
      "train loss:0.056810222988350326\n",
      "train loss:0.03342828757560998\n",
      "train loss:0.04917481911380258\n",
      "train loss:0.055829230415689185\n",
      "train loss:0.026786967231761896\n",
      "train loss:0.044564678536829126\n",
      "train loss:0.021392275673016867\n",
      "train loss:0.015294880522966276\n",
      "train loss:0.032185977004370335\n",
      "train loss:0.11671524794092467\n",
      "train loss:0.010020733367670115\n",
      "train loss:0.013564184599324751\n",
      "train loss:0.026173701250198834\n",
      "train loss:0.015118787715444086\n",
      "train loss:0.06792657541820257\n",
      "train loss:0.03215731837753985\n",
      "train loss:0.023433984358073784\n",
      "train loss:0.047809602659096485\n",
      "train loss:0.0376001459432854\n",
      "train loss:0.08232163691602469\n",
      "train loss:0.0091600681792536\n",
      "train loss:0.045658664818354816\n",
      "train loss:0.025427591843720166\n",
      "train loss:0.059035988875987676\n",
      "train loss:0.08163768449469275\n",
      "train loss:0.01228156949418584\n",
      "train loss:0.02189508149207245\n",
      "train loss:0.021407109359281966\n",
      "train loss:0.029075302115815736\n",
      "train loss:0.023007242305208134\n",
      "train loss:0.01851435475597991\n",
      "train loss:0.017796027461509836\n",
      "train loss:0.02841021334146771\n",
      "train loss:0.018874924704887542\n",
      "train loss:0.015691042664850382\n",
      "train loss:0.013380267137181354\n",
      "train loss:0.009323241611243297\n",
      "train loss:0.031416728062806515\n",
      "train loss:0.011233687666111974\n",
      "train loss:0.007026828480738633\n",
      "train loss:0.017588798849643806\n",
      "train loss:0.012782080113556667\n",
      "train loss:0.028101673503567612\n",
      "train loss:0.02229843149638027\n",
      "train loss:0.018659161137528482\n",
      "train loss:0.016487935448542216\n",
      "train loss:0.017828541267844503\n",
      "train loss:0.03457673305238196\n",
      "train loss:0.014934120082619795\n",
      "train loss:0.048990723337697294\n",
      "train loss:0.005458524956057468\n",
      "train loss:0.03310695148956737\n",
      "train loss:0.07717979457104641\n",
      "train loss:0.009253999003339824\n",
      "train loss:0.022241155846666975\n",
      "train loss:0.04348348039716667\n",
      "train loss:0.03730510092473856\n",
      "train loss:0.03399884913882555\n",
      "train loss:0.03237732670709389\n",
      "train loss:0.034837183968545425\n",
      "train loss:0.010084472590384691\n",
      "train loss:0.013795585503823971\n",
      "train loss:0.0696731938718768\n",
      "train loss:0.01917844718440886\n",
      "train loss:0.029474050751273073\n",
      "train loss:0.01294892070088137\n",
      "train loss:0.07306811097311075\n",
      "train loss:0.025975545197658816\n",
      "train loss:0.026870959914258873\n",
      "train loss:0.05534266755160134\n",
      "train loss:0.021030274830377085\n",
      "train loss:0.026552225830298433\n",
      "train loss:0.025792528074238347\n",
      "train loss:0.011130727469661903\n",
      "train loss:0.018586579484175057\n",
      "train loss:0.07291947087262775\n",
      "train loss:0.04030646760239956\n",
      "train loss:0.02537294437346208\n",
      "train loss:0.009437991837089599\n",
      "train loss:0.02082132129309341\n",
      "train loss:0.009797653545312958\n",
      "train loss:0.012387240046572576\n",
      "train loss:0.012025718161242056\n",
      "train loss:0.010189361416295879\n",
      "train loss:0.0058930172553577605\n",
      "train loss:0.03850143487168465\n",
      "train loss:0.015930871463110233\n",
      "train loss:0.03162198933527915\n",
      "train loss:0.01445599880041372\n",
      "train loss:0.07306952821222946\n",
      "train loss:0.06557072131571791\n",
      "train loss:0.01104055208075736\n",
      "train loss:0.010474532391029988\n",
      "train loss:0.030471273321459883\n",
      "train loss:0.0070562211500233216\n",
      "train loss:0.0099948628470839\n",
      "train loss:0.007791249207214481\n",
      "train loss:0.019083683463784498\n",
      "train loss:0.00785935397961336\n",
      "train loss:0.012849446568312366\n",
      "train loss:0.11873775482312465\n",
      "train loss:0.0822796297160757\n",
      "train loss:0.010568931963116434\n",
      "train loss:0.011093548430089363\n",
      "train loss:0.05396147819150537\n",
      "train loss:0.036100343344223754\n",
      "train loss:0.021697723329424986\n",
      "train loss:0.017222802427442757\n",
      "train loss:0.0495285384120523\n",
      "train loss:0.08136737620266637\n",
      "train loss:0.05246890103728547\n",
      "train loss:0.09544207947552896\n",
      "train loss:0.1917899739647033\n",
      "train loss:0.022843428729806017\n",
      "train loss:0.034436906274265516\n",
      "train loss:0.06104258687306162\n",
      "train loss:0.04971469801422988\n",
      "train loss:0.05702460517429293\n",
      "train loss:0.031313700599534755\n",
      "train loss:0.035833239889194485\n",
      "train loss:0.06855461693263533\n",
      "train loss:0.06589112539045637\n",
      "train loss:0.027841390152217645\n",
      "train loss:0.05808830250447987\n",
      "train loss:0.02822908417814289\n",
      "train loss:0.020561914871701906\n",
      "train loss:0.027374462067795983\n",
      "train loss:0.025588082116621776\n",
      "train loss:0.026697890941919877\n",
      "train loss:0.05116324917305817\n",
      "train loss:0.009497447287818275\n",
      "train loss:0.03191943059868414\n",
      "train loss:0.06492703231615592\n",
      "train loss:0.1160705665628698\n",
      "train loss:0.01702830382323093\n",
      "train loss:0.03536620883123784\n",
      "train loss:0.03766957234286868\n",
      "train loss:0.009590896517963637\n",
      "train loss:0.011704500940280931\n",
      "train loss:0.02729657342955066\n",
      "train loss:0.04737053850096126\n",
      "train loss:0.015465887937352487\n",
      "train loss:0.055970067861612495\n",
      "train loss:0.03359144888471659\n",
      "train loss:0.015127014551799505\n",
      "train loss:0.04635384208621498\n",
      "train loss:0.02992841401683724\n",
      "train loss:0.01960755434260472\n",
      "train loss:0.011345255778983463\n",
      "train loss:0.017468539660294487\n",
      "train loss:0.019166866628599427\n",
      "train loss:0.06507790379506034\n",
      "train loss:0.0408605536382069\n",
      "train loss:0.02507022023492181\n",
      "train loss:0.06929222136971867\n",
      "train loss:0.07367120611846108\n",
      "train loss:0.12330296735301652\n",
      "train loss:0.023312929247082517\n",
      "train loss:0.051875194818198925\n",
      "train loss:0.02562422137230604\n",
      "train loss:0.035653435115188536\n",
      "train loss:0.04037946693696916\n",
      "train loss:0.029011907275081284\n",
      "train loss:0.009243042751065404\n",
      "train loss:0.009909557815039167\n",
      "train loss:0.03709393296424806\n",
      "train loss:0.06106635372113603\n",
      "train loss:0.057559292882753736\n",
      "train loss:0.040511683244111606\n",
      "train loss:0.007738546182250082\n",
      "train loss:0.011218760576683438\n",
      "train loss:0.011099225260107799\n",
      "train loss:0.06679354010806664\n",
      "train loss:0.03818896999189531\n",
      "train loss:0.10955275630685414\n",
      "train loss:0.008868150553987937\n",
      "train loss:0.02343265071041226\n",
      "train loss:0.055717809235791185\n",
      "train loss:0.03505033107855403\n",
      "train loss:0.1301820804056909\n",
      "train loss:0.03867953739494958\n",
      "train loss:0.03418053336166085\n",
      "train loss:0.013784531374256418\n",
      "train loss:0.05314292883499722\n",
      "train loss:0.07578503347991246\n",
      "train loss:0.045327507629967005\n",
      "train loss:0.04374156151790408\n",
      "train loss:0.03148831621134992\n",
      "train loss:0.013069139048804093\n",
      "train loss:0.04793398580034391\n",
      "train loss:0.015191285315208308\n",
      "train loss:0.024995196786701954\n",
      "train loss:0.01956006872878868\n",
      "train loss:0.04655692283644946\n",
      "train loss:0.046021504172252724\n",
      "train loss:0.06584523670224858\n",
      "train loss:0.0341661102718676\n",
      "train loss:0.01979970434814768\n",
      "train loss:0.011354623706347756\n",
      "train loss:0.036796481834094\n",
      "train loss:0.007114779427134367\n",
      "train loss:0.006282640033104981\n",
      "train loss:0.02228179985906471\n",
      "train loss:0.015192724259335535\n",
      "train loss:0.012051740611042227\n",
      "train loss:0.018021481737924342\n",
      "train loss:0.025870498639047158\n",
      "train loss:0.14623346746236549\n",
      "train loss:0.0071154597493113415\n",
      "train loss:0.049374387150434906\n",
      "train loss:0.02019873082654083\n",
      "train loss:0.007120446369740433\n",
      "train loss:0.01771234737201949\n",
      "train loss:0.01321148719568732\n",
      "train loss:0.018097520973961143\n",
      "train loss:0.037809035901088056\n",
      "train loss:0.02024937694985081\n",
      "train loss:0.01396357345695101\n",
      "train loss:0.008076903036019756\n",
      "train loss:0.011050501326275455\n",
      "train loss:0.026728510321783493\n",
      "train loss:0.030988613765377607\n",
      "train loss:0.002856026334488204\n",
      "train loss:0.035269728268787444\n",
      "train loss:0.03568949617533005\n",
      "train loss:0.02832105767112515\n",
      "train loss:0.09728542554958931\n",
      "train loss:0.016790217936330464\n",
      "train loss:0.03852218044174966\n",
      "train loss:0.05027967975700364\n",
      "train loss:0.016556861904431387\n",
      "train loss:0.01192011289589081\n",
      "train loss:0.01662932230153148\n",
      "train loss:0.01413202278587692\n",
      "train loss:0.01691850750844639\n",
      "train loss:0.04555559519246815\n",
      "train loss:0.013717466015930541\n",
      "train loss:0.024012774816451733\n",
      "train loss:0.020743422087524985\n",
      "train loss:0.02421431819630706\n",
      "train loss:0.013433270280033638\n",
      "train loss:0.041745849494947994\n",
      "train loss:0.0061000165805912545\n",
      "train loss:0.14004729653188636\n",
      "train loss:0.02276404544718698\n",
      "train loss:0.02484172783221422\n",
      "train loss:0.011823084407719764\n",
      "train loss:0.020824910304646938\n",
      "train loss:0.027854078141170066\n",
      "train loss:0.01838486835829124\n",
      "train loss:0.024704996771104583\n",
      "train loss:0.01593038041981992\n",
      "train loss:0.025715423949934802\n",
      "train loss:0.050066979147554605\n",
      "train loss:0.008256624314574142\n",
      "train loss:0.060623651200071106\n",
      "train loss:0.03593816422023886\n",
      "train loss:0.017497561599403452\n",
      "train loss:0.026220543368729133\n",
      "train loss:0.0709678525467661\n",
      "train loss:0.02848830547057001\n",
      "train loss:0.052225087625833166\n",
      "train loss:0.02174137630465974\n",
      "train loss:0.055966503001587294\n",
      "train loss:0.06749491416327708\n",
      "train loss:0.022891079812211196\n",
      "train loss:0.060863898144308454\n",
      "train loss:0.016840567691248646\n",
      "train loss:0.007526418666395377\n",
      "train loss:0.040232699602697305\n",
      "train loss:0.03881855810440317\n",
      "train loss:0.010542177628499788\n",
      "train loss:0.013184599711555514\n",
      "train loss:0.008329454236152578\n",
      "train loss:0.021724333054541002\n",
      "train loss:0.03541215824436273\n",
      "train loss:0.023430981783448983\n",
      "train loss:0.1305001399101187\n",
      "train loss:0.13592397221829652\n",
      "train loss:0.03628063303062465\n",
      "train loss:0.026365633020445617\n",
      "train loss:0.040165231612040796\n",
      "train loss:0.032785364296563646\n",
      "train loss:0.018201778206223443\n",
      "train loss:0.012589219093365864\n",
      "train loss:0.040433273768149064\n",
      "train loss:0.1807782969035424\n",
      "train loss:0.031807762069694684\n",
      "train loss:0.014624176226061207\n",
      "train loss:0.020778670623043182\n",
      "train loss:0.032674577468543416\n",
      "train loss:0.007421749479591319\n",
      "train loss:0.033411773208513434\n",
      "train loss:0.034586424693506014\n",
      "train loss:0.007010898417524063\n",
      "train loss:0.02502751573831337\n",
      "train loss:0.04211555011870322\n",
      "train loss:0.01773535598217025\n",
      "train loss:0.02116503903207386\n",
      "train loss:0.05070631923519693\n",
      "train loss:0.09809884952805226\n",
      "train loss:0.0055160126066251414\n",
      "train loss:0.016221127060315893\n",
      "train loss:0.01599766042174108\n",
      "train loss:0.015081523581423761\n",
      "train loss:0.032982970936749116\n",
      "train loss:0.0707819967283895\n",
      "train loss:0.017388941406352273\n",
      "train loss:0.01890864765398978\n",
      "train loss:0.011519610898847095\n",
      "train loss:0.021725760365041164\n",
      "train loss:0.03862446969858779\n",
      "train loss:0.04599582340374011\n",
      "train loss:0.02690388741893015\n",
      "train loss:0.01708115364608672\n",
      "train loss:0.03602494293463967\n",
      "train loss:0.03467183032686855\n",
      "train loss:0.025345548248887528\n",
      "train loss:0.010952279914316082\n",
      "train loss:0.015272267048335196\n",
      "train loss:0.013865225562914923\n",
      "train loss:0.02361952575378983\n",
      "train loss:0.031526548033582284\n",
      "train loss:0.0322467244547919\n",
      "train loss:0.019635217978803715\n",
      "train loss:0.010233594138448347\n",
      "train loss:0.038350862434594075\n",
      "train loss:0.03201800964944094\n",
      "train loss:0.028583153641745532\n",
      "train loss:0.00636125484774949\n",
      "train loss:0.003577011917250988\n",
      "train loss:0.013111832043628415\n",
      "train loss:0.03643242481690445\n",
      "train loss:0.01669565899752054\n",
      "train loss:0.0885645486130911\n",
      "train loss:0.06017305118733882\n",
      "train loss:0.015532825240373855\n",
      "train loss:0.02481824345257404\n",
      "train loss:0.020196927277265096\n",
      "train loss:0.05878715875609296\n",
      "train loss:0.03628554474066124\n",
      "train loss:0.04859544123749237\n",
      "train loss:0.011707000198712039\n",
      "train loss:0.058637410786423115\n",
      "train loss:0.03227922565956527\n",
      "train loss:0.0032204231716973885\n",
      "train loss:0.019885908314265396\n",
      "train loss:0.007539871416806802\n",
      "train loss:0.019027481346254108\n",
      "train loss:0.0864602971362693\n",
      "train loss:0.005837023965902152\n",
      "train loss:0.014366159366894317\n",
      "train loss:0.016312513504986116\n",
      "train loss:0.035250672843227936\n",
      "train loss:0.010075922683846472\n",
      "train loss:0.004065435001917342\n",
      "train loss:0.030279993047210664\n",
      "train loss:0.01890677675154822\n",
      "train loss:0.03980479530347125\n",
      "train loss:0.027720856404552765\n",
      "train loss:0.027144906930203362\n",
      "train loss:0.05604273216214982\n",
      "train loss:0.010259755101768997\n",
      "train loss:0.02962336328128927\n",
      "train loss:0.045617816967726404\n",
      "train loss:0.003784787803034339\n",
      "train loss:0.015883488427224823\n",
      "train loss:0.016326003830774826\n",
      "train loss:0.01810843711995953\n",
      "train loss:0.06050526547466099\n",
      "train loss:0.020465817758083506\n",
      "train loss:0.021619896109135915\n",
      "train loss:0.07426044879692364\n",
      "train loss:0.11273921855475408\n",
      "train loss:0.036391533458583876\n",
      "train loss:0.022505484868501232\n",
      "train loss:0.012467585754464626\n",
      "train loss:0.026074286216130407\n",
      "train loss:0.008164740570156878\n",
      "train loss:0.022503496562876077\n",
      "train loss:0.042540948807104566\n",
      "train loss:0.004661972941679609\n",
      "train loss:0.03589309104883117\n",
      "train loss:0.021778813919711874\n",
      "train loss:0.009906092900248395\n",
      "=== epoch:6, train acc:0.987, test acc:0.988 ===\n",
      "train loss:0.05391684560908824\n",
      "train loss:0.009512421182096614\n",
      "train loss:0.02059528683375283\n",
      "train loss:0.019334669144557903\n",
      "train loss:0.0621793590949193\n",
      "train loss:0.034740464298198985\n",
      "train loss:0.031123857435472257\n",
      "train loss:0.018475238663663247\n",
      "train loss:0.03622481315110105\n",
      "train loss:0.007717124763419855\n",
      "train loss:0.019702553996924554\n",
      "train loss:0.017065246095288252\n",
      "train loss:0.04689783986982597\n",
      "train loss:0.08449255743155361\n",
      "train loss:0.03209396074521598\n",
      "train loss:0.011861351410086858\n",
      "train loss:0.007970465481227582\n",
      "train loss:0.025175916407585656\n",
      "train loss:0.030259145885249583\n",
      "train loss:0.01935703454717142\n",
      "train loss:0.09978691641590819\n",
      "train loss:0.06333926699515319\n",
      "train loss:0.011337429919438681\n",
      "train loss:0.0066746961815523495\n",
      "train loss:0.012053953339055354\n",
      "train loss:0.09246758990572093\n",
      "train loss:0.041286071963562855\n",
      "train loss:0.012799009283137597\n",
      "train loss:0.0212670541766337\n",
      "train loss:0.026433535234635337\n",
      "train loss:0.01935442457901738\n",
      "train loss:0.053274904988122825\n",
      "train loss:0.01993931556607376\n",
      "train loss:0.04221666135771316\n",
      "train loss:0.00890156377554341\n",
      "train loss:0.044457054321707504\n",
      "train loss:0.06869397470521965\n",
      "train loss:0.024888257726707025\n",
      "train loss:0.0175971173337567\n",
      "train loss:0.02024670976381257\n",
      "train loss:0.006129192053755828\n",
      "train loss:0.005357935313832273\n",
      "train loss:0.023138167825851586\n",
      "train loss:0.004370949116514244\n",
      "train loss:0.02536089037392505\n",
      "train loss:0.02401685885955415\n",
      "train loss:0.027985009816274248\n",
      "train loss:0.01184120972512815\n",
      "train loss:0.011631046152760411\n",
      "train loss:0.027995510032018722\n",
      "train loss:0.015721268036270913\n",
      "train loss:0.0070064629221873505\n",
      "train loss:0.012661502345097045\n",
      "train loss:0.021833790709417968\n",
      "train loss:0.03994143766034547\n",
      "train loss:0.03807935884169258\n",
      "train loss:0.011333616097124941\n",
      "train loss:0.06796772369417811\n",
      "train loss:0.016334848426913177\n",
      "train loss:0.025890724287845456\n",
      "train loss:0.011619181044243454\n",
      "train loss:0.016205938319866654\n",
      "train loss:0.01757320104169787\n",
      "train loss:0.012325052893297408\n",
      "train loss:0.05443919088195887\n",
      "train loss:0.010294070566572007\n",
      "train loss:0.0487430553103202\n",
      "train loss:0.01486963123384957\n",
      "train loss:0.013133213040899414\n",
      "train loss:0.010871733181113447\n",
      "train loss:0.012285418963282944\n",
      "train loss:0.027399431384487954\n",
      "train loss:0.015749752366405922\n",
      "train loss:0.01626277433146951\n",
      "train loss:0.030478167489098363\n",
      "train loss:0.0257158319594639\n",
      "train loss:0.00585393309586864\n",
      "train loss:0.010516419274352817\n",
      "train loss:0.011100774185288374\n",
      "train loss:0.007886543477673973\n",
      "train loss:0.013695077246588203\n",
      "train loss:0.025917790664289394\n",
      "train loss:0.03440476155973029\n",
      "train loss:0.01696322960517492\n",
      "train loss:0.010172778133531511\n",
      "train loss:0.00784267626122768\n",
      "train loss:0.004090200608551457\n",
      "train loss:0.0029628762475006386\n",
      "train loss:0.007059636638872641\n",
      "train loss:0.0248902880872786\n",
      "train loss:0.03544503303995607\n",
      "train loss:0.05060387933922596\n",
      "train loss:0.006332399301068682\n",
      "train loss:0.008868594753711281\n",
      "train loss:0.035310611059222535\n",
      "train loss:0.04382966156134453\n",
      "train loss:0.004837487535272925\n",
      "train loss:0.0353478097620148\n",
      "train loss:0.0203474141894442\n",
      "train loss:0.007182944428684771\n",
      "train loss:0.01669497851071544\n",
      "train loss:0.027704202708026503\n",
      "train loss:0.01108714536542576\n",
      "train loss:0.056490688060170616\n",
      "train loss:0.008429387695646632\n",
      "train loss:0.0035896364133151458\n",
      "train loss:0.008048647662645753\n",
      "train loss:0.035916202472435634\n",
      "train loss:0.04675686838689946\n",
      "train loss:0.04471979258116449\n",
      "train loss:0.016606268987329306\n",
      "train loss:0.05368007875277859\n",
      "train loss:0.028877300983472208\n",
      "train loss:0.005873439274199413\n",
      "train loss:0.022970165759055604\n",
      "train loss:0.017843362803320658\n",
      "train loss:0.04546956046759797\n",
      "train loss:0.01283697890728436\n",
      "train loss:0.03505212911332845\n",
      "train loss:0.02209144288171274\n",
      "train loss:0.006140449536251928\n",
      "train loss:0.014617804779079051\n",
      "train loss:0.0383878717172813\n",
      "train loss:0.004976941603834257\n",
      "train loss:0.014253502209380193\n",
      "train loss:0.02379260125101693\n",
      "train loss:0.009053812212583983\n",
      "train loss:0.027122673690410402\n",
      "train loss:0.034270223623742144\n",
      "train loss:0.004751941900343878\n",
      "train loss:0.009054100862296908\n",
      "train loss:0.013574922196428895\n",
      "train loss:0.04465840042244284\n",
      "train loss:0.06931991146260351\n",
      "train loss:0.006388484903454884\n",
      "train loss:0.016335647254177202\n",
      "train loss:0.019233622560738534\n",
      "train loss:0.011297024603768795\n",
      "train loss:0.019100630560079192\n",
      "train loss:0.021090959357132205\n",
      "train loss:0.008614246504650971\n",
      "train loss:0.002941723426576902\n",
      "train loss:0.005804416688410066\n",
      "train loss:0.01908684992214822\n",
      "train loss:0.0969671776117098\n",
      "train loss:0.008503027978053843\n",
      "train loss:0.01234122861958033\n",
      "train loss:0.03125414652214037\n",
      "train loss:0.019343759806746342\n",
      "train loss:0.004005967881271679\n",
      "train loss:0.04258473680414049\n",
      "train loss:0.018682992092777618\n",
      "train loss:0.011551264428112037\n",
      "train loss:0.012626404432830281\n",
      "train loss:0.014032079357151239\n",
      "train loss:0.020969638546955216\n",
      "train loss:0.03920317696811793\n",
      "train loss:0.007009586945004829\n",
      "train loss:0.06761126570602975\n",
      "train loss:0.02280727379223348\n",
      "train loss:0.01997083989502988\n",
      "train loss:0.07059666373870048\n",
      "train loss:0.011473737607493532\n",
      "train loss:0.019782531918733456\n",
      "train loss:0.0049146115415723965\n",
      "train loss:0.02822071646783807\n",
      "train loss:0.011282229087507891\n",
      "train loss:0.015169143869685786\n",
      "train loss:0.018302428810593675\n",
      "train loss:0.00202567594937802\n",
      "train loss:0.026292376844889342\n",
      "train loss:0.005793180452007615\n",
      "train loss:0.012451714604790977\n",
      "train loss:0.021975108585465877\n",
      "train loss:0.016360329112368538\n",
      "train loss:0.03164837944322836\n",
      "train loss:0.054422311536848476\n",
      "train loss:0.011548943018178038\n",
      "train loss:0.01596057930191183\n",
      "train loss:0.06597442147001936\n",
      "train loss:0.05834361973613951\n",
      "train loss:0.06970374157006907\n",
      "train loss:0.084796163254656\n",
      "train loss:0.020363351988884112\n",
      "train loss:0.019156796784572758\n",
      "train loss:0.008213976333140582\n",
      "train loss:0.0136146078224918\n",
      "train loss:0.025946596703974225\n",
      "train loss:0.05261338297578119\n",
      "train loss:0.07862718683174476\n",
      "train loss:0.05242621583489887\n",
      "train loss:0.015176419626452595\n",
      "train loss:0.008960854663801003\n",
      "train loss:0.05126697869883616\n",
      "train loss:0.017477092906019738\n",
      "train loss:0.011845479279682889\n",
      "train loss:0.04989345390764402\n",
      "train loss:0.0722343300644667\n",
      "train loss:0.04521453685230657\n",
      "train loss:0.017453010720969872\n",
      "train loss:0.0465921485193506\n",
      "train loss:0.017528656406631597\n",
      "train loss:0.02288551656525977\n",
      "train loss:0.019163714124526256\n",
      "train loss:0.04186629858156962\n",
      "train loss:0.009243297234049425\n",
      "train loss:0.03097465783045448\n",
      "train loss:0.04092023391123302\n",
      "train loss:0.05315747764134923\n",
      "train loss:0.0313224526517999\n",
      "train loss:0.017796437364253032\n",
      "train loss:0.01990817076162716\n",
      "train loss:0.013042295018640356\n",
      "train loss:0.049457930517114425\n",
      "train loss:0.010984777821971426\n",
      "train loss:0.054811210410491536\n",
      "train loss:0.020211580656229557\n",
      "train loss:0.0065456640133336776\n",
      "train loss:0.02917932845578267\n",
      "train loss:0.0291895701914562\n",
      "train loss:0.01594458917404722\n",
      "train loss:0.015734166478604077\n",
      "train loss:0.009220365085688275\n",
      "train loss:0.12217801341481335\n",
      "train loss:0.027213129956113545\n",
      "train loss:0.025864756631813857\n",
      "train loss:0.01656573622351283\n",
      "train loss:0.012518078616140762\n",
      "train loss:0.027536479075558184\n",
      "train loss:0.01784672862838213\n",
      "train loss:0.005747771133214189\n",
      "train loss:0.010580824637154772\n",
      "train loss:0.01697126550418695\n",
      "train loss:0.019697716627939125\n",
      "train loss:0.015170410765168769\n",
      "train loss:0.01574157581698514\n",
      "train loss:0.028164498583142118\n",
      "train loss:0.023607105342901398\n",
      "train loss:0.00905515152458441\n",
      "train loss:0.01578687606552659\n",
      "train loss:0.03138916479025297\n",
      "train loss:0.011396134649737107\n",
      "train loss:0.020447149617431114\n",
      "train loss:0.040480110794067735\n",
      "train loss:0.0036735129042137953\n",
      "train loss:0.025031373504922065\n",
      "train loss:0.03247939033670415\n",
      "train loss:0.015388481299559187\n",
      "train loss:0.023579024950124337\n",
      "train loss:0.004516382499326551\n",
      "train loss:0.013401850505435275\n",
      "train loss:0.006764771360138814\n",
      "train loss:0.018118423205615432\n",
      "train loss:0.08114191635392012\n",
      "train loss:0.03511089387924998\n",
      "train loss:0.031966560289435016\n",
      "train loss:0.043463466681593725\n",
      "train loss:0.0058394074302575375\n",
      "train loss:0.07855747414326011\n",
      "train loss:0.005080202479853976\n",
      "train loss:0.010560028369856214\n",
      "train loss:0.01229980635394738\n",
      "train loss:0.01290234483449158\n",
      "train loss:0.023941062127643336\n",
      "train loss:0.013541726685514873\n",
      "train loss:0.034111482302972916\n",
      "train loss:0.0027083825042636743\n",
      "train loss:0.022777746856593107\n",
      "train loss:0.010077400612657666\n",
      "train loss:0.009310844050390234\n",
      "train loss:0.0522291053693503\n",
      "train loss:0.007190778891691694\n",
      "train loss:0.033425253062927734\n",
      "train loss:0.006110529309871017\n",
      "train loss:0.018042146971513153\n",
      "train loss:0.00922883634194798\n",
      "train loss:0.03080690945514153\n",
      "train loss:0.0018186963316143728\n",
      "train loss:0.013849084273966826\n",
      "train loss:0.007562500668248335\n",
      "train loss:0.029782842388071936\n",
      "train loss:0.02425432140353572\n",
      "train loss:0.022306159826191104\n",
      "train loss:0.024941659966260684\n",
      "train loss:0.0023641626792045957\n",
      "train loss:0.009035544546052716\n",
      "train loss:0.015954826936329443\n",
      "train loss:0.007942186009304153\n",
      "train loss:0.027931697560477656\n",
      "train loss:0.007824461577625851\n",
      "train loss:0.03989417746975401\n",
      "train loss:0.016221541800368017\n",
      "train loss:0.004538096790015036\n",
      "train loss:0.0016560081814819697\n",
      "train loss:0.04263902408985705\n",
      "train loss:0.005755002256293138\n",
      "train loss:0.020957853265137036\n",
      "train loss:0.005168955174675363\n",
      "train loss:0.043938855083692226\n",
      "train loss:0.015958401261899433\n",
      "train loss:0.0011338788803312014\n",
      "train loss:0.010579025849824673\n",
      "train loss:0.0245854039260524\n",
      "train loss:0.0320110434764907\n",
      "train loss:0.009395463734402181\n",
      "train loss:0.016306425760172962\n",
      "train loss:0.026251113726983303\n",
      "train loss:0.0018549772693391032\n",
      "train loss:0.07316626330324326\n",
      "train loss:0.010939184779111733\n",
      "train loss:0.011598264924190837\n",
      "train loss:0.017519368146073622\n",
      "train loss:0.012577514335269315\n",
      "train loss:0.043127868242563636\n",
      "train loss:0.046304601949665375\n",
      "train loss:0.028457773871910397\n",
      "train loss:0.04011958913364906\n",
      "train loss:0.04334490192363259\n",
      "train loss:0.020618656016480306\n",
      "train loss:0.0042909761654755935\n",
      "train loss:0.009131129206663749\n",
      "train loss:0.03349291249859431\n",
      "train loss:0.018410816986194966\n",
      "train loss:0.012921257605554692\n",
      "train loss:0.013923195044422165\n",
      "train loss:0.03730688812560641\n",
      "train loss:0.028434133814537875\n",
      "train loss:0.0191718188002683\n",
      "train loss:0.026238337539734494\n",
      "train loss:0.004389102065245097\n",
      "train loss:0.012336618803019254\n",
      "train loss:0.1830835493171804\n",
      "train loss:0.044669086314375855\n",
      "train loss:0.024225534601649546\n",
      "train loss:0.053656659667240716\n",
      "train loss:0.020855366874445544\n",
      "train loss:0.015000967418826219\n",
      "train loss:0.002104545321596529\n",
      "train loss:0.017573592052782687\n",
      "train loss:0.013758235022181773\n",
      "train loss:0.026022380686766523\n",
      "train loss:0.006921884992781605\n",
      "train loss:0.015819750995030288\n",
      "train loss:0.015416531364250806\n",
      "train loss:0.1670253880151246\n",
      "train loss:0.046878260652566374\n",
      "train loss:0.08352165389885988\n",
      "train loss:0.06701178776611326\n",
      "train loss:0.007858814005300982\n",
      "train loss:0.05405144918521172\n",
      "train loss:0.020062155413474333\n",
      "train loss:0.00832966162796483\n",
      "train loss:0.033657658775492615\n",
      "train loss:0.01413518601309503\n",
      "train loss:0.00994471671045592\n",
      "train loss:0.012677358643811252\n",
      "train loss:0.035696316779101414\n",
      "train loss:0.025925164070276997\n",
      "train loss:0.016848263193504277\n",
      "train loss:0.013399995366510603\n",
      "train loss:0.013797211245716489\n",
      "train loss:0.009259234137236375\n",
      "train loss:0.009875553518201545\n",
      "train loss:0.007707685901650333\n",
      "train loss:0.004018829012557783\n",
      "train loss:0.04219400415137603\n",
      "train loss:0.018216795358406958\n",
      "train loss:0.0370435902723464\n",
      "train loss:0.015877673493282073\n",
      "train loss:0.027264893117341208\n",
      "train loss:0.02836457050257633\n",
      "train loss:0.01750186917109735\n",
      "train loss:0.0028394137276310165\n",
      "train loss:0.008881276617898082\n",
      "train loss:0.022042522359148012\n",
      "train loss:0.024036804168689162\n",
      "train loss:0.009384778836845187\n",
      "train loss:0.010818616499211498\n",
      "train loss:0.0073766938463983075\n",
      "train loss:0.029980484770427848\n",
      "train loss:0.005771247145091769\n",
      "train loss:0.018088857962011835\n",
      "train loss:0.017574739647652337\n",
      "train loss:0.013676567530004933\n",
      "train loss:0.03745818547606616\n",
      "train loss:0.04477828167298352\n",
      "train loss:0.013068774359869646\n",
      "train loss:0.0539235908732727\n",
      "train loss:0.005037897298086796\n",
      "train loss:0.02250676779945661\n",
      "train loss:0.057919322391822854\n",
      "train loss:0.009690372623323834\n",
      "train loss:0.02803073512284394\n",
      "train loss:0.004887594022507928\n",
      "train loss:0.018442859214559924\n",
      "train loss:0.009238221118863936\n",
      "train loss:0.02365467914423124\n",
      "train loss:0.006035374060576963\n",
      "train loss:0.05279774521907063\n",
      "train loss:0.013108738936724356\n",
      "train loss:0.009502706431019603\n",
      "train loss:0.013046947415603255\n",
      "train loss:0.014723968318337301\n",
      "train loss:0.037248640546181676\n",
      "train loss:0.014257427132001888\n",
      "train loss:0.006865029781047583\n",
      "train loss:0.022146418404255602\n",
      "train loss:0.1004455199388247\n",
      "train loss:0.010943467231563612\n",
      "train loss:0.007961229744315292\n",
      "train loss:0.007009655608238189\n",
      "train loss:0.022427867847555048\n",
      "train loss:0.017903104205755267\n",
      "train loss:0.04087579617028195\n",
      "train loss:0.02447417119510943\n",
      "train loss:0.027557523622404392\n",
      "train loss:0.016512141842120374\n",
      "train loss:0.012994528178878426\n",
      "train loss:0.034016138789726195\n",
      "train loss:0.004997419245903708\n",
      "train loss:0.00559462710986627\n",
      "train loss:0.004717275435210726\n",
      "train loss:0.0059383491530174805\n",
      "train loss:0.02690193490953945\n",
      "train loss:0.004658465802340229\n",
      "train loss:0.020006498404104435\n",
      "train loss:0.13667932602135224\n",
      "train loss:0.026826680457836063\n",
      "train loss:0.005279755351791483\n",
      "train loss:0.021373286449371448\n",
      "train loss:0.02426054821476166\n",
      "train loss:0.01330468557883838\n",
      "train loss:0.019334206839794543\n",
      "train loss:0.009147592376762674\n",
      "train loss:0.01344704711820699\n",
      "train loss:0.024810827229195226\n",
      "train loss:0.010887299635570287\n",
      "train loss:0.0030283177915392734\n",
      "train loss:0.0124335729655394\n",
      "train loss:0.01769432487366738\n",
      "train loss:0.045195820577453645\n",
      "train loss:0.017737155030592765\n",
      "train loss:0.03115868184478602\n",
      "train loss:0.007210586968968086\n",
      "train loss:0.038206824281122834\n",
      "train loss:0.007133966179500344\n",
      "train loss:0.029090212929734963\n",
      "train loss:0.010430077207093246\n",
      "train loss:0.012680464108264691\n",
      "train loss:0.04084757887330937\n",
      "train loss:0.007564154901609076\n",
      "train loss:0.013470229744055491\n",
      "train loss:0.007851669494714855\n",
      "train loss:0.010428392382577615\n",
      "train loss:0.04724529650205796\n",
      "train loss:0.012254018399313182\n",
      "train loss:0.04158199705826424\n",
      "train loss:0.009269384340541953\n",
      "train loss:0.01722187483091254\n",
      "train loss:0.021297233439090048\n",
      "train loss:0.00222389434118389\n",
      "train loss:0.02276915185038576\n",
      "train loss:0.01203339079980116\n",
      "train loss:0.02385732029867921\n",
      "train loss:0.02686355581037687\n",
      "train loss:0.009226493692798883\n",
      "train loss:0.013460630071889062\n",
      "train loss:0.019790143509990386\n",
      "train loss:0.026262412356891925\n",
      "train loss:0.023392270033241583\n",
      "train loss:0.010352816219028583\n",
      "train loss:0.03883169881541449\n",
      "train loss:0.014500807011690537\n",
      "train loss:0.03217977935426017\n",
      "train loss:0.007140256177569689\n",
      "train loss:0.004994229304372913\n",
      "train loss:0.1667618902868353\n",
      "train loss:0.025355082394465468\n",
      "train loss:0.0615956827635504\n",
      "train loss:0.03848203712949075\n",
      "train loss:0.10938504467897942\n",
      "train loss:0.020843435936139763\n",
      "train loss:0.016549996730834526\n",
      "train loss:0.012864580632054267\n",
      "train loss:0.023442519167736476\n",
      "train loss:0.04689813563092216\n",
      "train loss:0.020223319478519537\n",
      "train loss:0.0018351354963980152\n",
      "train loss:0.00709032413034909\n",
      "train loss:0.008592078063241216\n",
      "train loss:0.005406928622156132\n",
      "train loss:0.004128831761168906\n",
      "train loss:0.026423622511039503\n",
      "train loss:0.01144238617635557\n",
      "train loss:0.006849096863039806\n",
      "train loss:0.024116013448709773\n",
      "train loss:0.02213148259899652\n",
      "train loss:0.01818671145583294\n",
      "train loss:0.030524622350041327\n",
      "train loss:0.0048247464293009554\n",
      "train loss:0.030844858139154357\n",
      "train loss:0.08570609030604319\n",
      "train loss:0.007605143892419316\n",
      "train loss:0.030902889215650257\n",
      "train loss:0.015936003398461497\n",
      "train loss:0.054223917485809923\n",
      "train loss:0.024099851424972032\n",
      "train loss:0.018971149566917266\n",
      "train loss:0.08009514900377775\n",
      "train loss:0.01378453666655347\n",
      "train loss:0.010633623848687011\n",
      "train loss:0.03702418321572953\n",
      "train loss:0.013182550346012473\n",
      "train loss:0.02610101493919066\n",
      "train loss:0.005531582407816699\n",
      "train loss:0.013348235726919115\n",
      "train loss:0.009780733941619579\n",
      "train loss:0.0051739876921752135\n",
      "train loss:0.02909612984871957\n",
      "train loss:0.02881159834516833\n",
      "train loss:0.0026588256020736523\n",
      "train loss:0.017417863309778955\n",
      "train loss:0.009306323977873367\n",
      "train loss:0.023845509113153668\n",
      "train loss:0.015781203912022475\n",
      "train loss:0.04654126833478671\n",
      "train loss:0.012915657621801935\n",
      "train loss:0.02463371992485559\n",
      "train loss:0.008223889500962013\n",
      "train loss:0.004655234060393022\n",
      "train loss:0.009475984604118698\n",
      "train loss:0.007598203937389248\n",
      "train loss:0.01736888549187568\n",
      "train loss:0.01387515563731591\n",
      "train loss:0.03439020734160944\n",
      "train loss:0.00681353411791433\n",
      "train loss:0.002219602826567815\n",
      "train loss:0.01566014931739657\n",
      "train loss:0.007433729946915828\n",
      "train loss:0.019112050558897087\n",
      "train loss:0.00959197041041018\n",
      "train loss:0.031009961706521774\n",
      "train loss:0.013468821520179508\n",
      "train loss:0.006909120682999994\n",
      "train loss:0.031209713738368516\n",
      "train loss:0.004879072237278278\n",
      "train loss:0.00957008995588446\n",
      "train loss:0.019983421003703084\n",
      "train loss:0.09309806620553869\n",
      "train loss:0.054782827037600415\n",
      "train loss:0.018743654823578208\n",
      "train loss:0.020704200824523428\n",
      "train loss:0.0354120473359977\n",
      "train loss:0.015192795651669987\n",
      "train loss:0.009568155140599166\n",
      "train loss:0.003605194648163757\n",
      "train loss:0.008760218309806101\n",
      "train loss:0.01316748826191202\n",
      "train loss:0.023648566564090832\n",
      "train loss:0.007080563573585417\n",
      "train loss:0.06978570964215111\n",
      "train loss:0.024122123734368995\n",
      "train loss:0.0289643067130275\n",
      "train loss:0.01186700072319264\n",
      "train loss:0.028709375843375615\n",
      "train loss:0.009976890702354494\n",
      "train loss:0.03691359363454685\n",
      "train loss:0.056564751069153874\n",
      "train loss:0.016097828304882107\n",
      "train loss:0.011415926656205904\n",
      "train loss:0.02718281779280114\n",
      "train loss:0.046826919886167724\n",
      "train loss:0.023373069508408107\n",
      "train loss:0.011107875356558026\n",
      "train loss:0.014017616685401463\n",
      "train loss:0.08701362828456753\n",
      "train loss:0.031866373380180106\n",
      "train loss:0.012410653023064872\n",
      "train loss:0.08283059097535679\n",
      "train loss:0.005915484464553725\n",
      "train loss:0.0020634311000519677\n",
      "train loss:0.010682202869325252\n",
      "train loss:0.007303091119967864\n",
      "train loss:0.010364419621241761\n",
      "train loss:0.031700220894483475\n",
      "train loss:0.06010107176918525\n",
      "train loss:0.033090362091010676\n",
      "train loss:0.006604183146230911\n",
      "train loss:0.061894460897770696\n",
      "train loss:0.10697661270100824\n",
      "train loss:0.006312452886664808\n",
      "train loss:0.01169761508859453\n",
      "train loss:0.025840077151495187\n",
      "train loss:0.011587196786282436\n",
      "train loss:0.009813536172090197\n",
      "train loss:0.019520999509050424\n",
      "train loss:0.04972458061976742\n",
      "train loss:0.019929187630267863\n",
      "train loss:0.030219309116970218\n",
      "train loss:0.007637953698948682\n",
      "=== epoch:7, train acc:0.985, test acc:0.986 ===\n",
      "train loss:0.00969376577651369\n",
      "train loss:0.06198586022657108\n",
      "train loss:0.015410767100992508\n",
      "train loss:0.006502149733188593\n",
      "train loss:0.016315718131642808\n",
      "train loss:0.012117237674774047\n",
      "train loss:0.01654106008322941\n",
      "train loss:0.01111170034273444\n",
      "train loss:0.00851108356369377\n",
      "train loss:0.014613705093173668\n",
      "train loss:0.034192407527683405\n",
      "train loss:0.01918475488289739\n",
      "train loss:0.004704687951063493\n",
      "train loss:0.014898493070146792\n",
      "train loss:0.005388095842561069\n",
      "train loss:0.04533701596638969\n",
      "train loss:0.06818782127245517\n",
      "train loss:0.011308501379282805\n",
      "train loss:0.03490183187566117\n",
      "train loss:0.014418917454139535\n",
      "train loss:0.025713245105511765\n",
      "train loss:0.018799077247525175\n",
      "train loss:0.03326037135570435\n",
      "train loss:0.033881548611925334\n",
      "train loss:0.010080302370554038\n",
      "train loss:0.022425325941333216\n",
      "train loss:0.03177856339518498\n",
      "train loss:0.03689328628198262\n",
      "train loss:0.009726901785962125\n",
      "train loss:0.007546405993451863\n",
      "train loss:0.016919200185669996\n",
      "train loss:0.051156032692802354\n",
      "train loss:0.0038518466976728586\n",
      "train loss:0.011971250921104588\n",
      "train loss:0.018648733046219015\n",
      "train loss:0.014719571190399365\n",
      "train loss:0.009987462101888467\n",
      "train loss:0.016325268260797905\n",
      "train loss:0.04953205128519709\n",
      "train loss:0.00899776154239809\n",
      "train loss:0.008727809182726204\n",
      "train loss:0.008714227476875584\n",
      "train loss:0.01204327477308159\n",
      "train loss:0.006898552922520252\n",
      "train loss:0.04664700158570801\n",
      "train loss:0.02585846376266796\n",
      "train loss:0.0036151057875769936\n",
      "train loss:0.04272615139705078\n",
      "train loss:0.010321857440462793\n",
      "train loss:0.04402606965638541\n",
      "train loss:0.010300017384896087\n",
      "train loss:0.03562924247748517\n",
      "train loss:0.006253287223189734\n",
      "train loss:0.022744582325296595\n",
      "train loss:0.08179281479198487\n",
      "train loss:0.01687759585964245\n",
      "train loss:0.027793556245841632\n",
      "train loss:0.009961576802819843\n",
      "train loss:0.01507954389690493\n",
      "train loss:0.028276242618159766\n",
      "train loss:0.019628846948027523\n",
      "train loss:0.022331103763537544\n",
      "train loss:0.04747913628073344\n",
      "train loss:0.0337913769580478\n",
      "train loss:0.007905608968159745\n",
      "train loss:0.04644654850060708\n",
      "train loss:0.02125860188926683\n",
      "train loss:0.004480863460099348\n",
      "train loss:0.01580955724894574\n",
      "train loss:0.0143998584511404\n",
      "train loss:0.012078551466451554\n",
      "train loss:0.01800523907562354\n",
      "train loss:0.01052755252086357\n",
      "train loss:0.015254397554373118\n",
      "train loss:0.030403552248404096\n",
      "train loss:0.014562891398014557\n",
      "train loss:0.0629409366219484\n",
      "train loss:0.024457856393892138\n",
      "train loss:0.0035343150090699055\n",
      "train loss:0.0016482795412783918\n",
      "train loss:0.005834432159042958\n",
      "train loss:0.009606327686866829\n",
      "train loss:0.02622780569620323\n",
      "train loss:0.14435659393784248\n",
      "train loss:0.01225294530331033\n",
      "train loss:0.012289034507486111\n",
      "train loss:0.03181955758084114\n",
      "train loss:0.018655136190044438\n",
      "train loss:0.031056200804419124\n",
      "train loss:0.039764796762832906\n",
      "train loss:0.010856098687101266\n",
      "train loss:0.037131261424629566\n",
      "train loss:0.022639495900688367\n",
      "train loss:0.02332048718863485\n",
      "train loss:0.012579831512251179\n",
      "train loss:0.0018629575755150876\n",
      "train loss:0.009225148395861358\n",
      "train loss:0.011384101061754534\n",
      "train loss:0.09143914352762618\n",
      "train loss:0.009052397049176876\n",
      "train loss:0.04231093379469217\n",
      "train loss:0.025621283328893595\n",
      "train loss:0.005114542071751005\n",
      "train loss:0.027907149391005404\n",
      "train loss:0.037221650687667245\n",
      "train loss:0.01707952230979367\n",
      "train loss:0.015589242572735407\n",
      "train loss:0.004909756234492621\n",
      "train loss:0.015647308843639712\n",
      "train loss:0.006405100620363251\n",
      "train loss:0.06347604460226487\n",
      "train loss:0.02560401257223715\n",
      "train loss:0.03973915348287489\n",
      "train loss:0.020552721417630537\n",
      "train loss:0.012632031765199815\n",
      "train loss:0.020733727508533015\n",
      "train loss:0.0098534892718568\n",
      "train loss:0.022560883811732352\n",
      "train loss:0.005298969585680577\n",
      "train loss:0.01275817652090549\n",
      "train loss:0.042436588073853934\n",
      "train loss:0.011454237600135801\n",
      "train loss:0.011424453742201619\n",
      "train loss:0.019540003376793912\n",
      "train loss:0.01576419940751544\n",
      "train loss:0.03258797601555434\n",
      "train loss:0.009113711701689132\n",
      "train loss:0.008646634784774282\n",
      "train loss:0.009862403797437542\n",
      "train loss:0.008141270802963486\n",
      "train loss:0.029921727952650638\n",
      "train loss:0.010488197234489785\n",
      "train loss:0.021499539794082714\n",
      "train loss:0.013898647958364829\n",
      "train loss:0.008702753081238024\n",
      "train loss:0.003051727643913765\n",
      "train loss:0.006451285607932173\n",
      "train loss:0.0031446427583695594\n",
      "train loss:0.020102807445838387\n",
      "train loss:0.004645218999113357\n",
      "train loss:0.023669693723686658\n",
      "train loss:0.011106093998893549\n",
      "train loss:0.016334531067538266\n",
      "train loss:0.008153620127061398\n",
      "train loss:0.03318837285081727\n",
      "train loss:0.012975684861157648\n",
      "train loss:0.08111448213303854\n",
      "train loss:0.026051793624604986\n",
      "train loss:0.013274726812757452\n",
      "train loss:0.009250533572127543\n",
      "train loss:0.01605646806815233\n",
      "train loss:0.01094885737500281\n",
      "train loss:0.003518225285031074\n",
      "train loss:0.017462204793319547\n",
      "train loss:0.00692068168316545\n",
      "train loss:0.009107918470798834\n",
      "train loss:0.00962529782654497\n",
      "train loss:0.00603737885856128\n",
      "train loss:0.01192249328644892\n",
      "train loss:0.00493890619020562\n",
      "train loss:0.045405940769370365\n",
      "train loss:0.05422997574044341\n",
      "train loss:0.016284790818788374\n",
      "train loss:0.006908787739427322\n",
      "train loss:0.042234111871148505\n",
      "train loss:0.004836296781826552\n",
      "train loss:0.010272257792243483\n",
      "train loss:0.003003861929005538\n",
      "train loss:0.0039195219632599835\n",
      "train loss:0.011181136964032312\n",
      "train loss:0.09088279042846657\n",
      "train loss:0.004833437128367901\n",
      "train loss:0.033970466125818964\n",
      "train loss:0.039632207177868944\n",
      "train loss:0.007219187495260587\n",
      "train loss:0.030029788765194586\n",
      "train loss:0.06807380417098444\n",
      "train loss:0.01614832944717734\n",
      "train loss:0.025094464976010172\n",
      "train loss:0.014277142713555988\n",
      "train loss:0.021082195363203923\n",
      "train loss:0.015807077999867292\n",
      "train loss:0.04816697629392893\n",
      "train loss:0.005681998096356412\n",
      "train loss:0.06048824653584441\n",
      "train loss:0.026003200653564207\n",
      "train loss:0.011159270838277757\n",
      "train loss:0.013844264306699178\n",
      "train loss:0.0018710024702287356\n",
      "train loss:0.03731109428924644\n",
      "train loss:0.01099932724337327\n",
      "train loss:0.08276625390158936\n",
      "train loss:0.012851228783502349\n",
      "train loss:0.028106792173657497\n",
      "train loss:0.018306841573959044\n",
      "train loss:0.012304169126279452\n",
      "train loss:0.019586596610618557\n",
      "train loss:0.004300273427139271\n",
      "train loss:0.012651681444936856\n",
      "train loss:0.03012020921047217\n",
      "train loss:0.0048385050458815285\n",
      "train loss:0.016597938690588912\n",
      "train loss:0.004056777902190944\n",
      "train loss:0.006623017500306649\n",
      "train loss:0.00903585117147774\n",
      "train loss:0.005689062195598835\n",
      "train loss:0.029366304289261833\n",
      "train loss:0.015330109487911586\n",
      "train loss:0.006174882694929334\n",
      "train loss:0.028345537595727714\n",
      "train loss:0.005463140346271282\n",
      "train loss:0.0017439567842408412\n",
      "train loss:0.02533991377151756\n",
      "train loss:0.009799162879404867\n",
      "train loss:0.006632001908712436\n",
      "train loss:0.013992964430827244\n",
      "train loss:0.016727261293019494\n",
      "train loss:0.026079029019051327\n",
      "train loss:0.013651277496834766\n",
      "train loss:0.056090256531084084\n",
      "train loss:0.013452586521911704\n",
      "train loss:0.024436281845081505\n",
      "train loss:0.05808096884735548\n",
      "train loss:0.035637882997060014\n",
      "train loss:0.032924429827946085\n",
      "train loss:0.015550532701982145\n",
      "train loss:0.01093591334691746\n",
      "train loss:0.007252898659061494\n",
      "train loss:0.021527583299891028\n",
      "train loss:0.005977224954660311\n",
      "train loss:0.013606202403209805\n",
      "train loss:0.10650423588581967\n",
      "train loss:0.003557799727358922\n",
      "train loss:0.0037191431913821582\n",
      "train loss:0.06341626325128805\n",
      "train loss:0.06237094024624378\n",
      "train loss:0.01864414428812752\n",
      "train loss:0.021628574512312176\n",
      "train loss:0.0191605567815915\n",
      "train loss:0.03797408272647814\n",
      "train loss:0.008012652970458766\n",
      "train loss:0.007507725318909816\n",
      "train loss:0.033929323373951985\n",
      "train loss:0.012299384121390372\n",
      "train loss:0.05503672928652683\n",
      "train loss:0.008943186433672659\n",
      "train loss:0.0189392624181055\n",
      "train loss:0.05026568353530523\n",
      "train loss:0.008510258381048234\n",
      "train loss:0.017022103873169266\n",
      "train loss:0.008755973765089032\n",
      "train loss:0.015136910660115698\n",
      "train loss:0.022350347080142847\n",
      "train loss:0.039665712879644426\n",
      "train loss:0.013646137571187291\n",
      "train loss:0.013392193266522889\n",
      "train loss:0.011603751104625647\n",
      "train loss:0.010565709112839486\n",
      "train loss:0.0039208311596473115\n",
      "train loss:0.0015635509955003067\n",
      "train loss:0.024075649665712642\n",
      "train loss:0.005945307636879077\n",
      "train loss:0.006139372076089948\n",
      "train loss:0.02066499364227857\n",
      "train loss:0.012660078656047244\n",
      "train loss:0.01674231689167334\n",
      "train loss:0.0077017027160915\n",
      "train loss:0.0038324997291133878\n",
      "train loss:0.01662252219700355\n",
      "train loss:0.015064713879866305\n",
      "train loss:0.01685755289503626\n",
      "train loss:0.006002521145388145\n",
      "train loss:0.03772395788803244\n",
      "train loss:0.009089982472521766\n",
      "train loss:0.010506690880660587\n",
      "train loss:0.018992188275416103\n",
      "train loss:0.013856214171602514\n",
      "train loss:0.0031826570338356454\n",
      "train loss:0.017260405163257875\n",
      "train loss:0.018484153935178698\n",
      "train loss:0.034937073801808195\n",
      "train loss:0.015815379363886238\n",
      "train loss:0.0160706960259109\n",
      "train loss:0.00845181065098189\n",
      "train loss:0.012560393147156608\n",
      "train loss:0.006892438417111869\n",
      "train loss:0.034150116175126755\n",
      "train loss:0.008178239988706423\n",
      "train loss:0.01592108321191147\n",
      "train loss:0.02715138560762837\n",
      "train loss:0.015206768365447678\n",
      "train loss:0.0054787409492255855\n",
      "train loss:0.0066846163210655994\n",
      "train loss:0.021348642576187817\n",
      "train loss:0.02887945221340359\n",
      "train loss:0.0057474968467758234\n",
      "train loss:0.019402829515397565\n",
      "train loss:0.0033352546815257284\n",
      "train loss:0.0030105460912242038\n",
      "train loss:0.02443897993937826\n",
      "train loss:0.049560288336307545\n",
      "train loss:0.017279977389625462\n",
      "train loss:0.006885305159916486\n",
      "train loss:0.006373324010068994\n",
      "train loss:0.01804992728103087\n",
      "train loss:0.005564420720994024\n",
      "train loss:0.013822740829451684\n",
      "train loss:0.010391316240809688\n",
      "train loss:0.003563525082362969\n",
      "train loss:0.016729467495115655\n",
      "train loss:0.003689691979660228\n",
      "train loss:0.0039815121621987765\n",
      "train loss:0.0020818467721305496\n",
      "train loss:0.012744366862102348\n",
      "train loss:0.005091116872301526\n",
      "train loss:0.005681844496829708\n",
      "train loss:0.0057432170330962784\n",
      "train loss:0.008517905330489997\n",
      "train loss:0.012337780320971799\n",
      "train loss:0.017448690145292316\n",
      "train loss:0.06167722617917947\n",
      "train loss:0.03616798082042885\n",
      "train loss:0.007077201169603534\n",
      "train loss:0.00413523030539528\n",
      "train loss:0.0019149997660012586\n",
      "train loss:0.02215405574210426\n",
      "train loss:0.012020844755809936\n",
      "train loss:0.013981591024919488\n",
      "train loss:0.006380885487212243\n",
      "train loss:0.00515029254435371\n",
      "train loss:0.008497019794956524\n",
      "train loss:0.004842941666991019\n",
      "train loss:0.005306295620570797\n",
      "train loss:0.011776309901315203\n",
      "train loss:0.059987403019661885\n",
      "train loss:0.02709280084821152\n",
      "train loss:0.01505797769027164\n",
      "train loss:0.0039464869234729595\n",
      "train loss:0.004618707024658138\n",
      "train loss:0.037451200610166734\n",
      "train loss:0.003635345284165644\n",
      "train loss:0.0022986934724291224\n",
      "train loss:0.013302571732913049\n",
      "train loss:0.06433653781661743\n",
      "train loss:0.016788818986547438\n",
      "train loss:0.0023464477914391197\n",
      "train loss:0.0025252972839001774\n",
      "train loss:0.004991944102827708\n",
      "train loss:0.007002194985003765\n",
      "train loss:0.004946885554457484\n",
      "train loss:0.0060502256415929405\n",
      "train loss:0.017629370366834346\n",
      "train loss:0.013919300827612642\n",
      "train loss:0.006817140889966488\n",
      "train loss:0.032373022955297426\n",
      "train loss:0.002654441307207802\n",
      "train loss:0.0026963491621927104\n",
      "train loss:0.015448154252386326\n",
      "train loss:0.008885191704804798\n",
      "train loss:0.02018069394516033\n",
      "train loss:0.024646895759913457\n",
      "train loss:0.002355882192981133\n",
      "train loss:0.010501768590932724\n",
      "train loss:0.007558709772541919\n",
      "train loss:0.017896315134759432\n",
      "train loss:0.008891564645298111\n",
      "train loss:0.052359357573276866\n",
      "train loss:0.017124298521609436\n",
      "train loss:0.02927660887331539\n",
      "train loss:0.04178757874685665\n",
      "train loss:0.007627401862451433\n",
      "train loss:0.014021263866511719\n",
      "train loss:0.03023440027942621\n",
      "train loss:0.02102128467894053\n",
      "train loss:0.002538903017544591\n",
      "train loss:0.00965347468906747\n",
      "train loss:0.019523972805720897\n",
      "train loss:0.0014260697994489286\n",
      "train loss:0.013624909800025337\n",
      "train loss:0.006319518507948639\n",
      "train loss:0.009005149980738314\n",
      "train loss:0.007864927947053383\n",
      "train loss:0.02869370495800943\n",
      "train loss:0.016978946275787377\n",
      "train loss:0.012124883444528783\n",
      "train loss:0.018060588676461913\n",
      "train loss:0.012175386396296941\n",
      "train loss:0.06604875500822514\n",
      "train loss:0.022120991716206108\n",
      "train loss:0.018770497724278336\n",
      "train loss:0.01084799793583282\n",
      "train loss:0.035754086944678555\n",
      "train loss:0.02035901834947789\n",
      "train loss:0.010788311132374336\n",
      "train loss:0.004986904127462865\n",
      "train loss:0.02627732332283319\n",
      "train loss:0.03603403703264549\n",
      "train loss:0.01573272930698441\n",
      "train loss:0.012806163969737903\n",
      "train loss:0.0049374266925727925\n",
      "train loss:0.028002955955383105\n",
      "train loss:0.005499188934592983\n",
      "train loss:0.017755389183709433\n",
      "train loss:0.012258864480693086\n",
      "train loss:0.005906038017499044\n",
      "train loss:0.005881381521021091\n",
      "train loss:0.005176951464501992\n",
      "train loss:0.024459110900136073\n",
      "train loss:0.003039258158395925\n",
      "train loss:0.009672897998734685\n",
      "train loss:0.017817245858647218\n",
      "train loss:0.004619478903526433\n",
      "train loss:0.01369112507234689\n",
      "train loss:0.014109838213489727\n",
      "train loss:0.018591372908099885\n",
      "train loss:0.029489806074519623\n",
      "train loss:0.03819813796468958\n",
      "train loss:0.019617713638104765\n",
      "train loss:0.013619365904730583\n",
      "train loss:0.006252389390571101\n",
      "train loss:0.01705360646948488\n",
      "train loss:0.003317751335891652\n",
      "train loss:0.011233599523938802\n",
      "train loss:0.04597169462404091\n",
      "train loss:0.004303126961746548\n",
      "train loss:0.07400265799849666\n",
      "train loss:0.017460613097131622\n",
      "train loss:0.01400405292475702\n",
      "train loss:0.006262290762177492\n",
      "train loss:0.007715150513993003\n",
      "train loss:0.0013091247233197436\n",
      "train loss:0.004988095344951159\n",
      "train loss:0.06365887992812908\n",
      "train loss:0.05236813132551652\n",
      "train loss:0.010275312121483802\n",
      "train loss:0.013412083555496142\n",
      "train loss:0.019717929951726366\n",
      "train loss:0.015670900505345052\n",
      "train loss:0.005218379494278217\n",
      "train loss:0.008891686947016485\n",
      "train loss:0.02135582228351562\n",
      "train loss:0.009931861693314257\n",
      "train loss:0.006949143159414709\n",
      "train loss:0.04369553667556735\n",
      "train loss:0.014722956397682466\n",
      "train loss:0.008932024361708453\n",
      "train loss:0.024446092740560907\n",
      "train loss:0.0017652999142193082\n",
      "train loss:0.05545973611565\n",
      "train loss:0.003703002568159001\n",
      "train loss:0.013081280463985519\n",
      "train loss:0.036916679671068285\n",
      "train loss:0.10007159435575876\n",
      "train loss:0.009351816186425655\n",
      "train loss:0.015049382201737172\n",
      "train loss:0.0023060615658048304\n",
      "train loss:0.01318067244517988\n",
      "train loss:0.04458220273292093\n",
      "train loss:0.011546577909295157\n",
      "train loss:0.008122591316927545\n",
      "train loss:0.009844038343377983\n",
      "train loss:0.02441309881998705\n",
      "train loss:0.03930931642276431\n",
      "train loss:0.011269970633795548\n",
      "train loss:0.04741891722390619\n",
      "train loss:0.007130344641813708\n",
      "train loss:0.04103762876031251\n",
      "train loss:0.0316192330425574\n",
      "train loss:0.00870476929275458\n",
      "train loss:0.007053588022250293\n",
      "train loss:0.01211279040941728\n",
      "train loss:0.007370134054891972\n",
      "train loss:0.003385877144645855\n",
      "train loss:0.05132097631567888\n",
      "train loss:0.006002425015891718\n",
      "train loss:0.004216279525196522\n",
      "train loss:0.014379702533687396\n",
      "train loss:0.024282031462160743\n",
      "train loss:0.0192240253120614\n",
      "train loss:0.024014604632002733\n",
      "train loss:0.04339796089543813\n",
      "train loss:0.0417227749977057\n",
      "train loss:0.02277005056301803\n",
      "train loss:0.03436559070672061\n",
      "train loss:0.008874791961149141\n",
      "train loss:0.010435634931411005\n",
      "train loss:0.006371936537899348\n",
      "train loss:0.016327803458409756\n",
      "train loss:0.012705998189234225\n",
      "train loss:0.07479547010073173\n",
      "train loss:0.011490055470483217\n",
      "train loss:0.025861479124388423\n",
      "train loss:0.026771095365018933\n",
      "train loss:0.015482789876396326\n",
      "train loss:0.01497969812356908\n",
      "train loss:0.016384764154048204\n",
      "train loss:0.011352202034995327\n",
      "train loss:0.01874930253097392\n",
      "train loss:0.012531227376397116\n",
      "train loss:0.007679134942435059\n",
      "train loss:0.025222474133469147\n",
      "train loss:0.007811228813712651\n",
      "train loss:0.0014319868118343555\n",
      "train loss:0.0028755955103509836\n",
      "train loss:0.007727000060427889\n",
      "train loss:0.006874837389958894\n",
      "train loss:0.030921764680659325\n",
      "train loss:0.009956684801475596\n",
      "train loss:0.0033514610167501035\n",
      "train loss:0.03881143819106304\n",
      "train loss:0.03194888035587871\n",
      "train loss:0.0174986251997537\n",
      "train loss:0.008331198789596879\n",
      "train loss:0.005786017329014455\n",
      "train loss:0.006338171522674809\n",
      "train loss:0.008747742925972322\n",
      "train loss:0.005678065301000868\n",
      "train loss:0.0095578397673025\n",
      "train loss:0.0029694554590967014\n",
      "train loss:0.005161849489017433\n",
      "train loss:0.022566140241457592\n",
      "train loss:0.03065333370771921\n",
      "train loss:0.00911513771717559\n",
      "train loss:0.016635258648841827\n",
      "train loss:0.007200042673535345\n",
      "train loss:0.0033005256522615546\n",
      "train loss:0.04810673532639062\n",
      "train loss:0.0017672744549470437\n",
      "train loss:0.008546996225439894\n",
      "train loss:0.04546186619616074\n",
      "train loss:0.019990431621460667\n",
      "train loss:0.006200055919660965\n",
      "train loss:0.029719009302677266\n",
      "train loss:0.03817253905279761\n",
      "train loss:0.023997203055585867\n",
      "train loss:0.01839211993466558\n",
      "train loss:0.0025521979882498925\n",
      "train loss:0.03381946221684912\n",
      "train loss:0.04247572486697915\n",
      "train loss:0.021222674970852336\n",
      "train loss:0.003738557491271795\n",
      "train loss:0.007264133335520805\n",
      "train loss:0.03060684000743701\n",
      "train loss:0.007518835745884764\n",
      "train loss:0.013904343679830957\n",
      "train loss:0.0067823811361411536\n",
      "train loss:0.017230428698680855\n",
      "train loss:0.020871881713840496\n",
      "train loss:0.013955211199231918\n",
      "train loss:0.016008601484247136\n",
      "train loss:0.03813229043493214\n",
      "train loss:0.005657188239441242\n",
      "train loss:0.03794282380516406\n",
      "train loss:0.07694125208068092\n",
      "train loss:0.009054386567263857\n",
      "train loss:0.03549263181664026\n",
      "train loss:0.007911310444438081\n",
      "train loss:0.004715869201913688\n",
      "train loss:0.01154577234838084\n",
      "train loss:0.010984861516968772\n",
      "train loss:0.005206147983811784\n",
      "train loss:0.02583530948854273\n",
      "train loss:0.006685084207570484\n",
      "train loss:0.03514131745547347\n",
      "train loss:0.015240872790982645\n",
      "train loss:0.009342504671622233\n",
      "train loss:0.007997009024254674\n",
      "train loss:0.00895610473651693\n",
      "train loss:0.030192613399401776\n",
      "train loss:0.002696354249088075\n",
      "train loss:0.029675359136849357\n",
      "train loss:0.006835143105998819\n",
      "train loss:0.004659627951332787\n",
      "train loss:0.07201765574279805\n",
      "train loss:0.004450970643285605\n",
      "train loss:0.003959283051432609\n",
      "train loss:0.01191759300235793\n",
      "train loss:0.016012447317370768\n",
      "train loss:0.014697430048301872\n",
      "train loss:0.0040717525966407235\n",
      "train loss:0.030362356230590245\n",
      "train loss:0.03999462329128973\n",
      "train loss:0.0054136173579319834\n",
      "train loss:0.0037717466528880334\n",
      "train loss:0.009846188169048437\n",
      "train loss:0.0046095425249005305\n",
      "train loss:0.006583417735583823\n",
      "train loss:0.007992816119129205\n",
      "train loss:0.0119078127199618\n",
      "train loss:0.006606653154417464\n",
      "train loss:0.021015153086888443\n",
      "train loss:0.013781445554928828\n",
      "train loss:0.04456083515377985\n",
      "train loss:0.0059066015420543225\n",
      "train loss:0.01490136738658503\n",
      "train loss:0.00809299410150161\n",
      "train loss:0.011502511788086282\n",
      "train loss:0.009972068021567382\n",
      "train loss:0.015674854346224926\n",
      "train loss:0.02248299458082924\n",
      "=== epoch:8, train acc:0.992, test acc:0.987 ===\n",
      "train loss:0.002759495015232721\n",
      "train loss:0.023684979593773945\n",
      "train loss:0.0035391441817296714\n",
      "train loss:0.006270086963730023\n",
      "train loss:0.01778730642538088\n",
      "train loss:0.04869012178437649\n",
      "train loss:0.053944177220223766\n",
      "train loss:0.003411578365525848\n",
      "train loss:0.005312596749711073\n",
      "train loss:0.011583453737299784\n",
      "train loss:0.025430133188585922\n",
      "train loss:0.054409338908246674\n",
      "train loss:0.00776740132459313\n",
      "train loss:0.008102196102571606\n",
      "train loss:0.02008741294729597\n",
      "train loss:0.044300641853861296\n",
      "train loss:0.012926379399637864\n",
      "train loss:0.008532092838892303\n",
      "train loss:0.0016347845256114279\n",
      "train loss:0.014261265119794276\n",
      "train loss:0.06493599851912171\n",
      "train loss:0.01771227842605572\n",
      "train loss:0.008365180413273714\n",
      "train loss:0.008589401638047308\n",
      "train loss:0.01324302115928207\n",
      "train loss:0.007142515076249618\n",
      "train loss:0.013038042250579305\n",
      "train loss:0.027541269100628058\n",
      "train loss:0.03596416084314399\n",
      "train loss:0.028306947154426224\n",
      "train loss:0.09420030270982414\n",
      "train loss:0.011946282757686293\n",
      "train loss:0.010014274400062372\n",
      "train loss:0.010042259767491108\n",
      "train loss:0.004375834226334741\n",
      "train loss:0.014589333718004463\n",
      "train loss:0.03133441059623277\n",
      "train loss:0.011895369449477616\n",
      "train loss:0.04181235351103965\n",
      "train loss:0.006553614843718194\n",
      "train loss:0.014800677729220042\n",
      "train loss:0.005985632134814972\n",
      "train loss:0.014529818781546496\n",
      "train loss:0.009316001292438626\n",
      "train loss:0.01822395952748148\n",
      "train loss:0.028836724833441317\n",
      "train loss:0.014267087914197838\n",
      "train loss:0.007604172205368318\n",
      "train loss:0.002276467026656099\n",
      "train loss:0.005370088153744363\n",
      "train loss:0.03617866340455702\n",
      "train loss:0.008759735743584008\n",
      "train loss:0.00376746787308601\n",
      "train loss:0.006698843282448128\n",
      "train loss:0.0055560833242003895\n",
      "train loss:0.05023359221812301\n",
      "train loss:0.009310104393862153\n",
      "train loss:0.04853392877734766\n",
      "train loss:0.0038880655204424886\n",
      "train loss:0.020009772789724874\n",
      "train loss:0.007453733997442614\n",
      "train loss:0.0032914605651505457\n",
      "train loss:0.00395768489727673\n",
      "train loss:0.0065551477067457295\n",
      "train loss:0.009115625491075318\n",
      "train loss:0.004817234961354488\n",
      "train loss:0.008204069800109168\n",
      "train loss:0.005788928391999699\n",
      "train loss:0.03526335144660884\n",
      "train loss:0.004373248854509997\n",
      "train loss:0.01739533990686467\n",
      "train loss:0.015956428249298146\n",
      "train loss:0.01245719675341158\n",
      "train loss:0.0028035665166455054\n",
      "train loss:0.031597359080891896\n",
      "train loss:0.04911235589876504\n",
      "train loss:0.02568718456364626\n",
      "train loss:0.005388205975456977\n",
      "train loss:0.0726510284356939\n",
      "train loss:0.01939099278343675\n",
      "train loss:0.01659499890079768\n",
      "train loss:0.006276834161377697\n",
      "train loss:0.0027230288290525195\n",
      "train loss:0.0057576051780759375\n",
      "train loss:0.0013032888146288493\n",
      "train loss:0.007858958900375568\n",
      "train loss:0.0026763623482528092\n",
      "train loss:0.003903200108558361\n",
      "train loss:0.013722169994774214\n",
      "train loss:0.0639110585708236\n",
      "train loss:0.013676389249665135\n",
      "train loss:0.0440438397548688\n",
      "train loss:0.023078951145463007\n",
      "train loss:0.010081979084655903\n",
      "train loss:0.010822179369885188\n",
      "train loss:0.007497839201404024\n",
      "train loss:0.0065919433669926805\n",
      "train loss:0.07682178610420513\n",
      "train loss:0.004596610458160688\n",
      "train loss:0.012408582491347084\n",
      "train loss:0.005048998860468401\n",
      "train loss:0.01699701431161636\n",
      "train loss:0.009883065276616705\n",
      "train loss:0.07330332322130802\n",
      "train loss:0.013931169995979466\n",
      "train loss:0.025272271936399412\n",
      "train loss:0.006238140505734842\n",
      "train loss:0.012829774212205915\n",
      "train loss:0.05550352309927586\n",
      "train loss:0.013367803583234139\n",
      "train loss:0.014676190417432728\n",
      "train loss:0.01936019814306834\n",
      "train loss:0.009659564740892381\n",
      "train loss:0.005152334637499783\n",
      "train loss:0.017801656253268117\n",
      "train loss:0.018081398268484632\n",
      "train loss:0.0041796894090310515\n",
      "train loss:0.007863810156365765\n",
      "train loss:0.06968563756337096\n",
      "train loss:0.005589076922607489\n",
      "train loss:0.03346149511331875\n",
      "train loss:0.0032854387649455486\n",
      "train loss:0.010561550485187534\n",
      "train loss:0.011024320259427038\n",
      "train loss:0.01521824956285818\n",
      "train loss:0.006116636230452815\n",
      "train loss:0.03463200012965464\n",
      "train loss:0.009710876827809863\n",
      "train loss:0.0186270358431981\n",
      "train loss:0.010564084302052738\n",
      "train loss:0.014391418009015278\n",
      "train loss:0.027310877446601812\n",
      "train loss:0.024842845522287074\n",
      "train loss:0.038627338556246345\n",
      "train loss:0.009587777256748548\n",
      "train loss:0.028204943062508914\n",
      "train loss:0.004712911367411244\n",
      "train loss:0.04514171092079983\n",
      "train loss:0.02004707534021023\n",
      "train loss:0.1168651170197298\n",
      "train loss:0.011722561120127949\n",
      "train loss:0.02135435185642062\n",
      "train loss:0.03760935996541339\n",
      "train loss:0.0019617246722557886\n",
      "train loss:0.0072603038246233575\n",
      "train loss:0.015302276264964065\n",
      "train loss:0.031816814613658345\n",
      "train loss:0.03301839737198401\n",
      "train loss:0.004112209179105419\n",
      "train loss:0.026251088425659777\n",
      "train loss:0.009075330696763124\n",
      "train loss:0.02140874450362553\n",
      "train loss:0.005911454644565054\n",
      "train loss:0.0042310811225048026\n",
      "train loss:0.013197396795503483\n",
      "train loss:0.01387215326629627\n",
      "train loss:0.012986893372871509\n",
      "train loss:0.012741482946560688\n",
      "train loss:0.013190113329922022\n",
      "train loss:0.03796534149505189\n",
      "train loss:0.004305046280628404\n",
      "train loss:0.010755015638250463\n",
      "train loss:0.013064954301009114\n",
      "train loss:0.0038695534559314\n",
      "train loss:0.016484871791974807\n",
      "train loss:0.018169900892116123\n",
      "train loss:0.054710922794127315\n",
      "train loss:0.01490139043506251\n",
      "train loss:0.04371558089639099\n",
      "train loss:0.02651566143291208\n",
      "train loss:0.0075838443527816\n",
      "train loss:0.0016450776152004555\n",
      "train loss:0.03476628574364544\n",
      "train loss:0.011046603657708248\n",
      "train loss:0.026001874329579566\n",
      "train loss:0.004038824969995923\n",
      "train loss:0.04093963829108401\n",
      "train loss:0.0221721641955598\n",
      "train loss:0.045676879997960204\n",
      "train loss:0.0048314331552424495\n",
      "train loss:0.01456356209531246\n",
      "train loss:0.003759557580559958\n",
      "train loss:0.007899849150265951\n",
      "train loss:0.003949944081316422\n",
      "train loss:0.010260025978174343\n",
      "train loss:0.020515231093159697\n",
      "train loss:0.015537885782625199\n",
      "train loss:0.09430611821580849\n",
      "train loss:0.014589636684436724\n",
      "train loss:0.009868107611229415\n",
      "train loss:0.01361614904199283\n",
      "train loss:0.007469723004013251\n",
      "train loss:0.04298411825180911\n",
      "train loss:0.0028260412340128734\n",
      "train loss:0.01650875613130132\n",
      "train loss:0.0011232554732312674\n",
      "train loss:0.01036735736457832\n",
      "train loss:0.007589924920817103\n",
      "train loss:0.0328750011438875\n",
      "train loss:0.005146696513837966\n",
      "train loss:0.006218955953177062\n",
      "train loss:0.07212573414777176\n",
      "train loss:0.008347152733812533\n",
      "train loss:0.03846040795566639\n",
      "train loss:0.009239173315083617\n",
      "train loss:0.044916366036140375\n",
      "train loss:0.01088903220536727\n",
      "train loss:0.03147581908355148\n",
      "train loss:0.017162244021772344\n",
      "train loss:0.021658509013883186\n",
      "train loss:0.0019456104999495368\n",
      "train loss:0.009753212146154272\n",
      "train loss:0.019133905891142534\n",
      "train loss:0.002361592986978475\n",
      "train loss:0.01255797953684462\n",
      "train loss:0.008060922352442872\n",
      "train loss:0.042577465970337985\n",
      "train loss:0.034025417002656815\n",
      "train loss:0.006616475930050752\n",
      "train loss:0.0011808740019791657\n",
      "train loss:0.031347251575249244\n",
      "train loss:0.014885270384109726\n",
      "train loss:0.003039043928642467\n",
      "train loss:0.008955802712348098\n",
      "train loss:0.0469074356829835\n",
      "train loss:0.016551677394469284\n",
      "train loss:0.03693160281944877\n",
      "train loss:0.012203113232631369\n",
      "train loss:0.0032738823091909073\n",
      "train loss:0.016365638132588192\n",
      "train loss:0.026523727674663503\n",
      "train loss:0.0534142038144598\n",
      "train loss:0.01689594085650569\n",
      "train loss:0.01076270140770783\n",
      "train loss:0.005754597937482853\n",
      "train loss:0.009018661652647263\n",
      "train loss:0.011308469893462746\n",
      "train loss:0.008316638288617966\n",
      "train loss:0.01914274947382689\n",
      "train loss:0.010269897640100245\n",
      "train loss:0.007229330311050855\n",
      "train loss:0.05646793079374505\n",
      "train loss:0.03837801091498712\n",
      "train loss:0.02146183860202422\n",
      "train loss:0.055820960093447634\n",
      "train loss:0.008312632695345849\n",
      "train loss:0.005870067107075726\n",
      "train loss:0.004974430869249874\n",
      "train loss:0.004234341149894872\n",
      "train loss:0.05110395636945179\n",
      "train loss:0.01651508472269242\n",
      "train loss:0.041305198005455325\n",
      "train loss:0.009353615179074034\n",
      "train loss:0.004774445033938034\n",
      "train loss:0.028937105186283354\n",
      "train loss:0.014101151791760563\n",
      "train loss:0.007449080537500488\n",
      "train loss:0.025707705526481\n",
      "train loss:0.0035840447636880358\n",
      "train loss:0.012546090210269463\n",
      "train loss:0.01363140782480211\n",
      "train loss:0.02312107937258292\n",
      "train loss:0.009738426286355307\n",
      "train loss:0.010518980101651993\n",
      "train loss:0.039099948618052174\n",
      "train loss:0.001997749752829124\n",
      "train loss:0.06066390332370619\n",
      "train loss:0.016253665714419364\n",
      "train loss:0.0028091262730584564\n",
      "train loss:0.0058207995089373885\n",
      "train loss:0.006476360331976485\n",
      "train loss:0.059478632394803305\n",
      "train loss:0.014105307016608852\n",
      "train loss:0.006199243256578156\n",
      "train loss:0.003316490827762729\n",
      "train loss:0.014517405235186602\n",
      "train loss:0.008615519197776844\n",
      "train loss:0.002466875243984038\n",
      "train loss:0.00736691740193438\n",
      "train loss:0.038323993285356114\n",
      "train loss:0.005101279527895456\n",
      "train loss:0.013023213781388399\n",
      "train loss:0.02084228261249017\n",
      "train loss:0.006862276238004279\n",
      "train loss:0.004453041949324569\n",
      "train loss:0.010011690285771084\n",
      "train loss:0.006246773731040132\n",
      "train loss:0.036964374263464814\n",
      "train loss:0.010305944458348674\n",
      "train loss:0.005709993455529219\n",
      "train loss:0.02897707697652894\n",
      "train loss:0.012265100096981096\n",
      "train loss:0.03792795390983414\n",
      "train loss:0.020207243626244483\n",
      "train loss:0.002962922308139851\n",
      "train loss:0.005730180847825783\n",
      "train loss:0.013658071895061727\n",
      "train loss:0.0048045023176132426\n",
      "train loss:0.026384740555569674\n",
      "train loss:0.02202519327654572\n",
      "train loss:0.04165693730787183\n",
      "train loss:0.003747270443637062\n",
      "train loss:0.017388478299434067\n",
      "train loss:0.007782961022841816\n",
      "train loss:0.013692687424037994\n",
      "train loss:0.04817147800928089\n",
      "train loss:0.02851865657332636\n",
      "train loss:0.012896357952093776\n",
      "train loss:0.005493624198249339\n",
      "train loss:0.0036517435946049025\n",
      "train loss:0.014459762011678154\n",
      "train loss:0.006778696193365109\n",
      "train loss:0.012383094415256044\n",
      "train loss:0.001859339137839116\n",
      "train loss:0.010234133940054395\n",
      "train loss:0.010635881411916424\n",
      "train loss:0.032184295239112155\n",
      "train loss:0.005860877357859153\n",
      "train loss:0.021806312824341072\n",
      "train loss:0.006644875204768729\n",
      "train loss:0.02595115981657005\n",
      "train loss:0.008146625508800982\n",
      "train loss:0.007962421445776288\n",
      "train loss:0.04548917730863347\n",
      "train loss:0.012622772597705494\n",
      "train loss:0.021198687008419065\n",
      "train loss:0.02010518140436042\n",
      "train loss:0.025798806659692808\n",
      "train loss:0.005489947628830344\n",
      "train loss:0.015496360842150196\n",
      "train loss:0.022381807501095464\n",
      "train loss:0.06880843723143155\n",
      "train loss:0.013768079741232054\n",
      "train loss:0.01980389609174267\n",
      "train loss:0.024802081345685754\n",
      "train loss:0.007204000825368033\n",
      "train loss:0.001568868270930571\n",
      "train loss:0.004514492290252631\n",
      "train loss:0.009646246088990373\n",
      "train loss:0.027873656752660868\n",
      "train loss:0.0053916140493594465\n",
      "train loss:0.004343671813028853\n",
      "train loss:0.027272371884342415\n",
      "train loss:0.006107224385353504\n",
      "train loss:0.020071761570689187\n",
      "train loss:0.01692001640003021\n",
      "train loss:0.008202636882115128\n",
      "train loss:0.0063374065420622235\n",
      "train loss:0.002253732028260972\n",
      "train loss:0.006329747947828024\n",
      "train loss:0.015874245480310144\n",
      "train loss:0.02069265043956179\n",
      "train loss:0.0030814364699802664\n",
      "train loss:0.018292253995946774\n",
      "train loss:0.0016865001309396002\n",
      "train loss:0.03610590034631475\n",
      "train loss:0.010259258152857354\n",
      "train loss:0.06818579595935505\n",
      "train loss:0.0074732695503312244\n",
      "train loss:0.010245872286897438\n",
      "train loss:0.009257400404468836\n",
      "train loss:0.007307149949809319\n",
      "train loss:0.016799132425440188\n",
      "train loss:0.014552930379801496\n",
      "train loss:0.007613817235045644\n",
      "train loss:0.09182902472722083\n",
      "train loss:0.006037727199797026\n",
      "train loss:0.0032844575906851507\n",
      "train loss:0.0077953132930557315\n",
      "train loss:0.0077248352100248\n",
      "train loss:0.00610322283650545\n",
      "train loss:0.02237944408189863\n",
      "train loss:0.011014403263528025\n",
      "train loss:0.06421266469385162\n",
      "train loss:0.012257612044484738\n",
      "train loss:0.012793003569997921\n",
      "train loss:0.028678866726016286\n",
      "train loss:0.011847792154146384\n",
      "train loss:0.003670860127323322\n",
      "train loss:0.003425965859607846\n",
      "train loss:0.016628516320034484\n",
      "train loss:0.013786920265806031\n",
      "train loss:0.005574657589144427\n",
      "train loss:0.01828425922284509\n",
      "train loss:0.013611526853891377\n",
      "train loss:0.009788072586897307\n",
      "train loss:0.06652846954521546\n",
      "train loss:0.005955998837553732\n",
      "train loss:0.00664974123819515\n",
      "train loss:0.011111867751324802\n",
      "train loss:0.00607369259370324\n",
      "train loss:0.05325295339061224\n",
      "train loss:0.0032380072425330286\n",
      "train loss:0.007104584800601219\n",
      "train loss:0.009981693723621905\n",
      "train loss:0.02175361706963734\n",
      "train loss:0.00614313513186316\n",
      "train loss:0.0039513289929508045\n",
      "train loss:0.03476436088524019\n",
      "train loss:0.0016163884929254813\n",
      "train loss:0.0038525227664901853\n",
      "train loss:0.008118247593065947\n",
      "train loss:0.004249162771595536\n",
      "train loss:0.0014122660386711747\n",
      "train loss:0.004683876739513621\n",
      "train loss:0.07895331767481432\n",
      "train loss:0.015877468278779713\n",
      "train loss:0.016687873094645294\n",
      "train loss:0.0037812498033029836\n",
      "train loss:0.0014753981547543965\n",
      "train loss:0.005591534258892027\n",
      "train loss:0.004107907429855083\n",
      "train loss:0.005773350545215863\n",
      "train loss:0.058948349707924105\n",
      "train loss:0.013413334835412717\n",
      "train loss:0.015019526329947915\n",
      "train loss:0.0068901281116756155\n",
      "train loss:0.011464317726496018\n",
      "train loss:0.033612363563884674\n",
      "train loss:0.01167740190884921\n",
      "train loss:0.008406438062748114\n",
      "train loss:0.0009868272584199827\n",
      "train loss:0.004750957170310664\n",
      "train loss:0.008109164283134235\n",
      "train loss:0.007524239577534421\n",
      "train loss:0.003344718112856612\n",
      "train loss:0.01585073172777179\n",
      "train loss:0.014509083040627353\n",
      "train loss:0.0058187535872209015\n",
      "train loss:0.02753992968124321\n",
      "train loss:0.02416983459328836\n",
      "train loss:0.020351947000606732\n",
      "train loss:0.0024082445935086037\n",
      "train loss:0.003964918209305205\n",
      "train loss:0.005365850167019087\n",
      "train loss:0.0027833702739712817\n",
      "train loss:0.004299093496432668\n",
      "train loss:0.017797353446774265\n",
      "train loss:0.017249659235465887\n",
      "train loss:0.017905808075325977\n",
      "train loss:0.024183140202783383\n",
      "train loss:0.0026952672718326526\n",
      "train loss:0.011190033372393542\n",
      "train loss:0.007986082869564887\n",
      "train loss:0.023539441304005652\n",
      "train loss:0.01970279496941141\n",
      "train loss:0.008774433641765755\n",
      "train loss:0.01941759258750435\n",
      "train loss:0.023030687290722347\n",
      "train loss:0.008421758751403618\n",
      "train loss:0.0038273595342664464\n",
      "train loss:0.026125663002620994\n",
      "train loss:0.007992034411556435\n",
      "train loss:0.10606700571746709\n",
      "train loss:0.0022251651408631682\n",
      "train loss:0.028119998285076497\n",
      "train loss:0.009473329476903103\n",
      "train loss:0.04204297397722132\n",
      "train loss:0.023893514598891635\n",
      "train loss:0.008036912004307168\n",
      "train loss:0.005113902319114679\n",
      "train loss:0.009448753238625074\n",
      "train loss:0.021329418712685372\n",
      "train loss:0.01036592391418716\n",
      "train loss:0.001511973776506168\n",
      "train loss:0.0025923545136328483\n",
      "train loss:0.03533595391688702\n",
      "train loss:0.008672880566466323\n",
      "train loss:0.004539365142298933\n",
      "train loss:0.02446462263872239\n",
      "train loss:0.010011365471660157\n",
      "train loss:0.0021783682475169162\n",
      "train loss:0.015406711196765534\n",
      "train loss:0.015047084129670617\n",
      "train loss:0.023670212297685574\n",
      "train loss:0.009819309972820631\n",
      "train loss:0.0036591846527056658\n",
      "train loss:0.007928499424847673\n",
      "train loss:0.02735230619686445\n",
      "train loss:0.02511751096486056\n",
      "train loss:0.01441177345668036\n",
      "train loss:0.00222862642096352\n",
      "train loss:0.05579747625850727\n",
      "train loss:0.0018494023178399794\n",
      "train loss:0.019191897065324096\n",
      "train loss:0.06963719949150433\n",
      "train loss:0.007825115070341332\n",
      "train loss:0.004597538513327217\n",
      "train loss:0.011696361071388972\n",
      "train loss:0.010080898634116707\n",
      "train loss:0.005504204398258361\n",
      "train loss:0.001990116190564611\n",
      "train loss:0.02316638324468179\n",
      "train loss:0.013913326545198645\n",
      "train loss:0.003194408770237138\n",
      "train loss:0.003385602388628733\n",
      "train loss:0.009508080553195353\n",
      "train loss:0.011076133350124645\n",
      "train loss:0.016179992641452662\n",
      "train loss:0.06116338659896679\n",
      "train loss:0.039363582703017846\n",
      "train loss:0.010890693470757202\n",
      "train loss:0.014741535945888095\n",
      "train loss:0.005560665410160932\n",
      "train loss:0.00751337496919255\n",
      "train loss:0.035276673754573926\n",
      "train loss:0.02480465741390295\n",
      "train loss:0.028668518468670294\n",
      "train loss:0.028706140436708858\n",
      "train loss:0.012626620786190722\n",
      "train loss:0.006015195872320611\n",
      "train loss:0.006574101222409204\n",
      "train loss:0.010481931007442316\n",
      "train loss:0.002794195160453122\n",
      "train loss:0.004467309993949688\n",
      "train loss:0.004147727867400758\n",
      "train loss:0.011087911236496653\n",
      "train loss:0.005225320011370145\n",
      "train loss:0.009879060471952803\n",
      "train loss:0.01791747534623948\n",
      "train loss:0.0004476247497004123\n",
      "train loss:0.005250825302160388\n",
      "train loss:0.0048602722985378885\n",
      "train loss:0.003419340004282675\n",
      "train loss:0.005694513071008516\n",
      "train loss:0.0029788054852874835\n",
      "train loss:0.006635333750948895\n",
      "train loss:0.0007160189810272498\n",
      "train loss:0.003028336355687073\n",
      "train loss:0.0016835540010990552\n",
      "train loss:0.020968021812525012\n",
      "train loss:0.005874707857793873\n",
      "train loss:0.01406218574922933\n",
      "train loss:0.01266874396247466\n",
      "train loss:0.007005472963006116\n",
      "train loss:0.007783449672237195\n",
      "train loss:0.031242364893642148\n",
      "train loss:0.003968024439613876\n",
      "train loss:0.0006821108930380813\n",
      "train loss:0.018823261337544847\n",
      "train loss:0.014239982245111632\n",
      "train loss:0.002312251152553389\n",
      "train loss:0.014398297368926212\n",
      "train loss:0.01600358563989475\n",
      "train loss:0.0025348505383716025\n",
      "train loss:0.02215211333611796\n",
      "train loss:0.0028852285641301333\n",
      "train loss:0.005205373177836064\n",
      "train loss:0.012311740597606419\n",
      "train loss:0.0043238485245942855\n",
      "train loss:0.0015592104191360543\n",
      "train loss:0.009953920685767595\n",
      "train loss:0.048643987391017746\n",
      "train loss:0.04063661325294098\n",
      "train loss:0.006625441856201802\n",
      "train loss:0.06371233268633358\n",
      "train loss:0.006374693491900011\n",
      "train loss:0.009390377039662768\n",
      "train loss:0.007752778638663448\n",
      "train loss:0.004704665106249407\n",
      "train loss:0.013848973786188903\n",
      "train loss:0.01663386062813523\n",
      "train loss:0.013661249805658762\n",
      "train loss:0.013948053442111885\n",
      "train loss:0.009996737540864754\n",
      "train loss:0.008073406940720748\n",
      "train loss:0.005138075191618246\n",
      "train loss:0.0032721852688827866\n",
      "train loss:0.0010787363633282636\n",
      "train loss:0.0020245716772223435\n",
      "train loss:0.0020890636942494157\n",
      "train loss:0.01674012793003069\n",
      "train loss:0.01913305683683105\n",
      "train loss:0.03516093936396658\n",
      "train loss:0.026155678716167613\n",
      "train loss:0.009639288046836635\n",
      "train loss:0.0041872070975286285\n",
      "train loss:0.003830241655804812\n",
      "train loss:0.0030709917735259958\n",
      "train loss:0.002247165575674823\n",
      "train loss:0.009141843163983863\n",
      "train loss:0.1034328036386423\n",
      "train loss:0.015534331804107017\n",
      "train loss:0.009329327046643967\n",
      "train loss:0.011111172800374745\n",
      "train loss:0.029523789520080147\n",
      "train loss:0.01871832688560158\n",
      "train loss:0.010054258136158085\n",
      "train loss:0.024932146848199045\n",
      "train loss:0.004079482623206532\n",
      "train loss:0.041205051925280796\n",
      "train loss:0.013261078624789726\n",
      "train loss:0.007464957124743189\n",
      "train loss:0.0045991143374292795\n",
      "train loss:0.019087452758735343\n",
      "train loss:0.0027983756300191754\n",
      "train loss:0.009032253883994451\n",
      "train loss:0.009271007421805439\n",
      "train loss:0.011656192882112695\n",
      "train loss:0.003589427118234721\n",
      "=== epoch:9, train acc:0.99, test acc:0.988 ===\n",
      "train loss:0.014100082284929729\n",
      "train loss:0.01547323484741387\n",
      "train loss:0.0034423731318016697\n",
      "train loss:0.007515062170886644\n",
      "train loss:0.00923723742522145\n",
      "train loss:0.016387507462235062\n",
      "train loss:0.030484199512363218\n",
      "train loss:0.02157666332496542\n",
      "train loss:0.0031399668292142065\n",
      "train loss:0.022872302182675986\n",
      "train loss:0.028996743457011354\n",
      "train loss:0.0063598030823148076\n",
      "train loss:0.00892453060812319\n",
      "train loss:0.00964768345246738\n",
      "train loss:0.008862216772825381\n",
      "train loss:0.006720890499055044\n",
      "train loss:0.0008362470824937362\n",
      "train loss:0.002486582996278681\n",
      "train loss:0.007508711496707774\n",
      "train loss:0.013011077360284402\n",
      "train loss:0.00439659921224758\n",
      "train loss:0.012756374327231257\n",
      "train loss:0.009674915088576467\n",
      "train loss:0.061419016658865094\n",
      "train loss:0.0021770475707237067\n",
      "train loss:0.013436954954665908\n",
      "train loss:0.019603204593712956\n",
      "train loss:0.004322205797167181\n",
      "train loss:0.011563854959341818\n",
      "train loss:0.009909725327171943\n",
      "train loss:0.01179893624646668\n",
      "train loss:0.0026476063606872308\n",
      "train loss:0.00679958753739447\n",
      "train loss:0.015319124235536823\n",
      "train loss:0.006625825539762257\n",
      "train loss:0.005629382014175036\n",
      "train loss:0.009033085167983592\n",
      "train loss:0.0030993556897641206\n",
      "train loss:0.009137150472822275\n",
      "train loss:0.044212616599933925\n",
      "train loss:0.006045677350285964\n",
      "train loss:0.002153940378853184\n",
      "train loss:0.007969056997258707\n",
      "train loss:0.011916615263207156\n",
      "train loss:0.014796771613689919\n",
      "train loss:0.05028359328330362\n",
      "train loss:0.0013230364128029512\n",
      "train loss:0.007050584835570668\n",
      "train loss:0.029325913079934724\n",
      "train loss:0.01583736274117102\n",
      "train loss:0.08295005428312009\n",
      "train loss:0.024966163580905837\n",
      "train loss:0.01187960731060676\n",
      "train loss:0.07652982366980271\n",
      "train loss:0.020337488521914887\n",
      "train loss:0.014660507271099563\n",
      "train loss:0.03390159245309036\n",
      "train loss:0.011693128581282797\n",
      "train loss:0.014155865560332461\n",
      "train loss:0.005783848739033711\n",
      "train loss:0.004861854007136251\n",
      "train loss:0.023492069706819274\n",
      "train loss:0.009575708528696672\n",
      "train loss:0.01714150142631805\n",
      "train loss:0.00803003878854572\n",
      "train loss:0.009777053847973595\n",
      "train loss:0.013865933572089897\n",
      "train loss:0.00738471971998529\n",
      "train loss:0.017242377156395098\n",
      "train loss:0.01820054595853982\n",
      "train loss:0.009311682991285099\n",
      "train loss:0.014616115664258167\n",
      "train loss:0.008271643291323324\n",
      "train loss:0.008249400030645226\n",
      "train loss:0.027876576718540463\n",
      "train loss:0.024081121771534097\n",
      "train loss:0.024750992266531272\n",
      "train loss:0.0015426933995804797\n",
      "train loss:0.030264583257285555\n",
      "train loss:0.013396309364575197\n",
      "train loss:0.001734007076441959\n",
      "train loss:0.025277264904917184\n",
      "train loss:0.002487372017915023\n",
      "train loss:0.007938894300309663\n",
      "train loss:0.018405265188664435\n",
      "train loss:0.0075308927624595035\n",
      "train loss:0.012596229464898303\n",
      "train loss:0.019522349174407996\n",
      "train loss:0.031074027376673148\n",
      "train loss:0.005501314204924165\n",
      "train loss:0.0064494806627821595\n",
      "train loss:0.0323026961257252\n",
      "train loss:0.003301648456582147\n",
      "train loss:0.08571159461114927\n",
      "train loss:0.007155272633207043\n",
      "train loss:0.004740046955659604\n",
      "train loss:0.032294506008013625\n",
      "train loss:0.004504694056170361\n",
      "train loss:0.02876193735350725\n",
      "train loss:0.005648047757143084\n",
      "train loss:0.010861222180213395\n",
      "train loss:0.0052783462793961535\n",
      "train loss:0.007562962162970639\n",
      "train loss:0.0025372639583707028\n",
      "train loss:0.02036173548650842\n",
      "train loss:0.006991692154005174\n",
      "train loss:0.016897098424266666\n",
      "train loss:0.040261706224525816\n",
      "train loss:0.019401348377686745\n",
      "train loss:0.014484402626887605\n",
      "train loss:0.0253477769265273\n",
      "train loss:0.019798904563374943\n",
      "train loss:0.0025012318069497085\n",
      "train loss:0.0023127513222338477\n",
      "train loss:0.003907001997283186\n",
      "train loss:0.003898054956123092\n",
      "train loss:0.01649236592036578\n",
      "train loss:0.016760829486272662\n",
      "train loss:0.019996747866745282\n",
      "train loss:0.0016078263612649103\n",
      "train loss:0.007704668538519644\n",
      "train loss:0.0012716182309420255\n",
      "train loss:0.03477170347975887\n",
      "train loss:0.006610630688042759\n",
      "train loss:0.004495024738589371\n",
      "train loss:0.0053278488855175166\n",
      "train loss:0.006548596307935178\n",
      "train loss:0.012444867297406428\n",
      "train loss:0.02770581850860057\n",
      "train loss:0.0017046683311308002\n",
      "train loss:0.012142026334552946\n",
      "train loss:0.1164678078898047\n",
      "train loss:0.003572328028557454\n",
      "train loss:0.08130869240160335\n",
      "train loss:0.01118067568773235\n",
      "train loss:0.05143094729242136\n",
      "train loss:0.0019915369147933486\n",
      "train loss:0.004217886130353022\n",
      "train loss:0.03361073523083974\n",
      "train loss:0.006992124051396601\n",
      "train loss:0.015177267386890769\n",
      "train loss:0.029667270845204274\n",
      "train loss:0.0618653963588342\n",
      "train loss:0.03725064582321802\n",
      "train loss:0.012336715677669302\n",
      "train loss:0.009307502659147706\n",
      "train loss:0.0068884121174914185\n",
      "train loss:0.004818257518858754\n",
      "train loss:0.003099642799506212\n",
      "train loss:0.004143934599745262\n",
      "train loss:0.012305672537494204\n",
      "train loss:0.015660445280886556\n",
      "train loss:0.009181724447783179\n",
      "train loss:0.023671107291972925\n",
      "train loss:0.010123023964062608\n",
      "train loss:0.010829444755382039\n",
      "train loss:0.013348206290943333\n",
      "train loss:0.013965573822090875\n",
      "train loss:0.0062784635576945145\n",
      "train loss:0.004907597163534913\n",
      "train loss:0.008367196737045201\n",
      "train loss:0.005608169337178107\n",
      "train loss:0.01095416260780509\n",
      "train loss:0.009947616197766957\n",
      "train loss:0.0244103063688264\n",
      "train loss:0.019643486379194632\n",
      "train loss:0.005193312194370525\n",
      "train loss:0.007321420736191606\n",
      "train loss:0.06308770383977551\n",
      "train loss:0.0077513371824065045\n",
      "train loss:0.008677179863420773\n",
      "train loss:0.01818915526081123\n",
      "train loss:0.03326980348898874\n",
      "train loss:0.006031994574310135\n",
      "train loss:0.01902608576980457\n",
      "train loss:0.014652421695859674\n",
      "train loss:0.01996189907265467\n",
      "train loss:0.001444234862705972\n",
      "train loss:0.02518515017544121\n",
      "train loss:0.01893834104512252\n",
      "train loss:0.0021077076497938553\n",
      "train loss:0.030930047204027925\n",
      "train loss:0.03140336836585955\n",
      "train loss:0.037508850254806735\n",
      "train loss:0.01664498943315822\n",
      "train loss:0.004161726247595787\n",
      "train loss:0.009115404522221059\n",
      "train loss:0.008322063996032525\n",
      "train loss:0.014490486473402954\n",
      "train loss:0.0179111696437768\n",
      "train loss:0.0034447907366136356\n",
      "train loss:0.004236160057517937\n",
      "train loss:0.006073783143300673\n",
      "train loss:0.0022751607045833106\n",
      "train loss:0.006174441877493405\n",
      "train loss:0.007162616292634261\n",
      "train loss:0.003052557776536942\n",
      "train loss:0.0012220017323327204\n",
      "train loss:0.01725832126611766\n",
      "train loss:0.01942465014485945\n",
      "train loss:0.0011394104472840466\n",
      "train loss:0.014445231974425179\n",
      "train loss:0.002416439223107712\n",
      "train loss:0.0029144635132957834\n",
      "train loss:0.009685383108188313\n",
      "train loss:0.0127970698913691\n",
      "train loss:0.009728905425952094\n",
      "train loss:0.008934877670106875\n",
      "train loss:0.008529327462906983\n",
      "train loss:0.007436242863362501\n",
      "train loss:0.006762445283590299\n",
      "train loss:0.005396443942467336\n",
      "train loss:0.001948141467751698\n",
      "train loss:0.03178490069760318\n",
      "train loss:0.00540277559420353\n",
      "train loss:0.00850573883295212\n",
      "train loss:0.017281967499623692\n",
      "train loss:0.00154959909210124\n",
      "train loss:0.006962140756328256\n",
      "train loss:0.01268793102445124\n",
      "train loss:0.009497856627541353\n",
      "train loss:0.0110801225352932\n",
      "train loss:0.0018184826399069867\n",
      "train loss:0.005758789823852654\n",
      "train loss:0.0186700734076427\n",
      "train loss:0.0023240998052523436\n",
      "train loss:0.0017015623670394845\n",
      "train loss:0.0047289623091780995\n",
      "train loss:0.003922564484873755\n",
      "train loss:0.0022762416984374724\n",
      "train loss:0.005082405821253537\n",
      "train loss:0.009228899793409993\n",
      "train loss:0.002635938846138729\n",
      "train loss:0.001026989764597235\n",
      "train loss:0.020970377840442485\n",
      "train loss:0.0023478786636894904\n",
      "train loss:0.0020909865332432655\n",
      "train loss:0.010016857074003481\n",
      "train loss:0.003303454708779681\n",
      "train loss:0.015352872726725572\n",
      "train loss:0.021677232797238145\n",
      "train loss:0.04098547190268117\n",
      "train loss:0.006085445982894883\n",
      "train loss:0.009287484517656637\n",
      "train loss:0.005282225088792454\n",
      "train loss:0.007353022464605615\n",
      "train loss:0.02267384889826356\n",
      "train loss:0.02304511899985538\n",
      "train loss:0.03358238867002678\n",
      "train loss:0.006681729031345661\n",
      "train loss:0.003185766219844509\n",
      "train loss:0.04199630601599577\n",
      "train loss:0.00772000270626737\n",
      "train loss:0.008505457905675237\n",
      "train loss:0.0071974046711784445\n",
      "train loss:0.010005100469973587\n",
      "train loss:0.00870437623133129\n",
      "train loss:0.05224985909622979\n",
      "train loss:0.005013385111988258\n",
      "train loss:0.009153486992112235\n",
      "train loss:0.01358106334041255\n",
      "train loss:0.002098256324027563\n",
      "train loss:0.008136520241244186\n",
      "train loss:0.002763302557847917\n",
      "train loss:0.00886278516077691\n",
      "train loss:0.004569555529479528\n",
      "train loss:0.007651938469653524\n",
      "train loss:0.010755936267499329\n",
      "train loss:0.009532373086508407\n",
      "train loss:0.006585156952612182\n",
      "train loss:0.006827452830947323\n",
      "train loss:0.016964513444344565\n",
      "train loss:0.0040723039001627805\n",
      "train loss:0.02568900228479275\n",
      "train loss:0.0010428427418458653\n",
      "train loss:0.015472484193458423\n",
      "train loss:0.003614284311944443\n",
      "train loss:0.004331535794572353\n",
      "train loss:0.02218807045347737\n",
      "train loss:0.012988917423792292\n",
      "train loss:0.004283127557116814\n",
      "train loss:0.006497007982774437\n",
      "train loss:0.00781511875113896\n",
      "train loss:0.002994214982009423\n",
      "train loss:0.002752853450605662\n",
      "train loss:0.01191784140973956\n",
      "train loss:0.007741596135945705\n",
      "train loss:0.04812997240256967\n",
      "train loss:0.006156598166444166\n",
      "train loss:0.018317989361196295\n",
      "train loss:0.034141377501291294\n",
      "train loss:0.02543098797235234\n",
      "train loss:0.011180760395075264\n",
      "train loss:0.0019495710672700814\n",
      "train loss:0.0024045398885512346\n",
      "train loss:0.05031896437289366\n",
      "train loss:0.0023776274837901735\n",
      "train loss:0.007586078428273912\n",
      "train loss:0.00974390315976936\n",
      "train loss:0.07571392964129968\n",
      "train loss:0.005532147014413773\n",
      "train loss:0.007633853312248704\n",
      "train loss:0.006080045266760484\n",
      "train loss:0.002978801881509396\n",
      "train loss:0.006929680680889323\n",
      "train loss:0.00530255845823888\n",
      "train loss:0.02990939099946402\n",
      "train loss:0.0040708041157827834\n",
      "train loss:0.0054953736170354475\n",
      "train loss:0.07015432870500203\n",
      "train loss:0.004697897792520115\n",
      "train loss:0.0017572017150861057\n",
      "train loss:0.016829093242758643\n",
      "train loss:0.030090904391179996\n",
      "train loss:0.0008330453654474555\n",
      "train loss:0.003162235212856561\n",
      "train loss:0.05151630416929395\n",
      "train loss:0.015847577875830073\n",
      "train loss:0.0034361860352397936\n",
      "train loss:0.01966971537785555\n",
      "train loss:0.046263538344618126\n",
      "train loss:0.002230038529082834\n",
      "train loss:0.001826758184472925\n",
      "train loss:0.0039838882563240354\n",
      "train loss:0.004301742089513739\n",
      "train loss:0.04906537309682504\n",
      "train loss:0.005436029889140569\n",
      "train loss:0.024906925074402238\n",
      "train loss:0.0023391695577120587\n",
      "train loss:0.033224554701513795\n",
      "train loss:0.0030241605953895846\n",
      "train loss:0.007615548927748527\n",
      "train loss:0.023499846363319067\n",
      "train loss:0.016336082780066475\n",
      "train loss:0.016460121523996184\n",
      "train loss:0.004150151787140207\n",
      "train loss:0.0028615862498085924\n",
      "train loss:0.016144454090577017\n",
      "train loss:0.00657929866921075\n",
      "train loss:0.006687236531291497\n",
      "train loss:0.02258942110361722\n",
      "train loss:0.003983713014980735\n",
      "train loss:0.005824694914280287\n",
      "train loss:0.0015977040214837255\n",
      "train loss:0.00806434597436945\n",
      "train loss:0.004583115474204546\n",
      "train loss:0.0260775287277357\n",
      "train loss:0.045891706350403745\n",
      "train loss:0.0029682678679823996\n",
      "train loss:0.11193377971719105\n",
      "train loss:0.010825689011390893\n",
      "train loss:0.010266570611170298\n",
      "train loss:0.005639079323452071\n",
      "train loss:0.005621620595629806\n",
      "train loss:0.050248044587049644\n",
      "train loss:0.009700846454956436\n",
      "train loss:0.013829115246299633\n",
      "train loss:0.0030005577711239547\n",
      "train loss:0.0029145622986020796\n",
      "train loss:0.000767907417893005\n",
      "train loss:0.007018616592657418\n",
      "train loss:0.009379565704184632\n",
      "train loss:0.06412264962683119\n",
      "train loss:0.007886285930819836\n",
      "train loss:0.0013859989289267396\n",
      "train loss:0.004256400197289258\n",
      "train loss:0.0024578333176147413\n",
      "train loss:0.00704087777574841\n",
      "train loss:0.003492474491684898\n",
      "train loss:0.011300402620072647\n",
      "train loss:0.011325464430545447\n",
      "train loss:0.011592724525706716\n",
      "train loss:0.011211346691274887\n",
      "train loss:0.0030087388157373166\n",
      "train loss:0.03775339508533102\n",
      "train loss:0.0011854315311731875\n",
      "train loss:0.02892221840415745\n",
      "train loss:0.03047872798625639\n",
      "train loss:0.008139700068522368\n",
      "train loss:0.00804770423436587\n",
      "train loss:0.01775123040648612\n",
      "train loss:0.0038216794423876085\n",
      "train loss:0.02022943537481876\n",
      "train loss:0.007694673868966741\n",
      "train loss:0.008567551739600715\n",
      "train loss:0.007255816619424562\n",
      "train loss:0.021386409841587604\n",
      "train loss:0.009390662436132373\n",
      "train loss:0.006718664257445138\n",
      "train loss:0.006042836146292322\n",
      "train loss:0.010271174337842097\n",
      "train loss:0.0013773995257702766\n",
      "train loss:0.0074152701322312145\n",
      "train loss:0.003110916139927812\n",
      "train loss:0.006694677164114222\n",
      "train loss:0.010077036147021394\n",
      "train loss:0.010761964279022701\n",
      "train loss:0.004021700245094173\n",
      "train loss:0.007728406100668708\n",
      "train loss:0.011957022823429371\n",
      "train loss:0.004185184616286597\n",
      "train loss:0.0032757795546022373\n",
      "train loss:0.008127994458919891\n",
      "train loss:0.008405925472802951\n",
      "train loss:0.003038758976889027\n",
      "train loss:0.006094631657998276\n",
      "train loss:0.011192610186050986\n",
      "train loss:0.0018482615185031349\n",
      "train loss:0.010452367848557213\n",
      "train loss:0.0010290684774772348\n",
      "train loss:0.0036223620551650336\n",
      "train loss:0.030571692432123525\n",
      "train loss:0.004344515405328194\n",
      "train loss:0.003373787811497471\n",
      "train loss:0.013363660147500296\n",
      "train loss:0.001548657859996308\n",
      "train loss:0.004038591357361227\n",
      "train loss:0.0016366986239981023\n",
      "train loss:0.012820414574913487\n",
      "train loss:0.004688351699633815\n",
      "train loss:0.0033572431497591775\n",
      "train loss:0.0016019433155847942\n",
      "train loss:0.01079250774686174\n",
      "train loss:0.030426425261357876\n",
      "train loss:0.00911590051524501\n",
      "train loss:0.0027156469188334975\n",
      "train loss:0.004419869425209796\n",
      "train loss:0.008818137023578249\n",
      "train loss:0.001205322278347426\n",
      "train loss:0.011879631519910939\n",
      "train loss:0.011389037868711774\n",
      "train loss:0.01116158753853759\n",
      "train loss:0.03998113289004983\n",
      "train loss:0.002064234798176112\n",
      "train loss:0.005482388479428467\n",
      "train loss:0.004081284004077619\n",
      "train loss:0.0033981884336870953\n",
      "train loss:0.0073536345692595305\n",
      "train loss:0.010787392425356111\n",
      "train loss:0.030321675377067682\n",
      "train loss:0.002734510868251777\n",
      "train loss:0.014709795609144104\n",
      "train loss:0.0020661943408548837\n",
      "train loss:0.0071804510048915785\n",
      "train loss:0.00241097063131284\n",
      "train loss:0.010806170414608393\n",
      "train loss:0.0010175177957234879\n",
      "train loss:0.007226768131308145\n",
      "train loss:0.018415042643183843\n",
      "train loss:0.026419842505990516\n",
      "train loss:0.013617493513724804\n",
      "train loss:0.013692676071067684\n",
      "train loss:0.004959294109157295\n",
      "train loss:0.006785591256786912\n",
      "train loss:0.0020003827259376153\n",
      "train loss:0.03486417240053668\n",
      "train loss:0.01160640898845237\n",
      "train loss:0.03981660055365546\n",
      "train loss:0.02044607045788558\n",
      "train loss:0.025293834238760478\n",
      "train loss:0.0057976813686271824\n",
      "train loss:0.004731226493439765\n",
      "train loss:0.009279802967284458\n",
      "train loss:0.01664673953958847\n",
      "train loss:0.011011855409659594\n",
      "train loss:0.004763827694643664\n",
      "train loss:0.011096725743741225\n",
      "train loss:0.04219759152968349\n",
      "train loss:0.0025809733774275335\n",
      "train loss:0.007886274710978686\n",
      "train loss:0.007507767818819813\n",
      "train loss:0.0065506488676957235\n",
      "train loss:0.04572132183347911\n",
      "train loss:0.013249014471433484\n",
      "train loss:0.011301899720451756\n",
      "train loss:0.021539225229599456\n",
      "train loss:0.005286994384991053\n",
      "train loss:0.05921838121497151\n",
      "train loss:0.0065497062335934855\n",
      "train loss:0.11100428879215983\n",
      "train loss:0.0009223774860953951\n",
      "train loss:0.0012765862566126234\n",
      "train loss:0.028144401703513844\n",
      "train loss:0.01789985712194187\n",
      "train loss:0.013857669235318582\n",
      "train loss:0.009001921416767054\n",
      "train loss:0.0031462728616123386\n",
      "train loss:0.0026840546094603086\n",
      "train loss:0.005520058166809378\n",
      "train loss:0.0016487686823955334\n",
      "train loss:0.004865829764057518\n",
      "train loss:0.0019082275897526332\n",
      "train loss:0.003830228978338644\n",
      "train loss:0.014333958701076312\n",
      "train loss:0.0027916362639987995\n",
      "train loss:0.008111873114662986\n",
      "train loss:0.004371346912998053\n",
      "train loss:0.02138684971444998\n",
      "train loss:0.0167919558844725\n",
      "train loss:0.004531730917635044\n",
      "train loss:0.0029968653364637545\n",
      "train loss:0.014718550258162271\n",
      "train loss:0.0096453160922931\n",
      "train loss:0.01072296400312072\n",
      "train loss:0.02763922827382429\n",
      "train loss:0.009176093097736737\n",
      "train loss:0.0009635808334579319\n",
      "train loss:0.00918834560153285\n",
      "train loss:0.014933595372387452\n",
      "train loss:0.005742712880577579\n",
      "train loss:0.00537245043789504\n",
      "train loss:0.004241161097946558\n",
      "train loss:0.007493730389852552\n",
      "train loss:0.005902802628710807\n",
      "train loss:0.004029772907585827\n",
      "train loss:0.00494543741118004\n",
      "train loss:0.011273282575282319\n",
      "train loss:0.03278978074494121\n",
      "train loss:0.02447383341746028\n",
      "train loss:0.002180496838166022\n",
      "train loss:0.0018845682087370267\n",
      "train loss:0.006033027998707884\n",
      "train loss:0.05939734730836863\n",
      "train loss:0.003155957238109596\n",
      "train loss:0.02605956123562657\n",
      "train loss:0.014990717257806836\n",
      "train loss:0.006979879579302748\n",
      "train loss:0.016160049753128146\n",
      "train loss:0.011131535902431733\n",
      "train loss:0.024773476295078055\n",
      "train loss:0.008478707046480911\n",
      "train loss:0.02113820886419944\n",
      "train loss:0.011326271421240723\n",
      "train loss:0.014262477612352574\n",
      "train loss:0.003555045907959875\n",
      "train loss:0.005736014271720312\n",
      "train loss:0.006714042719464352\n",
      "train loss:0.0042481816484551994\n",
      "train loss:0.002028243823783405\n",
      "train loss:0.0036934845420066448\n",
      "train loss:0.018698994028677404\n",
      "train loss:0.007267721357035028\n",
      "train loss:0.01875171587706973\n",
      "train loss:0.0013261925856971025\n",
      "train loss:0.004360060517698205\n",
      "train loss:0.016790824074707212\n",
      "train loss:0.011028761244462407\n",
      "train loss:0.003226732378570435\n",
      "train loss:0.003467679686301471\n",
      "train loss:0.0052063588455474145\n",
      "train loss:0.00032132524585777914\n",
      "train loss:0.014266010068931993\n",
      "train loss:0.005420099944782789\n",
      "train loss:0.009986690368023153\n",
      "train loss:0.019152883202555912\n",
      "train loss:0.0044548251077782845\n",
      "train loss:0.0012913861225849845\n",
      "train loss:0.014938500065565989\n",
      "train loss:0.0015944645985865391\n",
      "train loss:0.0007126588248559998\n",
      "train loss:0.007290238013662007\n",
      "train loss:0.0030876449185652122\n",
      "train loss:0.003181412864777779\n",
      "train loss:0.004128286874523281\n",
      "train loss:0.001954995490844675\n",
      "train loss:0.002094233361786874\n",
      "train loss:0.006349783773511597\n",
      "train loss:0.01088590832666011\n",
      "train loss:0.013784350897316755\n",
      "train loss:0.06991322360852051\n",
      "train loss:0.0015023330122788494\n",
      "train loss:0.0010371136481926253\n",
      "train loss:0.0014345916574259243\n",
      "train loss:0.026573357922356183\n",
      "train loss:0.016532937017584518\n",
      "train loss:0.010642074573119551\n",
      "train loss:0.010711113293848766\n",
      "train loss:0.02611064160174585\n",
      "train loss:0.015968878454252412\n",
      "train loss:0.0028447431816258513\n",
      "train loss:0.002103528004508214\n",
      "train loss:0.004157917738744082\n",
      "train loss:0.02265402848038781\n",
      "train loss:0.012292944100164484\n",
      "train loss:0.013495545670172754\n",
      "train loss:0.004805000011789536\n",
      "train loss:0.005438867110165984\n",
      "train loss:0.005839626674336307\n",
      "train loss:0.0032736120995743617\n",
      "train loss:0.004366372484734412\n",
      "train loss:0.02337820659417251\n",
      "train loss:0.017434041271957336\n",
      "train loss:0.017170753052707866\n",
      "train loss:0.011780228667989145\n",
      "train loss:0.018749788412536286\n",
      "train loss:0.005833651909027157\n",
      "train loss:0.003574952492906051\n",
      "train loss:0.003431320358563657\n",
      "train loss:0.016898481680439587\n",
      "train loss:0.0060770154788036095\n",
      "=== epoch:10, train acc:0.993, test acc:0.985 ===\n",
      "train loss:0.006737190440520338\n",
      "train loss:0.01039502564224935\n",
      "train loss:0.02829156212505358\n",
      "train loss:0.020045279556217602\n",
      "train loss:0.016209309568446687\n",
      "train loss:0.0026672277603262858\n",
      "train loss:0.015592849050377167\n",
      "train loss:0.022702315553310246\n",
      "train loss:0.00742935570993218\n",
      "train loss:0.031262998005097714\n",
      "train loss:0.018645275266861647\n",
      "train loss:0.0018315208102151171\n",
      "train loss:0.004238192019320167\n",
      "train loss:0.0005361195251108653\n",
      "train loss:0.01103570526500272\n",
      "train loss:0.022664009547094098\n",
      "train loss:0.06462287422775935\n",
      "train loss:0.028551967887845645\n",
      "train loss:0.0021047054497763703\n",
      "train loss:0.03921630889334408\n",
      "train loss:0.043898625981104375\n",
      "train loss:0.0023377372236406345\n",
      "train loss:0.0077872068564190725\n",
      "train loss:0.003768266997917013\n",
      "train loss:0.021975162274731275\n",
      "train loss:0.02988922460327269\n",
      "train loss:0.021867453542341174\n",
      "train loss:0.05053900061627473\n",
      "train loss:0.013234006484879659\n",
      "train loss:0.00820414623422503\n",
      "train loss:0.0029555483918958835\n",
      "train loss:0.008943512956042805\n",
      "train loss:0.005420757137552738\n",
      "train loss:0.006495381004594823\n",
      "train loss:0.004361367735363867\n",
      "train loss:0.031179958447324435\n",
      "train loss:0.0051617739351362355\n",
      "train loss:0.023437459868335054\n",
      "train loss:0.0005335230874897648\n",
      "train loss:0.0049244703571248785\n",
      "train loss:0.0054456602792436635\n",
      "train loss:0.0029663059231176403\n",
      "train loss:0.004707153469068721\n",
      "train loss:0.023927445919556325\n",
      "train loss:0.0074547248032539185\n",
      "train loss:0.03143713246016175\n",
      "train loss:0.006762240095254756\n",
      "train loss:0.01306941681128714\n",
      "train loss:0.002456489025509711\n",
      "train loss:0.024141328682394034\n",
      "train loss:0.004142054124090249\n",
      "train loss:0.006795892298627532\n",
      "train loss:0.0037948480696797125\n",
      "train loss:0.03654102453458771\n",
      "train loss:0.014113281931517061\n",
      "train loss:0.024789862030494155\n",
      "train loss:0.015652083683129456\n",
      "train loss:0.003429633087568718\n",
      "train loss:0.0025965448602697063\n",
      "train loss:0.00872212150918996\n",
      "train loss:0.005340474356671391\n",
      "train loss:0.003068280077712522\n",
      "train loss:0.0019798669350341524\n",
      "train loss:0.037052886004426015\n",
      "train loss:0.004821356465985216\n",
      "train loss:0.0037240725483476305\n",
      "train loss:0.013892565683708396\n",
      "train loss:0.004014681490696468\n",
      "train loss:0.039158628299816554\n",
      "train loss:0.016051386505157715\n",
      "train loss:0.011381699525915903\n",
      "train loss:0.01142863568234493\n",
      "train loss:0.0038355437822175475\n",
      "train loss:0.01218601609998468\n",
      "train loss:0.003746073316648922\n",
      "train loss:0.005214696480053361\n",
      "train loss:0.010110060257967622\n",
      "train loss:0.029039083725757107\n",
      "train loss:0.011124559994689693\n",
      "train loss:0.0018210403444680495\n",
      "train loss:0.015355975099805842\n",
      "train loss:0.0051999196093082765\n",
      "train loss:0.020799156704307195\n",
      "train loss:0.011498550037371925\n",
      "train loss:0.013071651464333844\n",
      "train loss:0.00392852859776703\n",
      "train loss:0.004015115403676104\n",
      "train loss:0.011759881329878756\n",
      "train loss:0.006401404027979726\n",
      "train loss:0.0072984903527398325\n",
      "train loss:0.005748365436918056\n",
      "train loss:0.002679067865835094\n",
      "train loss:0.07122793051200077\n",
      "train loss:0.004566793629190381\n",
      "train loss:0.029971200610724467\n",
      "train loss:0.012778285070881992\n",
      "train loss:0.007961239523197942\n",
      "train loss:0.04339092213118444\n",
      "train loss:0.009303198020613735\n",
      "train loss:0.003040520843399358\n",
      "train loss:0.015792220852606457\n",
      "train loss:0.006216622143327497\n",
      "train loss:0.0036028179437867748\n",
      "train loss:0.01519635459105166\n",
      "train loss:0.029988285222598363\n",
      "train loss:0.007151728471728692\n",
      "train loss:0.003127930481615209\n",
      "train loss:0.0013623967384799845\n",
      "train loss:0.014501148063004968\n",
      "train loss:0.007506517735313044\n",
      "train loss:0.005778969591465669\n",
      "train loss:0.007518585993890192\n",
      "train loss:0.03252674452806067\n",
      "train loss:0.00036211750862455254\n",
      "train loss:0.008174159867359608\n",
      "train loss:0.008076304704543923\n",
      "train loss:0.0017105140701895807\n",
      "train loss:0.003938156282370473\n",
      "train loss:0.012795438780119541\n",
      "train loss:0.010582518583428607\n",
      "train loss:0.01673473487405995\n",
      "train loss:0.005691275238726047\n",
      "train loss:0.002325477616968305\n",
      "train loss:0.016407400920253553\n",
      "train loss:0.01095889350706056\n",
      "train loss:0.032117868466708936\n",
      "train loss:0.0031691779064289293\n",
      "train loss:0.024240578650469375\n",
      "train loss:0.008307454506195593\n",
      "train loss:0.007715132651783857\n",
      "train loss:0.07553599111103733\n",
      "train loss:0.006383583252756478\n",
      "train loss:0.029876692167465867\n",
      "train loss:0.0055805130110081255\n",
      "train loss:0.0037839665509659445\n",
      "train loss:0.0043919202987363504\n",
      "train loss:0.0030324974014300505\n",
      "train loss:0.0008136958287732437\n",
      "train loss:0.0012321919984600417\n",
      "train loss:0.013467727470712293\n",
      "train loss:0.003877431040190044\n",
      "train loss:0.01819231887223019\n",
      "train loss:0.003226972357974281\n",
      "train loss:0.010315702762319787\n",
      "train loss:0.005881452416483178\n",
      "train loss:0.007517773181290067\n",
      "train loss:0.0031511368965686988\n",
      "train loss:0.004980888794530565\n",
      "train loss:0.006482529605234051\n",
      "train loss:0.003923268490622358\n",
      "train loss:0.005111727502581325\n",
      "train loss:0.0059523813276889746\n",
      "train loss:0.0012753617235816096\n",
      "train loss:0.015000692828540345\n",
      "train loss:0.011265115051486483\n",
      "train loss:0.005767383900485085\n",
      "train loss:0.010399426716076554\n",
      "train loss:0.028671764498396497\n",
      "train loss:0.013947235344281706\n",
      "train loss:0.001912235081635427\n",
      "train loss:0.009742721648603314\n",
      "train loss:0.021349045601393\n",
      "train loss:0.006618904970160246\n",
      "train loss:0.003120836434815372\n",
      "train loss:0.01163069738123443\n",
      "train loss:0.011114827956982671\n",
      "train loss:0.006961880487597224\n",
      "train loss:0.012477280773010807\n",
      "train loss:0.009841280639151259\n",
      "train loss:0.0066155161263482766\n",
      "train loss:0.10610214792026275\n",
      "train loss:0.001349681664163657\n",
      "train loss:0.0020790777908554544\n",
      "train loss:0.016010803386393165\n",
      "train loss:0.00555295322475284\n",
      "train loss:0.010734087860383091\n",
      "train loss:0.0011978165519233811\n",
      "train loss:0.02278951624010146\n",
      "train loss:0.02605715303116767\n",
      "train loss:0.0019562395274417203\n",
      "train loss:0.07341049953045582\n",
      "train loss:0.008538868214663842\n",
      "train loss:0.009746895385558554\n",
      "train loss:0.019371893107743433\n",
      "train loss:0.008149207168561682\n",
      "train loss:0.024268233660313863\n",
      "train loss:0.0027841846426220128\n",
      "train loss:0.008374147866169432\n",
      "train loss:0.025985620626356808\n",
      "train loss:0.011170093339177239\n",
      "train loss:0.012580840110608045\n",
      "train loss:0.002778797893342802\n",
      "train loss:0.0014850937800223716\n",
      "train loss:0.002129467749704709\n",
      "train loss:0.0012739727152934247\n",
      "train loss:0.016491514154406337\n",
      "train loss:0.006205418208861873\n",
      "train loss:0.0018005530508264879\n",
      "train loss:0.0072226115568296565\n",
      "train loss:0.02690113216291931\n",
      "train loss:0.0036948391134393526\n",
      "train loss:0.009814180990475983\n",
      "train loss:0.007864605301871976\n",
      "train loss:0.026238191452594695\n",
      "train loss:0.008026133064401537\n",
      "train loss:0.001544859034452067\n",
      "train loss:0.0019744093906817126\n",
      "train loss:0.004095885906101744\n",
      "train loss:0.005651141306888581\n",
      "train loss:0.006696614186492986\n",
      "train loss:0.003544775057426114\n",
      "train loss:0.007049128903121745\n",
      "train loss:0.019722950408354433\n",
      "train loss:0.002185452817799198\n",
      "train loss:0.023379931586206965\n",
      "train loss:0.02462022274909659\n",
      "train loss:0.013932183504842398\n",
      "train loss:0.004789769224323528\n",
      "train loss:0.008877320499555813\n",
      "train loss:0.0017702668687859862\n",
      "train loss:0.005947834764171798\n",
      "train loss:0.008823760191384717\n",
      "train loss:0.0021997442609629574\n",
      "train loss:0.005328697436919491\n",
      "train loss:0.004195266141575207\n",
      "train loss:0.008741495910026046\n",
      "train loss:0.04355867170376122\n",
      "train loss:0.0010292772798122475\n",
      "train loss:0.0018149833372487223\n",
      "train loss:0.0070651654318942755\n",
      "train loss:0.0031403291800175034\n",
      "train loss:0.008680023621125502\n",
      "train loss:0.0023171700468546334\n",
      "train loss:0.007629145890948415\n",
      "train loss:0.005677435915377988\n",
      "train loss:0.0065339606750699185\n",
      "train loss:0.0016050832310035123\n",
      "train loss:0.0032271153000807806\n",
      "train loss:0.011024544955971336\n",
      "train loss:0.002609213414465683\n",
      "train loss:0.007067604271195282\n",
      "train loss:0.00478543028967366\n",
      "train loss:0.0035124899675466202\n",
      "train loss:0.02029476848423826\n",
      "train loss:0.0038400588292895077\n",
      "train loss:0.006643816950071652\n",
      "train loss:0.03477732370253818\n",
      "train loss:0.0063204467193217866\n",
      "train loss:0.018295774197221278\n",
      "train loss:0.0018641162725940052\n",
      "train loss:0.01733866257789126\n",
      "train loss:0.04388012285160586\n",
      "train loss:0.0030343549122166923\n",
      "train loss:0.0027011883956975986\n",
      "train loss:0.008729905511461999\n",
      "train loss:0.014122220894139049\n",
      "train loss:0.009359324884569888\n",
      "train loss:0.0060703564164822045\n",
      "train loss:0.0014674441577429364\n",
      "train loss:0.012658424023456323\n",
      "train loss:0.008447845549443993\n",
      "train loss:0.0046116623273985856\n",
      "train loss:0.0006207481643050514\n",
      "train loss:0.001442182047640712\n",
      "train loss:0.002782807176595883\n",
      "train loss:0.004012237120871836\n",
      "train loss:0.0019260297211686293\n",
      "train loss:0.020739646532705828\n",
      "train loss:0.00624276459574767\n",
      "train loss:0.0007715327014890358\n",
      "train loss:0.0011559048831071871\n",
      "train loss:0.0028417909988767477\n",
      "train loss:0.0016116508952768599\n",
      "train loss:0.006601436409691459\n",
      "train loss:0.024404197850480404\n",
      "train loss:0.00103554747790199\n",
      "train loss:0.014540211362557016\n",
      "train loss:0.007869108131353185\n",
      "train loss:0.007157690508920003\n",
      "train loss:0.0063118914453363685\n",
      "train loss:0.0036421398445800218\n",
      "train loss:0.022256518930334236\n",
      "train loss:0.004391393578326443\n",
      "train loss:0.019155867692162834\n",
      "train loss:0.0018184447224301\n",
      "train loss:0.005622900895448434\n",
      "train loss:0.005410121945064112\n",
      "train loss:0.005115780773128679\n",
      "train loss:0.009010668881896791\n",
      "train loss:0.001413409146016681\n",
      "train loss:0.000509007562415095\n",
      "train loss:0.021960937579754276\n",
      "train loss:0.0015093981719491616\n",
      "train loss:0.016488829748498565\n",
      "train loss:0.0038219580807681936\n",
      "train loss:0.008811589716234228\n",
      "train loss:0.0007148868794545284\n",
      "train loss:0.0034278949029092546\n",
      "train loss:0.0394826078071333\n",
      "train loss:0.0013107065217885638\n",
      "train loss:0.0005905579603244255\n",
      "train loss:0.001962649536153845\n",
      "train loss:0.0083307538632646\n",
      "train loss:0.003175971766692082\n",
      "train loss:0.005654845876014625\n",
      "train loss:0.02547731369135343\n",
      "train loss:0.0066500495241032034\n",
      "train loss:0.0045795121503642185\n",
      "train loss:0.003385414741606059\n",
      "train loss:0.005182619609964716\n",
      "train loss:0.013638762464476342\n",
      "train loss:0.020481494033570562\n",
      "train loss:0.00030435772440918205\n",
      "train loss:0.006076992769635677\n",
      "train loss:0.014841296118030398\n",
      "train loss:0.010437872832009965\n",
      "train loss:0.03162955391309462\n",
      "train loss:0.003534869190435849\n",
      "train loss:0.010354111531468552\n",
      "train loss:0.01167606780329669\n",
      "train loss:0.015865678603914274\n",
      "train loss:0.017155630885142006\n",
      "train loss:0.00773213924301234\n",
      "train loss:0.0043077492990864705\n",
      "train loss:0.004840119158903139\n",
      "train loss:0.00221217213026885\n",
      "train loss:0.002210010564573916\n",
      "train loss:0.003088640444117061\n",
      "train loss:0.005991581907689367\n",
      "train loss:0.0046130083407626326\n",
      "train loss:0.00307073858042372\n",
      "train loss:0.006508681681131989\n",
      "train loss:0.0018163240751406674\n",
      "train loss:0.006924487943464657\n",
      "train loss:0.0037742321797406696\n",
      "train loss:0.00905567011263664\n",
      "train loss:0.00458322628570387\n",
      "train loss:0.0003460816945481854\n",
      "train loss:0.029609456029293574\n",
      "train loss:0.0012854544834326436\n",
      "train loss:0.018893361437448627\n",
      "train loss:0.013781290619019799\n",
      "train loss:0.03022961362461516\n",
      "train loss:0.0029824887907419833\n",
      "train loss:0.011304726706242616\n",
      "train loss:0.0020494674449239234\n",
      "train loss:0.01146825609292327\n",
      "train loss:0.0017150511525754494\n",
      "train loss:0.008104342710105463\n",
      "train loss:0.017230492263303004\n",
      "train loss:0.007894191525795326\n",
      "train loss:0.007531843050261627\n",
      "train loss:0.004295013966357466\n",
      "train loss:0.011234186090565946\n",
      "train loss:0.007185809375895246\n",
      "train loss:0.006049249408063182\n",
      "train loss:0.002918265751790272\n",
      "train loss:0.005321789445391427\n",
      "train loss:0.004181909442892813\n",
      "train loss:0.014257548986708446\n",
      "train loss:0.011951541771316696\n",
      "train loss:0.04752592719267316\n",
      "train loss:0.009080939098304288\n",
      "train loss:0.00596302055148089\n",
      "train loss:0.005336455236535092\n",
      "train loss:0.003152536856896629\n",
      "train loss:0.0009547644116076918\n",
      "train loss:0.0031867697230854248\n",
      "train loss:0.011943144114957654\n",
      "train loss:0.0014615299846563062\n",
      "train loss:0.0014532113882826808\n",
      "train loss:0.001245641985886914\n",
      "train loss:0.0026578144878851677\n",
      "train loss:0.019890706997190238\n",
      "train loss:0.020931095089987684\n",
      "train loss:0.005042655276967326\n",
      "train loss:0.0031454619924254214\n",
      "train loss:0.0063927289806631924\n",
      "train loss:0.02193036142325867\n",
      "train loss:0.007018830664527003\n",
      "train loss:0.0035407770398671796\n",
      "train loss:0.0028331334138731795\n",
      "train loss:0.006855086792207266\n",
      "train loss:0.006288408172283009\n",
      "train loss:0.02477952361831439\n",
      "train loss:0.0057425875019455905\n",
      "train loss:0.002653437803138142\n",
      "train loss:0.011445422569896446\n",
      "train loss:0.029273122947396275\n",
      "train loss:0.004532249760516488\n",
      "train loss:0.005253997927207055\n",
      "train loss:0.0016055225137583382\n",
      "train loss:0.005685067500671928\n",
      "train loss:0.00905813584359303\n",
      "train loss:0.013930035199116857\n",
      "train loss:0.012004663433819635\n",
      "train loss:0.004829272557730688\n",
      "train loss:0.009043842942470303\n",
      "train loss:0.004924768194511067\n",
      "train loss:0.00432939835019051\n",
      "train loss:0.005507432395605693\n",
      "train loss:0.0027607928870542003\n",
      "train loss:0.004499020004985119\n",
      "train loss:0.0017585908779376516\n",
      "train loss:0.006947904563834663\n",
      "train loss:0.006179028849026512\n",
      "train loss:0.00182065729523313\n",
      "train loss:0.006149156344419396\n",
      "train loss:0.049413854771104965\n",
      "train loss:0.002926015846161547\n",
      "train loss:0.006294123750714056\n",
      "train loss:0.06059661070565493\n",
      "train loss:0.0008469273062255325\n",
      "train loss:0.0007599224529502871\n",
      "train loss:0.0014236593163740071\n",
      "train loss:0.004283711437253731\n",
      "train loss:0.009447128377312701\n",
      "train loss:0.0022153629628444316\n",
      "train loss:0.025168404703144246\n",
      "train loss:0.0010012630367066332\n",
      "train loss:0.0022103962474834486\n",
      "train loss:0.0010658193681834442\n",
      "train loss:0.0027337842568116084\n",
      "train loss:0.003213358300742393\n",
      "train loss:0.00498298919178396\n",
      "train loss:0.01641776727826142\n",
      "train loss:0.04328571180934773\n",
      "train loss:0.010583044814339842\n",
      "train loss:0.005271709418601159\n",
      "train loss:0.0038407684295356957\n",
      "train loss:0.010784908352166462\n",
      "train loss:0.013190510887273093\n",
      "train loss:0.0012140428541944958\n",
      "train loss:0.005228783207870221\n",
      "train loss:0.003772094523772479\n",
      "train loss:0.012320977038876819\n",
      "train loss:0.0010791804563650203\n",
      "train loss:0.011307776506953344\n",
      "train loss:0.0033342484791638533\n",
      "train loss:0.004678669226561501\n",
      "train loss:0.007937387016186731\n",
      "train loss:0.002549131216698657\n",
      "train loss:0.0013815355013625566\n",
      "train loss:0.007093202749120103\n",
      "train loss:0.002708702483314911\n",
      "train loss:0.0010321324995336111\n",
      "train loss:0.011959658013065184\n",
      "train loss:0.005878516227217607\n",
      "train loss:0.015406887933426509\n",
      "train loss:0.003001654579791461\n",
      "train loss:0.008522664006562626\n",
      "train loss:0.009869084321585614\n",
      "train loss:0.01063795897748555\n",
      "train loss:0.004983407688738058\n",
      "train loss:0.0016840816439524884\n",
      "train loss:0.0032290681234248466\n",
      "train loss:0.016841496833751825\n",
      "train loss:0.0029542424701846813\n",
      "train loss:0.004830024591281897\n",
      "train loss:0.004026441643497403\n",
      "train loss:0.012922939230543884\n",
      "train loss:0.003992322325378036\n",
      "train loss:0.003370392085724544\n",
      "train loss:0.003971791665880984\n",
      "train loss:0.1236791132471503\n",
      "train loss:0.00578846209467933\n",
      "train loss:0.0061102035169924305\n",
      "train loss:0.002247973432144095\n",
      "train loss:0.00526806123782446\n",
      "train loss:0.0024027363058697973\n",
      "train loss:0.007547320727870952\n",
      "train loss:0.0045181442192193\n",
      "train loss:0.0015408435369439583\n",
      "train loss:0.01930105406127733\n",
      "train loss:0.005749349630637227\n",
      "train loss:0.014997805164798783\n",
      "train loss:0.0063453265110732715\n",
      "train loss:0.004595857468832851\n",
      "train loss:0.005803158337707487\n",
      "train loss:0.00436553047400435\n",
      "train loss:0.007346749852120356\n",
      "train loss:0.002278255096881625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 22\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m# 3층!\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    def __init__(self, input_dim=(1, 28, 28), \u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m                 hidden_size=100, output_size=10, weight_init_std=0.01):\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     18\u001b[0m trainer_3 \u001b[38;5;241m=\u001b[39m Trainer(network, x_train, t_train, x_test, t_test,\n\u001b[1;32m     19\u001b[0m                   epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, mini_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     20\u001b[0m                   optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer_param\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0.001\u001b[39m},\n\u001b[1;32m     21\u001b[0m                   evaluate_sample_num_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mtrainer_3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/2024/DeepLearningFromScratch/1/ch08/../common/trainer.py:73\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter):\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m걸린 시간 : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;241m-\u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/2024/DeepLearningFromScratch/1/ch08/../common/trainer.py:48\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mgradient(x_batch, t_batch)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mparams, grads)\n\u001b[0;32m---> 48\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loss_list\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(loss))\n",
      "File \u001b[0;32m~/2024/DeepLearningFromScratch/1/ch08/../ch07/simple_convnet.py:75\u001b[0m, in \u001b[0;36mSimpleConvNet.loss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m     68\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"손실 함수를 구한다.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    t : 정답 레이블\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_layer\u001b[38;5;241m.\u001b[39mforward(y, t)\n",
      "File \u001b[0;32m~/2024/DeepLearningFromScratch/1/ch08/../ch07/simple_convnet.py:63\u001b[0m, in \u001b[0;36mSimpleConvNet.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m---> 63\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/2024/DeepLearningFromScratch/1/ch08/../common/layers.py:223\u001b[0m, in \u001b[0;36mConvolution.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    220\u001b[0m col \u001b[38;5;241m=\u001b[39m im2col(x, FH, FW, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad)\n\u001b[1;32m    221\u001b[0m col_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW\u001b[38;5;241m.\u001b[39mreshape(FN, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m--> 223\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_W\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb\u001b[49m\n\u001b[1;32m    224\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mreshape(N, out_h, out_w, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from ch07.simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = SimpleConvNet()  \n",
    "'''\n",
    "# 3층!\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "'''\n",
    "trainer_3 = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=20, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer_3.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8318525-0442-4ff1-b178-a7dc325b546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100], output_size=10,\n",
    "                             activation='relu', weight_init_std=0.01)  \n",
    "\n",
    "trainer_4 = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=20, mini_batch_size=100, # 60000개 데이터 // 100 = 600개 * 20번\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000) # 총 12000번!!\n",
    "\n",
    "trainer_4.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83338cb2-7378-4905-a914-409ca02aee47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caluculate accuracy (float64) ... \n",
      "0.9926\n",
      "caluculate accuracy (float16) ... \n",
      "0.9926\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deep_convnet import DeepConvNet\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = DeepConvNet()\n",
    "network.load_params(\"deep_convnet_params.pkl\")\n",
    "\n",
    "sampled = 10000 # 고속화를 위한 표본추출\n",
    "x_test = x_test[:sampled]\n",
    "t_test = t_test[:sampled]\n",
    "\n",
    "print(\"caluculate accuracy (float64) ... \")\n",
    "print(network.accuracy(x_test, t_test))\n",
    "\n",
    "# float16(반정밀도)로 형변환\n",
    "x_test = x_test.astype(np.float16)\n",
    "for param in network.params.values():\n",
    "    param[...] = param.astype(np.float16)\n",
    "\n",
    "print(\"caluculate accuracy (float16) ... \")\n",
    "print(network.accuracy(x_test, t_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfce25fc-c622-4417-9708-e813fdf13ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating test accuracy ... \n",
      "test accuracy:0.9926\n",
      "======= misclassified result =======\n",
      "{view index: (label, inference), ...}\n",
      "{1: (4, 9), 2: (6, 5), 3: (6, 0), 4: (4, 9), 5: (3, 5), 6: (8, 2), 7: (6, 4), 8: (2, 1), 9: (1, 7), 10: (7, 4), 11: (3, 5), 12: (3, 5), 13: (8, 9), 14: (6, 5), 15: (9, 4), 16: (7, 1), 17: (1, 6), 18: (6, 0), 19: (2, 7), 20: (9, 4)}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deep_convnet import DeepConvNet\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = DeepConvNet()\n",
    "network.load_params(\"deep_convnet_params.pkl\")\n",
    "\n",
    "print(\"calculating test accuracy ... \")\n",
    "#sampled = 1000\n",
    "#x_test = x_test[:sampled]\n",
    "#t_test = t_test[:sampled]\n",
    "\n",
    "classified_ids = []\n",
    "\n",
    "acc = 0.0\n",
    "batch_size = 100\n",
    "\n",
    "for i in range(int(x_test.shape[0] / batch_size)):\n",
    "    tx = x_test[i*batch_size:(i+1)*batch_size]\n",
    "    tt = t_test[i*batch_size:(i+1)*batch_size]\n",
    "    y = network.predict(tx, train_flg=False)\n",
    "    y = np.argmax(y, axis=1)\n",
    "    classified_ids.append(y)\n",
    "    acc += np.sum(y == tt)\n",
    "    \n",
    "acc = acc / x_test.shape[0]\n",
    "print(\"test accuracy:\" + str(acc))\n",
    "\n",
    "classified_ids = np.array(classified_ids)\n",
    "classified_ids = classified_ids.flatten()\n",
    " \n",
    "max_view = 20\n",
    "current_view = 1\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.2, wspace=0.2)\n",
    "\n",
    "mis_pairs = {}\n",
    "for i, val in enumerate(classified_ids == t_test):\n",
    "    if not val:\n",
    "        ax = fig.add_subplot(4, 5, current_view, xticks=[], yticks=[])\n",
    "        ax.imshow(x_test[i].reshape(28, 28), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "        mis_pairs[current_view] = (t_test[i], classified_ids[i])\n",
    "            \n",
    "        current_view += 1\n",
    "        if current_view > max_view:\n",
    "            break\n",
    "\n",
    "print(\"======= misclassified result =======\")\n",
    "print(\"{view index: (label, inference), ...}\")\n",
    "print(mis_pairs)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7c1dfa-ebe3-45ee-86cb-29c7f8d1e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=False, flatten=False)\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2b91b1-32a5-4b04-bf89-6afe40f13be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=False)\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b5eb44-0e46-4e8f-9c4e-9dc3a0ec9851",
   "metadata": {},
   "source": [
    "    if normalize:\n",
    "        for key in ('train_img', 'test_img'):\n",
    "            dataset[key] = dataset[key].astype(np.float32)\n",
    "            dataset[key] /= 255.0\n",
    "            -> 이거 전체에 대해 나누는거야 전체 elment-wise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7cd328-edb6-48cb-871a-df4e0fdbe42f",
   "metadata": {},
   "source": [
    "    def _change_one_hot_label(X):\n",
    "        T = np.zeros((X.size, 10))\n",
    "        for idx, row in enumerate(T):\n",
    "            row[X[idx]] = 1\n",
    "            \n",
    "        return T    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbddaf62-d132-4405-9825-d317f11ec003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2bce48d-f010-40bb-b7e4-92f5cff86317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = np.zeros((t_train.size, 10))\n",
    "T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8043c05c-1adb-4ef3-a3ce-ba3fbb7f55d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "867c4fb5-7b40-4c7d-9f2c-3b9597dfad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in enumerate(T):\n",
    "    row[t_train[idx]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad193ee9-5dde-4e25-b1cd-9f7e85fbd84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afe86f65-c21f-44c9-9c74-c0fce344312b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3741a6-b6f4-4be6-8f61-0a9c6c7e7a45",
   "metadata": {},
   "source": [
    "    if not flatten:  -> 이 말은 즉 원래 data가 flatten돼서 나온단 것\r\n",
    "         for key in ('train_img', 'test_img'):\r\n",
    "            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\r\n",
    "\r\n",
    "    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd7c6a84-378c-4303-8fe5-87aecd4089a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 1, 28, 28)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=False)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9a76555-7291-4f0b-a486-eec8dce08312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7a673b90-d020-491d-81c9-8d834b263cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import pickle\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "# from collections import OrderedDict\n",
    "\n",
    "class DeepConvNet:\n",
    "    \"\"\" 정확도 99% 이상의 고정밀 합성곱 신경망CNN\n",
    "    (채널 수 더 늘어나고, 중간 데이터 공간 크기 줄여나감)\n",
    "    \n",
    "    conv - relu - conv - relu - pool -\n",
    "    conv - relu - conv - relu - pool -\n",
    "    conv - relu - conv - relu - pool - \n",
    "    affine - relu - dropout - affine - dropout - softmax\n",
    "\n",
    "    He 초깃값 & ReLU & Adam optimizer & dropout\n",
    "    필터 3x3 (small)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), # (N,1,28,28)\n",
    "                 # 채널 수 점 점 늘어남 (FN,C,FH,FW) -> (N,FN,OH,OW)\n",
    "                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1}, # conv1\n",
    "                 conv_param_2 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1}, # conv2\n",
    "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1}, # conv3\n",
    "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1}, # conv4\n",
    "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1}, # conv5\n",
    "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1}, # conv6\n",
    "                 hidden_size=50, output_size=10): # affine(7,8)\n",
    "    \n",
    "        # 1. 가중치filter 초기화==============================================================================\n",
    "        # 각 층의 뉴런 하나당 앞 층의 몇 개 뉴런과 연결되는가 (N)\n",
    "        # 6개의 합성곱 계층과 2개의 완전연결 계층에 대한 앞 층의 뉴런의 개수 배열 (C/FN,FH,FW)\n",
    "        \"\"\"\n",
    "        [Input] --(1*3*3)--> [Conv1] --(16*3*3)--> [Conv2] --(16*3*3)--> [Conv3] --(32*3*3)--> [Conv4] --(32*3*3)--> [Conv5] --(64*3*3)--> [Conv6] \n",
    "        --(64*4*4)--> [Hidden] --(hidden_size)--> [Output]\n",
    "        \"\"\"\n",
    "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n",
    "        weight_init_scales = np.sqrt(2.0 / pre_node_nums) # He 초깃값 (ReLu 사용시 권장)\n",
    "\n",
    "        # 2. 파라미터 초기화==================================================================================\n",
    "        self.params = {}\n",
    "        pre_channel_num = input_dim[0] #(1,28,28)\n",
    "        # 2(1) 합성곱 계층 (FN,C,FH,FW), (FN,)\n",
    "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
    "            self.params['W' + str(idx+1)] = weight_init_scales[idx] \\\n",
    "                                            * np.random.rands(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
    "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
    "            pre_channel_num = conv_param['filter_num']\n",
    "        # 2(2) 완전연결 게층 (input/filter,hidden/output)\n",
    "        self.params['W7'] = weight_init_scales[6] * np.random.randn(64*4*4, hidden_size)\n",
    "        self.params['b7'] = np.zeros(hidden_size)\n",
    "        self.params['W8'] = weight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b8'] = np.zeros(output_size)\n",
    "\n",
    "        # 3. layer 생성======================================================================================\n",
    "        self.layers = [] # OrderedDict()\n",
    "        # 3(1) (self, W, b, stride=1, pad=0) / (self, pool_h, pool_w, stride=1, pad=0)\n",
    "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], conv_param_1['stride'], conv_param_1['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        \n",
    "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], conv_param_2['stride'], conv_param_2['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        \n",
    "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], conv_param_3['stride'], conv_param_3['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        \n",
    "        self.layers.append(Convolution(self.params['W4'], self.params['b4'], conv_param_4['stride'], conv_param_4['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        \n",
    "        self.layers.append(Convolution(self.params['W5'], self.params['b5'], conv_param_5['stride'], conv_param_5['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        \n",
    "        self.layers.append(Convolution(self.params['W6'], self.params['b6'], conv_param_6['stride'], conv_param_6['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))         \n",
    "        \n",
    "        # 3(2) (self, W, b) / (self, dropout_ratio=0.5)\n",
    "        self.layers.append(Affine(self.params['W7'], self, params['b7']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Dropout(0.5))\n",
    "\n",
    "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
    "        self.layers.append(Dropout(0.5))\n",
    "                           \n",
    "        # 3(3)\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    # 4. predict(forward)=====================================================================================\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout): # 주어진 인스턴스가 어떤 클래스/데이터 타입인지\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "    # 5. loss(forward~loss)====================================================================================\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x, train_flg=True) # train시 true default\n",
    "        return self.last_layer.forward(y, t)\n",
    "        \n",
    "    # 6. accuracy==============================================================================================\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        # 원핫->label\n",
    "        if t.dim != 1 : t = np.argmax(t, axis=1)             \n",
    "        # 정확도 초기화\n",
    "        acc = 0.0\n",
    "        # 1 epoch만큼!! (즉 데이터 전체 다 돎)\n",
    "        for i in range(int(x.shape[0] / batch_size)): # 버림\n",
    "            tx = x[i*batch_size:(i+1)*batch_size] # 0*100 ~ 1*100\n",
    "            tt = t[i*batch_size:(i+1)*batch_size] # -> slicing\n",
    "            y = self.predict(tx, train_flag=False) # regularization 끄기\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    # 7. for&back&grads=========================================================================================\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        tmp_layers = self.layers.copy() # layers = list(self.layers.values())\n",
    "        tmp_layers.reverse()\n",
    "        for layer in tmp_layers:\n",
    "            dout = layer.backward(dout)\n",
    "        # save grads\n",
    "        grads = {}                    # c-r - c-r-p - c-r - c-r-p - c-r - c-r-p - a-r-d - a-d - s\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            grads['w' + str(i+1)] = self.layers[layer_idx].dW\n",
    "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
    "        return grads\n",
    "\n",
    "    # 9. 가중치 저장(pickle)======================================================================================\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        # 새로운 dict에 옮겨주기\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        # 저장\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "            \n",
    "    # 9. 가중치 로드(pickle)======================================================================================\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        # self.params에 덮어쓰기\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "        # layer 별로 직접 가중치 값 할당\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
    "            self.layers[layer_idx].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a2d8be-7416-4d99-a35e-b496c859609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from deep_convnet import DeepConvNet \n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = DeepConvNet()\n",
    "# Trainer : optimizer, epoch당 acc, 최종 test acc\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=20, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "\n",
    "triner.train()\n",
    "\n",
    "# 매개변수 보관\n",
    "network.save_params(\"deep_convnet_params.pkl\")\n",
    "print(\"Saved Network Parameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fe81802d-97c6-4a65-bf20-cf300d0f414e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating test accuracy ... \n",
      "test accuracy:0.9926\n",
      "======= misclassified result =======\n",
      "{view index: (label, inference), ...}\n",
      "{1: (4, 9), 2: (6, 5), 3: (6, 0), 4: (4, 9), 5: (3, 5), 6: (8, 2), 7: (6, 4), 8: (2, 1), 9: (1, 7), 10: (7, 4), 11: (3, 5), 12: (3, 5), 13: (8, 9), 14: (6, 5), 15: (9, 4), 16: (7, 1), 17: (1, 6), 18: (6, 0), 19: (2, 7), 20: (9, 4)}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAo4AAAH0CAYAAACzX6zaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA+9klEQVR4nO3dZ3yUxfr/8QlSpCQgnUhABUUQpSmKgnpUmooUxUJV9NDkgAIqTUU6KBZAQFARKaIcioiACEe6DZAiAtJBQ/VIstIh+39wfs5/roFsZpO9N7vJ5/1ovq/ZvXd0x2S87yszMX6/368AAACANOTI7AEAAAAgOrBwBAAAgBMWjgAAAHDCwhEAAABOWDgCAADACQtHAAAAOGHhCAAAACcsHAEAAOAkp8uLUlJSVGJiooqNjVUxMTFejwlB8vv9yufzqfj4eJUjh/f/L8B8iHzMCZiYD7AxJ2AKZj44LRwTExNVQkJCSAYH7xw4cECVLl3a889hPkQP5gRMzAfYmBMwucwHp4VjbGysvmBcXFzGR4aQSk5OVgkJCfp78hrzIfIxJ2BiPsDGnIApmPngtHD8+7ZyXFwcX3gEC9ftf+ZD9GBOwMR8gI05AZPLfOCPYwAAAOCEhSMAAACcsHAEAACAExaOAAAAcMLCEQAAAE5YOAIAAMAJC0cAAAA4YeEIAAAAJywcAQAA4ISFIwAAAJywcAQAAIATp7Oqs4Px48eL3KlTJ5FnzZql282aNQvLmBCdzp8/L/LatWtF/umnn1Lt2759u8gVKlQQuUuXLrpdrVq1DI0TAIBgcccRAAAATlg4AgAAwAkLRwAAADjJtjWOY8eOFdmsHbuU2NhYL4eDKHPu3Dnd/vHHH0XfG2+8IfKcOXPS/TmrV68Wef369bpt1krCzahRo0Tu2rVrJo0EQDCGDBkickxMjG5XrlxZ9DVq1CgsY8quuOMIAAAAJywcAQAA4CRbPapes2aNbnfr1k305cmTR+SpU6eKXLduXe8Ghohnb5Njzp+vvvoq3dctWrSoyDfeeGPA148ZMybdn5VdnDhxQrd79eol+vbs2SMyj6rhFfNnhl2+8ttvv4ls/wx56qmndPuDDz7wYHTRp2/fviKbj6pz5col+i6//PKwjMnv94vcv39/kXPnzp3qexcvXixy69atRW7evHnGBuch7jgCAADACQtHAAAAOGHhCAAAACdZusbxl19+Efnxxx9P9bUjRowQ+eGHH/ZkTIhM9jGBL7/8ssjvvvuuyD6fL9VrFSxYUGT7+MonnnhCt4sXLy76SpYsmfZgEZBZx2h/bz/88EO4h4Nswq6bN2sTT506FfC9Zr2eUkotWbIkdAPLBszt0S6VvWLXOPbs2TPd1zp69KjI1DgCAAAg6rFwBAAAgBMWjgAAAHCSpWoc9+7dK3L9+vVFTkxM1O233npL9P3rX//ybFyIfL179xbZ3nctEHue2e+1j8OCt8xaM3tfzHDt74asZ/PmzSKPGzdOZHu/RbtuOhjVqlVL93uzKvvft70PYiDHjh0TedWqVSEZU3bFHUcAAAA4YeEIAAAAJywcAQAA4CSqaxztGpLu3buL/Pvvv4v8/PPP63awZ9ReuHBBt3PkkOttew8uRCZ7vphnn6ZV02ifhdqlSxfdHjx4sOjLmzdveoeIdPj6669FNv9b3bhxo2efu2vXLpGPHz+u2zVq1BB933zzjcirV692/pwqVaqI3KhRI+f3ImN+/vln3b7vvvtEn103lxFxcXEim7+r8D8dO3YMmAOx98WsV69eSMZ01VVXiXzttdcGfP0NN9yg20WKFBF9zZo1C8mYwoE7jgAAAHDCwhEAAABOovpR9dtvvy3ynDlzRLaPGBw5cqTztVNSUlK9lr39yjPPPON8XWQe89G0UhcfM2kqW7asyK+++qrITz31VOgGhgxZtGiRyHYpSTDMLbuaNGkS8LXJyckinzlzRrevvPJK0Wc/1vz111+dx1SsWDGR7bnJMYrpZz6KVkqp999/X+RZs2bptn0kXChLlOz5ctddd4Xs2lDqt99+S/d7c+aUy6Q+ffroduvWrUVfuXLl0v050YQ7jgAAAHDCwhEAAABOWDgCAADASdTVOO7bt0+3R40aJfrs48XsurRg2DUR//73v3V769atoq9Vq1Yic6xZ5rC32wnmGMHcuXOLPGPGDJFvu+22DI4OoWLWISql1KZNm0Q269TWrl0r+sqUKSNy8eLFRW7Xrp1u2zWMfr9f5B07dqQ6xrZt24psbhGk1MVbOAVi19bVrFnT+b2QTp06JbL9M2LBggXO1ypRooTIAwcOFLlfv366feTIkYDXCtX2MPgfn88nsn3EcCD2NjmTJk0S+cEHH0z/wLII7jgCAADACQtHAAAAOGHhCAAAACdRV+M4bNgw3T5w4IDo69Gjh8jXX3+983XPnTsnsr3nn8mubaGmMTJ8/PHHIqd1jKDJPpKKmsbIZdcUL1u2TOQOHTro9v79+0XftGnTRLZrHPPnz6/bM2fOFH12jePhw4dTHeOdd94psv2zyqyZVkqpPXv26PbZs2dFX4MGDUT+8MMPU/1cXMysRX322WdF39KlS9N93QIFCoj83nvviZxWXaPJrqOrVauWbjdv3jwdo8veVq1aJfL27dud33v69GmR7Z8DZrb323zyySdFzsiespEsa/5TAQAAIORYOAIAAMAJC0cAAAA4ifgax507d4ps1oI88MADoq9r167p/hy7Bmnq1KmpvjatM2wRPmYtS/fu3QO+NleuXCKPGzdOt2vXrh3agSFkvv/+e5HtvRmrVasmslkHbZ9PX7hw4YCfZZ5NHErly5cX2T5f2qy9s3/2lCpVSmT77GpIdm3hmDFjdDsjNY22Xbt2hexa9p6hPXv21G1qHIPXsGFDkc09NZVS6pVXXkn1vSdOnBB5ypQpqb7Wrqu3/7u2f+c899xzum3/d5wvXz6R7TOyIwl3HAEAAOCEhSMAAACcRO690P/zxRdfiHzmzBndTklJCdnn2H9yHwiPDjKPvSWK+RgqKSkp4HvNrVaUknPp5MmToi9v3rwiZ9VtFaKBvc3JX3/9JbK9PU+NGjV0e/r06d4NLAPsrXwClcYgsI0bN4psb18UzLY4XrG37nnmmWdELleunMj2zypkjP0zwnwMbG/RtWXLlnR/zoQJEwL2v/vuu6n2Pf744yLbj9crVaqU7nGFGr8NAQAA4ISFIwAAAJywcAQAAICTGL9dNHYJycnJqmDBgiopKUnFxcWFY1zaL7/8InKVKlV0+/z586KvadOmIvfp00fkm2++WbfNY6iUUqp69eoi23VUZk2KXXOV2fVv4f5+MnM+2Fsl2LVDofKvf/1L5N69e4tsb5ESaaJ9TgwYMEC3Bw0aJPrM49iUuniLlUjcxqJ///4iDx8+XOROnTrp9pAhQ0TfZZddJrK9xYeLaJ8PpvXr14s8dOhQkWfPnh3Sz/ub/asyJiYm4OsrV66s24sWLRJ9kfDzIyvNCS99/vnnIpvbg61Zs0b0LV++3LNxPProo7pds2ZN0Wcft5wewXw/3HEEAACAExaOAAAAcMLCEQAAAE4irxjIYu9d9Oqrr+r2yy+/LPrmzJkj8jfffCOyWRdg1ybYNY123eLAgQNT7UP42N+5V0aPHi3ykiVLRDbnVokSJcIypuzE/O/criWz//uLxJpGu77666+/Fvmll14SuX79+rp9+eWXezewKLVu3Trdvvfee0WffVyfV9L6c4Abb7xR5BdeeEG3I6GmEenTuHHjVPPZs2dFn7k3sFIX1zJv2LBBtxcsWBDUOMy9pu39rfPkySNyly5dgrp2sFgBAQAAwAkLRwAAADhh4QgAAAAnkVcclAbz/Mby5cuLvhdffFHkAwcOiLx48WLnz7nzzjtFLlmypPN74R27VswUGxsr8gcffOB83f/85z8ijx8/XuStW7eK/PHHH+u2WcsE79k1bYcOHRI5XP+trl27VuRx48bp9pQpU0SfXePWpk0bka+55poQjy5r+fbbb3Xb5/OJvrT2Uwykbt26Il999dUiBzp72P7czp07i2yfj4ysJ3fu3AGzvQftqVOndPvPP/8Ufb/99pvI5r6NSim1f/9+3T59+rTo69q1q8jUOAIAACAisHAEAACAk6h7VG16/PHHRW7WrJnIFy5cEHnLli26fcstt4i+/Pnzi/zRRx+FYIQIpyeffFLk5s2bO7/XflRoP6q27dmzx/naCK2ffvpJ5LZt24r8ySef6HbhwoXT/TmbNm0S2dwOQymlRowYIfL999+v2/YRg3bpC4+mvfPAAw+IXLVqVd3u2LGj6CtUqJDI9jG2gR5VV6xYUeTHHnssiFEiO8qbN+8l20optXv3bpHt8hbzUXVm444jAAAAnLBwBAAAgBMWjgAAAHAS1TWONvtP4W32sYImu56gbNmyIRkTwseuUw3G4MGDQzgSZJS51dbRo0dFn70dj71Fk1n7PHbsWNFnb520c+fOVMdgf4695YV5DJ5SSsXHx+t2RmorcbGWLVvqtr2Fjs3+2R3oCMfDhw+LfPfdd6f62goVKoi8cOFCke16SWSe999/X+SJEyeKfMMNN+j2hx9+6Nk4Vq5cKbJ5JKH9s8neEi6YozTtrQm9xh1HAAAAOGHhCAAAACcsHAEAAOAkS9U4psXed81Uv379MI4EmeHcuXMiv/TSS7o9Z86cgO+1a2DN9yL0duzYodvt2rUTfQUKFBD5u+++E3nJkiW6fd111wX1ubly5dLtbt26ib57771X5MqVKwd1baTfFVdcccl2Ro0ePVrkX3/9NdXX9uzZU+SEhISQjQMZc+zYMZHtmvV9+/aJbH7P9h6rlSpVCvhZX331lW4vW7ZM9OXIIe/FrVq1SmSzxjEj7Hr+QEfxeoE7jgAAAHDCwhEAAABOWDgCAADASZaucdywYYPIZu0TolO9evVE/vnnn3V7+vTpos+ufTt9+nTA/kCGDh0qMvt8hk+vXr1Ets95ts9wfeihh3Tb3qcvLa+99ppud+7cOaj3IvLZc+Wjjz4SOWdO+SuxevXquv300097Ni5kjF1LeOjQoYCvT0pK0m27hjoYfr9f5JiYmHRfy95v9KqrrhK5YMGCut23b1/RF+7fR9xxBAAAgBMWjgAAAHCSpR9V20cMmtux2Ft6mEdaIXINGzZMZHM7hPXr14s++7FUICVLlhTZfjTdpk0b52shtNLaUsd+dG2WLwCnTp3S7TfffFP0HTx4UGT76LZvv/3Wu4EhZJo0aSLyrbfeKvLWrVtFto8x9Yq9bY65lVSnTp1EX5UqVUS+//77vRtYBnHHEQAAAE5YOAIAAMAJC0cAAAA4ydI1jsWLFxc5b968ul2jRg3RV6tWrbCMCRljHgmnlFIdO3bU7QkTJoi+tWvXimxurWHnl19+WfSVKVMmQ+MEEBk2b96s2/YRg4ULFxZ53rx5YRkTvGUfBXjgwAGRGzdurNu7d+8OeC1zey+llKpZs6bzOCpWrCiyfWxptOKOIwAAAJywcAQAAIATFo4AAABwkqVrHO39306ePJlJI4FX/vnPf16yDQBpyZcvn8gVKlTIpJHASwkJCSLbe/4iONxxBAAAgBMWjgAAAHDCwhEAAABOsnSNIwAgeytdurRu33DDDaIvKSkp3MMBoh53HAEAAOCEhSMAAACc8KgaAJBlxcfH6/amTZsycSRA1sAdRwAAADhh4QgAAAAnLBwBAADghIUjAAAAnLBwBAAAgBMWjgAAAHDitB2P3+9XSimVnJzs6WCQPn9/L39/T15jPkQ+5gRMzAfYmBMwBTMfnBaOPp9PKaVUQkJCBoYFr/l8PlWwYMGwfI5SzIdowJyAifkAG3MCJpf5EON3WF6mpKSoxMREFRsbq2JiYkI2QISG3+9XPp9PxcfHqxw5vK8+YD5EPuYETMwH2JgTMAUzH5wWjgAAAAB/HAMAAAAnLBwBAADghIUjAAAAnLBwBAAAgBMWjgAAAHDCwhEAAABOWDgCAADACQtHAAAAOGHhCAAAACcsHAEAAOCEhSMAAACcsHAEAACAExaOAAAAcMLCEQAAAE5YOAIAAMAJC0cAAAA4YeEIAAAAJzldXpSSkqISExNVbGysiomJ8XpMCJLf71c+n0/Fx8erHDm8/38B5kPkY07AxHyAjTkBUzDzwWnhmJiYqBISEkIyOHjnwIEDqnTp0p5/DvMhejAnYGI+wMacgMllPjgtHGNjY/UF4+LiMj4yhFRycrJKSEjQ35PXmA+RjzkBE/MBNuYETMHMB6eF49+3lePi4vjCI1i4bv8zH6IHcwIm5gNszAmYXOYDfxwDAAAAJywcAQAA4ISFIwAAAJywcAQAAIATpz+OAbK6Ro0aifzNN9+IvGLFCpGrV6/u+ZgAAIg03HEEAACAExaOAAAAcMLCEQAAAE6ocUS2tWrVKt22axpPnjwp8ptvviny1KlTvRsYAAARijuOAAAAcMLCEQAAAE5YOAIAAMBJtq1xtA/ybtasmch+v1/kG264QbcHDhzo3cAQNm+88YZunzp1KuBrt2zZ4vVwAACIeNxxBAAAgBMWjgAAAHDCwhEAAABOqHH8P3PnzhXZrnH8/PPPdbtatWqiz66PRGT69ttvRV6yZEkmjQTRYP/+/SI/8sgjIv/444+pvrdnz54iv/7666EbGICQmTBhgsgdOnQQ2V4rpKSkeD6mSMcdRwAAADhh4QgAAAAn2fZR9fjx4wP29+vXT+Rjx47p9tChQ0Ufj6oj05EjR0R+9tlnRU5rCx5Tp06dQjImRJY1a9aIPGTIEN0+ePCg6Pvpp59Eth9hFSpUSLdbtGgRohEiM5nlCJs2bRJ9Xbp0Ebl+/foijxw5UrfLlSvnwejgBfu/azuDO44AAABwxMIRAAAATlg4AgAAwEm2rXFs3759wP7169eLPHHiRC+HgxC4cOGCyHYN0oYNG5yvNWbMGJHTmi+ITEePHhX5008/FdmuZU5OTk73Zx0/fly3P/nkE9Fnb+GFyLB161aRR40aJfKkSZN0Oy4uTvSdOXNG5Hnz5oncp08f3abGMXKtXLlSZHsrvmLFioVzOFGBO44AAABwwsIRAAAATlg4AgAAwEm2rXEMlln3UKdOnUwcCVJj7pumlFKzZs0SOdB+XG3bthW5c+fOoRsYQsquQ/zjjz9EnjNnjm5//PHHos/eiw/Zy8aNG0W+7777RM6ZU/5K7N27t263bt1a9FWvXl3kjNTHIrzM2udVq1aJPvZxTBt3HAEAAOCEhSMAAACcsHAEAACAE2ocU2HWSSkl6xyaNm0a7uEgFXv37tXtXr16ib60alPM+qa0zi5H5rHPFG/VqpXI8+fPD9lnNWrUSLfz5Mkj+v7973+H7HOQOQoWLCjyjBkzRC5fvrzIZcuW1e2GDRuKPrumsUSJEgEzIse+ffsu2Vbq4n0cjxw5InKzZs10e+rUqaIvX758oRpiROOOIwAAAJywcAQAAIATHlWnwn4cPWHCBN1mO57Ms3v3bpEbNGjg/N74+HiRhw4dqtu5c+fO2MDgmdOnT4scykfTt912m8iTJ0/W7aVLl4o+HlVHv6uuuipgDmTRokUi26UwHTt2FNl8zI3Ism3bNt1Oq6TJ7p87d65u21s0DRo0SOSKFSumc4SRjTuOAAAAcMLCEQAAAE5YOAIAAMBJtq1xNI8cUkrWuyl18XY8lSpV8nxMuJi53Y5SSvXr10/kXbt2OV/rqaeeEtk+Miwz2P98AwYMEHnhwoUiHzx40OshRR17ex57iwzTQw89JHLLli1FLlSokG5PmzYt44NDVLOPozPZW/v885//9Ho4CBHze7W33ylTpozId955p8hTpkzRbXudsHLlSpH79OkjsvnzplixYkGMOLJwxxEAAABOWDgCAADACQtHAAAAOMnSNY72UUJmTYFdB/X222+LbB8dtHz58tAODpe0c+dOkevWrSuy/Z2a7FoV+/iwdu3aZXB06XPo0CGRzbpFu6Yx0D8f/ichIUFk+99ZYmJiqu+NjY0VOX/+/Km+1q4vRdZ3/vx5kV955ZVUX1u7dm2R7X1iEbnMelR7n8aBAweKXLRoUZFPnDih2+aejkopdezYMZF79Ogh8jvvvHPJMSh1cT1kJOOOIwAAAJywcAQAAIATFo4AAABwkqVrHGvWrCnyyJEjdXvYsGGiz65zsOsNrr/++hCPDn87d+6cbnfv3l307d+/X+RA54rmypVL5DfffFPkYM6lDcavv/4q8ltvvSXye++9J3Kgf4a0zk3Njux64xkzZoh85swZkUuWLJnuz5o0aZJuX7hwId3XQXRKSkoSedmyZam+9pFHHvF4NPBKjRo1Ltl2MWvWLN2ePXu26LP3dbT3ATX37bX3JLZrHiN5n0fuOAIAAMAJC0cAAAA4yVKPqu3bxkeOHBF5yJAhqfZVrFhR5Gj60/hot379et3+8ssv030d8/tVSqkHH3ww3deyHz//8MMPIpvbOX333Xeiz+fzpftzcbE8efKIXKtWLc8+yzy+sHPnzqLP3qrFZj4it+ciosP27dudX9uoUSMPR4Jo0KxZs4DZ3p6nQ4cOum1v5WMfe2yXWkUS7jgCAADACQtHAAAAOGHhCAAAACdRV+O4detW3Tb/LF4ppYYPHy6yvbWJuX3Cli1bRJ9dbzBo0CCR7T+dR+jYRzwF44YbbtBt+3inYMycOVPkLl26iGzXqnilXLlyInft2jUsn5tdbd68WeT+/fvrdlo1jbYcOf7//4fnzBl1P1qhlFq3bl1mDwFZiH0M7i+//JJqn72N26lTp0QeN25ciEeXftxxBAAAgBMWjgAAAHDCwhEAAABOIr4QZ9++fSL37dtXt+3jfe666y6RzeN9lFKqRYsWun3ixAnRV6lSJZFffvllkc3j6sy93pBxCxYs0O20jtwzaxqVUmrJkiWpvtbeq/Pjjz9ONW/btk302fVtXh0F2Lx5c5EHDx4ssl3ziNA6evSoyObPlLvvvlv0BTp+DllDYmKiyGYdmn0EnH3EKWCz1yjmPqH27xR7ftlHEEYS7jgCAADACQtHAAAAOGHhCAAAACcRX+PYpk0bkVetWqXbxYsXF3322Y5lypQRuWjRorp98uRJ0WefVd20aVORzdqzfPnyiT77fEoEx97PKpjXvvHGG7pt16CtXbs2LGNKS4kSJUQeNmyYbrdt2zZkn4Pg5c6dW+TnnntOt+2fCWnVOHK+ffTZvXu3yPbvELMOzd4nODY21ruBISpNnTpVZPtvJQLVzB4+fNi7gYUYdxwBAADghIUjAAAAnETco2p7e4wVK1aIbG65k5HtMezHzbbq1auLbB5vaD8iN7fqudR7EdiTTz6p2/aWOTbzyCY724+Xg9lCx35kbG/ls3DhQudrPfDAAyKbj6aVunjrJ2Se2rVrizxp0iTd7tChQ1DXsh9tI/KZx0QqpdRll10m8rlz53S7fv36YRkTood5BLJSFx97ax9Va/5OmjJlincD8xh3HAEAAOCEhSMAAACcsHAEAACAk4ircbSP6LHr1OxtcjKDXZtg191R4xgccwsM+whB+wiwQOwj4pYvXy5yjRo1RDa3WLKPq5wwYYLIwdQ49uvXT2RqGiPXr7/+KvLx48ed39ukSRORb775Zuf32jW05vZgds00vFOoUCGR7a2zzCNv7a2bANv1118v8sqVK0U262Tr1asXljF5gTuOAAAAcMLCEQAAAE5YOAIAAMBJxNU4mscCXiq/9957up2QkCD6vDz6b/bs2br98MMPiz67DrNVq1aejSMruuKKK3R7zZo1os+uOduyZYvIZj2YXcO4bds2kQsUKCDy1VdfneqYWrduLfI777wjsn1Umenpp58WuWfPniJzzGDksGtbgzn2a+/evSKPGDFCt++44w7Rl5ycLPLmzZtFNvcv/eyzz0Tfrbfe6jwmBOf06dMi20fRmuzfRYC9d6v9O8deG2SVY0m54wgAAAAnLBwBAADghIUjAAAAnERcjaNdp7h//36R33//fd22a8Xs+oKM1BMMGjRI5OHDh+u2Xbdg79uH9LPrVu184403Ol8rmNfaChYsKLJ9hrZ5xvEtt9wS8FqPPPJIuscBb7399tsiP/HEE87v3bBhQ6rZnj8lS5YU+dFHHxX53nvv1W32/Qwfn88nsr2/JhCIvU6w549dQ12nTh3PxxQO3HEEAACAExaOAAAAcBJxj6ptzz33nMjmkT0NGzYUfe3bt0/359jbr9iPvc2tGCZPniz6vNwGCJGhVq1aIl+4cCGTRoJQKlasmCfXtcsX7OMJ7Udar7/+um7HxsZ6MiZczP7+7eMezSMHjx07JvrYnid7MrfmM0vYlLr40bR5nG5Wwh1HAAAAOGHhCAAAACcsHAEAAOAk4mscbeYRP1OmTEn3dbZu3Sry3LlzRe7du7fIZv0ktS1A1lCkSBGRH3jgAd3OlSuX6Bs3bpzIpUqVSvW6VatWFblz584i29s7nT9/Ps2xIvQKFSokcvfu3UXu1q2bbr/xxhuib9iwYZ6NC5Fr8eLFun3ixAnRlzdvXpGrV68eljGFG3ccAQAA4ISFIwAAAJywcAQAAICTqKtxNGXk+B6zVlKpi4+eApD1ValSReQvvvhCt1u1aiX67Npme9/Yzz77TLfTOrqwTZs2QY0T4dGiRQuRzRrHTZs2hXs4iHD28aAZ+buLaMIdRwAAADhh4QgAAAAnLBwBAADgJKprHAHAK1OnTg3YP378+IAZ0cc+J7x58+a6/euvv4q+M2fOiJwnTx7vBoaIwX/n3HEEAACAIxaOAAAAcMKjagAAlFK5c+cW+dNPP82kkQCRizuOAAAAcMLCEQAAAE5YOAIAAMAJC0cAAAA4YeEIAAAAJywcAQAA4MRpOx6/36+UUio5OdnTwSB9/v5e/v6evMZ8iHzMCZiYD7AxJ2AKZj44LRx9Pp9SSqmEhIQMDAte8/l8qmDBgmH5HKWYD9GAOQET8wE25gRMLvMhxu+wvExJSVGJiYkqNjZWxcTEhGyACA2/3698Pp+Kj49XOXJ4X33AfIh8zAmYmA+wMSdgCmY+OC0cAQAAAP44BgAAAE5YOAIAAMAJC0cAAAA4YeEIAAAAJywcAQAA4ISFIwAAAJywcAQAAIATFo4AAABwwsIRAAAATlg4AgAAwAkLRwAAADhh4QgAAAAnLBwBAADghIUjAAAAnLBwBAAAgBMWjgAAAHCS0+VFKSkpKjExUcXGxqqYmBivx4Qg+f1+5fP5VHx8vMqRw/v/F2A+RD7mBEzMB9iYEzAFMx+cFo6JiYkqISEhJIODdw4cOKBKly7t+ecwH6IHcwIm5gNszAmYXOaD08IxNjZWXzAuLi7jI0NIJScnq4SEBP09eY35EPmYEzAxH2BjTsAUzHxwWjj+fVs5Li6OLzyChev2P/MhejAnYGI+wMacgMllPvDHMQAAAHDCwhEAAABOWDgCAADACQtHAAAAOGHhCAAAACcsHAEAAOCEhSMAAACcsHAEAACAExaOAAAAcMLCEQAAAE5YOAIAAMAJC0cAAAA4yZnZA0jLzp07Rb722mt1u2HDhqJv0KBBIlevXt27gQEAAGQz3HEEAACAExaOAAAAcMLCEQAAAE4ivsYxkIULF4q8bNkykffu3Sty8eLFPR4RosmqVatS7Vu6dKnIw4YNE7lu3bq63bRpU9H3j3/8Q+SrrroqnSNEqPl8PpHHjBmT6msXL14s8nfffSfy888/L3KPHj10u0iRIukdIoAIZv7sX7Jkiei75pprRN61a1dYxhRu3HEEAACAExaOAAAAcBJ1j6pLlSql29OnTxd99iPCe+65R+Sff/7Zu4EhLM6dOydyUlKSbl9++eWi78033xT5k08+EXnbtm26HRMTE9Q45s+fr9tffPGF6OvVq5fIQ4cODeraCJ3t27eLXLNmTZH/+uuvVN/r9/tFtufI8OHDRTYfew8ZMkT0denSJe3BAog4/fr1E9kuYzKVLl3a6+FEBO44AgAAwAkLRwAAADhh4QgAAAAnEV/jWLRoUZH79++v23feeafoGzBggMivvPKKyD/99JNuV6tWLUQjRDh17NhR5EmTJul22bJlRd++fftC9rn2XFuxYkXIro3QOnbsmG536tRJ9AWqacyoEydO6PaLL74o+uytfebNm+fZOBB5/vjjD5FPnjwZ8PWHDh3Sbftnjb3VU8uWLUXOlStXeoaI/2PWzSt1cU2jWfscGxsr+l599VXvBhZBuOMIAAAAJywcAQAA4ISFIwAAAJxEfI1joUKFRG7fvn2qr+3Zs6fIdo2jWR85Z84c0ZcjB2voSGQf6/bhhx+KbO6tZ9c0XnfddSLb++5de+21l7yOUkpNmDBB5PXr16c6xqpVq4rcoEGDVF+L0Dty5IjIZs3X8uXLwz0cpZRSZ86cEfno0aOZMg6Ejn1E6ZYtW0S259qmTZt0+7fffhN9dh1dRhw8eFDk3r17h+za2ZH9e8I+atQ0aNAgke29oydPniyyuZf066+/nt4hZjpWSwAAAHDCwhEAAABOWDgCAADAScTXOAbDrlMrUKCAyObeaWfPnhV99jnHiAwlSpRwfq291+K0adNEvvLKK1N9r1n/qpRSU6dOFdneh61ChQq6vWjRItEXzJiRcXa98n/+8x/n99p73g0ePFi37fk0a9YskaO5Rim7On36tMirV68W2Z5L5ndu7wFaqVIlke+++26R27Ztq9tVqlQRfSVLlnQb8CWY+xErpVT16tVFpsYxYxITEwP29+3bV7efffZZ0Xfq1CmRhw4dKvKOHTt0++GHHxZ9t912W1DjzEzccQQAAIATFo4AAABwkqUeVduPm++77z6R586dq9v2kU88qo5MvXr1Etl+DGMe/1S5cmXRZ2/HZD4mUEqplStXpvq5FStWFNl+JGE/2kbmMY+dDJa9ZVOPHj1Sfe3333+f7s9B5tm7d69u29/v/PnzRbZ/hgwbNky3H3jgAdFnH4cbLp988onIbP+VMcnJySJ/9dVXItvHCpolCJdddpnoGzdunMjbt28X2Vxn5MuXL/jBRgjuOAIAAMAJC0cAAAA4YeEIAAAAJ1mqxtFm136YNY6333676LO3UClcuLDIY8aM0e1A27rAWytWrBC5WbNmuj127NigrvXSSy/ptr01gl3jaG/thOhkf6+ff/6583vt7Z2CUbx48XS/F4H5fD6RR4wYIfLIkSN1u1WrVqJv8+bNIts1r5Hgo48+EnnJkiUi2zV5CI793/WhQ4dEtv9Wwjyq1vb7778H/KwiRYro9k033eQ6xIjDHUcAAAA4YeEIAAAAJywcAQAA4CRL1zh26NBB5ED7sNlHTZ04cULkpUuX6rZ95FO5cuXSO0QEqU6dOiLXqFFDtxcvXhzUtf7880/dPnbsmOijpjFrKl++vMhXX311qq/94osvRN6wYUO6P7dbt27pfi8ke9+9hx56SORdu3aJ/Omnn+p2o0aNvBtYCJl7zC5YsED02b/H7GMzERx7f1/bPffc43ytYI47jWbccQQAAIATFo4AAABwwsIRAAAATrJ0jaPtww8/dO6z9wt88MEHdfvRRx8VfV9//bXI9h6Q8I5Zmzpv3jzRN3v2bJHts6knTJig2/b3b++xZZ+RbZ5bmzdv3iBGjIzauHGjyPv27XN+b0JCgvNr161bJ/LZs2ed31uhQgWRI3F/wGhi7q3XtGlT0VeoUCGR7fkRjT+Pq1Spotvvvvuu6KOmMbxuvPHGVPuOHj0q8sGDBwNeyz7vPFpxxxEAAABOWDgCAADASbZ6VB2MO++8U+Rq1arptv0Y297+IRofjUQr8zHxY489JvrsfPjwYZHXrFmj2+3atRN99pZLdnnCI488ottt2rQRfWZZA0Jv9+7dIh85csT5vUlJSSKfPn1a5AEDBuj2sGHDRF9MTIzz59iPxEuXLu38XlzM3GrL/g4XLlwosv3oOhD7Wn/88YfI11xzjfO1QikuLi5TPjc7On78eMD+SpUqpdo3depUke3fMbb8+fM7jyuScccRAAAATlg4AgAAwAkLRwAAADihxtGRWfNo1zgGU/uEzFOiRAmRzW097C0+lixZInKnTp1Enjlz5iXbSik1ePBgkfv06RP8YJEq+7u6+eabRf7xxx9Tfa9dk2Rnk9/vT8fo/mfQoEHpfi8u9tlnn+l28+bNRV8wNY221q1bi7x8+XKRzZ/7zZo1E312jo2NFTlHDu7LRKqTJ0/qtn2kYzDsn/3ZBTMbAAAATlg4AgAAwAkLRwAAADihxhG4hPvuu09kc89HpZSaMmWKbg8cOFD09e/fX2T7iLAePXroNnVQGWfXGHtVc0wtc+Yx98ebP3++6HvttdfSfV37mNIDBw6I/OWXX+r2+PHjRZ+992vjxo1FHj16tG4Hc9QlvLd//37dTk5OFn3XXnutyHZtvHnM4J49e4L63IoVKwb1+kjFby0AAAA4YeEIAAAAJywcAQAA4IQaR8BBsWLFRO7evbtu16xZU/TZ55y/9NJLIpu1UUWKFAnVELMt+9+veY44sobJkyfrdtWqVUVf586dRbZrjIsXL+78OXYtYseOHXX7mWeeEX32GdkTJ04UuXLlyro9a9Ys0WfXUCO8rrzySt0uUKCA6NuxY4fIdt3rjBkzdPvQoUMBP8eub7f3nI1W3HEEAACAExaOAAAAcMKjakfHjx/P7CFkGefPnxf59OnTum0/NogGt956q8j2dg72o49p06bpdteuXb0bWDZRr149kRcvXqzbY8eODfjezZs3i7xr166QjOmtt94Sefr06SG5bnZVqVIl3X7nnXdEn32kp/koUSl5RKF9xGDt2rWdx5Azp/x12ahRo4D5hRdeSPVzf/rpJ5FLlizpPA5knHk8pFlSoJRS33zzjchNmjQRefv27c6fY3/v1apVc35vJOOOIwAAAJywcAQAAIATFo4AAABwQo2jo0WLFmX2ELIMs8ZPKaVef/113bbrlVq0aBGWMWWEveVCWscInjlzxsvhZDv58+cX+d57771k+1K2bt0qsl3vlF5//PFHSK6Di3Xq1ClgHjlypMhLly7V7YYNG4o+s75aqbTnSyDmsYhKKbVhwwbdNrf1UYqaxkhib+9k1zgGqmm8/PLLRbbn0/r160U+d+6cbtu/N6IJdxwBAADghIUjAAAAnLBwBAAAgBNqHFMxdepUkX/77Tfdrlu3ruirWLFiWMaUVTRu3Fjk4cOH63bLli1Fn70nm7k3mlJK1alTJ8SjC569T6N9DJXf7xc5mCPQ4C3z6LFQsmub7H37ssp+bpGoR48eqeajR4+Kvn379om8cePGVK+7evVqke+4446A4zD3dzX3oURksY+stPds3rNnj8gPP/ywbpvrAqXk7zKllNq0aZPI5vyK5uMHueMIAAAAJywcAQAA4ISFIwAAAJxEfI3j77//LvJ///vfVF+7cuXKgNcy6xHsehXb999/L3LhwoV129x3UKmL95FDYIUKFRL5hx9+0O1HH31U9JnnDiul1I8//ijy22+/rdv2Hm1xcXEZGGVg5n5dzz77rOhLSkoS2d7ryz7TFlmP/XPqzz//zKSRwFSsWLGAOVDd2dNPP+3JmJC5ypcvL/KHH37o/N609v0sU6aMyNFc12jijiMAAACcsHAEAACAk0x/VJ2SkiJy//79RTYfRSqllM/n0237EeA//vEPke1jnb799lvdtrdhsDVr1kzkXr166fZNN90U8L0IToECBXR7wYIFom/VqlUit27dWuQnnnhCt+3v+/333xf5/vvvdx7TwYMHRf76669FNuelvdVKTEyMyB06dBDZLHsAAESnI0eOBOyPhO3ivMAdRwAAADhh4QgAAAAnLBwBAADgJNNrHNetWyfy6NGjRX7xxRdFvvHGG3W7du3aoq9IkSIhHh0ym/0db968WWSzBrJ9+/air0mTJiLbxxmaR//Z112zZo3IycnJqY6xatWqIg8bNkxku/YWkcOuR82dO7dunzlzJt3XrVChgsjXXXdduq8FIHKYRxLaW6/Zatas6fFoMgd3HAEAAOCEhSMAAACcsHAEAACAk0yvcbzllltE5mguBGLu+aiUUg0aNNDtDRs2iL5t27YFvJZZi7ho0aKAr23VqpXI5j6ftWrVEn0lSpQIeC1EjtjYWJHNeRBsbWrlypV1u3fv3qKvdOnS6RgdgEhj7iVttpVS6pFHHhHZrqvPKrjjCAAAACcsHAEAAOCEhSMAAACcZHqNIxAq9hnQt99+e8DXz5s3z8vhIArdddddup2SkpKJIwEQiRISEnQ7u/5NBnccAQAA4ISFIwAAAJywcAQAAIATFo4AAABwwsIRAAAATlg4AgAAwAkLRwAAADhh4QgAAAAnLBwBAADgxOnkGL/fr5RSKjk52dPBIH3+/l7+/p68xnyIfMwJmJgPsDEnYApmPjgtHH0+n1JKHrWDyOPz+VTBggXD8jlKMR+iAXMCJuYDbMwJmFzmQ4zfYXmZkpKiEhMTVWxsrIqJiQnZABEafr9f+Xw+FR8fr3Lk8L76gPkQ+ZgTMDEfYGNOwBTMfHBaOAIAAAD8cQwAAACcsHAEAACAExaOAAAAcMLCEQAAAE5YOAIAAMAJC0cAAAA4YeEIAAAAJywcAQAA4ISFIwAAAJywcAQAAIATFo4AAABwwsIRAAAATlg4AgAAwAkLRwAAADhh4QgAAAAnLBwBAADghIUjAAAAnOR0eVFKSopKTExUsbGxKiYmxusxIUh+v1/5fD4VHx+vcuTw/v8FmA+RjzkBE/MBNuYETMHMB6eFY2JiokpISAjJ4OCdAwcOqNKlS3v+OcyH6MGcgIn5ABtzAiaX+eC0cIyNjdUXjIuLy/jIEFLJyckqISFBf09eYz5EPuYETMwH2JgTMAUzH5wWjn/fVo6Li+MLj2Dhuv3PfIgezAmYmA+wMSdgcpkP/HEMAAAAnLBwBAAAgBMWjgAAAHDCwhEAAABOWDgCAADACQtHAAAAOGHhCAAAACdO+zgCQKTo1q2byKNGjRK5du3aur106VLRlzt3bu8GBgDZAHccAQAA4ISFIwAAAJywcAQAAIATahwBj02ePFnkuXPn6vbAgQNFX+XKlcMxpKj2yy+/iGyfrbp69WrdPnjwoOgrW7asdwMDgGyAO44AAABwwsIRAAAATlg4AgAAwEnU1TgeO3ZMt+vWrSv6tm3bJvJtt90msvn6nj17ij72d0Oo7N+/X+QOHTqI3LJlS92Oi4sLy5ii2ZQpU0RetmxZwNfXr19ftxMSErwYEoAsIiUlReR58+aJ3LRpU5Hr1Kmj2/Pnzxd92eXnOXccAQAA4ISFIwAAAJxE3aPqo0eP6vbGjRsDvnb58uUim4+4/vzzT9H3+uuvZ3xwyDb8fr9uv/fee6Lv5ZdfTvW1SsktYcqUKePB6LKWQYMGiXzhwoWAr3/ooYd0O0cO/t8YQOrs7b2aNWsmsr3d16pVq3TbfqzdqlWrEI8uMvFTFQAAAE5YOAIAAMAJC0cAAAA4iboax/Lly+u2XZswfPjwgO/96KOPdNuujzx58qTI+fLlS+cIkR189tlnut2pU6eArz1x4oTIzK3g/P777wH7y5UrJ/LTTz/t5XCQjcyYMUO37W22gjFnzhyR169fL3KePHlEXrJkiW7XrFkz3Z+LtNl1iraZM2eKbK47mjRp4sWQIh53HAEAAOCEhSMAAACcsHAEAACAk6irccyVK5duX3/99aJv0qRJAd9r1jiaNSRKKbV69WqR7eMMkb198803Is+aNSvV1zZs2FBku34JwTGPEFTq4nqxEiVKiJxZx4du375dt8eMGSP67J8n5l6TCK2tW7eK/NVXX+m2fSzt9OnTA17r1KlTun3+/PkQjO7Szp49K/J9992n28nJyZ59bnZl/jsdNWqU6LP31r3jjjtEfvjhh70bWJTgjiMAAACcsHAEAACAExaOAAAAcBJ1NY4Z8eyzz+r22LFjRZ9d60KNY/Zinyc9d+5ckbt16ybygQMHdNvey6tly5YiX3bZZRkfYDZm1x/bEhMTwzQSKVBdmr335Pjx40UuXbq0yGYd3nXXXReqIWZJ9r6p5p6qSil17tw5kX0+n+djCrXTp0/r9rp160RfjRo1wj2cLMf8G4cjR46IvgEDBohcsmTJsIwpmnDHEQAAAE5YOAIAAMBJtnpU3bNnT902t+ZRSj4qUko+ilRKqYSEBM/Ghcy3Y8cOkZs1axbw9aVKldJt+9FZvXr1QjewbOrQoUO6nZSUFPC17dq183o4l2RvyRToaMQLFy6IvG/fPpH79++v22ltEZMd2FurmY8P7aP/UlJSwjKmcDIftw8ePFj0zZ49O9zDyVYOHz6c2UOIeNxxBAAAgBMWjgAAAHDCwhEAAABOslWN41VXXaXbbdq0EX32dhknT54Mx5CQicxaqWCPgDPrGqlpDL3Fixfr9pkzZwK+NkeOzPn/X3sbmIz44osvdNuu2cusf75wGj16tMhdu3bNpJGk7p577hE5Pj5e5JUrV4ps17Gml33EJjLO3NKoUKFCos8+jvivv/4SuUCBAp6NK1pk/Z9IAAAACAkWjgAAAHDCwhEAAABOslWNo6lSpUqZPQSE2d69e0Vu0KCBbm/fvj3ge5955hmR69SpE7Jx4WLLli1zfm379u29G0gAf/75Z8iudeLECd2eNm2a6GvdunXIPidSrVq1KrOHoJRS6rbbbhN58uTJun3llVeKPvtow4YNG4qckRpHs6513Lhx6b4OLq1s2bK6XbVqVdFn/+zZsGGDyLVr1w7JGObPny/yu+++K/LEiRNFto8pzUzccQQAAIATFo4AAABwwsIRAAAATrJtjSOyPvt8YHtvuEB1jXfccYfIQ4cOFblo0aIZHB0C2bZtW6p95n6sSimVJ08ej0cDrwWzJ6b9/ffo0UNkex++bt266XZa/93mz59f5GuuuUa37ZrWjz/+WOTvvvsu4LUDyZUrl8ijRo3S7cyq4c0u7LlXvHhxkd977z2Rb7/9dt0Odo/VHTt26LY5L5W6eP/WYsWKBXXtcOKOIwAAAJywcAQAAIATHlUjy+rbt6/I5rFuttjYWJHnzp0rMo+mw6tw4cKp9uXMKX9sxcTEeD0cRBC75KRLly4Bc0b88MMPuj148GDRN2/evJB9zvPPPy9yx44dQ3ZtBJY3b96A/fb2WNWqVdPt7t27B3yvfQyledSxvV3TunXrRI7kEhzuOAIAAMAJC0cAAAA4YeEIAAAAJ9m2xrFPnz4i+/3+gBmR78yZMyKbx4VdyuWXX67bU6ZMEX3UNGauypUr6/aCBQtE386dO0U+ffq0yAUKFPBuYPCEvcWOfTzoY489ptuhrGG02XVm9erV0+2kpCTPPtc+zhDhY/4eUOribZbMukSllOrVq5dup7WN1Pr160U+f/68br/wwgui76abbkp7sBGCO44AAABwwsIRAAAATlg4AgAAwEm2rXG0935LKyPyffnllyIfOnQo4Os7deqk240bN/ZkTEifVq1a6fbEiRNFn330m30k2+zZs70bmOHWW28VecWKFem+Vu7cuXXb3Ccuu2jXrp3Ir7zyisjmnqz28XwZMXPmTJGHDBkicqjqGu09+W655RaRS5UqFZLPQfAuu+wykZ944gmRt2zZIvKkSZN02zxCUKmLfzbZOnTooNv2XLPHEcm44wgAAAAnLBwBAADghIUjAAAAnGSrGsfNmzfr9tmzZ0Vf+fLlRY6LiwvLmBA6M2bMCNjftGlTkQcMGODlcJAB5j6OTz75pOh76623RLbPgzVrW0uWLBn6wf2fbt26iTx27FjdPnHiRFDXatKkiW6b/+zZRYsWLUSuWbOmyBUrVvTkc0eMGCHyhg0bPPmcKlWqiGzPWUQOu9Zw6NChqeY//vhD9NWqVUvk/fv3i1y1atVUPyeacMcRAAAATlg4AgAAwEm2elR977336rZ9PN0dd9whMtsjRIdz587pdpEiRQK+tlGjRiJzNF10eO6550SeM2eOyPbxdKNGjdLtgQMHir5QPh6Kj48X2dxSZvTo0UFdyz4CNbspV65cwByNunfvrtt2WQOyhpSUFJHNIwWVUur2228X2dyOJ5pxxxEAAABOWDgCAADACQtHAAAAOMlWNY5Hjx7VbY4UzBq2b9+u2+PHj8/EkcArCQkJIptHRSql1EsvvSTysGHDdNs8yk8ppfr37x/awRn++9//Or/2pptuEvmaa64J9XDgoGjRop5de/Xq1bptH0X34osvilyiRAmRr7jiCs/GhdDZvXu3yHa9tX2UZlbBHUcAAAA4YeEIAAAAJywcAQAA4CRb1TgGklVrEbK6woUL63b9+vVF3+HDh0WuUKFCWMYEb9k1jgcPHhR54sSJuj1kyBDRt2zZMpHtvT0DOX78uMiffvqpyHv27En1vfYRptWrVxfZrI9kf9Hw+eCDD0Ru1qyZyN9//326r22+177OpEmTRL755ptFrlSpkm6/8847oq9QoULpHhNCa+HChZk9hEzBHUcAAAA4YeEIAAAAJywcAQAA4IQax/+zc+dOkevUqZNJI0EwzPOC7TONX3vtNZGXL18ucvny5XXbPud6//79Itt1Z8WKFQt6rAgN+7t48803U82vvPKK6LP3+lyxYkWIR/c/Zo2aUkpNmDBBZPsMW2QO+7xx+xz0Rx99VLfXrl0r+k6fPh2ycdjXNvO1114r+vr16xeyz0XG2Ps42lq0aBGmkYQXdxwBAADghIUjAAAAnGSrR9UNGjTQ7a+++kr02Vu5IPqY369SSn300Uci9+nTR+TRo0frtn0EnD0/+vbtK/KgQYPSO0yE0YABA0R++umnRU5KShLZ3K5n7ty5qfYppVTjxo1Fvu6663TbfpwYGxvrMlxkslKlSom8cuVK3bbLDbp27SrymTNnQjaOXLly6XbDhg1Ddl2Elr3Nlm3GjBki27+DohV3HAEAAOCEhSMAAACcsHAEAACAk2xV42jXKCFrmz59ushXXHGFyHv37tXtRYsWBbzWQw89FLJxIfOULVs2YL9Z62rXsCF7a9++vch2PaR95ORnn32m26tXrw7qs3Lk+P/3dGrUqBHUexE+Xbp0EXnatGkiz5w5U2TzaOOSJUt6NzCPcccRAAAATlg4AgAAwAkLRwAAADjJVjWO5vFj9r58yHrMOiGllBo3blwmjQRAVtOoUaOA/W3bttXtkSNHir4RI0YEfG+ePHnSPzCEzfXXXy+yvX/r/PnzRU5MTNRtahwBAACQ5bFwBAAAgBMWjgAAAHAS4/f7/Wm9KDk5WRUsWFAlJSWleTYjwi/c3w/zIfIxJ2BiPsDGnIApmO+HO44AAABwwsIRAAAATlg4AgAAwAkLRwAAADhh4QgAAAAnLBwBAADghIUjAAAAnLBwBAAAgBMWjgAAAHCS0+VFfx8uk5yc7OlgkD5/fy8OhwCFBPMh8jEnYGI+wMacgCmY+eC0cPT5fEoppRISEjIwLHjN5/OpggULhuVzlGI+RAPmBEzMB9iYEzC5zAens6pTUlJUYmKiio2NVTExMSEbIELD7/crn8+n4uPjVY4c3lcfMB8iH3MCJuYDbMwJmIKZD04LRwAAAIA/jgEAAIATFo4AAABwwsIRAAAATlg4AgAAwAkLRwAAADhh4QgAAAAnLBwBAADg5P8Bq8OSxZJHSVcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deep_convnet import DeepConvNet\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# trainer 필요없다! 바로 test\n",
    "network = DeepConvNet()\n",
    "network.load_params(\"deep_convnet_params.pkl\")\n",
    "print(\"calculating test accuracy ... \")\n",
    "\n",
    "# 분류 결과\n",
    "classified_ids = []\n",
    "\n",
    "acc = 0.0\n",
    "batch_size = 100\n",
    "# 1 epoch (전체 데이터셋) : 100번 (0~99)\n",
    "for i in range(int(x_test.shape[0] / batch_size)):\n",
    "    tx = x_test[i*batch_size:(i+1)*batch_size]\n",
    "    tt = t_test[i*batch_size:(i+1)*batch_size]\n",
    "    y = network.predict(tx, train_flg=False) # logits(score)\n",
    "    y = np.argmax(y, axis=1)\n",
    "    classified_ids.append(y) # 1회당 100개의 set에 대해\n",
    "    acc += np.sum(y == tt)\n",
    "\n",
    "acc = acc / x_test.shape[0]\n",
    "print(\"test accuracy:\" + str(acc))\n",
    "\n",
    "classified_ids = np.array(classified_ids) # list to array\n",
    "classified_ids = classified_ids.flatten() # 레이블(10000,)과 비교하려고!\n",
    "                                          # 안하면 (100,100)\n",
    "\n",
    "# 틀린 이미지 20개까지만 보여주기 위함\n",
    "max_view = 20\n",
    "current_view = 1\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.2, wspace=0.2)\n",
    "\n",
    "mis_pairs = {}\n",
    "for i, val in enumerate(classified_ids == t_test): # 1. boolean 배열 반환 (classified_ids == t_test)\n",
    "                                                   # 2. 배열 순회 1~1000개(i)\n",
    "    if not val: # False, 같지 않다면\n",
    "        ax = fig.add_subplot(4, 5, current_view, xticks=[], yticks=[]) # add_subplot(r,c,idx)\n",
    "        ax.imshow(x_test[i].reshape(28, 28), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "        mis_pairs[current_view] = (t_test[i], classified_ids[i])\n",
    "\n",
    "        current_view += 1\n",
    "        if current_view > max_view:\n",
    "            break\n",
    "\n",
    "print(\"======= misclassified result =======\")\n",
    "print(\"{view index: (label, inference), ...}\")\n",
    "print(mis_pairs)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "20ebce1f-bd21-4f07-b7c3-b3bb120e994c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating test accuracy ... \n",
      "test accuracy:0.9926\n",
      "======= misclassified result =======\n",
      "{view index: (label, inference), ...}\n",
      "{1: (4, 9), 2: (6, 5), 3: (6, 0), 4: (4, 9), 5: (3, 5), 6: (8, 2), 7: (6, 4), 8: (2, 1), 9: (1, 7), 10: (7, 4), 11: (3, 5), 12: (3, 5), 13: (8, 9), 14: (6, 5), 15: (9, 4), 16: (7, 1), 17: (1, 6), 18: (6, 0), 19: (2, 7), 20: (9, 4)}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deep_convnet import DeepConvNet\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# trainer 필요없다! 바로 test\n",
    "network = DeepConvNet()\n",
    "network.load_params(\"deep_convnet_params.pkl\")\n",
    "print(\"calculating test accuracy ... \")\n",
    "\n",
    "# 분류 결과\n",
    "classified_ids = []\n",
    "\n",
    "acc = 0.0\n",
    "batch_size = 100\n",
    "# 1 epoch (전체 데이터셋) : 100번 (0~99)\n",
    "for i in range(int(x_test.shape[0] / batch_size)):\n",
    "    tx = x_test[i*batch_size:(i+1)*batch_size]\n",
    "    tt = t_test[i*batch_size:(i+1)*batch_size]\n",
    "    y = network.predict(tx, train_flg=False) # logits(score)\n",
    "    y = np.argmax(y, axis=1)\n",
    "    classified_ids.append(y) # 1회당 100개의 set에 대해\n",
    "    acc += np.sum(y == tt)\n",
    "\n",
    "acc = acc / x_test.shape[0]\n",
    "print(\"test accuracy:\" + str(acc))\n",
    "\n",
    "classified_ids = np.array(classified_ids) # list to array\n",
    "classified_ids = classified_ids.flatten() # 레이블(10000,)과 비교하려고!\n",
    "                                          # 안하면 (100,100)\n",
    "\n",
    "# 틀린 이미지 20개까지만 보여주기 위함\n",
    "max_view = 20\n",
    "current_view = 1\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.2, wspace=0.2)\n",
    "\n",
    "mis_pairs = {}\n",
    "for i, val in enumerate(classified_ids == t_test):\n",
    "    if not val:\n",
    "        ax = fig.add_subplot(4, 5, current_view, xticks=[], yticks=[])\n",
    "        ax.imshow(x_test[i].reshape(28, 28), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "        \n",
    "        # 왼쪽 상단에 실제 레이블 (label) 추가 (서브플롯 상대적 위치 좌표, 0~1)\n",
    "        ax.text(0.05, 0.95, t_test[i], ha='left', va='top', transform=ax.transAxes) # x왼끝/y위끝 지정된 좌표에\n",
    "        # 오른쪽 하단에 예측 레이블 (inference) 추가\n",
    "        ax.text(0.95, 0.05, classified_ids[i], ha='right', va='bottom', transform=ax.transAxes)\n",
    "        \n",
    "        mis_pairs[current_view] = (t_test[i], classified_ids[i])\n",
    "\n",
    "        current_view += 1\n",
    "        if current_view > max_view:\n",
    "            break\n",
    "\n",
    "print(\"======= misclassified result =======\")\n",
    "print(\"{view index: (label, inference), ...}\")\n",
    "print(mis_pairs)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fb3ea8dd-dd52-48f5-a7cf-b213d41a26d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classified_ids = np.array(classified_ids)\n",
    "classified_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "410f9e0c-f797-41e2-ab1f-12a0f3daff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_ids = classified_ids.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3de88fbb-e0d6-40bf-b127-ae2d916a7075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 False\n",
      "1 False\n",
      "2 False\n",
      "3 False\n",
      "4 False\n"
     ]
    }
   ],
   "source": [
    "t_test = [3, 4, 7, 3, 2]\n",
    "for i, val in enumerate(classified_ids == t_test):\n",
    "    print(i,val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "314b0bc6-61b2-4911-8331-64ff49015c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating test accuracy ... \n",
      "test accuracy:0.9926\n",
      "[[7 2 1 ... 7 6 9]\n",
      " [6 0 5 ... 6 4 2]\n",
      " [3 6 1 ... 0 2 8]\n",
      " ...\n",
      " [2 9 5 ... 3 2 1]\n",
      " [0 0 1 ... 0 1 7]\n",
      " [8 9 0 ... 4 5 6]]\n",
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deep_convnet import DeepConvNet\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = DeepConvNet()\n",
    "network.load_params(\"deep_convnet_params.pkl\")\n",
    "\n",
    "print(\"calculating test accuracy ... \")\n",
    "#sampled = 1000\n",
    "#x_test = x_test[:sampled]\n",
    "#t_test = t_test[:sampled]\n",
    "\n",
    "classified_ids = []\n",
    "\n",
    "acc = 0.0\n",
    "batch_size = 100\n",
    "\n",
    "for i in range(int(x_test.shape[0] / batch_size)):\n",
    "    tx = x_test[i*batch_size:(i+1)*batch_size]\n",
    "    tt = t_test[i*batch_size:(i+1)*batch_size]\n",
    "    y = network.predict(tx, train_flg=False)\n",
    "    y = np.argmax(y, axis=1)\n",
    "    classified_ids.append(y)\n",
    "    acc += np.sum(y == tt)\n",
    "    \n",
    "acc = acc / x_test.shape[0]\n",
    "print(\"test accuracy:\" + str(acc))\n",
    "\n",
    "classified_ids = np.array(classified_ids)\n",
    "print(classified_ids)\n",
    "print(classified_ids.shape)\n",
    "#classified_ids = classified_ids.flatten() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c526107e-db05-4e25-b7a9-848f8f358071",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(int(x_test.shape[0] / batch_size)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd109e8f-1f62-4cd3-a10b-6ceb423e59d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(int(x_test.shape[0] / batch_size)):\n",
    "    print(i*batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677af0c1-7f84-4fbb-8645-b4a0427d224e",
   "metadata": {},
   "source": [
    "이 코드는 Matplotlib의 subplot들 간의 여백을 조정하는 역할을 합니다. 여기서 사용되는 `subplots_adjust()` 메서드는 subplot들의 간격 및 여백을 조절하는 데 사용됩니다.\n",
    "\n",
    "구체적으로 각 매개변수가 하는 일은 다음과 같습니다:\n",
    "\n",
    "- `left`: subplot 영역의 왼쪽 가장자리와 figure 영역의 왼쪽 가장자리 사이의 공간을 나타냅니다. 왼쪽 여백을 설정합니다.\n",
    "- `right`: subplot 영역의 오른쪽 가장자리와 figure 영역의 오른쪽 가장자리 사이의 공간을 나타냅니다. 오른쪽 여백을 설정합니다.\n",
    "- `bottom`: subplot 영역의 아래쪽 가장자리와 figure 영역의 아래쪽 가장자리 사이의 공간을 나타냅니다. 아래쪽 여백을 설정합니다.\n",
    "- `top`: subplot 영역의 위쪽 가장자리와 figure 영역의 위쪽 가장자리 사이의 공간을 나타냅니다. 위쪽 여백을 설정합니다.\n",
    "- `hspace`: 수직 공간을 나타냅니다. subplot들 간의 세로 간격을 설정합니다.\n",
    "- `wspace`: 수평 공간을 나타냅니다. subplot들 간의 가로 간격을 설정합니다.\n",
    "\n",
    "위 코드에서는 모든 subplot들의 여백을 최소화하기 위해 `left`, `right`, `bottom`, `top`을 0으로 설정하고, subplot들 간의 간격을 조정하기 위해 `hspace`, `wspace`를 0.2로 설정하고 있습니다. 이를 통해 subplot들 간의 간격을 설정하여 레이아웃을 보다 조밀하게 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90aae73-43a8-4ce2-b013-3ded97df2325",
   "metadata": {},
   "source": [
    "transform=ax.transAxes는 텍스트 위치의 좌표계를 설정하는 옵션입니다. 이 옵션을 사용하면 x와 y 좌표를 서브플롯의 축에 상대적인 값으로 지정할 수 있습니다.\n",
    "\n",
    "ax.transAxes는 서브플롯의 축을 기준으로 하는 좌표계를 의미하며, 이 좌표계에서는 (0, 0)이 서브플롯의 왼쪽 하단이고, (1, 1)이 오른쪽 상단이 됩니다. 따라서, 예를 들어 (0, 1)은 서브플롯의 왼쪽 상단, (1, 0)은 서브플롯의 오른쪽 하단을 나타냅니다.\n",
    "\n",
    "이 옵션을 사용하면 서브플롯의 크기와 위치에 상관없이 텍스트가 항상 서브플롯 내에 위치하도록 할 수 있습니다. 이는 특히 여러 개의 서브플롯이 있는 그림에서 유용합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
