- SGD 학습 멈추면 -> local minima -> '모멘텀'

- Negative Sampling?
	- Word2Vec의 CBOW와 Skip-gram 모두 단어 개수가 많아질수록 계산 복잡도가 증가하여 연산 속도가 저하된다는 한계점을 보완하기 위해 제안
 	- Negative Sampling은 전체 문장에서 자주 사용되는 단어에 높은 가중치를 부여하고, 우선적으로 해당 단어를 선별
	- 학습 데이터셋이 작을 때는 5~20개 사이의 Negative sample을 추출하는 게 효과적이고, 학습 데이터셋이 클 경우 2~5개 사이의 sample을 선정하는 게 효과적
	- Skip-gram with Negative Sampling(SGNS)

- 전체 통계정보? 비교?(불균형?)

- syntatic semantic : intrinsic evaluation

- correlation evaluation : anology

- 벡터 여러개로? 한개로?

- 자코비안 n->m

- dimension과 맞아 떨어지도록

- 체인룰 - 합성함수 미분의 곱 - 각 차원 일치 시켜야 

- ReLU : 회귀/연속변수 적합(범위 무한), gradient vanishing 없어

- Analystic Gradient / Numeric Gradient